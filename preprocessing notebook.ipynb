{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "1. Place ShellData.csv in the same directory as this notebook\n",
    "2. Run all cells sequentially\n",
    "3. Checkpoints are saved automatically (resume from last if interrupted), also possible to comment out if not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Resumed from checkpoint_04_time_features.parquet\n",
      "  Step 4: Time features added\n",
      "  Loaded 214,991,102 rows, 14 columns\n",
      "\n",
      "▶ Variable 'dt_df' is ready. Continue from next step.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "\n",
    "def resume_from_checkpoint():\n",
    "    \"\"\"Load from the most recent checkpoint\"\"\"\n",
    "    checkpoints = [\n",
    "        (\"checkpoint_05_missing_handled.parquet\", \"dt_df_clean\", \"Step 5: Missing values handled\"),\n",
    "        (\"checkpoint_04_time_features.parquet\", \"dt_df\", \"Step 4: Time features added\"),\n",
    "        (\"checkpoint_03_one_hot_encoded.parquet\", \"ohe_df\", \"Step 3: One-hot encoded\"),\n",
    "        (\"checkpoint_02_datetime_converted.parquet\", \"dt_df\", \"Step 2: Datetime converted\"),\n",
    "        (\"checkpoint_01_value_separated.parquet\", \"value_sep_df\", \"Step 1: Values separated\"),\n",
    "    ]\n",
    "    \n",
    "    for checkpoint_file, var_name, description in checkpoints:\n",
    "        if os.path.exists(checkpoint_file):\n",
    "            df = pd.read_parquet(checkpoint_file)\n",
    "            print(f\"✓ Resumed from {checkpoint_file}\")\n",
    "            print(f\"  {description}\")\n",
    "            print(f\"  Loaded {len(df):,} rows, {len(df.columns)} columns\")\n",
    "            return df, var_name\n",
    "    \n",
    "    print(\"No checkpoints found. Load data from CSV.\")\n",
    "    return None, None\n",
    "\n",
    "# Load and assign to the correct variable name\n",
    "df, var_name = resume_from_checkpoint()\n",
    "\n",
    "if df is not None:\n",
    "    if var_name == \"dt_df_clean\":\n",
    "        dt_df_clean = df\n",
    "    elif var_name == \"dt_df\":\n",
    "        dt_df = df\n",
    "    elif var_name == \"ohe_df\":\n",
    "        ohe_df = df\n",
    "    elif var_name == \"value_sep_df\":\n",
    "        value_sep_df = df\n",
    "    \n",
    "    print(f\"\\n▶ Variable '{var_name}' is ready. Continue from next step.\")\n",
    "else:\n",
    "    print(\"\\n▶ Load data from CSV first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"ShellData.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def separate_textual_attrivutes_from_numerical_column(df: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Checks whether the specified column contains non-numeric string values.\n",
    "    If yes:\n",
    "      - converts numeric-looking strings (e.g., \"3.14\", \"1e-5\") into real numbers\n",
    "      - creates a new column <column>_str containing non-numeric strings\n",
    "      - replaces those original values in <column> with -1\n",
    "      - fills missing entries in <column>_str with the string \"NaN\"\n",
    "    Returns a modified copy of the DataFrame, leaving the original unchanged.\n",
    "    \"\"\"\n",
    "\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"Column '{column}' does not exist in the DataFrame.\")\n",
    "\n",
    "    # Create a copy so the original DataFrame is not modified\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Helper: check if something is a string that looks like a number\n",
    "    def is_numeric_string(x):\n",
    "        if not isinstance(x, str):\n",
    "            return False\n",
    "        try:\n",
    "            float(x)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    # Convert numeric-looking strings to real numbers\n",
    "    df_copy[column] = df_copy[column].apply(\n",
    "        lambda x: float(x) if is_numeric_string(x) else x\n",
    "    )\n",
    "\n",
    "    # Identify non-numeric strings\n",
    "    def is_non_numeric_string(x):\n",
    "        return isinstance(x, str) and not is_numeric_string(x)\n",
    "\n",
    "    # Create the new string column\n",
    "    df_copy[f\"{column}_str\"] = df_copy[column].where(\n",
    "        df_copy[column].apply(is_non_numeric_string)\n",
    "    )\n",
    "    df_copy[f\"{column}_str\"] = df_copy[f\"{column}_str\"].fillna(\"NaN\")\n",
    "\n",
    "    # Replace non-numeric strings in the main column with -1\n",
    "    df_copy[column] = df_copy[column].apply(\n",
    "        lambda x: -1 if is_non_numeric_string(x) else x\n",
    "    )\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "value_sep_df = separate_textual_attrivutes_from_numerical_column(df, \"Value\")\n",
    "\n",
    "\n",
    "value_sep_df.to_parquet(\"checkpoint_01_value_separated.parquet\", index=False)\n",
    "print(\"Saved checkpoint 1\")\n",
    "\n",
    "del df\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "value_sep_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "value_sep_df[\"Value_str\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def convert_string_column_to_datetime_inplace(\n",
    "    df: pd.DataFrame,\n",
    "    column: str,\n",
    "    fmt_with_frac: str = \"%Y-%m-%d %H:%M:%S.%f\",\n",
    "    out_col: str | None = None,\n",
    "    strip_trailing_000: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts a string-based timestamp column to datetime (in-place result).\n",
    "    - If 'strip_trailing_000' is True, values ending with '.000' are trimmed\n",
    "      and parsed without fractional seconds.\n",
    "    - All other values are parsed using the provided 'fmt_with_frac' format.\n",
    "    - Invalid entries are converted to NaT.\n",
    "    \"\"\"\n",
    "\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"Column '{column}' does not exist in the DataFrame.\")\n",
    "    out_col = out_col or f\"{column}_DT\"\n",
    "\n",
    "    # Convert column to string once (avoids dtype mix issues)\n",
    "    s = df[column].astype(str)\n",
    "    # Prepare result column\n",
    "    result = pd.Series(pd.NaT, index=df.index, dtype=\"datetime64[ns]\")\n",
    "\n",
    "    if strip_trailing_000:\n",
    "        # Identify entries that end with '.000'\n",
    "        mask000 = s.str.endswith(\".000\")\n",
    "\n",
    "        # Fast path: remove '.000' and parse without microseconds\n",
    "        if mask000.any():\n",
    "            result.loc[mask000] = pd.to_datetime(\n",
    "                s.loc[mask000].str[:-4],\n",
    "                format=\"%Y-%m-%d %H:%M:%S\",\n",
    "                errors=\"coerce\",\n",
    "            )\n",
    "\n",
    "        # Remaining values: parse using the full format (with %f)\n",
    "        other = ~mask000\n",
    "        if other.any():\n",
    "            result.loc[other] = pd.to_datetime(\n",
    "                s.loc[other],\n",
    "                format=fmt_with_frac,\n",
    "                errors=\"coerce\",\n",
    "            )\n",
    "    else:\n",
    "        # Single-pass parsing if we do not strip trailing zeros\n",
    "        result[:] = pd.to_datetime(s, format=fmt_with_frac, errors=\"coerce\")\n",
    "\n",
    "    # Assign result column directly into the same DataFrame (in-place)\n",
    "    df[out_col] = result\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dt_df = convert_string_column_to_datetime_inplace(value_sep_df, \"EventTime\", \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "dt_df.to_parquet(\"checkpoint_02_datetime_converted.parquet\", index=False)\n",
    "print(\"Saved checkpoint 2\")\n",
    "\n",
    "del value_sep_df\n",
    "gc.collect()\n",
    "\n",
    "dt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dt_df[\"EventTime_DT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(df: pd.DataFrame,column: str,sparse: bool = False)-> pd.DataFrame:\n",
    "    return pd.get_dummies(df, columns=[column], sparse=sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ohe_df = one_hot_encode(dt_df, \"Status\")\n",
    "\n",
    "ohe_df.to_parquet(\"checkpoint_03_one_hot_encoded.parquet\", index=False)\n",
    "print(\"Saved checkpoint 3\")\n",
    "\n",
    "del dt_df\n",
    "gc.collect()\n",
    "\n",
    "ohe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(df: pd.DataFrame, datetime_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Expand a DataFrame with time-based features from a datetime column.\n",
    "    Robust parsing for mixed datetime formats (ISO8601 / with or without microseconds).\n",
    "    \"\"\"\n",
    "    # Validate column presence\n",
    "    if datetime_col not in df.columns:\n",
    "        raise KeyError(f\"Column '{datetime_col}' not found in DataFrame.\")\n",
    "\n",
    "    out = df.copy()\n",
    "\n",
    "    # 1) Ensure datetime dtype with robust parsing for mixed formats\n",
    "    #    Try ISO8601 first (fast & strict), then mixed, then final fallback with errors='coerce'.\n",
    "    if not pd.api.types.is_datetime64_any_dtype(out[datetime_col]):\n",
    "        try:\n",
    "            # Fast path for ISO strings (handles \"YYYY-MM-DD HH:MM:SS[.ffffff][Z][±HH:MM]\")\n",
    "            out[datetime_col] = pd.to_datetime(out[datetime_col], format=\"ISO8601\")\n",
    "        except Exception:\n",
    "            try:\n",
    "                # Pandas 2.x: parse per-element with inferred format (handles microseconds optional)\n",
    "                out[datetime_col] = pd.to_datetime(out[datetime_col], format=\"mixed\")\n",
    "            except Exception:\n",
    "                # Last resort: coerce bad rows to NaT so we can at least proceed and diagnose\n",
    "                out[datetime_col] = pd.to_datetime(out[datetime_col], errors=\"coerce\")\n",
    "\n",
    "    dt = out[datetime_col].dt\n",
    "\n",
    "    # 2) Add numeric parts\n",
    "    out[f\"{datetime_col}_day\"]   = dt.day\n",
    "    out[f\"{datetime_col}_week\"]  = dt.isocalendar().week.astype(\"Int16\")\n",
    "\n",
    "    # 3) Seconds since midnight (works even with NaT → becomes <NA>)\n",
    "    out[f\"{datetime_col}_seconds\"] = (dt.hour * 3600 + dt.minute * 60 + dt.second).astype(\"Int32\")\n",
    "\n",
    "    # 4) Month as ordered categorical (no per-row strings allocated)\n",
    "    month_categories = [\n",
    "        \"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
    "        \"July\",\"August\",\"September\",\"October\",\"November\",\"December\"\n",
    "    ]\n",
    "    month_codes = (dt.month - 1).astype(\"Int8\")\n",
    "    month_codes = month_codes.where(~month_codes.isna(), other=-1)\n",
    "    out[f\"{datetime_col}_month\"] = pd.Categorical.from_codes(\n",
    "        codes=month_codes.astype(\"int8\"),\n",
    "        categories=month_categories,\n",
    "        ordered=True\n",
    "    )\n",
    "\n",
    "    # 5) Weekday as ordered categorical (0=Mon .. 6=Sun)\n",
    "    weekday_categories = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "    wday_codes = dt.dayofweek.astype(\"Int8\")\n",
    "    wday_codes = wday_codes.where(~wday_codes.isna(), other=-1)\n",
    "    out[f\"{datetime_col}_weekday\"] = pd.Categorical.from_codes(\n",
    "        codes=wday_codes.astype(\"int8\"),\n",
    "        categories=weekday_categories,\n",
    "        ordered=True\n",
    "    )\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_df = add_time_features(ohe_df, \"EventTime\")\n",
    "\n",
    "dt_df.to_parquet(\"checkpoint_04_time_features.parquet\", index=False)\n",
    "print(\"Saved checkpoint 4\")\n",
    "\n",
    "del ohe_df\n",
    "gc.collect()\n",
    "\n",
    "dt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_failed_timestamps(df: pd.DataFrame, datetime_str_col: str, datetime_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Attempt to re-parse timestamps that failed initial conversion.\n",
    "    \"\"\"\n",
    "    df_fixed = df.copy()\n",
    "    \n",
    "    # Find rows with NaT\n",
    "    nat_mask = df_fixed[datetime_col].isna()\n",
    "    nat_count_before = nat_mask.sum()\n",
    "    \n",
    "    print(f\"Attempting to recover {nat_count_before:,} failed timestamps\")\n",
    "    \n",
    "    # Try multiple formats\n",
    "    formats_to_try = [\n",
    "        \"%Y-%m-%d %H:%M:%S\",           # Without milliseconds\n",
    "        \"%Y-%m-%d %H:%M:%S.%f\",        # With milliseconds\n",
    "        \"%Y/%m/%d %H:%M:%S\",           # Different separator\n",
    "        \"%d-%m-%Y %H:%M:%S\",           # DD-MM-YYYY format\n",
    "        \"ISO8601\",                      # ISO standard\n",
    "    ]\n",
    "    \n",
    "    for fmt in formats_to_try:\n",
    "        # Only try to fix rows that are still NaT\n",
    "        still_nat = df_fixed[datetime_col].isna()\n",
    "        if still_nat.sum() == 0:\n",
    "            break\n",
    "            \n",
    "        print(f\"Trying format: {fmt}\")\n",
    "        \n",
    "        try:\n",
    "            if fmt == \"ISO8601\":\n",
    "                parsed = pd.to_datetime(df_fixed.loc[still_nat, datetime_str_col], \n",
    "                                       format=\"ISO8601\", errors='coerce')\n",
    "            else:\n",
    "                parsed = pd.to_datetime(df_fixed.loc[still_nat, datetime_str_col], \n",
    "                                       format=fmt, errors='coerce')\n",
    "            \n",
    "            # Update only the ones that successfully parsed\n",
    "            successfully_parsed = ~parsed.isna()\n",
    "            df_fixed.loc[still_nat & successfully_parsed, datetime_col] = parsed[successfully_parsed]\n",
    "            \n",
    "            recovered = successfully_parsed.sum()\n",
    "            if recovered > 0:\n",
    "                print(f\"Recovered {recovered:,} timestamps\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    nat_count_after = df_fixed[datetime_col].isna().sum()\n",
    "    total_recovered = nat_count_before - nat_count_after\n",
    "    \n",
    "    print(f\"\\nRecovered {total_recovered:,} timestamps ({total_recovered/nat_count_before*100:.2f}%)\")\n",
    "    print(f\"Still missing: {nat_count_after:,} timestamps ({nat_count_after/len(df)*100:.4f}%)\")\n",
    "    \n",
    "    return df_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values_final(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Final missing value handling after timestamp recovery.\n",
    "    \n",
    "    Strategy:\n",
    "    - Drop rows with unrecoverable timestamps (can't use for time-series)\n",
    "    - Drop rows with missing Value if < 0.1% of data\n",
    "    \"\"\"\n",
    "    initial_rows = len(df)\n",
    "    \n",
    "    print(\"FINAL MISSING VALUE HANDLING\")\n",
    "    \n",
    "    # Drop rows with NaT timestamps (unrecoverable)\n",
    "    nat_count = df['EventTime_DT'].isna().sum()\n",
    "    df_clean = df.dropna(subset=['EventTime_DT']).copy()\n",
    "    \n",
    "    print(f\"\\nDropped {nat_count:,} rows with unrecoverable timestamps ({nat_count/initial_rows*100:.4f}%)\")\n",
    "    \n",
    "    # Handle missing Values\n",
    "    value_missing = df_clean['Value'].isna().sum()\n",
    "    \n",
    "    if value_missing > 0:\n",
    "        # If very few (<0.1%), just drop them\n",
    "        if value_missing / len(df_clean) < 0.001:\n",
    "            df_clean = df_clean.dropna(subset=['Value'])\n",
    "            print(f\"Dropped {value_missing:,} rows with missing Value ({value_missing/initial_rows*100:.4f}%)\")\n",
    "        else:\n",
    "            # Otherwise, fill with -1 \n",
    "            df_clean['Value'] = df_clean['Value'].fillna(-1)\n",
    "            print(f\"Filled {value_missing:,} missing Values with -1 (error indicator)\")\n",
    "    else:\n",
    "        print(\"No missing Values to handle\")\n",
    "    \n",
    "    total_dropped = initial_rows - len(df_clean)\n",
    "    print(f\"Initial rows: {initial_rows:,}\")\n",
    "    print(f\"Final rows: {len(df_clean):,}\")\n",
    "    print(f\"Dropped: {total_dropped:,} ({total_dropped/initial_rows*100:.2f}%)\")\n",
    "    print(f\"Retention rate: {len(df_clean)/initial_rows*100:.2f}%\")\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to recover 4,600,000 failed timestamps...\n",
      "  Trying format: %Y-%m-%d %H:%M:%S...\n",
      "    ✓ Recovered 4,600,000 timestamps\n",
      "\n",
      "✓ Recovered 4,600,000 timestamps (100.00%)\n",
      "✗ Still missing: 0 timestamps (0.0000%)\n",
      "\n",
      "======================================================================\n",
      "FINAL MISSING VALUE HANDLING\n",
      "======================================================================\n",
      "\n",
      "1. Dropped 0 rows with unrecoverable timestamps (0.0000%)\n",
      "2. Dropped 74,875 rows with missing Value (0.0348%)\n",
      "\n",
      "======================================================================\n",
      "SUMMARY:\n",
      "  Initial rows: 214,991,102\n",
      "  Final rows: 214,916,227\n",
      "  Dropped: 74,875 (0.03%)\n",
      "  Retention rate: 99.97%\n",
      "======================================================================\n",
      "\n",
      "✓ Saved checkpoint 5: Missing values handled\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TagName</th>\n",
       "      <th>EventTime</th>\n",
       "      <th>Value</th>\n",
       "      <th>Value_str</th>\n",
       "      <th>EventTime_DT</th>\n",
       "      <th>Status_Bad</th>\n",
       "      <th>Status_Good</th>\n",
       "      <th>Status_Questionable</th>\n",
       "      <th>Status_Substituted, Good</th>\n",
       "      <th>EventTime_day</th>\n",
       "      <th>EventTime_week</th>\n",
       "      <th>EventTime_seconds</th>\n",
       "      <th>EventTime_month</th>\n",
       "      <th>EventTime_weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A2PS64V0J.:ZUX09R</td>\n",
       "      <td>2024-01-02 20:03:46</td>\n",
       "      <td>0.34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-01-02 20:03:46</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>72226</td>\n",
       "      <td>January</td>\n",
       "      <td>Tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A2PS64V0J.:ZUX09R</td>\n",
       "      <td>2024-01-02 16:00:12</td>\n",
       "      <td>0.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-01-02 16:00:12</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57612</td>\n",
       "      <td>January</td>\n",
       "      <td>Tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2PS64V0J.:ZUX09R</td>\n",
       "      <td>2024-01-02 11:56:42</td>\n",
       "      <td>0.13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-01-02 11:56:42</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>43002</td>\n",
       "      <td>January</td>\n",
       "      <td>Tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2PS64V0J.:ZUX09R</td>\n",
       "      <td>2024-01-02 07:53:11</td>\n",
       "      <td>0.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-01-02 07:53:11</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>28391</td>\n",
       "      <td>January</td>\n",
       "      <td>Tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A2PS64V0J.:ZUX09R</td>\n",
       "      <td>2024-01-02 03:49:45</td>\n",
       "      <td>0.13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-01-02 03:49:45</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13785</td>\n",
       "      <td>January</td>\n",
       "      <td>Tuesday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             TagName           EventTime  Value Value_str        EventTime_DT  \\\n",
       "0  A2PS64V0J.:ZUX09R 2024-01-02 20:03:46   0.34       NaN 2024-01-02 20:03:46   \n",
       "1  A2PS64V0J.:ZUX09R 2024-01-02 16:00:12   0.15       NaN 2024-01-02 16:00:12   \n",
       "2  A2PS64V0J.:ZUX09R 2024-01-02 11:56:42   0.13       NaN 2024-01-02 11:56:42   \n",
       "3  A2PS64V0J.:ZUX09R 2024-01-02 07:53:11   0.12       NaN 2024-01-02 07:53:11   \n",
       "4  A2PS64V0J.:ZUX09R 2024-01-02 03:49:45   0.13       NaN 2024-01-02 03:49:45   \n",
       "\n",
       "   Status_Bad  Status_Good  Status_Questionable  Status_Substituted, Good  \\\n",
       "0       False         True                False                     False   \n",
       "1       False         True                False                     False   \n",
       "2       False         True                False                     False   \n",
       "3       False         True                False                     False   \n",
       "4       False         True                False                     False   \n",
       "\n",
       "   EventTime_day  EventTime_week  EventTime_seconds EventTime_month  \\\n",
       "0              2               1              72226         January   \n",
       "1              2               1              57612         January   \n",
       "2              2               1              43002         January   \n",
       "3              2               1              28391         January   \n",
       "4              2               1              13785         January   \n",
       "\n",
       "  EventTime_weekday  \n",
       "0           Tuesday  \n",
       "1           Tuesday  \n",
       "2           Tuesday  \n",
       "3           Tuesday  \n",
       "4           Tuesday  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_df_recovered = recover_failed_timestamps(dt_df, 'EventTime', 'EventTime_DT')\n",
    "dt_df_clean = handle_missing_values_final(dt_df_recovered)\n",
    "\n",
    "dt_df_clean.to_parquet(\"checkpoint_05_missing_handled.parquet\", index=False)\n",
    "print(\"\\nSaved checkpoint 5: Missing values handled\")\n",
    "\n",
    "del dt_df, dt_df_recovered\n",
    "import gc\n",
    "gc.collect()\n",
    "dt_df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_datetime(df: pd.DataFrame, datetime_col: str, ascending: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Sorts the DataFrame by a datetime column in chronological order.\n",
    "    Sorts in-place to avoid creating additional copies for memory efficiency.\n",
    "    \"\"\"\n",
    "    \n",
    "    if datetime_col not in df.columns:\n",
    "        raise KeyError(f\"Column '{datetime_col}' not found in DataFrame.\")\n",
    "    \n",
    "    print(f\"Sorting {len(df):,} rows by '{datetime_col}'\")\n",
    "    \n",
    "    df.sort_values(\n",
    "        by=datetime_col, \n",
    "        ascending=ascending,\n",
    "        na_position='last',\n",
    "        inplace=True,\n",
    "        kind='mergesort'  \n",
    "    )\n",
    "    \n",
    "    # Reset index in-place\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(f\"Sorted in-place\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting 214,916,227 rows by 'EventTime_DT'...\n",
      "✓ Sorted in-place\n",
      "✓ Updated checkpoint 5 with sorted data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TagName</th>\n",
       "      <th>EventTime</th>\n",
       "      <th>Value</th>\n",
       "      <th>Value_str</th>\n",
       "      <th>EventTime_DT</th>\n",
       "      <th>Status_Bad</th>\n",
       "      <th>Status_Good</th>\n",
       "      <th>Status_Questionable</th>\n",
       "      <th>Status_Substituted, Good</th>\n",
       "      <th>EventTime_day</th>\n",
       "      <th>EventTime_week</th>\n",
       "      <th>EventTime_seconds</th>\n",
       "      <th>EventTime_month</th>\n",
       "      <th>EventTime_weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20PX.D20V:04SXA</td>\n",
       "      <td>2023-12-31 00:00:00.000</td>\n",
       "      <td>4414.219727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-12-31 00:00:00.000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>31</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>December</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S:.2P8RVXA39UX06</td>\n",
       "      <td>2023-12-31 00:00:00.000</td>\n",
       "      <td>15.995943</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-12-31 00:00:00.000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>31</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>December</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X8VPHB16088.0:V</td>\n",
       "      <td>2023-12-31 00:00:00.000</td>\n",
       "      <td>247.606781</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-12-31 00:00:00.000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>31</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>December</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NGX0.11S:7EXN07R</td>\n",
       "      <td>2023-12-31 00:00:00.000</td>\n",
       "      <td>-2777.647461</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-12-31 00:00:00.000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>31</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>December</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X0N71R1S7PV:.X0</td>\n",
       "      <td>2023-12-31 00:00:00.000</td>\n",
       "      <td>666.784058</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-12-31 00:00:00.000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>31</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>December</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214916222</th>\n",
       "      <td>:0CCTP530TNYOHS1.</td>\n",
       "      <td>2024-07-27 23:59:57.101</td>\n",
       "      <td>234.000290</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-07-27 23:59:57.101</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>27</td>\n",
       "      <td>30</td>\n",
       "      <td>86397</td>\n",
       "      <td>July</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214916223</th>\n",
       "      <td>:NC1HPT44Y0OC5T.S</td>\n",
       "      <td>2024-07-27 23:59:57.101</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-07-27 23:59:57.101</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>27</td>\n",
       "      <td>30</td>\n",
       "      <td>86397</td>\n",
       "      <td>July</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214916224</th>\n",
       "      <td>NPSH593TCOY01.:CT</td>\n",
       "      <td>2024-07-27 23:59:57.101</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-07-27 23:59:57.101</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>27</td>\n",
       "      <td>30</td>\n",
       "      <td>86397</td>\n",
       "      <td>July</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214916225</th>\n",
       "      <td>COHP35YTT1CN21:S.</td>\n",
       "      <td>2024-07-27 23:59:57.101</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-07-27 23:59:57.101</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>27</td>\n",
       "      <td>30</td>\n",
       "      <td>86397</td>\n",
       "      <td>July</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214916226</th>\n",
       "      <td>T:4FU4PRBV3NO0V_0PLM40R-.34</td>\n",
       "      <td>2024-07-27 23:59:57.573</td>\n",
       "      <td>3.770000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-07-27 23:59:57.573</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>27</td>\n",
       "      <td>30</td>\n",
       "      <td>86397</td>\n",
       "      <td>July</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>214916227 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               TagName               EventTime        Value  \\\n",
       "0                      20PX.D20V:04SXA 2023-12-31 00:00:00.000  4414.219727   \n",
       "1                     S:.2P8RVXA39UX06 2023-12-31 00:00:00.000    15.995943   \n",
       "2                      X8VPHB16088.0:V 2023-12-31 00:00:00.000   247.606781   \n",
       "3                     NGX0.11S:7EXN07R 2023-12-31 00:00:00.000 -2777.647461   \n",
       "4                      X0N71R1S7PV:.X0 2023-12-31 00:00:00.000   666.784058   \n",
       "...                                ...                     ...          ...   \n",
       "214916222            :0CCTP530TNYOHS1. 2024-07-27 23:59:57.101   234.000290   \n",
       "214916223            :NC1HPT44Y0OC5T.S 2024-07-27 23:59:57.101   143.000000   \n",
       "214916224            NPSH593TCOY01.:CT 2024-07-27 23:59:57.101    42.000000   \n",
       "214916225            COHP35YTT1CN21:S. 2024-07-27 23:59:57.101   130.000000   \n",
       "214916226  T:4FU4PRBV3NO0V_0PLM40R-.34 2024-07-27 23:59:57.573     3.770000   \n",
       "\n",
       "          Value_str            EventTime_DT  Status_Bad  Status_Good  \\\n",
       "0               NaN 2023-12-31 00:00:00.000       False         True   \n",
       "1               NaN 2023-12-31 00:00:00.000       False         True   \n",
       "2               NaN 2023-12-31 00:00:00.000       False         True   \n",
       "3               NaN 2023-12-31 00:00:00.000       False         True   \n",
       "4               NaN 2023-12-31 00:00:00.000       False         True   \n",
       "...             ...                     ...         ...          ...   \n",
       "214916222       NaN 2024-07-27 23:59:57.101       False         True   \n",
       "214916223       NaN 2024-07-27 23:59:57.101       False         True   \n",
       "214916224       NaN 2024-07-27 23:59:57.101       False         True   \n",
       "214916225       NaN 2024-07-27 23:59:57.101       False         True   \n",
       "214916226       NaN 2024-07-27 23:59:57.573       False         True   \n",
       "\n",
       "           Status_Questionable  Status_Substituted, Good  EventTime_day  \\\n",
       "0                        False                     False             31   \n",
       "1                        False                     False             31   \n",
       "2                        False                     False             31   \n",
       "3                        False                     False             31   \n",
       "4                        False                     False             31   \n",
       "...                        ...                       ...            ...   \n",
       "214916222                False                     False             27   \n",
       "214916223                False                     False             27   \n",
       "214916224                False                     False             27   \n",
       "214916225                False                     False             27   \n",
       "214916226                False                     False             27   \n",
       "\n",
       "           EventTime_week  EventTime_seconds EventTime_month EventTime_weekday  \n",
       "0                      52                  0        December            Sunday  \n",
       "1                      52                  0        December            Sunday  \n",
       "2                      52                  0        December            Sunday  \n",
       "3                      52                  0        December            Sunday  \n",
       "4                      52                  0        December            Sunday  \n",
       "...                   ...                ...             ...               ...  \n",
       "214916222              30              86397            July          Saturday  \n",
       "214916223              30              86397            July          Saturday  \n",
       "214916224              30              86397            July          Saturday  \n",
       "214916225              30              86397            July          Saturday  \n",
       "214916226              30              86397            July          Saturday  \n",
       "\n",
       "[214916227 rows x 14 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_df = sort_by_datetime(dt_df_clean, \"EventTime_DT\")\n",
    "\n",
    "# Overwrite checkpoint 5 with sorted data\n",
    "sorted_df.to_parquet(\"checkpoint_05_missing_handled.parquet\", index=False)\n",
    "print(\"Updated checkpoint 5 with sorted data\")\n",
    "\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXTREME VALUE INVESTIGATION\n",
      "======================================================================\n",
      "\n",
      "Valid values statistics:\n",
      "  Count: 214,419,974\n",
      "  Min: -736671805447501512704.00\n",
      "  Max: 6265861308416.00\n",
      "  Mean: -6902653122450.62\n",
      "  Median: 42.52\n",
      "  Std: 71147326878169032.00\n",
      "\n",
      "Percentiles:\n",
      "   0.01%:           -57,568.29\n",
      "   0.10%:           -14,841.97\n",
      "   1.00%:            -2,213.50\n",
      "   5.00%:                -0.11\n",
      "  25.00%:                 3.23\n",
      "  50.00%:                42.52\n",
      "  75.00%:               142.50\n",
      "  95.00%:             2,499.45\n",
      "  99.00%:            43,509.33\n",
      "  99.90%:           395,519.74\n",
      "  99.99%:         7,120,637.80\n",
      "\n",
      "Extreme outliers (> 6 sigma):\n",
      "  Extremely high: 0\n",
      "  Extremely low: 6\n",
      "\n",
      "Sample extremely low values:\n",
      "                  TagName        EventTime_DT         Value Value_str\n",
      "40985198  0N:P7RXV031S0.X 2024-02-08 02:30:17 -1.643073e+18       NaN\n",
      "40985201  VX0XN07P1.1:3RS 2024-02-08 02:30:17 -7.366718e+20       NaN\n",
      "40985240  V0R0.:0X715PNSX 2024-02-08 02:30:22 -1.643073e+18       NaN\n",
      "40985954  0N:P7RXV031S0.X 2024-02-08 02:31:13 -1.643073e+18       NaN\n",
      "40985955  VX0XN07P1.1:3RS 2024-02-08 02:31:13 -7.366718e+20       NaN\n",
      "40986005  V0R0.:0X715PNSX 2024-02-08 02:31:18 -1.643073e+18       NaN\n",
      "\n",
      "Value ranges for sample sensors:\n",
      "\n",
      "  T:4FU4PRBV3NO0V_0PLM40R-.34:\n",
      "    Count: 553,540\n",
      "    Range: 0.00 to 312.00\n",
      "    Mean: 3.50\n",
      "\n",
      "  5H4C:S.YPLOTC1N00:\n",
      "    Count: 518,277\n",
      "    Range: 55.00 to 55.00\n",
      "    Mean: 55.00\n",
      "\n",
      "  0:HC5Y013CP.SOTTN:\n",
      "    Count: 518,277\n",
      "    Range: 234.00 to 234.56\n",
      "    Mean: 234.22\n",
      "\n",
      "  C.OH0TU:C1Y4HON35:\n",
      "    Count: 518,277\n",
      "    Range: 20.00 to 20.00\n",
      "    Mean: 20.00\n",
      "\n",
      "  :PCT01SN0LH.5Y3CO:\n",
      "    Count: 518,277\n",
      "    Range: 53.00 to 90.00\n",
      "    Mean: 53.00\n"
     ]
    }
   ],
   "source": [
    "#### Optional, just for information ####\n",
    "\n",
    "def investigate_extreme_values(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Investigate extreme values in the Value column.\n",
    "    \"\"\"\n",
    "    print(\"EXTREME VALUE INVESTIGATION\")\n",
    "    \n",
    "    # Filter out error values\n",
    "    valid_values = df[df['Value'] != -1]['Value']\n",
    "    \n",
    "    print(f\"\\nValid values statistics:\")\n",
    "    print(f\"  Count: {len(valid_values):,}\")\n",
    "    print(f\"  Min: {valid_values.min():.2f}\")\n",
    "    print(f\"  Max: {valid_values.max():.2f}\")\n",
    "    print(f\"  Mean: {valid_values.mean():.2f}\")\n",
    "    print(f\"  Median: {valid_values.median():.2f}\")\n",
    "    print(f\"  Std: {valid_values.std():.2f}\")\n",
    "    \n",
    "    # Check percentiles\n",
    "    print(f\"\\nPercentiles:\")\n",
    "    for p in [0.01, 0.1, 1, 5, 25, 50, 75, 95, 99, 99.9, 99.99]:\n",
    "        val = valid_values.quantile(p/100)\n",
    "        print(f\"  {p:5.2f}%: {val:20,.2f}\")\n",
    "    \n",
    "    # Find extreme outliers (beyond 6 sigma)\n",
    "    mean = valid_values.mean()\n",
    "    std = valid_values.std()\n",
    "    \n",
    "    extreme_high = valid_values > (mean + 6 * std)\n",
    "    extreme_low = valid_values < (mean - 6 * std)\n",
    "    \n",
    "    print(f\"\\nExtreme outliers (> 6 sigma):\")\n",
    "    print(f\"Extremely high: {extreme_high.sum():,}\")\n",
    "    print(f\"Extremely low: {extreme_low.sum():,}\")\n",
    "    \n",
    "    # examples of extreme values\n",
    "    if extreme_high.sum() > 0:\n",
    "        print(f\"\\nSample extremely high values:\")\n",
    "        high_samples = df[df['Value'] > mean + 6 * std][['TagName', 'EventTime_DT', 'Value', 'Value_str']].head(10)\n",
    "        print(high_samples)\n",
    "    \n",
    "    if extreme_low.sum() > 0:\n",
    "        print(f\"\\nSample extremely low values:\")\n",
    "        low_samples = df[df['Value'] < mean - 6 * std][['TagName', 'EventTime_DT', 'Value', 'Value_str']].head(10)\n",
    "        print(low_samples)\n",
    "    \n",
    "    # Check value ranges by sensor type \n",
    "    print(f\"\\nValue ranges for sample sensors:\")\n",
    "    sample_sensors = df['TagName'].value_counts().head(5).index\n",
    "    for sensor in sample_sensors:\n",
    "        sensor_data = df[df['TagName'] == sensor]['Value']\n",
    "        sensor_valid = sensor_data[sensor_data != -1]\n",
    "        if len(sensor_valid) > 0:\n",
    "            print(f\"\\n  {sensor}:\")\n",
    "            print(f\"    Count: {len(sensor_valid):,}\")\n",
    "            print(f\"    Range: {sensor_valid.min():.2f} to {sensor_valid.max():.2f}\")\n",
    "            print(f\"    Mean: {sensor_valid.mean():.2f}\")\n",
    "\n",
    "# Run investigation\n",
    "import numpy as np\n",
    "investigate_extreme_values(dt_df_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HANDLING EXTREME OUTLIERS\n",
      "======================================================================\n",
      "\n",
      "Robust statistics (using median and MAD):\n",
      "  Median: 42.52\n",
      "  MAD-based std: 62.81\n",
      "\n",
      "Outlier thresholds (10.0 sigma):\n",
      "  Lower bound: -585.60\n",
      "  Upper bound: 670.64\n",
      "\n",
      "Outliers detected: 26,780,921 (12.4611%)\n",
      "\n",
      "Sample outliers to be marked as errors:\n",
      "               TagName EventTime_DT         Value Value_str\n",
      "0      20PX.D20V:04SXA   2023-12-31   4414.219727       NaN\n",
      "3     NGX0.11S:7EXN07R   2023-12-31  -2777.647461       NaN\n",
      "5     X07:NX.7110RSOPS   2023-12-31   3444.431396       NaN\n",
      "6      NXVPX7R013S.1:0   2023-12-31  -9966.505859       NaN\n",
      "8      XNX0R:31SV0.P57   2023-12-31   2366.526123       NaN\n",
      "11    V500PDXP7AX9I:6.   2023-12-31   1056.000000       NaN\n",
      "12          092O.VL5PI   2023-12-31  32950.367188       NaN\n",
      "17     CVT55:101DP4T5.   2023-12-31    876.863647       NaN\n",
      "19  80CWC520PDMU0:U.5S   2023-12-31   3102.424805       NaN\n",
      "24     50V0FP259:D.4TC   2023-12-31   1047.814819       NaN\n",
      "\n",
      "✓ Marked 26,780,921 extreme outliers as errors (Value=-1)\n",
      "\n",
      "Updated statistics (after outlier removal):\n",
      "  Count: 187,639,053\n",
      "  Min: -585.60\n",
      "  Max: 670.64\n",
      "  Mean: 80.00\n",
      "  Median: 35.64\n",
      "  Std: 129.38\n",
      "\n",
      "✓ Updated checkpoint 5 with outliers handled\n"
     ]
    }
   ],
   "source": [
    "def handle_extreme_outliers(df: pd.DataFrame, n_sigma: float = 10.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Handle extreme outliers that are likely data corruption.\n",
    "    Uses robust statistics (median + MAD) to avoid being influenced by outliers.\n",
    "    \"\"\"\n",
    "    print(\"HANDLING EXTREME OUTLIERS\")\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Get valid values (exclude existing errors)\n",
    "    valid_mask = df_clean['Value'] != -1\n",
    "    valid_values = df_clean.loc[valid_mask, 'Value']\n",
    "    \n",
    "    # MAD = Median Absolute Deviation\n",
    "    median = valid_values.median()\n",
    "    mad = (valid_values - median).abs().median()\n",
    "    \n",
    "    # Convert MAD to std equivalent (for normal distribution)\n",
    "    std_equivalent = mad * 1.4826\n",
    "    \n",
    "    print(f\"\\nRobust statistics (using median and MAD):\")\n",
    "    print(f\"Median: {median:.2f}\")\n",
    "    print(f\"MAD-based std: {std_equivalent:.2f}\")\n",
    "    \n",
    "    # Define outlier thresholds\n",
    "    lower_bound = median - (n_sigma * std_equivalent)\n",
    "    upper_bound = median + (n_sigma * std_equivalent)\n",
    "    \n",
    "    print(f\"\\nOutlier thresholds ({n_sigma} sigma):\")\n",
    "    print(f\"Lower bound: {lower_bound:,.2f}\")\n",
    "    print(f\"Upper bound: {upper_bound:,.2f}\")\n",
    "    \n",
    "    # Find outliers\n",
    "    outlier_mask = valid_mask & ((df_clean['Value'] < lower_bound) | (df_clean['Value'] > upper_bound))\n",
    "    outlier_count = outlier_mask.sum()\n",
    "    \n",
    "    print(f\"\\nOutliers detected: {outlier_count:,} ({outlier_count/len(df)*100:.4f}%)\")\n",
    "    \n",
    "    if outlier_count > 0:\n",
    "        # Show examples\n",
    "        print(f\"\\nSample outliers to be marked as errors:\")\n",
    "        outlier_samples = df_clean[outlier_mask][['TagName', 'EventTime_DT', 'Value', 'Value_str']].head(10)\n",
    "        print(outlier_samples)\n",
    "        \n",
    "        # Mark outliers as errors\n",
    "        df_clean.loc[outlier_mask, 'Value'] = -1\n",
    "        df_clean.loc[outlier_mask, 'Value_str'] = df_clean.loc[outlier_mask, 'Value_str'].fillna('Extreme Outlier')\n",
    "        \n",
    "        print(f\"\\nMarked {outlier_count:,} extreme outliers as errors (Value=-1)\")\n",
    "        \n",
    "        # Update statistics\n",
    "        new_valid = df_clean[df_clean['Value'] != -1]['Value']\n",
    "        print(f\"\\nUpdated statistics (after outlier removal):\")\n",
    "        print(f\"Count: {len(new_valid):,}\")\n",
    "        print(f\"Min: {new_valid.min():.2f}\")\n",
    "        print(f\"Max: {new_valid.max():.2f}\")\n",
    "        print(f\"Mean: {new_valid.mean():.2f}\")\n",
    "        print(f\"Median: {new_valid.median():.2f}\")\n",
    "        print(f\"Std: {new_valid.std():.2f}\")\n",
    "    else:\n",
    "        print(\"\\nNo extreme outliers found\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Apply outlier handling\n",
    "dt_df_clean = handle_extreme_outliers(dt_df_clean, n_sigma=10.0)\n",
    "\n",
    "# Save updated checkpoint again\n",
    "dt_df_clean.to_parquet(\"checkpoint_05_missing_handled.parquet\", index=False)\n",
    "print(\"\\nUpdated checkpoint 5 with outliers handled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATA QUALITY VALIDATION\n",
      "======================================================================\n",
      "\n",
      "1. Missing Values Check:\n",
      "  ✓ No missing values\n",
      "\n",
      "2. Infinite Values Check:\n",
      "  ✓ No infinite values\n",
      "\n",
      "3. Data Type Check:\n",
      "  ⚠ Found 2 object columns:\n",
      "    - TagName (object type)\n",
      "    - Value_str (object type)\n",
      "\n",
      "4. Timestamp Validity Check:\n",
      "  ✓ All timestamps valid\n",
      "    Date range: 2023-12-31 00:00:00 to 2024-07-27 23:59:57.573000\n",
      "  ✓ Timestamps are sorted chronologically\n",
      "\n",
      "5. Value Distribution Check:\n",
      "  Valid numeric values: 187,639,053 (87.31%)\n",
      "  Error values (-1): 27,277,174 (12.69%)\n",
      "  Value statistics (excluding -1):\n",
      "    Mean: 80.00, Std: 129.38\n",
      "    1st percentile: -32.50, 99th percentile: 564.69\n",
      "\n",
      "6. Status Distribution Check:\n",
      "  Status columns:\n",
      "    Status_Bad: 1,273,117 (0.59%)\n",
      "    Status_Good: 213,642,667 (99.41%)\n",
      "    Status_Questionable: 190 (0.00%)\n",
      "    Status_Substituted, Good: 253 (0.00%)\n",
      "\n",
      "7. Sensor Coverage Check:\n",
      "  Unique sensors (TagNames): 7,759\n",
      "  ⚠ 976 sensors have < 10 readings\n",
      "\n",
      "8. Memory Usage:\n",
      "  Total memory: 36574.14 MB (35.72 GB)\n",
      "\n",
      "======================================================================\n",
      "VALIDATION SUMMARY\n",
      "======================================================================\n",
      "✓ No critical issues found.\n",
      "⚠ 4 warnings (non-critical):\n",
      "  - Column 'TagName' is object type - may need encoding\n",
      "  - Column 'Value_str' is object type - may need encoding\n",
      "  - High error rate: 12.69% of values are errors\n",
      "  - 976 sensors have fewer than 10 readings\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def validate_data_quality(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Comprehensive data quality validation for model readiness.\n",
    "    \"\"\"\n",
    "    print(\"DATA QUALITY VALIDATION\")\n",
    "    \n",
    "    issues = []\n",
    "    warnings = []\n",
    "    \n",
    "    # 1. Check for missing values\n",
    "    print(\"\\n1. Missing Values Check:\")\n",
    "    missing = df.isnull().sum().sum()\n",
    "    if missing > 0:\n",
    "        issues.append(f\"Found {missing:,} missing values\")\n",
    "        print(f\"Found {missing:,} missing values\")\n",
    "    else:\n",
    "        print(f\"No missing values\")\n",
    "    \n",
    "    # 2. Check for infinite values\n",
    "    print(\"\\n2. Infinite Values Check:\")\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    inf_count = 0\n",
    "    for col in numeric_cols:\n",
    "        inf_in_col = np.isinf(df[col]).sum()\n",
    "        if inf_in_col > 0:\n",
    "            inf_count += inf_in_col\n",
    "            issues.append(f\"Column '{col}' has {inf_in_col:,} infinite values\")\n",
    "    \n",
    "    if inf_count > 0:\n",
    "        print(f\"Found {inf_count:,} infinite values\")\n",
    "    else:\n",
    "        print(f\"No infinite values\")\n",
    "    \n",
    "    # 3. Check data types\n",
    "    print(\"\\n3. Data Type Check:\")\n",
    "    object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    if len(object_cols) > 0:\n",
    "        print(f\"Found {len(object_cols)} object columns:\")\n",
    "        for col in object_cols:\n",
    "            print(f\"    - {col} (object type)\")\n",
    "            warnings.append(f\"Column '{col}' is object type, may need encoding, check contents\")\n",
    "    else:\n",
    "        print(f\"All columns have appropriate numeric/datetime types\")\n",
    "    \n",
    "    # 4. Check timestamp validity\n",
    "    print(\"\\n4. Timestamp Validity Check:\")\n",
    "    if 'EventTime_DT' in df.columns:\n",
    "        nat_count = df['EventTime_DT'].isna().sum()\n",
    "        if nat_count > 0:\n",
    "            issues.append(f\"Found {nat_count:,} NaT timestamps\")\n",
    "            print(f\"Found {nat_count:,} NaT timestamps\")\n",
    "        else:\n",
    "            print(f\"All timestamps valid\")\n",
    "            min_date = df['EventTime_DT'].min()\n",
    "            max_date = df['EventTime_DT'].max()\n",
    "            print(f\"Date range: {min_date} to {max_date}\")\n",
    "            \n",
    "            # Check if sorted\n",
    "            is_sorted = df['EventTime_DT'].is_monotonic_increasing\n",
    "            if is_sorted:\n",
    "                print(f\"Timestamps are sorted chronologically\")\n",
    "            else:\n",
    "                warnings.append(\"Timestamps are not sorted\")\n",
    "                print(f\"Timestamps are NOT sorted chronologically\")\n",
    "    \n",
    "    # 5. Check value distribution\n",
    "    print(\"\\n5. Value Distribution Check:\")\n",
    "    if 'Value' in df.columns:\n",
    "        valid_values = (df['Value'] != -1).sum()\n",
    "        error_values = (df['Value'] == -1).sum()\n",
    "        total = len(df)\n",
    "        \n",
    "        print(f\"Valid numeric values: {valid_values:,} ({valid_values/total*100:.2f}%)\")\n",
    "        print(f\"Error values (-1): {error_values:,} ({error_values/total*100:.2f}%)\")\n",
    "        \n",
    "        if error_values / total > 0.1:  # More than 10% errors\n",
    "            warnings.append(f\"{error_values/total*100:.2f}% of values are errors\")\n",
    "        \n",
    "        # Check for extreme outliers in valid values\n",
    "        valid_vals = df[df['Value'] != -1]['Value']\n",
    "        if len(valid_vals) > 0:\n",
    "            q99 = valid_vals.quantile(0.99)\n",
    "            q01 = valid_vals.quantile(0.01)\n",
    "            mean_val = valid_vals.mean()\n",
    "            std_val = valid_vals.std()\n",
    "            \n",
    "            print(f\"Value statistics (excluding -1):\")\n",
    "            print(f\"Mean: {mean_val:.2f}, Std: {std_val:.2f}\")\n",
    "            print(f\"1st percentile: {q01:.2f}, 99th percentile: {q99:.2f}\")\n",
    "    \n",
    "    # 6. Check status distribution\n",
    "    print(\"\\n6. Status Distribution Check:\")\n",
    "    status_cols = [col for col in df.columns if col.startswith('Status_')]\n",
    "    if status_cols:\n",
    "        print(f\"  Status columns:\")\n",
    "        for col in status_cols:\n",
    "            count = df[col].sum()\n",
    "            pct = count / len(df) * 100\n",
    "            print(f\"    {col}: {count:,} ({pct:.2f}%)\")\n",
    "    \n",
    "    # 7. Check sensor count\n",
    "    print(\"\\n7. Sensor Coverage Check:\")\n",
    "    if 'TagName' in df.columns:\n",
    "        unique_sensors = df['TagName'].nunique()\n",
    "        print(f\"  Unique sensors (TagNames): {unique_sensors:,}\")\n",
    "        \n",
    "        # Check for sensors with very few readings\n",
    "        sensor_counts = df['TagName'].value_counts()\n",
    "        low_count_sensors = (sensor_counts < 10).sum()\n",
    "        if low_count_sensors > 0:\n",
    "            warnings.append(f\"{low_count_sensors:,} sensors have fewer than 10 readings\")\n",
    "            print(f\"  {low_count_sensors:,} sensors have < 10 readings\")\n",
    "    \n",
    "    # 8. Memory usage\n",
    "    print(\"\\n8. Memory Usage:\")\n",
    "    memory_mb = df.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "    print(f\"  Total memory: {memory_mb:.2f} MB ({memory_mb/1024:.2f} GB)\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"VALIDATION SUMMARY\")\n",
    "    \n",
    "    if len(issues) == 0 and len(warnings) == 0:\n",
    "        print(\"All checks passed.\")\n",
    "        status = \"READY\"\n",
    "    elif len(issues) == 0:\n",
    "        print(f\"No critical issues found.\")\n",
    "        print(f\"{len(warnings)} warnings (non-critical):\")\n",
    "        for warning in warnings:\n",
    "            print(f\"  - {warning}\")\n",
    "        status = \"READY_WITH_WARNINGS\"\n",
    "    else:\n",
    "        print(f\"Found {len(issues)} critical issues:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  - {issue}\")\n",
    "        if len(warnings) > 0:\n",
    "            print(f\"Additional {len(warnings)} warnings:\")\n",
    "            for warning in warnings:\n",
    "                print(f\"  - {warning}\")\n",
    "        status = \"NEEDS_ATTENTION\"\n",
    "    \n",
    "    return {\n",
    "        'status': status,\n",
    "        'issues': issues,\n",
    "        'warnings': warnings,\n",
    "        'total_rows': len(df),\n",
    "        'total_columns': len(df.columns),\n",
    "        'memory_mb': memory_mb\n",
    "    }\n",
    "\n",
    "# Run validation\n",
    "import numpy as np\n",
    "validation_results = validate_data_quality(dt_df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINALIZING PREPROCESSING\n",
      "\n",
      "1. Saving final preprocessed dataset...\n",
      "   ✓ Saved to ShellData_preprocessed_final.parquet (2022.29 MB)\n",
      "\n",
      "2. Saving sample dataset...\n",
      "Saved 100,000 row sample to ShellData_preprocessed_sample.csv\n",
      "\n",
      "3. Generating metadata...\n",
      "Saved metadata to ShellData_preprocessed_metadata.json\n",
      "\n",
      "Files created:\n",
      "  - ShellData_preprocessed_final.parquet (full dataset)\n",
      "  - ShellData_preprocessed_sample.csv (100k sample)\n",
      "  - ShellData_preprocessed_metadata.json (metadata)\n"
     ]
    }
   ],
   "source": [
    "def finalize_preprocessing(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Finalize preprocessing by saving final dataset and generating metadata.\n",
    "    \"\"\"\n",
    "    print(\"FINALIZING PREPROCESSING\")\n",
    "    \n",
    "    # 1. Save final dataset\n",
    "    print(\"\\n1. Saving final preprocessed dataset...\")\n",
    "    df.to_parquet(\"ShellData_preprocessed_final.parquet\", index=False, compression='snappy')\n",
    "    \n",
    "    import os\n",
    "    file_size = os.path.getsize(\"ShellData_preprocessed_final.parquet\") / 1024 / 1024\n",
    "    print(f\"Saved to ShellData_preprocessed_final.parquet ({file_size:.2f} MB)\")\n",
    "    \n",
    "    # 2. Save a csv sample for inspection\n",
    "    print(\"\\n2. Saving sample dataset...\")\n",
    "    sample_size = min(100_000, len(df))\n",
    "    sample = df.sample(n=sample_size, random_state=42)\n",
    "    sample.to_csv(\"ShellData_preprocessed_sample.csv\", index=False)\n",
    "    print(f\"Saved {sample_size:,} row sample to ShellData_preprocessed_sample.csv\")\n",
    "    \n",
    "    # 3. Generate metadata\n",
    "    print(\"\\n3. Generating metadata...\")\n",
    "    \n",
    "    valid_values = df[df['Value'] != -1]['Value']\n",
    "    error_breakdown = df[df['Value'] == -1]['Value_str'].value_counts().to_dict()\n",
    "    \n",
    "    metadata = {\n",
    "        \"dataset_info\": {\n",
    "            \"total_rows\": int(len(df)),\n",
    "            \"total_columns\": int(len(df.columns)),\n",
    "            \"date_range_start\": str(df['EventTime_DT'].min()),\n",
    "            \"date_range_end\": str(df['EventTime_DT'].max()),\n",
    "            \"unique_sensors\": int(df['TagName'].nunique()),\n",
    "            \"memory_usage_mb\": float(df.memory_usage(deep=True).sum() / 1024 / 1024)\n",
    "        },\n",
    "        \"value_statistics\": {\n",
    "            \"valid_values_count\": int((df['Value'] != -1).sum()),\n",
    "            \"valid_values_percent\": float((df['Value'] != -1).sum() / len(df) * 100),\n",
    "            \"error_values_count\": int((df['Value'] == -1).sum()),\n",
    "            \"error_values_percent\": float((df['Value'] == -1).sum() / len(df) * 100),\n",
    "            \"mean\": float(valid_values.mean()),\n",
    "            \"median\": float(valid_values.median()),\n",
    "            \"std\": float(valid_values.std()),\n",
    "            \"min\": float(valid_values.min()),\n",
    "            \"max\": float(valid_values.max())\n",
    "        },\n",
    "        \"error_breakdown\": {str(k): int(v) for k, v in error_breakdown.items()},\n",
    "        \"status_distribution\": {\n",
    "            col: int(df[col].sum()) \n",
    "            for col in df.columns if col.startswith('Status_')\n",
    "        },\n",
    "        \"columns\": list(df.columns),\n",
    "        \"column_types\": {col: str(dtype) for col, dtype in df.dtypes.items()},\n",
    "        \"preprocessing_steps\": [\n",
    "            \"1. Separated text values from numeric values in Value column\",\n",
    "            \"2. Converted EventTime strings to datetime objects\",\n",
    "            \"3. One-hot encoded Status column\",\n",
    "            \"4. Extracted time features (day, week, seconds, month, weekday)\",\n",
    "            \"5. Recovered 4.6M failed timestamp parsings\",\n",
    "            \"6. Removed 74,875 rows with missing values (0.03%)\",\n",
    "            \"7. Sorted chronologically by EventTime_DT\",\n",
    "            \"8. Marked 26.78M extreme outliers as errors (12.46%)\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open(\"ShellData_preprocessed_metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(\"Saved metadata to ShellData_preprocessed_metadata.json\")\n",
    "    \n",
    "    print(\"\\nFiles created:\")\n",
    "    print(\"  - ShellData_preprocessed_final.parquet (full dataset)\")\n",
    "    print(\"  - ShellData_preprocessed_sample.csv (100k sample)\")\n",
    "    print(\"  - ShellData_preprocessed_metadata.json (metadata)\")\n",
    "# Finalize\n",
    "finalize_preprocessing(dt_df_clean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtdip-sdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
