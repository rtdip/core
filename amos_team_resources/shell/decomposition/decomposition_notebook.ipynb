{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Time Series Decomposition - Shell Sensor Data\n",
    "\n",
    "This notebook performs MSTL (Multiple Seasonal-Trend decomposition using Loess) on Shell sensor data.\n",
    "\n",
    "**Goal**: Extract hourly and daily seasonal patterns from sensor data.\n",
    "\n",
    "**Steps**:\n",
    "1. Load raw data\n",
    "2. Select specific sensor(s)\n",
    "3. Preprocess Value column (handle non-numeric values)\n",
    "4. Check for missing timestamps and gaps\n",
    "5. Interpolate gaps and report details\n",
    "6. Filter to first 7 days\n",
    "7. Apply MSTL decomposition with hourly + daily seasonality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e969961f",
   "metadata": {},
   "source": [
    "# **Note**: It doesnt seem to be feasable to use 5-second intervall data as in this notebook because its too compute heavy even for one singular sensor. Also interpolating missing values with that intervall length would result in ~34% artificial values for the inspected sensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "# Step 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Add SDK to path\n",
    "SDK_PATH = Path().resolve().parents[2] / \"src\" / \"sdk\" / \"python\"\n",
    "sys.path.insert(0, str(SDK_PATH))\n",
    "\n",
    "# Import decomposition component\n",
    "from rtdip_sdk.pipelines.decomposition.pandas import MSTLDecomposition\n",
    "\n",
    "print(\"Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Step 2: Load Raw Data\n",
    "\n",
    "**Important**: We load **raw data** (not preprocessed) because preprocessing removes outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to raw data\n",
    "data_path = Path().resolve().parent / \"data\" / \"ShellData.parquet\"\n",
    "\n",
    "print(f\"Loading: {data_path}\")\n",
    "print(f\"File exists: {data_path.exists()}\")\n",
    "print()\n",
    "\n",
    "# Load only the columns we need for memory efficiency\n",
    "df = pd.read_parquet(data_path, columns=['TagName', 'EventTime', 'Value'])\n",
    "\n",
    "print(f\"Loaded {len(df):,} rows\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB\")\n",
    "print(f\"Unique sensors: {df['TagName'].nunique():,}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print()\n",
    "print(\"First few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Step 3: Select Specific Sensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select sensor\n",
    "selected_sensor = \"0SP116X5V:NXR.0\"\n",
    "\n",
    "print(f\"Filtering to sensor: {selected_sensor}\")\n",
    "df_sensor = df[df['TagName'] == selected_sensor].copy()\n",
    "\n",
    "print(f\"Selected sensor has {len(df_sensor):,} rows\")\n",
    "print(f\"Percentage of total data: {len(df_sensor)/len(df)*100:.2f}%\")\n",
    "\n",
    "# Clean up memory\n",
    "del df\n",
    "\n",
    "df_sensor.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 4: Preprocess Value Column\n",
    "\n",
    "The Value column may contain:\n",
    "- Numeric values (what we want)\n",
    "- Text strings like \"Calc Failed\" or \"Bad\"\n",
    "- Special values like -1 (error indicators)\n",
    "\n",
    "We need to:\n",
    "1. Convert EventTime to datetime\n",
    "2. Convert Value to numeric (coerce invalid values to NaN)\n",
    "3. Remove error values and NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preprocessing Value column...\")\n",
    "print()\n",
    "\n",
    "# Convert EventTime to datetime\n",
    "print(\"1. Converting EventTime to datetime...\")\n",
    "df_sensor['EventTime'] = pd.to_datetime(df_sensor['EventTime'], errors='coerce')\n",
    "invalid_times = df_sensor['EventTime'].isna().sum()\n",
    "print(f\"   Invalid timestamps: {invalid_times:,}\")\n",
    "\n",
    "# Drop invalid timestamps\n",
    "if invalid_times > 0:\n",
    "    df_sensor = df_sensor.dropna(subset=['EventTime']).copy()\n",
    "    print(f\"   Dropped {invalid_times:,} rows with invalid timestamps\")\n",
    "print()\n",
    "\n",
    "# Convert Value to numeric\n",
    "print(\"2. Converting Value to numeric...\")\n",
    "print(f\"   Original dtype: {df_sensor['Value'].dtype}\")\n",
    "\n",
    "# Check if already numeric\n",
    "if not pd.api.types.is_numeric_dtype(df_sensor['Value']):\n",
    "    df_sensor['Value'] = pd.to_numeric(df_sensor['Value'], errors='coerce')\n",
    "    print(f\"   Converted to numeric\")\n",
    "else:\n",
    "    print(f\"   Already numeric\")\n",
    "print()\n",
    "\n",
    "# Check for NaN after conversion\n",
    "print(\"3. Checking for invalid values...\")\n",
    "nan_count = df_sensor['Value'].isna().sum()\n",
    "error_count = (df_sensor['Value'] == -1).sum()\n",
    "print(f\"   NaN values: {nan_count:,} ({nan_count/len(df_sensor)*100:.2f}%)\")\n",
    "print(f\"   Error values (-1): {error_count:,} ({error_count/len(df_sensor)*100:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# Remove NaN and error values\n",
    "print(\"4. Removing invalid values...\")\n",
    "before_len = len(df_sensor)\n",
    "df_sensor = df_sensor[\n",
    "    (df_sensor['Value'].notna()) & \n",
    "    (df_sensor['Value'] != -1)\n",
    "].copy()\n",
    "removed = before_len - len(df_sensor)\n",
    "print(f\"   Removed {removed:,} rows ({removed/before_len*100:.2f}%)\")\n",
    "print(f\"   Remaining: {len(df_sensor):,} rows\")\n",
    "print()\n",
    "\n",
    "# Sort by time\n",
    "print(\"5. Sorting by time...\")\n",
    "df_sensor = df_sensor.sort_values('EventTime').reset_index(drop=True)\n",
    "print(f\"  Sorted\")\n",
    "print()\n",
    "\n",
    "# Show value statistics\n",
    "print(\"Value statistics:\")\n",
    "print(df_sensor['Value'].describe())\n",
    "print()\n",
    "print(f\"Time range: {df_sensor['EventTime'].min()} to {df_sensor['EventTime'].max()}\")\n",
    "print(f\"Duration: {df_sensor['EventTime'].max() - df_sensor['EventTime'].min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Sampling and Check for Missing Timestamps\n",
    "\n",
    "We need to understand:\n",
    "- What's the typical sampling interval?\n",
    "- Where are the gaps in timestamps?\n",
    "- How big are these gaps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analyzing sampling intervals...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate time differences between consecutive points\n",
    "time_diffs = df_sensor['EventTime'].diff().dropna()\n",
    "\n",
    "# Statistics\n",
    "print(\"\\nSampling interval statistics:\")\n",
    "print(f\"  Median:  {time_diffs.median()}\")\n",
    "print(f\"  Mean:    {time_diffs.mean()}\")\n",
    "print(f\"  Min:     {time_diffs.min()}\")\n",
    "print(f\"  Max:     {time_diffs.max()}\")\n",
    "print()\n",
    "print(\"Percentiles:\")\n",
    "for p in [25, 50, 75, 90, 95, 99]:\n",
    "    print(f\"  {p:2d}th: {time_diffs.quantile(p/100)}\")\n",
    "\n",
    "# Determine what's \"normal\" vs \"gap\"\n",
    "median_interval = time_diffs.median()\n",
    "print(f\"\\nMedian interval: {median_interval}\")\n",
    "\n",
    "# Define a gap as anything > 10x the median interval\n",
    "gap_threshold = median_interval * 10\n",
    "print(f\"Gap threshold (10x median): {gap_threshold}\")\n",
    "\n",
    "# Find gaps\n",
    "gaps = time_diffs[time_diffs > gap_threshold]\n",
    "print(f\"\\n✓ Found {len(gaps)} gaps larger than {gap_threshold}\")\n",
    "\n",
    "if len(gaps) > 0:\n",
    "    print(\"\\nLargest gaps:\")\n",
    "    for i, (idx, gap_size) in enumerate(gaps.nlargest(10).items(), 1):\n",
    "        gap_start = df_sensor.loc[idx-1, 'EventTime']\n",
    "        gap_end = df_sensor.loc[idx, 'EventTime']\n",
    "        print(f\"  {i:2d}. {gap_size} from {gap_start} to {gap_end}\")\n",
    "else:\n",
    "    print(\"\\nNo significant gaps found!\")\n",
    "\n",
    "# Visualize sampling intervals\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 8))\n",
    "\n",
    "# Histogram of intervals\n",
    "axes[0].hist(time_diffs.dt.total_seconds(), bins=100, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(median_interval.total_seconds(), color='red', linestyle='--', label=f'Median: {median_interval}')\n",
    "axes[0].set_xlabel('Interval (seconds)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution of Sampling Intervals')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Intervals over time\n",
    "axes[1].plot(df_sensor['EventTime'][1:], time_diffs.dt.total_seconds(), linewidth=0.5, alpha=0.7)\n",
    "axes[1].axhline(median_interval.total_seconds(), color='red', linestyle='--', alpha=0.5, label='Median')\n",
    "axes[1].axhline(gap_threshold.total_seconds(), color='orange', linestyle='--', alpha=0.5, label='Gap threshold')\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].set_ylabel('Interval (seconds)')\n",
    "axes[1].set_title('Sampling Intervals Over Time')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sampling_analysis.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 6: Resample to Regular Intervals\n",
    "\n",
    "MSTL requires data at regular intervals. We'll:\n",
    "1. Choose appropriate resampling frequency based on median interval\n",
    "2. Create a regular time grid\n",
    "3. Map our data to this grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resampling to regular intervals...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Determine appropriate resampling frequency\n",
    "median_interval = df_sensor['EventTime'].diff().median()\n",
    "print(f\"Median interval: {median_interval}\")\n",
    "\n",
    "if median_interval <= pd.Timedelta(seconds=10):\n",
    "    resample_freq = '5S'  # 5 seconds\n",
    "    freq_name = '5 seconds'\n",
    "elif median_interval <= pd.Timedelta(seconds=30):\n",
    "    resample_freq = '30S'  # 30 seconds\n",
    "    freq_name = '30 seconds'\n",
    "elif median_interval <= pd.Timedelta(minutes=5):\n",
    "    resample_freq = '1T'  # 1 minute\n",
    "    freq_name = '1 minute'\n",
    "else:\n",
    "    resample_freq = '5T'  # 5 minutes\n",
    "    freq_name = '5 minutes'\n",
    "\n",
    "print(f\"\\nResampling frequency: {resample_freq} ({freq_name})\")\n",
    "print()\n",
    "\n",
    "# Resample\n",
    "print(\"Creating regular time grid...\")\n",
    "df_resampled = df_sensor.set_index('EventTime').resample(resample_freq)['Value'].mean().reset_index()\n",
    "\n",
    "print(f\"Resampled to {len(df_resampled):,} regular intervals\")\n",
    "print(f\"Original points: {len(df_sensor):,}\")\n",
    "print(f\"Regular intervals: {len(df_resampled):,}\")\n",
    "\n",
    "# Check how many are missing\n",
    "missing_count = df_resampled['Value'].isna().sum()\n",
    "print(f\"\\nMissing values after resampling: {missing_count:,} ({missing_count/len(df_resampled)*100:.2f}%)\")\n",
    "\n",
    "df_resampled.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Step 7: Interpolate Gaps\n",
    "\n",
    "We'll use **linear interpolation** to fill missing values, but with a limit:\n",
    "- Small gaps (< 2 minutes): Interpolate\n",
    "- Large gaps (≥ 2 minutes): Keep as NaN and handle later\n",
    "\n",
    "This ensures we don't create unrealistic data across large time gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Interpolating gaps...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate interpolation limit based on resampling frequency\n",
    "# We'll interpolate up to 2 minutes of missing data\n",
    "max_gap_duration = pd.Timedelta(minutes=2)\n",
    "resample_timedelta = pd.Timedelta(resample_freq)\n",
    "interpolation_limit = int(max_gap_duration / resample_timedelta)\n",
    "\n",
    "print(f\"Resampling frequency: {resample_freq}\")\n",
    "print(f\"Maximum gap to interpolate: {max_gap_duration} ({interpolation_limit} consecutive missing values)\")\n",
    "print()\n",
    "\n",
    "# Before interpolation\n",
    "missing_before = df_resampled['Value'].isna().sum()\n",
    "print(f\"Missing values before interpolation: {missing_before:,}\")\n",
    "\n",
    "# Find consecutive NaN sequences\n",
    "is_nan = df_resampled['Value'].isna()\n",
    "nan_groups = (is_nan != is_nan.shift()).cumsum()\n",
    "nan_sequences = df_resampled[is_nan].groupby(nan_groups).size()\n",
    "\n",
    "if len(nan_sequences) > 0:\n",
    "    print(f\"\\nGap analysis before interpolation:\")\n",
    "    print(f\"  Number of gaps: {len(nan_sequences)}\")\n",
    "    print(f\"  Smallest gap: {nan_sequences.min()} points ({nan_sequences.min() * resample_timedelta})\")\n",
    "    print(f\"  Largest gap: {nan_sequences.max()} points ({nan_sequences.max() * resample_timedelta})\")\n",
    "    print(f\"  Mean gap size: {nan_sequences.mean():.1f} points ({nan_sequences.mean() * resample_timedelta})\")\n",
    "    \n",
    "    # Show distribution of gap sizes\n",
    "    print(f\"\\n  Gap size distribution:\")\n",
    "    small_gaps = (nan_sequences <= interpolation_limit).sum()\n",
    "    large_gaps = (nan_sequences > interpolation_limit).sum()\n",
    "    print(f\"    ≤ {interpolation_limit} points (will interpolate): {small_gaps} gaps\")\n",
    "    print(f\"    > {interpolation_limit} points (too large): {large_gaps} gaps\")\n",
    "    \n",
    "    if large_gaps > 0:\n",
    "        print(f\"\\n  Largest gaps (will NOT be interpolated):\")\n",
    "        for i, (group_id, size) in enumerate(nan_sequences.nlargest(5).items(), 1):\n",
    "            gap_indices = df_resampled[is_nan & (nan_groups == group_id)].index\n",
    "            gap_start = df_resampled.loc[gap_indices[0], 'EventTime']\n",
    "            gap_end = df_resampled.loc[gap_indices[-1], 'EventTime']\n",
    "            duration = size * resample_timedelta\n",
    "            print(f\"    {i}. {size} points ({duration}) from {gap_start} to {gap_end}\")\n",
    "print()\n",
    "\n",
    "# Perform interpolation\n",
    "print(f\"Interpolating gaps up to {interpolation_limit} consecutive missing values...\")\n",
    "df_resampled['Value'] = df_resampled['Value'].interpolate(\n",
    "    method='linear',\n",
    "    limit=interpolation_limit,\n",
    "    limit_direction='both'\n",
    ")\n",
    "\n",
    "# After interpolation\n",
    "missing_after = df_resampled['Value'].isna().sum()\n",
    "interpolated_count = missing_before - missing_after\n",
    "\n",
    "print(f\"\\nInterpolation complete\")\n",
    "print(f\"Interpolated: {interpolated_count:,} values ({interpolated_count/len(df_resampled)*100:.2f}%)\")\n",
    "print(f\"Remaining NaN: {missing_after:,} values ({missing_after/len(df_resampled)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Step 8: Handle Remaining Large Gaps\n",
    "\n",
    "For any remaining NaN values (large gaps that weren't interpolated), we'll:\n",
    "1. Identify continuous segments without NaN\n",
    "2. Select the longest continuous segment\n",
    "3. Use only that segment for decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Handling remaining gaps...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if df_resampled['Value'].isna().sum() > 0:\n",
    "    print(f\"Found {df_resampled['Value'].isna().sum():,} remaining NaN values\")\n",
    "    print(\"Finding longest continuous segment...\\n\")\n",
    "    \n",
    "    # Find continuous segments (groups of non-NaN values)\n",
    "    is_valid = df_resampled['Value'].notna()\n",
    "    segment_groups = (is_valid != is_valid.shift()).cumsum()\n",
    "    \n",
    "    # Get segments with valid data\n",
    "    valid_segments = df_resampled[is_valid].groupby(segment_groups)\n",
    "    segment_sizes = valid_segments.size()\n",
    "    \n",
    "    print(f\"Found {len(segment_sizes)} continuous segments\")\n",
    "    print(f\"\\nTop 5 longest segments:\")\n",
    "    for i, (group_id, size) in enumerate(segment_sizes.nlargest(5).items(), 1):\n",
    "        segment_data = df_resampled[is_valid & (segment_groups == group_id)]\n",
    "        start_time = segment_data['EventTime'].min()\n",
    "        end_time = segment_data['EventTime'].max()\n",
    "        duration = end_time - start_time\n",
    "        print(f\"  {i}. {size:,} points ({duration}) from {start_time} to {end_time}\")\n",
    "    \n",
    "    # Select longest segment\n",
    "    longest_segment_id = segment_sizes.idxmax()\n",
    "    df_continuous = df_resampled[is_valid & (segment_groups == longest_segment_id)].copy().reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nSelected longest segment: {len(df_continuous):,} points\")\n",
    "    print(f\"Start: {df_continuous['EventTime'].min()}\")\n",
    "    print(f\"End: {df_continuous['EventTime'].max()}\")\n",
    "    print(f\"Duration: {df_continuous['EventTime'].max() - df_continuous['EventTime'].min()}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No remaining NaN values - using complete dataset\")\n",
    "    df_continuous = df_resampled.copy()\n",
    "\n",
    "print(f\"\\nFinal dataset: {len(df_continuous):,} rows\")\n",
    "print(f\"No missing values: {df_continuous['Value'].notna().all()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Step 9: Filter to First 7 Days\n",
    "\n",
    "Now we'll take the first 7 days of the continuous data for decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Filtering to first 7 days...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get time range\n",
    "start_time = df_continuous['EventTime'].min()\n",
    "end_time = df_continuous['EventTime'].max()\n",
    "total_duration = end_time - start_time\n",
    "\n",
    "print(f\"Full continuous segment:\")\n",
    "print(f\"  Start: {start_time}\")\n",
    "print(f\"  End: {end_time}\")\n",
    "print(f\"  Duration: {total_duration}\")\n",
    "print(f\"  Points: {len(df_continuous):,}\")\n",
    "print()\n",
    "\n",
    "# Filter to first 7 days\n",
    "seven_days_later = start_time + pd.Timedelta(days=7)\n",
    "df_7days = df_continuous[df_continuous['EventTime'] <= seven_days_later].copy()\n",
    "\n",
    "print(f\"First 7 days:\")\n",
    "print(f\"  Start: {df_7days['EventTime'].min()}\")\n",
    "print(f\"  End: {df_7days['EventTime'].max()}\")\n",
    "print(f\"  Duration: {df_7days['EventTime'].max() - df_7days['EventTime'].min()}\")\n",
    "print(f\"  Points: {len(df_7days):,}\")\n",
    "print()\n",
    "\n",
    "# Calculate periods\n",
    "resample_timedelta = pd.Timedelta(resample_freq)\n",
    "hourly_period = int(pd.Timedelta(hours=1) / resample_timedelta)\n",
    "daily_period = int(pd.Timedelta(days=1) / resample_timedelta)\n",
    "\n",
    "print(f\"Periods for MSTL:\")\n",
    "print(f\"  Hourly period: {hourly_period} observations per hour\")\n",
    "print(f\"  Daily period: {daily_period} observations per day\")\n",
    "print()\n",
    "\n",
    "print(f\"Expected points for 7 days: {daily_period * 7:,}\")\n",
    "print(f\"Actual points: {len(df_7days):,}\")\n",
    "print(f\"Coverage: {len(df_7days) / (daily_period * 7) * 100:.1f}%\")\n",
    "\n",
    "# Check if we have enough data for decomposition\n",
    "# MSTL requires at least 2 complete cycles of the largest period\n",
    "min_required = daily_period * 2\n",
    "if len(df_7days) >= min_required:\n",
    "    print(f\"\\nSufficient data for decomposition (need >= {min_required:,} points)\")\n",
    "else:\n",
    "    print(f\"\\nWARNING: Insufficient data (need >= {min_required:,} points, have {len(df_7days):,})\")\n",
    "    print(f\"  Consider using more days or a coarser resampling frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Step 10: Visualize Data Before Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 5))\n",
    "\n",
    "ax.plot(df_7days['EventTime'], df_7days['Value'], linewidth=0.5, alpha=0.7)\n",
    "ax.set_title(f'Sensor {selected_sensor} - First 7 Days (Ready for Decomposition)')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Value')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data_before_decomposition.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Data characteristics:\")\n",
    "print(f\"  Mean: {df_7days['Value'].mean():.4f}\")\n",
    "print(f\"  Std: {df_7days['Value'].std():.4f}\")\n",
    "print(f\"  Min: {df_7days['Value'].min():.4f}\")\n",
    "print(f\"  Max: {df_7days['Value'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Step 11: Apply MSTL Decomposition with Hourly + Daily Seasonality\n",
    "\n",
    "Now we'll decompose the time series into:\n",
    "- **Trend**: Long-term direction\n",
    "- **Seasonal (Hourly)**: Repeating pattern every hour\n",
    "- **Seasonal (Daily)**: Repeating pattern every 24 hours\n",
    "- **Residual**: Random noise\n",
    "\n",
    "**Note**: MSTL can handle multiple seasonal periods simultaneously!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Applying MSTL Decomposition...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Sensor: {selected_sensor}\")\n",
    "print(f\"  Data points: {len(df_7days):,}\")\n",
    "print(f\"  Resampling frequency: {resample_freq}\")\n",
    "print(f\"  Hourly period: {hourly_period} observations\")\n",
    "print(f\"  Daily period: {daily_period} observations\")\n",
    "print(f\"  Duration: {df_7days['EventTime'].max() - df_7days['EventTime'].min()}\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    # Create MSTL decomposer with both hourly and daily seasonality\n",
    "    print(\"Creating MSTL decomposer with hourly + daily seasonality...\")\n",
    "    mstl = MSTLDecomposition(\n",
    "        df=df_7days,\n",
    "        value_column='Value',\n",
    "        timestamp_column='EventTime',\n",
    "        periods=[hourly_period, daily_period],  # Both hourly and daily seasonality\n",
    "        iterate=2\n",
    "    )\n",
    "    \n",
    "    # Perform decomposition\n",
    "    print(\"Decomposing (this may take a minute for 7 days of data)...\")\n",
    "    df_decomposed = mstl.decompose()\n",
    "    \n",
    "    print(\"\\nDecomposition successful\")\n",
    "    print(f\"\\nComponents:\")\n",
    "    for col in df_decomposed.columns:\n",
    "        if col not in ['EventTime', 'Value']:\n",
    "            print(f\"  - {col}\")\n",
    "    \n",
    "    print(f\"\\nShape: {df_decomposed.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nDecomposition failed\")\n",
    "    print(f\"Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    df_decomposed = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Step 12: Visualize Decomposition Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_decomposed is not None:\n",
    "    hourly_col = f'seasonal_{hourly_period}'\n",
    "    daily_col = f'seasonal_{daily_period}'\n",
    "    \n",
    "    fig, axes = plt.subplots(5, 1, figsize=(16, 15))\n",
    "    \n",
    "    # Original\n",
    "    axes[0].plot(df_decomposed['EventTime'], df_decomposed['Value'], linewidth=0.5, alpha=0.7)\n",
    "    axes[0].set_title(f'Original - {selected_sensor} (First 7 Days)')\n",
    "    axes[0].set_ylabel('Value')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Trend\n",
    "    axes[1].plot(df_decomposed['EventTime'], df_decomposed['trend'], linewidth=1.5, color='orange')\n",
    "    axes[1].set_title('Trend Component')\n",
    "    axes[1].set_ylabel('Trend')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Seasonal (Hourly)\n",
    "    axes[2].plot(df_decomposed['EventTime'], df_decomposed[hourly_col], linewidth=0.5, color='blue')\n",
    "    axes[2].set_title(f'Hourly Seasonal Component (Period = {hourly_period} points = 1 hour)')\n",
    "    axes[2].set_ylabel('Hourly Seasonal')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Seasonal (Daily)\n",
    "    axes[3].plot(df_decomposed['EventTime'], df_decomposed[daily_col], linewidth=0.5, color='green')\n",
    "    axes[3].set_title(f'Daily Seasonal Component (Period = {daily_period} points = 24 hours)')\n",
    "    axes[3].set_ylabel('Daily Seasonal')\n",
    "    axes[3].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residual\n",
    "    axes[4].plot(df_decomposed['EventTime'], df_decomposed['residual'], linewidth=0.5, color='red', alpha=0.5)\n",
    "    axes[4].set_title('Residual Component')\n",
    "    axes[4].set_xlabel('Time')\n",
    "    axes[4].set_ylabel('Residual')\n",
    "    axes[4].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    safe_tag = selected_sensor.replace('/', '_').replace('\\\\', '_').replace(':', '_')\n",
    "    plt.savefig(f'decomposition_{safe_tag}.png', dpi=150)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cannot visualize - decomposition failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Step 13: Variance Analysis\n",
    "\n",
    "How much of the total variance is explained by each component?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_decomposed is not None:\n",
    "    print(\"Variance Analysis\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    hourly_col = f'seasonal_{hourly_period}'\n",
    "    daily_col = f'seasonal_{daily_period}'\n",
    "    \n",
    "    total_var = df_decomposed['Value'].var()\n",
    "    trend_var = df_decomposed['trend'].dropna().var()\n",
    "    hourly_var = df_decomposed[hourly_col].dropna().var()\n",
    "    daily_var = df_decomposed[daily_col].dropna().var()\n",
    "    residual_var = df_decomposed['residual'].dropna().var()\n",
    "    \n",
    "    print(f\"\\nTotal variance: {total_var:.6f}\")\n",
    "    print()\n",
    "    print(f\"Component breakdown:\")\n",
    "    print(f\"  Trend:           {trend_var:>12.6f} ({trend_var/total_var*100:>6.2f}%)\")\n",
    "    print(f\"  Hourly Seasonal: {hourly_var:>12.6f} ({hourly_var/total_var*100:>6.2f}%)\")\n",
    "    print(f\"  Daily Seasonal:  {daily_var:>12.6f} ({daily_var/total_var*100:>6.2f}%)\")\n",
    "    print(f\"  Residual:        {residual_var:>12.6f} ({residual_var/total_var*100:>6.2f}%)\")\n",
    "    print()\n",
    "    \n",
    "    combined_seasonal = hourly_var + daily_var\n",
    "    print(f\"  Combined Seasonal: {combined_seasonal:>12.6f} ({combined_seasonal/total_var*100:>6.2f}%)\")\n",
    "else:\n",
    "    print(\"Cannot analyze variance - decomposition failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Step 14: Zoom into First 24 Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_decomposed is not None:\n",
    "    # Take first 24 hours\n",
    "    start = df_decomposed['EventTime'].min()\n",
    "    end = start + pd.Timedelta(days=1)\n",
    "    \n",
    "    df_day = df_decomposed[df_decomposed['EventTime'] <= end].copy()\n",
    "    hourly_col = f'seasonal_{hourly_period}'\n",
    "    daily_col = f'seasonal_{daily_period}'\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(16, 10))\n",
    "    \n",
    "    # Original vs Trend\n",
    "    axes[0].plot(df_day['EventTime'], df_day['Value'], linewidth=0.5, alpha=0.7, label='Original')\n",
    "    axes[0].plot(df_day['EventTime'], df_day['trend'], linewidth=2, color='orange', label='Trend')\n",
    "    axes[0].set_title('Original vs Trend (First 24 Hours)')\n",
    "    axes[0].set_ylabel('Value')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hourly seasonal pattern\n",
    "    axes[1].plot(df_day['EventTime'], df_day[hourly_col], linewidth=1, color='blue')\n",
    "    axes[1].set_title('Hourly Seasonal Pattern (First 24 Hours)')\n",
    "    axes[1].set_ylabel('Hourly Seasonal')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Daily seasonal pattern\n",
    "    axes[2].plot(df_day['EventTime'], df_day[daily_col], linewidth=1.5, color='green')\n",
    "    axes[2].set_title('Daily Seasonal Pattern (First 24 Hours)')\n",
    "    axes[2].set_xlabel('Time')\n",
    "    axes[2].set_ylabel('Daily Seasonal')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('daily_pattern_zoom.png', dpi=150)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cannot visualize patterns - decomposition failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Step 15: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_decomposed is not None:\n",
    "    print(\"Saving results...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    safe_tag = selected_sensor.replace('/', '_').replace('\\\\', '_').replace(':', '_')\n",
    "    hourly_col = f'seasonal_{hourly_period}'\n",
    "    daily_col = f'seasonal_{daily_period}'\n",
    "    \n",
    "    # Save decomposed data\n",
    "    parquet_file = f'decomposition_{safe_tag}_7days.parquet'\n",
    "    df_decomposed.to_parquet(parquet_file, index=False)\n",
    "    print(f\"Saved: {parquet_file}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    total_var = df_decomposed['Value'].var()\n",
    "    trend_var = df_decomposed['trend'].dropna().var()\n",
    "    hourly_var = df_decomposed[hourly_col].dropna().var()\n",
    "    daily_var = df_decomposed[daily_col].dropna().var()\n",
    "    residual_var = df_decomposed['residual'].dropna().var()\n",
    "    \n",
    "    metadata = {\n",
    "        'sensor': selected_sensor,\n",
    "        'duration': '7 days',\n",
    "        'data_points': len(df_decomposed),\n",
    "        'resampling_frequency': resample_freq,\n",
    "        'periods': {\n",
    "            'hourly': hourly_period,\n",
    "            'daily': daily_period\n",
    "        },\n",
    "        'time_range': {\n",
    "            'start': str(df_decomposed['EventTime'].min()),\n",
    "            'end': str(df_decomposed['EventTime'].max())\n",
    "        },\n",
    "        'variance_explained': {\n",
    "            'trend_pct': float(trend_var / total_var * 100),\n",
    "            'hourly_seasonal_pct': float(hourly_var / total_var * 100),\n",
    "            'daily_seasonal_pct': float(daily_var / total_var * 100),\n",
    "            'combined_seasonal_pct': float((hourly_var + daily_var) / total_var * 100),\n",
    "            'residual_pct': float(residual_var / total_var * 100)\n",
    "        },\n",
    "        'preprocessing': {\n",
    "            'source': 'raw data (not preprocessed)',\n",
    "            'interpolation_limit': f'{max_gap_duration} ({interpolation_limit} points)',\n",
    "            'gaps_interpolated': int(interpolated_count)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    metadata_file = f'decomposition_{safe_tag}_7days.json'\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"Saved: {metadata_file}\")\n",
    "    \n",
    "    print(\"\\nComplete\")\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"  Sensor: {selected_sensor}\")\n",
    "    print(f\"  Duration: 7 days\")\n",
    "    print(f\"  Hourly seasonality: {hourly_var/total_var*100:.2f}% of variance\")\n",
    "    print(f\"  Daily seasonality: {daily_var/total_var*100:.2f}% of variance\")\n",
    "    print(f\"  Combined seasonal: {(hourly_var + daily_var)/total_var*100:.2f}% of variance\")\n",
    "else:\n",
    "    print(\"Cannot save results - decomposition failed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtdip-sdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
