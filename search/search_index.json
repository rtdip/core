{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"api/authentication/","title":"Authentication","text":"<p>RTDIP REST APIs require Azure Active Directory Authentication and passing the token received as an <code>authorization</code> header in the form of a Bearer token. An example of the REST API header is <code>Authorization: Bearer &lt;&lt;token&gt;&gt;</code></p>"},{"location":"api/authentication/#end-user-authentication","title":"End User Authentication","text":"<p>If a developer or business user would like to leverage the RTDIP REST API suite, it is recommended that they use the Identity Packages provided by Azure to obtain a token.</p> <ul> <li>REST API</li> <li>.NET</li> <li>Java</li> <li>Python</li> <li>Javascript</li> </ul> <p>Note</p> <p>Note that the above packages have the ability to obtain tokens for end users and service principals and support all available authentication options. </p> <p>Ensure to install the relevant package and obtain a token.</p> <p>See the examples section to see various authentication methods implemented.</p>"},{"location":"api/examples/","title":"Examples","text":"<p>Below are examples of how to execute APIs using various authentication options and API methods.</p>"},{"location":"api/examples/#end-user-authentication","title":"End User Authentication","text":""},{"location":"api/examples/#python","title":"Python","text":"<p>A python example of obtaining a token as a user can be found below using the <code>azure-identity</code> python package to authenticate with Azure AD.</p> <p>POST Requests</p> <p>The POST request can be used to pass many tags to the API. This is the preferred method when passing large volumes of tags to the API.</p> GET RequestPOST Request <pre><code>from azure.identity import DefaultAzureCredential\nimport requests\n\nauthentication = DefaultAzureCredential()\naccess_token = authentication.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n\nparams = {\n\"business_unit\": \"Business Unit\",\n\"region\": \"Region\",\n\"asset\": \"Asset Name\",\n\"data_security_level\": \"Security Level\",\n\"data_type\": \"float\",\n\"tag_name\": \"TAG1\",\n\"tag_name\": \"TAG2\",\n\"start_date\": \"2022-01-01\",\n\"end_date\": \"2022-01-01\",\n\"include_bad_data\": True\n}\n\nurl = \"https://example.com/api/v1/events/raw\"\n\npayload={}\nheaders = {\n'Authorization': 'Bearer {}'.format(access_token)\n}\n\nresponse = requests.request(\"GET\", url, headers=headers, params=params, data=payload)\n\nprint(response.json())\n</code></pre> <pre><code>from azure.identity import DefaultAzureCredential\nimport requests\n\nauthentication = DefaultAzureCredential()\naccess_token = authentication.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n\nparams = {\n\"business_unit\": \"Business Unit\",\n\"region\": \"Region\",\n\"asset\": \"Asset Name\",\n\"data_security_level\": \"Security Level\",\n\"data_type\": \"float\",\n\"start_date\": \"2022-01-01T15:00:00\",\n\"end_date\": \"2022-01-01T16:00:00\",\n\"include_bad_data\": True    \n}\n\nurl = \"https://example.com/api/v1/events/raw\"\n\npayload={\"tag_name\": [\"TAG1\", \"TAG2\"]}\n\nheaders = {\n\"Authorization\": \"Bearer {}\".format(access_token),\n}\n\n# Requests automatically sets the Content-Type to application/json when the request body is passed via the json parameter\nresponse = requests.request(\"POST\", url, headers=headers, params=params, json=payload)\n\nprint(response.json())\n</code></pre>"},{"location":"api/examples/#service-principal-authentication","title":"Service Principal Authentication","text":""},{"location":"api/examples/#get-request","title":"GET Request","text":"<p>Authentication using Service Principals is similar to end user authentication. An example, using Python is provided below where the <code>azure-identity</code> package is not used, instead a direct REST API call is made to retrieve the token.</p> cURLPython <pre><code>curl --location --request POST 'https://login.microsoftonline.com/{tenant id}/oauth2/v2.0/token' \\\n--form 'grant_type=\"client_credentials\"' \\\n--form 'client_id=\"&lt;&lt;client id&gt;&gt;\"' \\\n--form 'client_secret=\"&lt;&lt;client secret&gt;&gt;\"' \\\n--form 'scope=\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\"'\n</code></pre> <pre><code>import requests\n\nurl = \"https://login.microsoftonline.com/{tenant id}/oauth2/v2.0/token\"\n\npayload={'grant_type': 'client_credentials',\n'client_id': '&lt;&lt;client id&gt;&gt;',\n'client_secret': '&lt;&lt;client secret&gt;&gt;',\n'scope': '2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default'}\nfiles=[]\nheaders = {}\n\nresponse = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n\naccess_token  = response.json()[\"access_token\"]\n\nparams = {\n    \"business_unit\": \"Business Unit\",\n    \"region\": \"Region\",\n    \"asset\": \"Asset Name\",\n    \"data_security_level\": \"Security Level\",\n    \"data_type\": \"float\",\n    \"tag_name\": \"TAG1\",\n    \"tag_name\": \"TAG2\",\n    \"start_date\": \"2022-01-01\",\n    \"end_date\": \"2022-01-01\",\n    \"include_bad_data\": True\n}\n\nurl = \"https://example.com/api/v1/events/raw\"\n\npayload={}\nheaders = {\n'Authorization': 'Bearer {}'.format(access_token)\n}\n\nresponse = requests.request(\"GET\", url, headers=headers, params=params, data=payload)\n\nprint(response.text)\n</code></pre>"},{"location":"api/overview/","title":"Overview","text":"![rest](images/rest-api-logo.png){width=50%}"},{"location":"api/overview/#rtdip-rest-apis","title":"RTDIP REST APIs","text":"<p>RTDIP provides REST API endpoints for querying data in the platform. The APIs are a wrapper to the python RTDIP SDK and provide similar functionality for users and applications that are unable to leverage the python RTDIP SDK. It is recommended to read the RTDIP SDK documentation and in particular the Functions section for more information about the options and logic behind each API. </p> <p>RTDIP API's are designed with the intention of running small to medium queries, rather than large queries to reduce network latency, increase performance and maintainability.  </p>"},{"location":"api/rest_apis/","title":"RTDIP REST API Endpoints","text":"<p>RTDIP REST API documentation is available in a number of formats, as described below. </p>  ![rest](images/open-api.png){width=50%}  <p>RTDIP REST APIs are built to OpenAPI standard 3.0.2. You can obtain the OpenAPI JSON schema at the following endpoint of your deployed APIs <code>https://{domain name}/api/openapi.json</code></p>  ![rest](images/swagger.png){width=50%}  <p>It is recommended to review the Swagger documentation that can be found at the following endpoint of your deployed APIs <code>https://{domain name}/docs</code> for more information about the parameters and options for each API. It is also possible to try out each API from this link.</p>  ![rest](images/redoc-logo.png){width=50%}  <p>Additionally, further information about each API can be found in Redoc format at  the following endpoint of your deployed APIs <code>https://{domain name}/redoc</code></p>"},{"location":"api/deployment/azure/","title":"Deploy RTDIP APIs to Azure","text":"<p>The RTDIP repository contains the code to deploy the RTDIP REST APIs to your own Azure Cloud environment. The APIs are built as part of the rtdip repository CI/CD pipelines and the image is deployed to Docker Hub repo <code>rtdip/api</code>. Below contains information on how to build and deploy the containers from source or to setup your function app to use the deployed container image provided by RTDIP.</p>"},{"location":"api/deployment/azure/#deploying-the-rtdip-apis","title":"Deploying the RTDIP APIs","text":""},{"location":"api/deployment/azure/#deployment-from-build","title":"Deployment from Build","text":"<p>To deploy the RTDIP APIs directly from the repository, follow the steps below:</p> <ol> <li>Build the docker image using the following command:     <pre><code>docker build --tag &lt;container_registry_url&gt;/rtdip-api:v0.1.0 -f src/api/Dockerfile .\n</code></pre></li> <li>Login to your container registry     <pre><code>docker login &lt;container_registry_url&gt;\n</code></pre></li> <li>Push the docker image to your container registry     <pre><code>docker push &lt;container_registry_url&gt;/rtdip-api:v0.1.0\n</code></pre></li> <li>Configure your Function App to use the docker image     <pre><code>az functionapp config container set --name &lt;function_app_name&gt; --resource-group &lt;resource_group_name&gt; --docker-custom-image-name &lt;container_registry_url&gt;/rtdip-api:v0.1.0\n</code></pre></li> </ol>"},{"location":"api/deployment/azure/#deployment-from-docker-hub","title":"Deployment from Docker Hub","text":"<p>To deploy the RTDIP APIs from Docker Hub, follow the steps below:</p> <ol> <li>Configure your Function App to use the docker image     <pre><code>az functionapp config container set --name &lt;function_app_name&gt; --resource-group &lt;resource_group_name&gt; --docker-custom-image-name rtdip/api:azure-&lt;version&gt;\n</code></pre></li> </ol>"},{"location":"api/deployment/azure/#environment-variables","title":"Environment Variables","text":""},{"location":"api/deployment/azure/#azure-active-directory","title":"Azure Active Directory","text":"<ol> <li>Once Authentication has been configured on the Azure Function App correctly, it is required to set the following Environment Variable with the Tenant ID of the relevant Active Directory:<ul> <li>TENANT_ID</li> </ul> </li> </ol>"},{"location":"api/deployment/azure/#databricks","title":"Databricks","text":"<ol> <li>The following Environment Variables are required and the values can be retrieved from your Databricks SQL Warehouse or Databricks Cluster:<ul> <li>DATABRICKS_SQL_SERVER_HOSTNAME</li> <li>DATABRICKS_SQL_HTTP_PATH</li> </ul> </li> </ol>"},{"location":"api/deployment/azure/#odbc-driver","title":"ODBC Driver","text":"<ol> <li>To allow the APIs to leverage Turbodbc for connectivity and possible performance improvements, it is possible to set the following environment variable:<ul> <li>RTDIP_ODBC_CONNECTION=turbodbc</li> </ul> </li> </ol>"},{"location":"blog/2022/11/01/delta-lakehouse-and-real-time-data-ingestion-platform/","title":"Delta Lakehouse and Real Time Data Ingestion Platform","text":"![Delta Lakehouse](../images/delta-lakehouse.svg){width=50%}  <p>Real Time Data Ingestion Platform leverages Delta and the concept of a Lakehouse to ingest, store and manage it's data. There are many benefits to Delta for performing data engineering tasks on files stored in a data lake including ACID transactions, maintenance, SQL query capability and performance at scale. To find out more about Delta Lakehouse please see here.</p> <p>The Real Time Data Ingestion Platform team would like to share some lessons learnt from the implementation of Delta and the Lakehouse concept so that hopefully it helps others on their Delta Lakehouse journey.</p> <p>For reference, please consider the typical layout of timeseries data ingested by RTDIP:</p> <p>Events</p> Column Name Description TagName Typically represents a sensor name or a measurement EventTime A timestamp for a recorded value Status Status of the recording, normally indicating if the measurement value is good or bad Value The value of the measurement and can be of a number of types - float, double, string, integer <p>Metadata</p> Column Name Description TagName Typically represents a sensor name or a measurement Description A description for the sensor UoM UoM for the measurement <p>Note</p> <p>Metadata can include a number of additional columns and depends on the system that provides the metadata. The above are the required columns for any sensor data ingested by RTDIP.</p>","tags":["Pipelines","Delta"]},{"location":"blog/2022/11/01/delta-lakehouse-and-real-time-data-ingestion-platform/#design-considerations","title":"Design Considerations","text":"<p>Delta, in its simplest definition, is a set of parquet files managed by an index file. This allows Spark to perform tasks like partition pruning and file pruning to find the exact parquet file to be used by any ACID transactions being performed on it. By reducing the number of files and the amount of data that Spark needs to read in a query means that it will perform much better. It is important to consider the following when designing a Delta Table to achieve performance benefits:</p> <ul> <li>Columns that are likely to be used in most reads and writes</li> <li>Partition column(s)</li> <li>File Sizes</li> </ul>","tags":["Pipelines","Delta"]},{"location":"blog/2022/11/01/delta-lakehouse-and-real-time-data-ingestion-platform/#partition-columns","title":"Partition Columns","text":"<p>The biggest benefit achieved using Delta is to include a partition column in the design of a Delta Table. This is the fastest way for Spark to isolate the dataset it needs to work with in a query. The general rule of thumb is that each partition size should be roughly 1gb in size, and ideally would be a column or columns that are used in every query to filter data for that table.</p> <p>This can be difficult to achieve. The most queried columns in RTDIP event data are TagName and EventTime, however, partitioning data by TagName creates far too many small partitions and a timestamp column like EventTime can not be used for partitioning for the same reason. The best outcome is typically to create an additional column that is an aggregation of the EventTime column, such as a Date, Month or Year Column, depending on the frequency of the data being ingested. </p> <p>Note</p> <p>Given the above, always query RTDIP delta events tables using EventDate in the filter to achieve the best results.</p> <p>One of the best methods to analyse Spark query performance is to analyse the query plan of a query. It is essential that a Spark query plan leverages a partition column. This can be identified by reviewing the query plan in Spark. As per the below query plan, it can be seen that for this particular query only one partition was read by Spark. Make sure to try different queries to identify that the expected number of partitions are being used by Spark in every query. If it does not match your expected number of partitions, it is important to investigate why partition pruning is not being leveraged in your query. </p>  ![query plan](../images/spark-query-plan.png)","tags":["Pipelines","Delta"]},{"location":"blog/2022/11/01/delta-lakehouse-and-real-time-data-ingestion-platform/#zorder-columns","title":"ZORDER Columns","text":"<p>Even though the rule is to achieve roughly 1gb partitions for a Delta Table, Delta is likely to divide that partition into a number of files. The default target size is around 128gb per file. Due to this, it is possible to improve performance above and beyond partitioning by telling Spark which files within in a partition to read. This is where ZORDER becomes useful. </p> <p>Zordering organises the data within each file, and along with the Delta Index file, directs Spark to the exact files to use in its reads(and merge writes) on the table. It is important to find the right number of columns to ZORDER - the best outcome would be a combination of columns that does not cause the index to grow too large. For example, ZORDERING by TagName creates a small index, but ZORDERING by TagName and EventTime created a huge index as there are far more combinations to be indexed.</p> <p>The most obvious column to ZORDER on in RTDIP is the TagName as every query is likely to use this in its filter. Like partition pruning, it is possible to identify the impact of ZORDERING on your queries by reviewing the files read attribute in the query plan. As per the query plan below, you can see that two files were read within the one partition.</p>  ![query plan](../images/spark-query-plan.png)","tags":["Pipelines","Delta"]},{"location":"blog/2022/11/01/delta-lakehouse-and-real-time-data-ingestion-platform/#merge-and-file-sizes","title":"MERGE and File Sizes","text":"<p>As stated above, the default target size for file sizes within a partition is 128gb. However, this is not always ideal and in certain scenarios, it is possible to improve performance of Spark jobs by reducing file sizes in certain scenarios: - MERGE commands - Queries that target very small subsets of data within a file</p> <p>Due to the nature of Merges, its typically an action where small updates are being made to the dataset. Due to this, it is possible to get much better MERGE performance by setting the following attribute on the Delta Table <code>delta.tuneFileSizesForRewrites=true</code>. This targets smaller file sizes to reduce the amount of data in each read a MERGE operation performs on the data. RTDIP gained a significant performance improvement in reading and writing and was able to reduce the Spark cluster size by half by implementing this setting on its Delta Tables.</p> <p>However, even more performance gain was achieved when Databricks released Low Shuffle Merge from DBR 9.0 onwards. This assists Spark to merge data into files without disrupting the ZORDER layout of Delta files, in turn assisting Merge commands to continue leveraging ZORDER performance benefits on an ongoing basis. RTDIP was able to improve MERGE performance by 5x with this change. To leverage Low Shuffle Merge, set the following Spark config in your notebook <code>spark.databricks.delta.merge.enableLowShuffle=true</code>.</p>","tags":["Pipelines","Delta"]},{"location":"blog/2022/11/01/delta-lakehouse-and-real-time-data-ingestion-platform/#delta-table-additional-attributes","title":"Delta Table Additional Attributes","text":"<p>It is recommended to consider setting the following two attributes on all Delta Tables:</p> <ul> <li><code>delta.autoOptimize.autoCompact=true</code></li> <li><code>delta.autoOptimize.optimizeWrite=true</code></li> </ul> <p>To understand more about optimization options you can set on Delta Tables, please refer to this link.</p>","tags":["Pipelines","Delta"]},{"location":"blog/2022/11/01/delta-lakehouse-and-real-time-data-ingestion-platform/#maintenance-tasks","title":"Maintenance Tasks","text":"<p>One important step to be included with every Delta Table is maintenance. Most developers forget these very important maintenance tasks that need to run on a regular basis to maintain performance and cost on Delta Tables.</p> <p>As a standard, run a maintenance job every 24 hours to perform OPTIMIZE and VACUUM commands on Delta Tables.</p>","tags":["Pipelines","Delta"]},{"location":"blog/2022/11/01/delta-lakehouse-and-real-time-data-ingestion-platform/#optimize","title":"OPTIMIZE","text":"<p>OPTIMIZE is a Spark SQL command that can be run on any Delta Table and is the simplest way to optimize the file layout of a Delta Table. The biggest benefit of running OPTIMIZE however, is to organize Delta files using ZORDER. Due to how effective ZORDER is on queries, its unlikely that OPTIMIZE would not be executed on a Delta Table regularly.</p> <p>It may be a question as to why one would run OPTIMIZE as well as set <code>delta.autoOptimize.autoCompact=true</code> on all its tables. Auto Compact does not ZORDER its data(at the time of writing this article), its task is simply to attempt to create larger files during writing and avoid the small file problem. Therefore, autoCompact does not provide ZORDER capability. Due to this, consider an OPTIMIZE strategy as follows: - Auto Compact is used by default for any new files written to an RTDIP Delta Table between the execution of maintenance jobs. This ensures that any new data ingested by RTDIP is still being written in a suitable and performant manner. - OPTIMIZE with ZORDER is run on a daily basis on any partitions that have changed(excluding the current day) to ensure ZORDER and updating of the Delta Index file is done. </p> <p>Note</p> <p>RTDIP data is going to typically be ingesting using Spark Streaming - given the nature of a real time data ingestion platform, it makes sense that data ingestion is performed in real time. One complication this introduces is the impact of the OPTIMIZE command being executed at the same time as files being written to a partition. Due to this, execute OPTIMIZE on partitions where the EventDate is not equal to the current date, minimizing the possibility of an OPTIMIZE command and a file write command being executed on a partition at the same time. This logic reduces issues experienced by both the maintenance job and Spark streaming job.</p>","tags":["Pipelines","Delta"]},{"location":"blog/2022/11/01/delta-lakehouse-and-real-time-data-ingestion-platform/#vacuum","title":"VACUUM","text":"<p>One of the most powerful features of Delta is time travel. This allows querying of data as at a certain point of time in the past, or a particular version of the Delta Table. Whilst incredibly useful, it does consume storage space and if these historical files are never removed, the size of Delta Tables can grow exponentially large and increase cost.</p> <p>To ensure only the required historical versions of a Delta Table are stored, its important to execute the VACUUM command every 24 hours. This deletes any files or versions that are outside the time travel retention period.</p>","tags":["Pipelines","Delta"]},{"location":"blog/2022/11/01/delta-lakehouse-and-real-time-data-ingestion-platform/#conclusion","title":"Conclusion","text":"<p>Delta and the Lakehouse transformed the way RTDIP ingests its data and provides integration with other projects, applications and platforms. We hope the above assists others with their Delta development and we look forward to posting more content on RTDIP and its use of Spark in the future.</p>","tags":["Pipelines","Delta"]},{"location":"blog/2025/02/05/enhancing-data-quality-in-real-time-our-experience-with-rtdip-and-the-amos-project/","title":"Enhancing Data Quality in Real-Time: Our Experience with RTDIP and the AMOS Project","text":"![blog](../images/agile.svg){width=60%} 1 <p>Real-time data integration and preparation are crucial in today's data-driven world, especially when dealing with time series data from often distributed heterogeneous data sources. As data scientists often spend no less than 80%<sup>2</sup> of their time finding, integrating, and cleaning datasets, the importance of automated ingestion pipelines rises inevitably. Building such ingestion and integration frameworks can be challenging and can entail all sorts of technical debt like glue code, pipeline jungles, or dead code paths, which calls for precise conception and development of such systems. Modern software development approaches try to mitigate technical debts and enhance quality results by introducing and utilizing agile and more iterative methodologies, which are designed to foster rapid feedback and continuous progress.</p> <p>As part of the Agile Methods and Open Source (AMOS) project, we had the unique opportunity to work in a SCRUM team consisting of students from TU Berlin and FAU Erlangen-N\u00fcrnberg, to build data quality measures for the RTDIP Ingestion Pipeline framework. With the goal of enhancing data quality, we got to work and built modular pipeline components that aim to help data scientists and engineers with data integration, data cleaning, and data preparation.</p> <p>But what does it mean to work in an agile framework? The Agile Manifesto is above all a set of guiding values, principles, ideals, and goals. The overarching goal is to gain performance and be most effective while adding business value. By prioritizing the right fundamentals like individuals and interactions, working software, customer collaboration, and responding to change, cross-functional teams can ship viable products easier and faster.</p> <p>How that worked out for us in building data quality measures? True to the motto \"User stories drive everything,\" we got together with contributors from the RTDIP Team to hear about concepts, the end users' stake in the project, and the current state to get a grasp on the expectations we can set on ourselves. With that, we got to work and planned our first sprint, and soon, we got the idea of how agile implementation is here to point out deficiencies in our processes. Through regular team meetings, we fostered a culture of continuous feedback and testing, leveraging reviews and retrospectives to identify roadblocks and drive necessary changes that enhance the overall development process.</p>"},{"location":"blog/2025/02/05/enhancing-data-quality-in-real-time-our-experience-with-rtdip-and-the-amos-project/#enhancing-data-quality-in-rtdips-pipeline-framework","title":"Enhancing Data Quality in RTDIP's Pipeline Framework","text":"<p>Coming up with modular steps that enhance data quality was the initial and arguably most critical step to start off a successful development process. So the question was: what exactly do the terms data integration, data cleaning, and data preparation entail? To expand on the key parts of that, this is what we did to pour these aspects into RTDIP components.</p>"},{"location":"blog/2025/02/05/enhancing-data-quality-in-real-time-our-experience-with-rtdip-and-the-amos-project/#data-validation-and-schema-alignment","title":"Data Validation and Schema Alignment","text":"<p>Data validation and schema alignment are critical for ensuring the reliability and usability of data, serving as a foundational step before implementing other quality measures. For the time series data at hand, we developed an InputValidator component to verify that incoming data adheres to predefined quality standards, including compliance with an expected schema, correct PySpark data types, and proper handling of null values, raising exceptions when inconsistencies are detected. Additionally, the component enforces schema integration, harmonizing data from multiple sources into a unified, predefined structure. To maintain a consistent and efficient workflow, we required all data quality components to inherit the validation functionality of the InputValidator.</p>"},{"location":"blog/2025/02/05/enhancing-data-quality-in-real-time-our-experience-with-rtdip-and-the-amos-project/#data-cleansing","title":"Data Cleansing","text":"<p>Data cleansing is a vital process in enhancing the quality of data within a data integration pipeline, ensuring consistency, reliability, and usability. We implemented functionalities such as duplicate detection, which identifies and removes redundant records to prevent skewed analysis, and flatline filters, which eliminate constant, non-informative data points. Interval and range filters are employed to validate the time series data against predefined temporal or value ranges, ensuring conformity with expected patterns. Additionally, a K-sigma anomaly detection component identifies outliers based on statistical deviations, enabling the isolation of erroneous or anomalous values. Together, these methods ensure the pipeline delivers high-quality, actionable data for downstream processes.</p>"},{"location":"blog/2025/02/05/enhancing-data-quality-in-real-time-our-experience-with-rtdip-and-the-amos-project/#missing-value-imputation","title":"Missing Value Imputation","text":"<p>With a dataset refined to exclude unwanted data points and accounting for potential sensor failures, the next step toward ensuring high-quality data is to address any missing values through imputation. The component we developed first identifies and flags missing values by leveraging PySpark\u2019s capabilities in windowing and UDF operations. With these techniques, we are able to dynamically determine the expected interval for each sensor by analyzing historical data patterns within defined partitions. Spline interpolation allows us to estimate missing values in time series data, seamlessly filling gaps with plausible and mathematically derived substitutes. By doing so, data scientists can not only improve the consistency of integrated datasets but also prevent errors or biases in analytics and machine learning models. To actually show how this is realized with this new RTDIP component, let me show you a short example on how a few lines of code can enhance an exemplary time series load profile: <pre><code>from rtdip_sdk.pipelines.data_quality import MissingValueImputation\nfrom pyspark.sql import SparkSession\nimport pandas as pd\n\nspark_session = SparkSession.builder.master(\"local[2]\").appName(\"test\").getOrCreate()\n\nsource_df = pd.read_csv('./solar_energy_production_germany_April02.csv')\nincomplete_spark_df = spark_session.createDataFrame(vi_april_df, ['Value', 'EventTime', 'TagName', 'Status'])\n\n#Before Missing Value Imputation\nspark_df.show()\n\n#Execute RTDIP Pipeline component\nclean_df = MissingValueImputation(spark_session, df=incomplete_spark_df).filter_data()\n\n#After Missing Value Imputation\nclean_df.show()\n</code></pre> To illustrate this visually, plotting the before-and-after DataFrames reveals that all gaps have been successfully filled with meaningful data.</p>   ![blog](../images/amos_mvi_raw.png){width=70%}  ![blog](../images/amos_mvi.png){width=70%}"},{"location":"blog/2025/02/05/enhancing-data-quality-in-real-time-our-experience-with-rtdip-and-the-amos-project/#normalization","title":"Normalization","text":"<p>Normalization is a critical step in ensuring data quality within data integration pipelines with various sources. Techniques like mean normalization, min-max scaling, and z-score standardization help transform raw time series data into a consistent scale, eliminating biases caused by differing units or magnitudes across features. It enables fair comparisons between variables, accelerates algorithm convergence, and ensures that data from diverse sources aligns seamlessly, supporting possible downstream processes such as entity resolution, data augmentation, and machine learning. To offer a variety of use cases within the RTDIP pipeline, we implemented normalization techniques like mean normalization, min-max scaling, and z-score standardization as well as their respective denormalization methods.</p>"},{"location":"blog/2025/02/05/enhancing-data-quality-in-real-time-our-experience-with-rtdip-and-the-amos-project/#data-monitoring","title":"Data Monitoring","text":"<p>Data monitoring is another aspect of enhancing data quality within the RTDIP pipeline, ensuring the reliability and consistency of incoming data streams. Techniques such as flatline detection identify periods of unchanging values, which may indicate sensor malfunctions or stale data. Missing data identification leverages predefined intervals or historical patterns to detect and flag gaps, enabling proactive resolution. By continuously monitoring for these anomalies, the pipeline maintains high data integrity, supporting accurate analysis for inconsistencies.</p>"},{"location":"blog/2025/02/05/enhancing-data-quality-in-real-time-our-experience-with-rtdip-and-the-amos-project/#data-prediction","title":"Data Prediction","text":"<p>Forecasting based on historical data patterns is essential for making informed decisions on a business level. Linear Regression is a simple yet powerful approach for predicting continuous outcomes by establishing a relationship between input features and the target variable. However, for time series data, the ARIMA (Autoregressive Integrated Moving Average) model is often preferred due to its ability to model temporal dependencies and trends in the data. The ARIMA model combines autoregressive (AR) and moving average (MA) components, along with differencing to stabilize the variance and trends in the time series. ARIMA with autonomous parameter selection takes this a step further by automatically optimizing the model\u2019s parameters (p, d, q) using techniques like grid search or other statistical criteria, ensuring that the model is well-suited to the data\u2019s underlying structure for more accurate predictions. To address this, we incorporated both an ARIMA component and an AUTO-ARIMA component, enabling the prediction of future time series data points for each sensor.</p> <p></p> <p>Working on the RTDIP Project within AMOS has been a fantastic journey, highlighting the importance of people and teamwork in agile development. By focusing on enhancing data quality, we\u2019ve significantly boosted the reliability, consistency, and usability of the data going through the RTDIP pipeline.</p> <p>To look back, our regular team meetings were the key to our success. Through open communication and collaboration, we tackled challenges and kept improving our processes. This showed us the power of working together in an agile framework and growing as a dedicated SCRUM team.</p> <p>We\u2019re excited about the future and how these advancements will help data scientists and engineers make better decisions.</p> <p></p> <p>1 Designed by Freepik 2 Michael Stonebraker, Ihab F. Ilyas: Data Integration: The Current Status and the Way Forward. IEEE Data Eng. Bull. 41(2) (2018)</p>"},{"location":"blog/2023/11/27/beyond-the-traditional-data-historian/","title":"Beyond the Traditional Data Historian","text":"<p>The Fourth Industrial Revolution, commonly known as IR4.0, has ushered in profound transformations in the landscape of industrial operations and manufacturing. Digital technologies and data derived from sensors are driving a wide array of innovations, spanning from advanced analytics and machine learning to the realms of augmented and virtual reality models.</p> <p>Handling sensor-based data poses a unique challenge for conventional relational databases, which is why data historians were originally conceived in the latter part of the 1980s. They were specifically designed for integration with industrial automation systems like SCADA (supervisory control and data acquisition). Initially, their primary application was within the process manufacturing sector, encompassing industries such as oil and gas, chemicals, pharmaceuticals, pipelines, and refining.</p> <p>This Historian system was developed as an ecosystem that provided a comprehensive solution, ranging from data interface software to data storage and data visualization. The industry 4.0 revolution has spurred automation in manufacturing, leveraging smart sensors and IoT devices to capture real-time data from the field. Furthermore, it has seen the increased utilization of Artificial Intelligence and Machine Learning for predictive analytics and decision support, both of which are data-hungry applications. So, with IR4.0 development what are the options available to cater requirements for real time data ingestion.</p>"},{"location":"blog/2023/11/27/beyond-the-traditional-data-historian/#what-are-the-alternatives","title":"What Are the Alternatives?","text":"<p>Hyperscalers have become a central part of global IT infrastructure and essential to operations and they also started providing industrial connectivity options out of many in this article I am going to focus on Azure , OPC UA &amp; Open source stack which provides you all the options of data ingestion from OPC UA source to your cloud database.</p> <p> </p>"},{"location":"blog/2023/11/27/beyond-the-traditional-data-historian/#components","title":"Components","text":""},{"location":"blog/2023/11/27/beyond-the-traditional-data-historian/#microsoft-opc-publisher-as-interface","title":"Microsoft OPC Publisher as Interface","text":"<p>OPC Publisher is a Microsoft-endorsed solution that acts as a vital link between industrial equipment and the Azure cloud infrastructure. Its core function involves linking OPC UA-enabled devices or industrial connectivity software to your Azure cloud environment, transmitting collected telemetry data to Azure IoT Hub in multiple formats, one of which is the standardized IEC62541 OPC UA PubSub format (from version 2.6 onwards). </p> <p>OPC Publisher is versatile in its deployment options, running either as an Azure IoT Edge Module or a Docker container on various platforms, thanks to its compatibility with the .NET cross-platform runtime. Notably, it can seamlessly operate on both Linux and Windows 10 systems.</p> <p>Refer to Microsoft OPC Publisher documentation for more information.</p>"},{"location":"blog/2023/11/27/beyond-the-traditional-data-historian/#rtdip-for-data-engineering","title":"RTDIP for Data Engineering","text":"<p>The Real-Time Data Ingestion Platform (RTDIP) is designed to offer convenient access to large volumes of historical and live operational data for analytical purposes, serving engineers and data scientists no matter where they are located.</p>"},{"location":"blog/2023/11/27/beyond-the-traditional-data-historian/#key-components-of-rtdip","title":"Key Components of RTDIP:","text":""},{"location":"blog/2023/11/27/beyond-the-traditional-data-historian/#delta-ingestion-engine","title":"Delta Ingestion Engine:","text":"<p>This component operates in the cloud and is responsible for processing time series data sourced from various streaming endpoints such as Eventhub, Kafka, and others, as well as data from files. It ingests and organizes this data into a Delta Lakehouse.</p> <p>To simplify the data ingestion from IoT hub, RTDIP has built Transformer components to convert the payload's defaultmessage encoding to json. The architecture above is using the transformer  OPCPublisherOPCUAJsonToPCDMTransformer to handle that scenario.</p>"},{"location":"blog/2023/11/27/beyond-the-traditional-data-historian/#query-engine","title":"Query Engine","text":"<p>Python SDK: The Python Software Development Kit (SDK) empowers users to interact with the data stored within the Delta Lakehouse. It provides a programmatic interface for working with the data.</p>"},{"location":"blog/2023/11/27/beyond-the-traditional-data-historian/#rest-apis","title":"REST APIs","text":"<p>In addition to the Python SDK, RTDIP offers RESTful Application Programming Interfaces (APIs) that deliver the same capabilities as the Python SDK. These APIs enable users to interact  and manipulate the data stored in the Delta Lakehouse through HTTP requests, making it accessible for a wide range of applications and services.</p> <p>See Real Time Data Ingestion Platform for more information.</p>"},{"location":"blog/2023/11/27/beyond-the-traditional-data-historian/#visualization","title":"Visualization","text":"<p>Real-time data visualization is a dynamic and essential component of data analytics and monitoring processes. It involves the presentation of data as it is generated or updated in real-time, allowing users to gain instant insights and make informed decisions. But for visualization there is always different requirement, in current open source scenario we have different options available which can be used as required:</p> <ul> <li>Plotly</li> <li>Dash</li> <li>Grafana</li> </ul>"},{"location":"blog/2023/11/27/beyond-the-traditional-data-historian/#conclusion","title":"Conclusion","text":"<p>To conclude this article, it is important to emphasize the wide array of open-source options at the disposal of historians. In today\u2019s market, numerous open-source components can be harnessed to create bespoke solutions that seamlessly link with your process control systems, enabling the integration of real-time data into modern data platforms. While this article focused on Azure as an illustration, it\u2019s worth noting that AWS offers similar capabilities, providing flexibility and adaptability to cater to your specific needs.</p>"},{"location":"blog/2024/06/24/ensuring-data-quality-at-speed-with-real-time-data/","title":"Ensuring Data Quality at Speed with Real Time Data","text":"![DataQualityImage](../images/data-quality.png){width=75%}   <p>High quality data plays a pivotal role in business success across various dimensions. Accurate and reliable data empowers business leaders to make well informed decisions and achieve operational efficiency, promoting growth and profitability. Data quality encompasses more than just accuracy it also includes completeness, consistency, and relevance. </p> <p>Maintaining consistent data quality becomes challenging without a robust data governance framework. Organizations often lack comprehensive data quality assessment procedures, so it\u2019s crucial to regularly evaluate data quality using metrics and automated checks. Integrating data from various sources can introduce inconsistencies, but implementing data integration best practices ensures seamless data flow. Manual data entry is prone to errors, so automation reduces reliance on manual input. To measure data quality, define clear metrics such as accuracy and completeness, and track them consistently. Additionally, automate data cleansing routines (e.g., deduplication, validation) to streamline processes and reduce manual effort. Lastly, use of automation  can help to identify incomplete or outdated records and regularly update data sources while retiring obsolete information.</p> <p>Maintaining data quality with time series data presents unique challenges. First, the high volume and velocity of incoming data makes real-time validation and processing difficult. Second, time series data often exhibits temporal dependencies, irregular sampling intervals, and missing values, requiring specialized handling. Lastly, dynamic data distribution due to seasonality, trends, or sudden events poses an ongoing challenge for adapting data quality checks. Ensuring data quality in time series streaming demands agility, adaptability and automation.</p>"},{"location":"blog/2024/06/24/ensuring-data-quality-at-speed-with-real-time-data/#data-quality-best-practices","title":"Data Quality Best Practices","text":""},{"location":"blog/2024/06/24/ensuring-data-quality-at-speed-with-real-time-data/#data-validation-at-ingestion","title":"Data Validation at Ingestion","text":"<p>Implementing data validation checks when data enters a pipeline before any transformation can prevent issues from becoming lost and hard to track. It is possible to set this with automated scripts that can validate incoming data against predefined rules, for example, it is possible to check for duplication, outliers, missing values, inconsistent data types and much more. </p>"},{"location":"blog/2024/06/24/ensuring-data-quality-at-speed-with-real-time-data/#continuous-monitoring","title":"Continuous Monitoring","text":"<p>Monitoring of data quality can support the data validation and cleaning allowing the support team or developer to be notified of detected inconsistencies in the data. Early detection and alerting allow for quick action and prompt investigation which will prevent data quality degradation.</p>"},{"location":"blog/2024/06/24/ensuring-data-quality-at-speed-with-real-time-data/#data-cleansing-and-preparation","title":"Data Cleansing and Preparation","text":"<p>Automating data cleansing can be run as both a routine job and as a job triggered by failed data validation. Cleansing routines automatically correct or remove erroneous data, ensuring the dataset remains accurate and reliable.</p>"},{"location":"blog/2024/06/24/ensuring-data-quality-at-speed-with-real-time-data/#data-profiling","title":"Data Profiling","text":"<p>Automated profiling tools can analyse data distributions, patterns, and correlations. By identifying these potential issues such as skewed distributions or duplicate records, businesses can proactively address them in their data validation and data cleansing processes.</p>"},{"location":"blog/2024/06/24/ensuring-data-quality-at-speed-with-real-time-data/#data-governance","title":"Data Governance","text":"<p>Data governance polices provide a clear framework to follow when ensuring data quality across a business. Managing access controls, data retention, and compliance, maintaining data quality and security.</p>"},{"location":"blog/2024/06/24/ensuring-data-quality-at-speed-with-real-time-data/#rtdip-and-data-quality","title":"RTDIP and Data Quality","text":"<p>RTDIP now includes data quality scripts that support the end user in developing strong data quality pass gates for their datasets. The RTDIP component has been built using the open source tool Great Exceptions which is a Python-based open source library for validating, documenting, and profiling your data. It helps you to maintain data quality and improve communication about data between teams.</p> <p>RTDIP believes that data quality should be considered an integral part of any data pipeline, more information about RTDIPs data quality components can be found at Examine Data Quality with Great Expectations.</p>"},{"location":"blog/2024/06/24/ensuring-data-quality-at-speed-with-real-time-data/#open-source-tools-and-data-quality","title":"Open Source Tools and Data Quality","text":"<p>RTDIP empowers energy professionals to share solutions, RTDIP welcomes contributions and recognises the importance of sharing code. There are also a number of great open source data quality tools which have gained notoriety due to their transparency, adaptability, and community driven enhancements.</p> <p>Choosing the right tool depends on your specific requirements and architecture. Some notable open open source data quality tools include:</p> <ul> <li>Built on Spark, Deequ is excellent for testing large datasets. It allows you to validate data using constraint suggestions and verification suites. </li> <li>dbt Core is a data pipeline development platform. Its automated testing features include data quality checks and validations. </li> <li>MobyDQ offers data profiling, monitoring, and validation. It helps maintain data quality by identifying issues and inconsistencies.</li> <li>Soda Core focuses on data monitoring and anomaly detection allowing the business to the track data quality over time and alerting.</li> </ul>"},{"location":"blog/2024/06/24/ensuring-data-quality-at-speed-with-real-time-data/#contribute","title":"Contribute","text":"<p>RTDIP empowers energy professionals to share solutions, RTDIP welcomes contributions and recognises the importance of sharing code. If you would like to contribute to RTDIP please follow our Contributing guide.</p>"},{"location":"blog/2024/04/08/energy-forecasting-utilising-the-power-of-tomorrows-data/","title":"Energy Forecasting: Utilising the Power of Tomorrow\u2019s Data","text":"![EnergyForecastingImage](../images/energy-forecasting.png){width=75%}   <p>Energy forecasting plays a pivotal role in our modern world, where energy consumption, production and pricing are critical factors. </p> <p>Energy forecasting involves predicting the demand load and price of various energy sources, including both fossil fuels and renewable energy resources like wind and solar.</p> <p>With an accurate energy usage forecast, a business can efficiently allocate and manage resources, this is crucial to maintain a stable energy supply to the consumer; energy forecasting is fundamental as we transition to renewable energy sources which do not produce consistent energy. Energy companies, grid operators and industrial consumers rely on forecasts to optimise their operations. Over- or undercontracting can lead to significant financial losses, so precise forecasts are essential.</p> <p>Energy load prices and forecasts greatly influence the energy sector and the decisions made across multiple departments in energy companies.  For example, medium to long-term energy forecasts are vital for planning and investing in new capacity, they guide decisions on new assets,  transmission lines and distribution networks. Another example is risk mitigation, unstable electricity prices can be handled with accurate forecasting of the market, companies can develop bidding strategies, production schedules and consumption patterns to minimize risk and maximize profits.</p> <p>Energy forecasting is focused on performance, i.e. how much over or under a forecast is and performance during extreme weather days. Quantifying a financial impact relative to market conditions can be diffcult. However, a rough estimate of savings from a 1% reduction in the mean absolute percentage error (MAPE) for a utility with a 1 GW peak load includes: </p> <ul> <li>$500,000 per year from long-term load forecasting</li> <li>$300,000 per year from short-term load forecasting</li> <li>$600,000 per year from short-term load and price forecasting</li> </ul> <p>Energy Forecasting allows for significant cost avoidance due to better price forecasts and risk management.</p>"},{"location":"blog/2024/04/08/energy-forecasting-utilising-the-power-of-tomorrows-data/#energy-forecasting-with-rtdip","title":"Energy Forecasting with RTDIP","text":"<p>RTDIP can be a powerful tool for businesses looking to forecast energy usage. RTDIP supports load forecasting applications, a critical technique used by RTOs (Regional Transmission Organisations)/TSO (Transmission System Operators), ISOs (Independent System Operators) and energy providers. Load forecasting allows a business to predict the power or energy needed to maintain the balance between energy demand and supply on the grid. Two primary inputs for load forecasting are weather data and meter data, RTDIP has developed pipeline components for these types of data.</p> <p>RTDIP provides example pipelines for weather forecast data ingestion. Accurate weather data helps predict energy production in renewable assets based on factors like temperature, humidity and wind patterns.</p> <p>RTDIP defines example pipelines for meter data from ISOs like MISO (Midcontinent ISO) and PJM (Pennsylvania-New Jersey-Maryland Interconnection). Meter data can include consumption patterns, load profiles and real-time measurements. The sources and transformers in RTDIP can aquire and translate meter data into suitable data models for efficient storage and analysis.</p> <p>The data models in RTDIP are aligned to the IEC CIM (Common Information Model) for time series and metering data. This aids compatibility with systems requiring data aligned with the IEC CIM standard.</p>"},{"location":"blog/2024/04/08/energy-forecasting-utilising-the-power-of-tomorrows-data/#building-pipelines-for-energy-forecasting","title":"Building Pipelines for Energy Forecasting","text":"<p>RTDIP allows you to develop and deploy cloud agnostic pipelines with popular orchestration engines. There are a number of RTDIP components focused on weather and metering data, these are all listed below.</p>"},{"location":"blog/2024/04/08/energy-forecasting-utilising-the-power-of-tomorrows-data/#sources","title":"Sources","text":"<p>Data aquistion via source connectors</p> <p>MISO Daily Load ISO</p> <p>MISO Historical Load ISO</p> <p>PJM Daily Load ISO </p> <p>PJM Historical Load ISO</p> <p>CAISO Daily Load ISO</p> <p>CAISO Historical Load ISO</p> <p>ERCOT Daily Load ISO</p> <p>Weather Forecast API V1 </p> <p>Weather Forecast API V1 Multi </p> <p>ECMWF MARS Weather Forecast</p>"},{"location":"blog/2024/04/08/energy-forecasting-utilising-the-power-of-tomorrows-data/#transformers","title":"Transformers","text":"<p>Data conversion into 'Meters Data Model' via transformers</p> <p>MISO To Meters Data Model</p> <p>PJM To Meters Data Model</p> <p>CAISO To Meters Data Model</p> <p>ERCOT To Meters Data Model</p> <p>Raw Forecast to Weather Data Mode</p> <p>ECMWF NC Forecast Extract Point To Weather Data Model</p> <p>ECMWF NC Forecast Extract Grid To Weather Data Model</p>"},{"location":"blog/2024/04/08/energy-forecasting-utilising-the-power-of-tomorrows-data/#contribute","title":"Contribute","text":"<p>RTDIP empowers energy professionals to share solutions, RTDIP welcomes contributions and recognises the importance of sharing code. There are multiple sources for weather and metering data crucial to forecasting energy needs, if you have anymore you\u2019d like to add to RTDIP please follow our Contributing guide.</p>"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/","title":"Rtdip ingestion pipelines","text":"# RTDIP Ingestion Pipeline Framework  ![blog](../images/framework.png){width=60%}  <p>RTDIP has been built to simplify ingesting and querying time series data. One of the most anticipated features of the Real Time Data Ingestion Platform for 2023 is the ability to create streaming and batch ingestion pipelines according to requirements of the source of the data and needs of the data consumer. Of equal importance is the need to query this data and an article that focuses on egress will follow in due course. </p>"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#overview","title":"Overview","text":"<p>The goal of the RTDIP Ingestion Pipeline framework is:</p> <ol> <li>Support python and pyspark to build pipeline components</li> <li>Enable execution of sources, transformers, sinks/destinations and utilities components in a framework that can execute them in a defined order</li> <li>Create modular components that can be leveraged as a step in a pipeline task using Object Oriented Programming techniques included Interfaces and Implementations per component type</li> <li>Deploy pipelines to popular orchestration engines</li> <li>Ensure pipelines can be constructed and executed using the RTDIP SDK and rest APIs</li> </ol>"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#pipeline-jobs","title":"Pipeline Jobs","text":"<p>The RTDIP Data Ingestion Pipeline Framework will follow the typical convention of a job that users will be familiar with if they have used orchestration engines such as Apache Airflow or Databricks Workflows.</p> <p>A pipeline job consists of the following components:</p> <pre><code>erDiagram\n  JOB ||--|{ TASK : contains\n  TASK ||--|{ STEP : contains\n  JOB {\n    string name\n    string description\n    list task_list\n  }\n  TASK {\n    string name\n    string description\n    string depends_on_task\n    list step_list\n    bool batch_task\n  }\n  STEP {\n    string name\n    string description\n    list depends_on_step\n    list provides_output_to_step\n    class component\n    dict component_parameters\n  }</code></pre> <p>As per the above, a pipeline job consists of a list of tasks. Each task consists of a list of steps. Each step consists of a component and a set of parameters that are passed to the component. Dependency Injection will ensure that each component is instantiated with the correct parameters.</p>"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#pipeline-runtime-environments","title":"Pipeline Runtime Environments","text":"Python Apache Spark Databricks Delta Live Tables <p>Pipelines will be able to run in multiple environment types. These will include:</p> <ul> <li>Python: Components will be written in python and executed on a python runtime</li> <li>Pyspark: Components will be written in pyspark and executed on an open source Apache Spark runtime</li> <li>Databricks: Components will be written in pyspark and executed on a Databricks runtime</li> <li>Delta Live Tables: Components will be written in pyspark and executed on a Databricks runtime and will write to Delta Live Tables</li> </ul> <p>Runtimes will take precedence depending on the list of components in a pipeline task.</p> <ul> <li>Pipelines with at least one Databricks or DLT component will be executed in a Databricks environment</li> <li>Pipelines with at least one Pyspark component will be executed in a Pyspark environment</li> <li>Pipelines with only Python components will be executed in a Python environment</li> </ul>"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#pipeline-clouds","title":"Pipeline Clouds","text":"<p>Certain components are related to cloud providers and in the tables below, it is indicated which cloud provider is related to its specific component. It does not mean that the component can only run in that cloud, instead its highlighting that the component is related to that cloud provider.</p> Cloud Target Azure Q1-Q2 2023 AWS Q2-Q4 2023 GCP 2024"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#pipeline-orchestration","title":"Pipeline Orchestration","text":"Airflow Databricks Dagster <p>Pipelines will be able to be deployed to orchestration engines so that users can schedule and execute jobs using their preferred orchestration engine.</p> Orchestration Engine Target Databricks Workflows Q2 2023 Airflow Q2 2023 Delta Live Tables Q3 2023 Dagster Q4 2023"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#pipeline-components","title":"Pipeline Components","text":"<p>The Real Time Data Ingestion Pipeline Framework will support the following components:</p> <ul> <li>Sources - connectors to source systems</li> <li>Transformers - perform transformations on data, including data cleansing, data enrichment, data aggregation, data masking, data encryption, data decryption, data validation, data conversion, data normalization, data de-normalization, data partitioning etc</li> <li>Destinations - connectors to sink/destination systems </li> <li>Utilities - components that perform utility functions such as logging, error handling, data object creation, authentication, maintenance etc</li> <li>Edge - components that will perform edge functionality such as connectors to protocols like OPC</li> </ul>"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#pipeline-component-types","title":"Pipeline Component Types","text":"Python Apache Spark Databricks <p>Component Types determine system requirements to execute the component:</p> <ul> <li>Python - components that are written in python and can be executed on a python runtime</li> <li>Pyspark - components that are written in pyspark can be executed on an open source Apache Spark runtime</li> <li>Databricks - components that require a Databricks runtime</li> </ul>"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#sources","title":"Sources","text":"<p>Sources are components that connect to source systems and extract data from them. These will typically be real time data sources, but will also support batch components as these are still important and necessary data souces in a number of circumstances in the real world.</p> Source Type Python Apache Spark Databricks Azure AWS Target Delta * Q1 2023 Delta Sharing * Q1 2023 Autoloader Q1 2023 Eventhub * Q1 2023 IoT Hub * Q2 2023 Kafka Q2 2023 Kinesis Q2 2023 SSIP PI Connector Q2 2023 Rest API Q2 2023 MongoDB Q3 2023 <p>* - target to deliver in the following quarter</p> <p>There is currently no spark connector for IoT Core. If you know a way to add it as a source component, please raise it by creating an issue on the GitHub repo.</p>"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#transformers","title":"Transformers","text":"<p>Transformers are components that perform transformations on data. These will target certain data models and common transformations that sources or destination components require to be performed on data before it can be ingested or consumed.</p> Transformer Type Python Apache Spark Databricks Azure AWS Target Eventhub Body Q1 2023 OPC UA Q2 2023 OPC AE Q2 2023 SSIP PI Q2 2023 OPC DA Q3 2023 <p>* - target to deliver in the following quarter</p> <p>This list will dynamically change as the framework is developed and new components are added.</p>"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#destinations","title":"Destinations","text":"<p>Destinations are components that connect to sink/destination systems and write data to them. </p> Destination Type Python Apache Spark Databricks Azure AWS Target Delta Append * Q1 2023 Eventhub * Q1 2023 Delta Merge Q2 2023 Kafka Q2 2023 Kinesis Q2 2023 Rest API Q2 2023 MongoDB Q3 2023 Polygon Blockchain Q3 2023 <p>* - target to deliver in the following quarter</p>"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#utilities","title":"Utilities","text":"<p>Utilities are components that perform utility functions such as logging, error handling, data object creation, maintenance and are normally components that can be executed as part of a pipeline or standalone.</p> Utility Type Python Apache Spark Databricks Azure AWS Target Delta Table Create * Q1 2023 Delta Optimize Q2 2023 Delta Vacuum * Q2 2023 Set ADLS Gen2 ACLs Q2 2023 Set S3 ACLs Q2 2023 Great Expectations Q3 2023 <p>* - target to deliver in the following quarter</p>"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#secrets","title":"Secrets","text":"<p>Secrets are components that perform authentication functions and are normally components that can be executed as part of a pipeline or standalone.</p> Secrets Type Python Apache Spark Databricks Azure AWS Target Databricks Secrets Q2 2023 Hashicorp Vault Q2 2023 Azure Key Vault Q3 2023 AWS Secrets Manager Q3 2023"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#edge","title":"Edge","text":"<p>Edge components are designed to provide a lightweight, low latency, low resource consumption, data ingestion framework for edge devices. These components will be designed to run on edge devices such as Raspberry Pi, Jetson Nano, etc. For cloud providers, this will be designed to run on AWS Greengrass and Azure IoT Edge.</p> Edge Type Azure IoT Edge AWS Greengrass Target OPC CloudPublisher Q3-Q4 2023 Fledge Q3-Q4 2023 Edge X Q3-Q4 2023"},{"location":"blog/2023/02/24/rtdip-ingestion-pipelines/#conclusion","title":"Conclusion","text":"<p>This is a very high level overview of the framework and the components that will be developed. As the framework is open source, the lists defined above and timelines can change depending on circumstances and resource availability. Its an exciting year for 2023 for the Real Time Data Ingestion Platform. Check back in regularly for updates and new features! If you would like to contribute, please visit our repository on Github and connect with us on our Slack channel on the LF Energy Foundation Slack workspace.</p>"},{"location":"domains/process_control/data_model/","title":"Process Control Data Model","text":"<p>The Process Control Data Model consists of two key components:</p> <ol> <li>Metadata about the sensor/object/measurement such as Description, Unit of Measure, Status and also provides metadata used in queries such as Step logic used in interpolation.</li> <li>Events contains transactional data and is simply capturing the name of the sensor/object/measurement, the timestamp of the event, the status of the event recording and the value.</li> </ol>"},{"location":"domains/process_control/data_model/#data-model","title":"Data Model","text":"<pre><code>erDiagram\n  METADATA ||--o{ EVENTS : contains\n  METADATA {\n    string TagName PK\n    string Description\n    string UoM\n    string DataType\n    boolean Step\n    string Status\n    dict Properties \"Key Value pairs of varying metadata\"\n  }\n  EVENTS {\n    string TagName PK\n    timestamp EventTime PK\n    string Status\n    dynamic Value \"Value can be of different Data Types\"\n  }</code></pre>"},{"location":"domains/process_control/data_model/#references","title":"References","text":"Reference Description IEC 61850 Relevant description to IEC 61850 IEC CIM Relevant description to IEC CIM"},{"location":"domains/process_control/data_model/#mappings","title":"Mappings","text":""},{"location":"domains/process_control/data_model/#fledge-opc-ua-south-plugin","title":"Fledge OPC UA South Plugin","text":"<p>Fledge provides support for sending data between various data sources and data destinations. The mapping below is for the OPC UA South Plugin that can be sent to message brokers like Kafka, Azure IoT Hub etc.</p> <p>This mapping is performed by the RTDIP Fledge to PCDM Component and can be used in an RTDIP Ingestion Pipeline.</p> From Data Model From Field From Type To Data Model To Field To Type Mapping Logic Fledge OPC UA Object ID string EVENTS TagName string Fledge OPC UA EventTime string EVENTS EventTime timestamp Converted to a timestamp EVENTS Status string Can be defaulted in RTDIP Fledge to PCDM Component otherwise Null Fledge OPC UA Value string EVENTS Value dynamic Converts Value into either a float number or string based on how it is received in the message"},{"location":"domains/process_control/data_model/#opc-publisher","title":"OPC Publisher","text":"<p>OPC Publisher connects to OPC UA assets and publishes data to the Microsoft Azure Cloud's IoT Hub.</p> <p>The mapping below is performed by the RTDIP OPC Publisher to PCDM Component and can be used in an RTDIP Ingestion Pipeline.</p> From Data Model From Field From Type To Data Model To Field To Type Mapping Logic OPC Publisher DisplayName string EVENTS TagName string From Field can be specified in Component OPC Publisher SourceTimestamp string EVENTS EventTime timestamp Converted to a timestamp OPC Publisher StatusCode.Symbol string EVENTS Status string Null values can be overridden in the RTDIP OPC Publisher to PCDM Component OPC Publisher Value.Value string EVENTS Value dynamic Converts Value into either a float number or string based on how it is received in the message"},{"location":"domains/process_control/data_model/#edgex","title":"EdgeX","text":"<p>EdgeX provides support for sending data between various data sources and data destinations. </p> <p>This mapping is performed by the RTDIP EdgeX to PCDM Component and can be used in an RTDIP Ingestion Pipeline.</p> From Data Model From Field From Type To Data Model To Field To Type Mapping Logic EdgeX deviceName string EVENTS TagName string EdgeX origin string EVENTS EventTime timestamp Converted to a timestamp EVENTS Status string Can be defaulted in RTDIP EdgeX to PCDM Component otherwise Null EdgeX value string EVENTS Value dynamic Converts Value into either a float number or string based on how it is received in the message"},{"location":"domains/process_control/data_model/#ssip-pi","title":"SSIP PI","text":"<p>SSIP PI connects to Osisoft PI Historians and sends the data to the Cloud.</p> <p>The mapping below is performed by the RTDIP SSIP PI to PCDM Component and can be used in an RTDIP Ingestion Pipeline.</p> From Data Model From Field From Type To Data Model To Field To Type Mapping Logic SSIP PI TagName string EVENTS TagName string SSIP PI EventTime string EVENTS EventTime timestamp SSIP PI Status string EVENTS Status string SSIP PI Value dynamic EVENTS Value dynamic"},{"location":"domains/process_control/overview/","title":"Process Control Domain Overview","text":"<p>For process control systems, RTDIP provides the ability to consume data from these sources, transform it and store the data in an open source format to enable:</p> <ul> <li>Data Science, ML and AI applications to consume the data</li> <li>Real time data in Digital Twins</li> <li>BI and Analytics </li> <li>Reporting</li> </ul>"},{"location":"domains/process_control/overview/#process-control-systems","title":"Process Control Systems","text":"<p>Process control systems monitor, control and safeguard production operations and generate vast amounts of data. Typical industry use cases include:</p> <ul> <li>Electricity Generation, Transmission and Distribution</li> <li>Chemicals, Gas, Oil Production and Distribution</li> <li>LNG Processing and Product Refining </li> </ul> <p>Process control systems record variables such as temperature, pressure, flow etc and automatically make adjustments to maintain preset specifications in a technical process.</p> <p>This data can be made available to other systems over a number of protocols, such as OPC UA. The protocols in turn make the data available to connectors that can send the data onwards to other systems and the cloud.</p>"},{"location":"domains/process_control/overview/#architecture","title":"Architecture","text":"<pre><code>graph LR\n  A(Process Control) --&gt; B(OPC UA Server);\n  B --&gt; C(Connectors);\n  C --&gt; D(Message Broker);\n  D --&gt; E(RTDIP);\n  E --&gt; F[(Destinations)];</code></pre>"},{"location":"domains/process_control/overview/#connectors","title":"Connectors","text":"<p>A number of connectors are available from various suppliers. Some open source options include:</p> [![Fledge](https://www.lfedge.org/wp-content/uploads/2019/09/fledge-horizontal-color.svg){width=40%}](https://www.lfedge.org/projects/fledge/)   [![Edge X Foundry](https://github.com/lf-edge/artwork/blob/master/edgexfoundry/horizontal/color/edgexfoundry-horizontal-color.png?raw=true){width=50%}](https://www.lfedge.org/projects/edgexfoundry/)"},{"location":"domains/process_control/overview/#message-brokers","title":"Message Brokers","text":"<p>Message Brokers support publishing of data from connectors and subscribing(pub/sub) to data from consumers. Popular options used with RTDIP are:</p> [![Kafka](../process_control/images/kafka-logo-wide.png){width=40%}](https://kafka.apache.org/)  [![MQTT](../process_control/images/mqtt.png){width=40%}](https://mqtt.org/)  [![Azure IoT Hub](../process_control/images/iot_hub.png){width=40%}](https://azure.microsoft.com/en-us/products/iot-hub)"},{"location":"domains/process_control/overview/#real-time-data-ingestion-platform","title":"Real Time Data Ingestion Platform","text":"<p>For more information about the Real Time Data Platform and its components to connect to data sources and destinations, please refer to this link.</p>"},{"location":"domains/smart_meter/data_model/","title":"Meters Data Model","text":"<p>Base Raw To Meters Data Model: here </p> <ul> <li>ISO:<ul> <li>MISO To Meters Data Model: here</li> <li>PJM  To Meters Data Model: here</li> </ul> </li> </ul>"},{"location":"domains/smart_meter/overview/","title":"Smart Meter / Energy Domain","text":""},{"location":"domains/smart_meter/overview/#overview","title":"Overview","text":"<p>Meter data is central to accelerating the electrification and decarbonisation of the energy grid. RTDIP provides the ability to consume meter data from exemplar sources, transform it and store it in an appropriate open-source format to enable domain-specific energy services, such as:</p> <ul> <li>Energy Load Forecasting</li> <li>Energy Generation Forecasting</li> <li>Other behind-the-meter services and insights</li> </ul> <p>At a high level, the electricity system (US example) works as follows:</p> <ul> <li>Generators, of various types (coal, oil, natural gas, nuclear, wind turbines &amp; PV, etc.) produce electricity </li> <li>Utilities, distribute and transmit the electricity from the Generators through the grid to the \u00a0point of consumption i.e. buildings and homes</li> <li>Suppliers, wholesale purchase the electricity and sell it as retail contracts to Buyers</li> <li>Buyers, consume electricity, via buildings, homes, and electric vehicles, etc.</li> <li>Consultants, facilitate these transactions and/or offer data insights e.g. load forecasting to tailor purchasing, targeting reduced risk, profit, and competitive costs for Buyers</li> </ul> <p>An Independent System Operator (ISO) sometimes called the Regional Transmission Organisation (RTO) is an organisation that is in charge of the entire process. They coordinate, control, and monitor the electric grid in a specific region, typically a multi-state area.</p>"},{"location":"domains/smart_meter/overview/#meter-data-pipelines","title":"Meter Data Pipelines","text":"<p>Load forecasting is a technique used by ISO's, and energy-providing companies to predict the power/energy needed to meet the demand and supply equilibrium of the energy grid. RTDIP defines and provides example pipelines for the two primary inputs to energy services like load forecasting, namely weather and meter data.</p> <p>Specifically, with respect to meter data RTDIP defines and provides two exemplar ISO's:</p> <ul> <li>the Midcontinent Independent System Operator, MISO </li> <li>the PJM Interconnection LLC Independent System Operator, PJM</li> </ul>"},{"location":"domains/smart_meter/overview/#architecture","title":"Architecture","text":"<p>The overall ETL flow of the pipeline is outlined below:</p> <pre><code>graph LR\n  A(External Meter Source e.g. MISO, PJM) --&gt; B(RTDIP Source/Connector);\n  B --&gt; C(RTDIP Transformer);\n  C --&gt; D(RTDIP Source/Connector);\n  D --&gt; E[(RTDIP Destination)];</code></pre> <ol> <li>Source: The specific source/connector acquires data from a specific external endpoint (MISO or PJM) and persists the raw data into Deltalake </li> <li>Transformer: \u00a0An RTDIP transformer translates this raw data into a meter specific Delta schema. \u00a0</li> <li>Destination: Essentially the function of loading is abstracted from the user and is handled by Deltalake. </li> </ol> <p>Indicative schema are available here. </p>"},{"location":"domains/smart_meter/overview/#real-time-data-ingestion-platform","title":"Real Time Data Ingestion Platform","text":"<p>For more information about the Real Time Data Platform and its components to connect to data sources and destinations, please refer to this link.</p>"},{"location":"domains/weather/data_model/","title":"Weather Data Model","text":"<p>The weather data model describes available weather data schemas commonly used in weather services and available via RTDIP. </p>"},{"location":"domains/weather/data_model/#15-day-hourly-forecast","title":"15-day Hourly Forecast","text":"<p>This model returns weather forecasts starting with the current day. Forecast data is using TAF format, which is similar to METAR but contains extra forecast variables</p> Type Name Description String class Data identifier Integer clds Hourly average cloud cover expressed as a percentage. String day_ind This data field indicates whether it is daytime or nighttime based on the Local Apparent Time of the location. Integer dewpt The temperature which air must be cooled at constant pressure to reach saturation. The Dew Point is also an indirect measure of the humidity of the air. The Dew Point will never exceed the Temperature. When the Dew Point and Temperature are equal, clouds or fog will typically form. The closer the values of Temperature and Dew Point, the higher the relative humidity. String dow Day of week Float expire_time_gmt Expiration time in UNIX seconds Float fcst_valid Time forecast is valid in UNIX seconds String fcst_valid_local Time forecast is valid in local apparent time Integer feels_like Hourly feels like temperature. An apparent temperature. It represents what the air temperature \u201cfeels like\u201d on exposed human skin due to the combined effect of the wind chill or heat index. String golf_category The Golf Index Category expressed as a worded phrase the weather conditions for playing golf Integer golf_index The Golf Index expresses on a scale of 0 to 10 the weather conditions for playing golf Not applicable at night. 0-2=Very Poor, 3=Poor, 4-5=Fair, 6-7=Good, 8-9=Very Good, 10=Excellent Integer gust The maximum expected wind gust speed. Integer hi Hourly maximum heat index. An apparent temperature. It represents what the air temperature \u201cfeels like\u201d on exposed human skin due to the combined effect of warm temperatures and high humidity. When the temperature is 70\u00b0F or higher, the Feels Like value represents the computed Heat Index. For temperatures between 40\u00b0F and 70\u00b0F, the Feels Like value and Temperature are the same, regardless of wind speed and humidity, so use the Temperature value. Integer icon_code This number is the key to the weather icon lookup. The data field shows the icon number that is matched to represent the observed weather conditions. Please refer to the Forecast Icon Code, Weather Phrases and Images document. Integer icon_extd Code representing explicit full set sensible weather. Please refer to the Forecast Icon Code, Weather Phrases and Images document. Float mslp Hourly mean sea level pressure Integer num This data field is the sequential number that identifies each of the forecasted days in the API. They start on day 1, which is the forecast for the current day. Then the forecast for tomorrow uses number 2, then number 3 for the day after tomorrow, and so forth. String phrase_12char Hourly sensible weather phrase String phrase_22char Hourly sensible weather phrase String phrase_32char Hourly sensible weather phrase Integer pop Hourly maximum probability of precipitation String precip_type The short text describing the expected type accumulation associated with the Probability of Precipitation (POP) display for the hour. Float qpf The forecasted measurable precipitation (liquid or liquid equivalent) during the hour. Integer rh The relative humidity of the air, which is defined as the ratio of the amount of water vapor in the air to the amount of vapor required to bring the air to saturation at a constant temperature. Relative humidity is always expressed as a percentage. Integer severity A number denoting how impactful is the forecasted weather for this hour. Can be used to determine the graphical treatment of the weather display such as using red font on weather.com Float snow_qpf The forecasted hourly snow accumulation during the hour. String subphrase_pt1 Part 1 of 3-part hourly sensible weather phrase String subphrase_pt2 Part 2 of 3-part hourly sensible weather phrase String subphrase_pt3 Part 3 of 3-part hourly sensible weather phrase Integer temp The temperature of the air, measured by a thermometer 1.5 meters (4.5 feet) above the ground that is shaded from the other elements. You will receive this data field in Fahrenheit degrees or Celsius degrees. String uv_desc The UV Index Description which complements the UV Index value by providing an associated level of risk of skin damage due to exposure. Integer uv_index Hourly maximum UV index Decimal uv_index_raw The non-truncated UV Index which is the intensity of the solar radiation based on a number of factors. Integer uv_warning TWC-created UV warning based on UV index of 11 or greater. Decimal vis Prevailing hourly visibility Integer wc Hourly minimum wind chill. An apparent temperature. It represents what the air temperature \u201cfeels like\u201d on exposed human skin due to the combined effect of the cold temperatures and wind speed. When the temperature is 61\u00b0F or lower the Feels Like value represents the computed Wind Chill so display the Wind Chill value. For temperatures between 61\u00b0F and 75\u00b0F, the Feels Like value and Temperature are the same, regardless of wind speed and humidity, so display the Temperature value. Integer wdir Hourly average wind direction in magnetic notation. String wdir_cardinal Hourly average wind direction in cardinal notation. Integer wspd The maximum forecasted hourly wind speed. The wind is treated as a vector; hence, winds must have direction and magnitude (speed). The wind information reported in the hourly current conditions corresponds to a 10-minute average called the sustained wind speed. Sudden or brief variations in the wind speed are known as \u201cwind gusts\u201d and are reported in a separate data field. Wind directions are always expressed as \"from whence the wind blows\" meaning that a North wind blows from North to South. If you face North in a North wind the wind is at your face. Face southward and the North wind is at your back. String wxman Code combining Hourly sensible weather and temperature conditions"},{"location":"domains/weather/data_model/#references","title":"References","text":"Reference Description METAR Description to METAR TAF Description of TAF"},{"location":"domains/weather/overview/","title":"Weather Services","text":""},{"location":"domains/weather/overview/#overview","title":"Overview","text":"<p>Many organizations need weather data for day-to-day operations. RTDIP provides the ability to consume data from examplar weather sources, transform it and store the data in an appropriate open source format to enable generic functions such as:</p> <ul> <li>Data Science, ML and AI applications to consume the data</li> <li>BI and Analytics</li> <li>Reporting</li> </ul> <p>A primary aim for RTDIP in 2023 is to demonstrate how the platform can be utilised for domain specific services such as:</p> <ul> <li>Consumption Load Forecasting</li> <li>Energy Generation Forecasting</li> <li>Other behind the meter services and insights</li> </ul> <p>Weather data is a primary driver,  together with meter data, of variance in load &amp; generation forecasting in the energy domain. </p>"},{"location":"domains/weather/overview/#weather-data-in-the-energy-domain","title":"Weather Data in the Energy Domain","text":"<p>One of the most widely used weather data standards is the combined METAR (Meteorological Aerodrome Report) and ICAO (International Civil Aviation Organization) standard. This standard is used by meteorological agencies and aviation organizations around the world to report weather conditions at airports and other aviation facilities. This standard is broadly utilised beyond the aviation industry including the energy domain. </p> <p>The METAR ICAO standard includes a set of codes and abbreviations that describe weather conditions in a standardized format. These codes include information such as temperature, wind speed and direction, visibility, cloud cover, and precipitation. The standard also includes codes for reporting special weather phenomena, such as thunderstorms or volcanic ash.</p> <p>Many actors in the energy domain utilise Historical, Forecast and near real-time METAR data as part of their services. Such data can be used to calculate average weather data by date and interval spanning multiple years, eg Historical Weather Data is often used to calculate an average or typical value for each weather variable eg. temperature, humidity over a given timeframe, which can be used for long range forecasting etc. </p>"},{"location":"domains/weather/overview/#architecture","title":"Architecture","text":"<p>An exemplar pipeline is defined and provided within RTDIP. The overall approach and weather data in general is agnostic but the exemplar utilises a specific external source. The overall ETL flow of the pipeline is outlined below:</p> <pre><code>graph LR\n  A(External Weather Source) --&gt; B(RTDIP Source/Connector);\n  B --&gt; C(RTDIP Transformer);\n  C --&gt; D(RTDIP Source/Connector);\n  D --&gt; E[(RTDIP Destination)];</code></pre> <ol> <li>Source: The specific source/connector aquires data from a specific external endpoint and persists the raw data into Deltalake </li> <li>Tranformer:  An RTDIP transformer translates this raw data into a weather specific Delta schema.  </li> <li>Destination: Essentially the function of loading is abstracted from the user and is handled by Deltalake. </li> </ol> <p>An indicative schema is available here. </p>"},{"location":"domains/weather/overview/#real-time-data-ingestion-platform","title":"Real Time Data Ingestion Platform","text":"<p>For more information about the Real Time Data Platform and its components to connect to data sources and destinations, please refer to this link.</p>"},{"location":"getting-started/about-rtdip/","title":"About RTDIP","text":""},{"location":"getting-started/about-rtdip/#rtdip-and-lf-energy","title":"RTDIP and LF Energy","text":"<p>By providing frameworks and reference implementations, LF Energy minimizes pain points such as cybersecurity, interoperability, control, automation, virtualization, and the orchestration of supply and demand.</p> <p>RTDIP is an LF Energy project and forms part of an overall open source energy ecosystem. To find out more about projects in LF Energy, please click here.</p>"},{"location":"getting-started/about-rtdip/#what-is-real-time-data-ingestion-platform","title":"What is Real Time Data Ingestion Platform","text":"<p>Organizations need data for day-to-day operations and to support advanced Data Science, Statistical and Machine Learning capabilities such as Optimization, Surveillance, Forecasting, and Predictive Analytics. Real Time Data forms a major part of the total data utilized in these activities.</p> <p>Real time data enables organizations to detect and respond to changes in their systems thus improving the efficiency of their operations. This data needs to be available in scalable and secure data platforms. </p> <p>Real Time Data Ingestion Platform (RTDIP) is the solution of choice leveraging PaaS (Platform as a Service) services along with some custom components to provide Data Ingestion, Data Transformation, and Data Sharing as a platform. RTDIP can interface with several data sources to ingest many different data types which include time series, alarms, video, photos and audio being provided from sources such as Historians, OPC Servers and Rest APIs, as well as data being sent from hardware such as IoT Sensors, Robots and Drones.</p>"},{"location":"getting-started/about-rtdip/#why-real-time-data-ingestion-platform","title":"\u200bWhy Real Time Data Ingestion Platform?","text":"<p>Real Time Data Ingestion Platform (RTDIP) enables users to consume Real Time Data at scale, including historical and real time streaming data. RTDIP has proven to be capable of ingesting over 3 million sensors in a production environment across every geographical location in the world.</p> <p>The Real Time Data Ingestion Platform can be run in a customers own environment, allowing them to accelerate their cloud deployments while leveraging a proven design to scale their time series data needs. </p> <p>RTDIP also provides a number popular integration options, including:</p> <ol> <li>ODBC</li> <li>JDBC</li> <li>Rest API</li> <li>Python SDK</li> </ol> <p>These options allow users to integrate with a wide variety of applications and tools, including:</p> <ol> <li>Data Visualization Tools such as Power BI, Seeq, Tableau, and Grafana</li> <li>Data Science Tools such as Jupyter Notebooks, R Studio, and Python</li> <li>Data Engineering Tools such as Apache Spark, Apache Kafka, and Apache Airflow</li> </ol> <p>RTDIP is architected to leverage Open Source technologies Apache Spark and Delta. This allows users to leverage the power of Open Source technologies to build their own custom applications and tools in whichever environment they prefer, whether that is in the cloud or on-premise on their own managed Spark Clusters.</p>"},{"location":"getting-started/installation/","title":"Getting started with RTDIP","text":"<p>RTDIP provides functionality to process and query real time data. The RTDIP SDK is central to building pipelines and querying data, so getting started with it is key to unlocking the capability of RTDIP.</p> <p>This article provides a guide on how to install the RTDIP SDK. Get started by ensuring you have all the prerequisites before following the simple installation steps.</p> <ul> <li> <p>Prerequisites</p> </li> <li> <p>Installation</p> </li> </ul>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"getting-started/installation/#python","title":"Python","text":"<p>There are a few things to note before using the RTDIP SDK. The following prerequisites will need to be installed on your local machine.</p> <p>Python version 3.10 or higher (and &lt; 3.13) should be installed. Check which python version you have with the following command:</p> <pre><code>python --version\n</code></pre> <p>Find the latest python version here and ensure your python path is set up correctly on your machine.</p>"},{"location":"getting-started/installation/#python-package-installers","title":"Python Package Installers","text":"<p>Installing the RTDIP can be done using a package installer, such as Pip, Conda or Micromamba.</p> PipCondaMicromamba <p>Ensure your pip python version matches the python version on your machine. Check which version of pip you have installed with the following command:</p> <pre><code>pip --version\n</code></pre> <p>There are two ways to ensure you have the correct versions installed. Either upgrade your Python and pip install or create an environment.</p> <pre><code>python -m pip install --upgrade pip\n</code></pre> <p>Check which version of Conda is installed with the following command:</p> <pre><code>conda --version\n</code></pre> <p>If necessary, upgrade Conda as follows:</p> <pre><code>conda update conda\n</code></pre> <p>Check which version of Micromamba is installed with the following command:</p> <pre><code>micromamba --version\n</code></pre> <p>If necessary, upgrade Micromamba as follows:</p> <pre><code>micromamba self-update\n</code></pre>"},{"location":"getting-started/installation/#odbc","title":"ODBC","text":"<p>To use pyodbc or turbodbc python libraries, ensure it is installed as per the below and the ODBC driver is installed as per these instructions.</p> PyodbcTurbodbc <ol> <li>If you plan to use pyodbc, Microsoft Visual C++ 14.0 or greater is required. Get it from Microsoft C++ Build Tools.</li> <li>If you are using linux, install unixodbc as per these instructions.</li> <li>Install the <code>pyodbc</code> python package into your python environment.</li> </ol> <ol> <li>To use turbodbc python library, ensure to follow the Turbodbc Getting Started section and ensure that Boost is installed correctly. </li> <li>Install the <code>turbodbc</code> python package into your python environment.</li> </ol>"},{"location":"getting-started/installation/#spark-connect","title":"Spark Connect","text":"<p>Spark Connect was released in Apache Spark 3.4.0 and enables a decoupled client-server architecture that allows remote connectivity to Spark Clusters. RTDIP SDK supports Spark Connect and can be configured using the Spark Connector and providing the Spark Connect connection string required to connect to your Spark Cluster.</p> <p>Please ensure that you have followed the instructions to enable Spark Connect on your Spark Cluster and that you are using a <code>pyspark&gt;=3.4.0</code>. If you are connecting to a Databricks Cluster, then you may prefer to install python package <code>databricks-connect&gt;=13.0.1</code> instead of <code>pyspark</code>.</p>"},{"location":"getting-started/installation/#java","title":"Java","text":"<p>To use RTDIP Pipelines components in your own environment that leverages pyspark and you do not want to leverage Spark Connect, Java 8 or later is a prerequisite. See below for suggestions to install Java in your development environment.</p> CondaJava <p>A fairly simple option is to use the conda openjdk package to install Java into your python virtual environment. An example of a conda environment.yml file to achieve this is below.</p> <pre><code>name: rtdip-sdk\nchannels:\n    - conda-forge\n    - defaults\ndependencies:\n    - python==3.12\n    - pip\n    - openjdk==11.0.15\n    - pip:\n        - rtdip-sdk\n</code></pre> <p>Pypi</p> <p>This package is not available from Pypi.</p> <p>Follow the official Java JDK installation documentation here.</p> <ul> <li>Windows</li> <li>Mac OS</li> <li>Linux</li> </ul> <p>Windows</p> <p>Windows requires an additional installation of a file called winutils.exe. Please see this repo for more information.</p>"},{"location":"getting-started/installation/#installing-the-rtdip-sdk","title":"Installing the RTDIP SDK","text":"<p>RTDIP SDK is a PyPi package that can be found here. On this page you can find the project description,  release history, statistics, project links and maintainers.</p> <p>Features of the SDK can be installed using different extras statements when installing the rtdip-sdk package:</p> QueriesPipelinesPipelines + Pyspark <p>When installing the package for only quering data, simply specify  in your preferred python package installer:</p> <pre><code>pip install rtdip-sdk\n</code></pre> <p>RTDIP SDK can be installed to include the packages required to build, execute and deploy pipelines. Specify the following extra [pipelines] when installing RTDIP SDK so that the required python packages are included during installation.</p> <pre><code>pip install \"rtdip-sdk[pipelines]\"\n</code></pre> <p>RTDIP SDK can also execute pyspark functions as a part of the pipelines functionality. Specify the following extra [pipelines,pyspark] when installing RTDIP SDK so that the required pyspark python packages are included during installation.</p> <pre><code>pip install \"rtdip-sdk[pipelines,pyspark]\"\n</code></pre> <p>Java</p> <p>Ensure that Java is installed prior to installing the rtdip-sdk with the [pipelines,pyspark]. See here for more information.</p> <p>The following provides examples of how to install the RTDIP SDK package with Pip, Conda or Micromamba. Please note the section above to update any extra packages to be installed as part of the RTDIP SDK.</p> PipCondaMicromamba <p>To install the latest released version of RTDIP SDK from PyPi use the following command:</p> <pre><code>pip install rtdip-sdk\n</code></pre> <p>If you have previously installed the RTDIP SDK and would like the latest version, see below:</p> <pre><code>pip install rtdip-sdk --upgrade\n</code></pre> <p>To create an environment, you will need to create a environment.yml file with the following:</p> <pre><code>name: rtdip-sdk\nchannels:\n    - conda-forge\n    - defaults\ndependencies:\n    - python==3.12\n    - pip\n    - pip:\n        - rtdip-sdk\n</code></pre> <p>Run the following command:</p> <pre><code>conda env create -f environment.yml\n</code></pre> <p>To update an environment previously created:</p> <pre><code>conda env update -f environment.yml\n</code></pre> <p>To create an environment, you will need to create a environment.yml file with the following:</p> <pre><code>name: rtdip-sdk\nchannels:\n    - conda-forge\n    - defaults\ndependencies:\n    - python==3.12\n    - pip\n    - pip:\n        - rtdip-sdk\n</code></pre> <p>Run the following command:</p> <pre><code>micromamba create -f environment.yml\n</code></pre> <p>To update an environment previously created:</p> <pre><code>micromamba update -f environment.yml\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next steps","text":"<p>Once the installation is complete you can learn how to use the SDK here.</p> <p>Note</p> <p>If you are having trouble installing the SDK, ensure you have installed all of the prerequisites. If problems persist please see Troubleshooting for more information. Please also reach out to the RTDIP team via Issues, we are always looking to improve the SDK and value your input.</p>"},{"location":"integration/power-bi/","title":"Integration of Power BI with RTDIP","text":""},{"location":"integration/power-bi/#integration-with-power-bi","title":"Integration with Power BI","text":"<p>Microsoft Power BI is a business analytics service that provides interactive visualizations with self-service business intelligence capabilities that enable end users to create reports and dashboards by themselves without having to depend on information technology staff or database administrators.</p> ![Power BI Databricks](images/databricks_powerbi.png){width=50%} <p>When you use Azure Databricks as a data source with Power BI, you can bring the advantages of Azure Databricks performance and technology beyond data scientists and data engineers to all business users.</p> <p>You can connect Power BI Desktop to your Azure Databricks clusters and Databricks SQL warehouses by using the built-in Azure Databricks connector. You can also publish Power BI reports to the Power BI service and enable users to access the underlying Azure Databricks data using single sign-on (SSO), passing along the same Azure Active Directory credentials they use to access the report.</p> <p>For more information on how to connect Power BI with databricks, see here.</p>"},{"location":"integration/power-bi/#power-bi-installation-instructions","title":"Power BI Installation Instructions","text":"<ol> <li>Install Power BI Desktop application from Microsoft Store using your Microsoft Account to sign in.</li> </ol> ![Power BI Desktop](images/power-bi-desktop.png) <ol> <li> <p>Open Power BI desktop.</p> </li> <li> <p>Click on Home, Get data and More... </p> </li> <li> <p>Search for Azure Databricks and click Connect.  </p> </li> <li> <p>Fill in the details and click OK.</p> </li> <li> <p>Connect to the RTDIP data using your Databricks SQL Warehouse connection details including Hostname and HTTP Path. For Data Connectivity mode, select DirectQuery.</p> </li> <li> <p>Click Azure Active Directory, Sign In and select Connect. In Power Query Editor, there are different tables for different data types. </p> </li> <li> <p>Once connected to the Databricks SQL Warehouse, navigate to the Business Unit in the navigator bar on the left and select the asset tables for the data you wish to use in your report. There is functionality to select multiple tables if required. Click Load to get the queried data.</p> </li> </ol>"},{"location":"releases/core/","title":"Macro Rendering Error","text":"<p>File: <code>releases/core.md</code></p> <p>RateLimitExceededException: 403 {\"message\": \"API rate limit exceeded for 68.154.115.181. (But here's the good news: Authenticated requests get a higher rate limit. Check out the documentation for more details.)\", \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"}</p> <pre><code>Traceback (most recent call last):\n  File \"/home/runner/micromamba/envs/rtdip-sdk/lib/python3.12/site-packages/mkdocs_macros/plugin.py\", line 523, in render\n    return md_template.render(**page_variables)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/runner/micromamba/envs/rtdip-sdk/lib/python3.12/site-packages/jinja2/environment.py\", line 1295, in render\n    self.environment.handle_exception()\n  File \"/home/runner/micromamba/envs/rtdip-sdk/lib/python3.12/site-packages/jinja2/environment.py\", line 942, in handle_exception\n    raise rewrite_traceback_stack(source=source)\n  File \"&lt;template&gt;\", line 3, in top-level template code\n  File \"/home/runner/work/core/core/docs/macros.py\", line 31, in github_releases\n    for release in repo.get_releases():\n                   ^^^^^^^^^^^^^^^^^^^\n  File \"/home/runner/micromamba/envs/rtdip-sdk/lib/python3.12/site-packages/github/PaginatedList.py\", line 98, in __iter__\n    newElements = self._grow()\n                  ^^^^^^^^^^^^\n  File \"/home/runner/micromamba/envs/rtdip-sdk/lib/python3.12/site-packages/github/PaginatedList.py\", line 109, in _grow\n    newElements = self._fetchNextPage()\n                  ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/runner/micromamba/envs/rtdip-sdk/lib/python3.12/site-packages/github/PaginatedList.py\", line 338, in _fetchNextPage\n    headers, data = self.__requester.requestJsonAndCheck(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/runner/micromamba/envs/rtdip-sdk/lib/python3.12/site-packages/github/Requester.py\", line 623, in requestJsonAndCheck\n    return self.__check(\n           ^^^^^^^^^^^^^\n  File \"/home/runner/micromamba/envs/rtdip-sdk/lib/python3.12/site-packages/github/Requester.py\", line 853, in __check\n    raise self.createException(status, responseHeaders, data)\ngithub.GithubException.RateLimitExceededException: 403 {\"message\": \"API rate limit exceeded for 68.154.115.181. (But here's the good news: Authenticated requests get a higher rate limit. Check out the documentation for more details.)\", \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"}\n</code></pre>"},{"location":"roadmap/roadmap-overview/","title":"RTDIP Roadmap","text":"![roadmap overview](images/roadmap-overview.jpeg){width=60%}  <p>This section provides periodical updates of what the RTDIP team will be working on over the short and long term. The team will provide updates at important periods of the year so that users of the platform can understand new capabilities and options coming to the platform.</p> <p>We welcome and encourage projects, developers, users and applications to contribute to our roadmaps. Please reach out to the RTDIP team if you have ideas or suggestions on what we could work on next.</p>"},{"location":"roadmap/yearly-roadmaps/2022-development-roadmap/","title":"RTDIP Development Roadmap in 2022","text":"<p>Defining a list of development items for RTDIP is always difficult because so much can change within  Digital Technologies in 12 months. However, as we head towards the end of the year of 2021, we have outlined themes of what RTDIP will look at in the Development and Innovation space in 2022.</p> <p>We welcome and encourage projects, developers, users and applications to contribute to our roadmaps. Please reach out to the RTDIP Technical Steering Committee team if you have ideas or suggestions on what we could innovate on in this space. We will continue to evolve these development items all through 2022 so please come back to this article throughout the year to see any new items that may be brought into scope.</p> <p>Note</p> <p>Development and Innovation items don't always make it to Production. </p>"},{"location":"roadmap/yearly-roadmaps/2022-development-roadmap/#tldr","title":"TL;DR","text":"<p>A brief summary of development and innovation items planned for 2022.</p> Item Description Estimated Quarter for Delivery Power BI Enable querying of RTDIP data via Power BI. While some work has started on this in 2021, this item explores rolling it out further and how users can combine RTDIP data with other data sources Q1 2022 Seeq Connector Enable querying of RTDIP data via Seeq. Scope is limited to simply querying RTDIP data, we may look at what else is possible with the connector once the base capability has been achieved Q1 2022 Delta Live Tables Leverage Delta Live Tables for ingestion of RTDIP data into Delta Format. Provides better processing, merging, data cleansing and monitoring capabilities to the RTDIP Delta Ingestion Pipelines Q1-Q2 2022 Multicloud Build certain existing RTDIP Azure capabilities on AWS. Enables RTDIP in the clouds aligned with the business but also to ensure multicloud is cost effective and that products in the architecture work in Cloud Environments Q1-Q3 2022 SDK An open source python SDK is developed to assist users with a simple python library for connecting, authenticating and querying RTDIP data Q1-Q4 2022 REST API Wrap the python SDK in a REST API to allow non Python users to get similar functionality to the python SDK Q1-Q4 2022 Unity Catalog Provides a multi-region Catalog of all data in RTDIP. Enables easier navigation and exploration of what datasets are available in RTDIP Q3 2022 Delta Sharing Enables sharing of Delta data via a managed service that handles security, authentication and delivery of data. Particularly useful for sharing RTDIP data with third parties Q4 2022"},{"location":"roadmap/yearly-roadmaps/2022-development-roadmap/#power-bi","title":"Power BI","text":"<p>Power BI is a popular tool amongst RTDIP End Users for querying and plotting RTDIP data. The use of Delta and Databricks SQL Warehouses in the RTDIP Platform brings native Power BI integration using connectors already available in Power BI versions after May 2021.</p> <p>The aim is to enable Power BI connectivity to RTDIP so that users can query their data by the end of Q1 2022.</p>"},{"location":"roadmap/yearly-roadmaps/2022-development-roadmap/#seeq","title":"Seeq","text":"<p>Similar to Power BI, Seeq is a popular tool amongst real time users to query and manipulate RTDIP data. Seeq and RTDIP are currently working on a connector that allows Seeq to query RTDIP data via the same Databricks SQL Warehouse that Power BI will use for querying data by the end of Q1 2022.</p>"},{"location":"roadmap/yearly-roadmaps/2022-development-roadmap/#delta-live-tables","title":"Delta Live Tables","text":"<p>For more information about the advantages of Delta Live Tables, please see this link and if you would like to see Bryce Bartmann, RTDIP team member, talking about Delta Live Tables at the Data &amp; AI Summit 2021, please see the session here.</p> <p>RTDIP has been converting it's data to the open source format Delta using standard PySpark structured streaming jobs. Whilst this has been working well for converting RTDIP data to Delta, Delta Live Tables from Databricks provides similar capabilities as standard spark code, but with additional benefits:</p> <ul> <li>Expectations: Allows developers to specify data cleansing rules on ingested data. This can assist to provide higher quality, more reliable data to users</li> <li>Data Flows: Visually describes the flow of the data through the data pipelines from source to target, including data schemas and record counts</li> <li>Maintenance: Delta Live Tables simplifies maintenance tasks required for Delta Tables by scheduling them automatically based on the deployment of the Delta Live Tables Job</li> <li>Monitoring: Delta Live Tables are easier to monitor as their graphical schematics help non-technical people to understand the status of the ingestion pipelines</li> </ul> <p>The RTDIP Team has actively worked with Databricks to build Delta Live Tables. Whilst the product is well understood, certain features like merging data needed to be made available before RTDIP could fully migrate existing spark jobs to Delta Live Tables. Databricks intend to provide the Merge function in late Q4 2021 which will then trigger this piece of work with a target of having a decision point to move to production in Q1 2022.</p>"},{"location":"roadmap/yearly-roadmaps/2022-development-roadmap/#multicloud","title":"Multicloud","text":"<p>As clouds mature, one of the most asked questions is how customers can leverage more than one cloud to provide a better and more economical solution to their customers. Even though this is a fairly new area to explore, there are a number of cloud agnostic technologies that are trying to help customers take advantage of and manage environments in more than one cloud.</p> <p>Multicloud design can be complex and requires significant analysis of existing technologies capabilities and how they translate into benefits for RTDIP customers. Databricks will be one good example of exploring their new multicloud environment management tool and how this could benefit businesses in the long run. We expect there to be more technologies that come out with multicloud capabilities throughout 2022 and we will continue to explore, test and understand how RTDIP can leverage these throughout 2022 and beyond.</p>"},{"location":"roadmap/yearly-roadmaps/2022-development-roadmap/#sdk-and-rest-api","title":"SDK and REST API","text":"<p>A common theme we are seeing amongst applications and users of RTDIP data is a simple way to authenticate, query and manipulate RTDIP data. In an effort to also build a stronger developer community around RTDIP, we will be building a python SDK that python users can use in their code for performing common functions with RTDIP Data:</p> <ul> <li>Authenticating with RTDIP</li> <li>Connecting to RTDIP data</li> <li>Querying RTDIP raw data</li> <li>Performing sampling on raw data</li> <li>Performing interpolation on sampled data</li> </ul> <p>We plan to deliver the first version of the python SDK early in 2022 and welcome all python developers to contribute to the repository. </p> <p>For non python users, we plan to wrap the SDK in a REST API. This facilitates a language agnostic way of benefitting from all the development of the python SDK. These REST APIs will be rolled out in line with functionality built with the python SDK.</p>"},{"location":"roadmap/yearly-roadmaps/2022-development-roadmap/#unity-catalog","title":"Unity Catalog","text":"<p>Cataloging data is a common activity when building data lakes that contain data from multiple sources and from multiple geographic regions. RTDIP will explore and deploy a catalog of all data sources currently being ingested into the platform.</p>"},{"location":"roadmap/yearly-roadmaps/2022-development-roadmap/#delta-sharing","title":"Delta Sharing","text":"<p>One of the most common requests the RTDIP team receive is how to share RTDIP data with third parties. Delta Sharing is an open source capability that allows sharing of Delta data via a managed service that provides authentication, connection management and supply of Delta data to third parties. </p> <p>We aim to see what more we can do in this space to make sharing of data simpler from an architecture perspective while still meeting all the security requirements around sharing of data with third parties.</p>"},{"location":"roadmap/yearly-roadmaps/2023-development-roadmap/","title":"RTDIP Development Roadmap in 2023","text":"<p>Shell\u2019s internally developed Shell Sensor Intelligence Platform was open sourced to LF Energy in 2022 and rebranded to Real Time Data Ingestion Platform. This was a major milestone for that project and has opened a number of strategic doors and expanded how the development roadmap is determined when considering it in an open source context. Below is the roadmap for 2023</p> <p>We welcome and encourage projects, developers, users and applications to contribute to our roadmaps. Please reach out to the RTDIP Technical Steering Committee team if you have ideas or suggestions on what we could innovate on in this space. We will continue to evolve these development items all through 2022 so please come back to this article throughout the year to see any new items that may be brought into scope.</p>"},{"location":"roadmap/yearly-roadmaps/2023-development-roadmap/#tldr","title":"TL;DR","text":"Item Description Estimated Quarter for Delivery IEC CIM RTDIP will target the IEC CIM data model and focus specifically on modelling data related to scada/process time series and sensor metadata Q1 2023 Ingestion Framework Develop a Data Ingestion framework, incorporated into the RTDIP SDK that supports building, testing and deploying streaming data pipelines into an RTDIP environment. The goal of this framework is to eventually facilitate low-code/no-code setup of streaming data pipelines that can be easily deployed by RTDIP users. In 2023, the first iteration of the framework will be designed and developed Q1-Q2 2023 Meter Data Design and build ingestion pipelines for meter data that meets the IEC CIM and Data Ingestion Framework items above Q2-Q4 2023 LF Energy Integration Increase integration of RTDIP with other LF Energy components. RTDIP will reach out to LF Energy projects to understand which data and integration opportunities exist and support building the necessary components to support these integrations Q1-Q4 2023 Microgrid Support Deployment of RTDIP and related LF Energy components to a pilot microgrid site Q1-Q4 2023 Multicloud Build certain existing RTDIP Azure capabilities on AWS. Enables RTDIP in the clouds aligned with the business but also to ensure multicloud is cost effective and that products in the architecture work in Cloud Environments Q1-Q3 2023 Adoption Adapt RTDIP to cater for a wider audience, particularly to increase RTDIP adoption in the energy sector. A simpler deployment process, facilitation of more environments RTDIP can setup in and catering for more orchestration engines are all initiatives to increase adoption in 2023 Q1-Q4 2023"},{"location":"roadmap/yearly-roadmaps/2023-development-roadmap/#iec-cim","title":"IEC CIM","text":"<p>RTDIP will define IEC CIM compliant data models that will be built in the RTDIP SDK for time series data and metering data. This will ensure that RTDIP can support systems that require data modelled to the IEC CIM standard. The data models will be defined and available within the RTDIP SDK so that it can be used in the ingestion and query layers of the RTDIP SDK.</p>"},{"location":"roadmap/yearly-roadmaps/2023-development-roadmap/#ingestion-framework","title":"Ingestion Framework","text":"<p>An ingestion framework will be incorporated into the RTDIP SDK that will facilitate the following: - Simple reuse of components - Standardisation of development of ingestion source, transformation and destination components - A common deployment pattern that supports the most popular spark environment setups - Targeting of popular orchestration engines and targeting Databricks Worflows and Airflow. Dagster/Flyte to be considered.</p> <p>The ingestion framework will be designed and developed in Q1-Q3 2023 and will be used to build the ingestion pipelines for meter data in Q2-Q4 2023.</p>"},{"location":"roadmap/yearly-roadmaps/2023-development-roadmap/#meter-data","title":"Meter Data","text":"<p>RTDIP will be extended to incorporate meter data as part of the platform. The ingestion framework above will be used to build ingestion pipelines for meter data. The pipelines will be designed to ingest meter data from a variety of meter data sources and will be designed to support the IEC CIM data model.</p>"},{"location":"roadmap/yearly-roadmaps/2023-development-roadmap/#lf-energy-integration","title":"LF Energy Integration","text":"<p>Increase integration of RTDIP with other LF Energy components. Data is at the centre of every component in an energy system and a data platform can play a major role in supporting integration across different components. RTDIP will reach out to LF Energy projects to understand which data and integration opportunities exist and support building the necessary components to support this integration. The scope of this will be more clearly defined after connecting with the relevant projects of LF Energy to define opportunities for integration.</p>"},{"location":"roadmap/yearly-roadmaps/2023-development-roadmap/#microgrid-support","title":"Microgrid Support","text":"<p>Deployment of RTDIP and related LF Energy components to a Shell pilot microgrid site. This exercise will help to identify how RTDIP supports an energy system better and demonstrate the integration of RTDIP with other LF Energy components in a real world environment.</p>"},{"location":"roadmap/yearly-roadmaps/2023-development-roadmap/#adoption","title":"Adoption","text":"<p>Adapt RTDIP to cater for a wider audience, particularly to increase RTDIP adoption in the energy sector. To do this, the roadmap for 2023 will focus on:</p> <ul> <li>Making it simpler for deploying and incorporating RTDIP into existing technology stacks</li> <li>Provide options for deploying RTDIP ingestion jobs in different Spark Environments. For 2023, this will be Databricks(Jobs with stretch target of DLT) and self-managed Spark Clusters setup using Open Source Spark</li> <li>Support deployment of RTDIP data pipelines to different workflow managers. For 2023, this will include Airflow and Databricks Workflows, with a stretch target of including Dagster</li> <li>Support more clouds. For 2023, the target is AWS.</li> </ul>"},{"location":"roadmap/yearly-roadmaps/2023-development-roadmap/#multicloud","title":"Multicloud","text":"<p>The target for 2023 is to enable RTDIP components to run in AWS. This includes:</p> <ul> <li>RTDIP API support on AWS Lambda</li> <li>Ingestion support on AWS Databricks</li> <li>BI support on AWS Databricks SQL</li> </ul>"},{"location":"sdk/overview/","title":"Overview","text":"![sdk](images/sdk-logo.png){width=40%}"},{"location":"sdk/overview/#what-is-the-rtdip-sdk","title":"What is the RTDIP SDK?","text":"<p>\u200b\u200bReal Time Data Ingestion Platform (RTDIP) SDK is a python software development kit built to provide users, data scientists and developers with the ability to interact with components of the Real Time Data Ingestion Platform, including:</p> <ul> <li>Building, Executing and Deploying Ingestion Pipelines</li> <li>Execution of queries on RTDIP data</li> <li>Authentication to securely interact with environments and data</li> </ul>"},{"location":"sdk/overview/#installation","title":"Installation","text":"<p>To get started with the RTDIP SDK, follow these installation instructions.</p>"},{"location":"sdk/overview/#pipelines","title":"Pipelines","text":"<p>Find out more about RTDIP Ingestion Pipeline Framework and the components here.</p>"},{"location":"sdk/overview/#queries","title":"Queries","text":"<p>Find out more about querying RTDIP data using built in time series functions here.</p>"},{"location":"sdk/overview/#authentication","title":"Authentication","text":"<p>Find out more about authentication to securely interact with evironments and data here.</p>"},{"location":"sdk/authentication/azure/","title":"Azure Active Directory","text":"<p>The RTDIP SDK includes several Azure AD authentication methods to cater to the preference of the user:</p> <ul> <li>Default Authentication - For authenticating users with Azure AD using the azure-identity package. Note the order that Default Authentication uses to sign in a user and how it does it in this documentation. From experience, the Visual Studio Code login is the easiest to setup, but the azure cli option is the most reliable option. This page is useful for troubleshooting issues with this option to authenticate.</li> </ul> <p>Visual Studio Code</p> <p>As per the guidance in the documentation - To authenticate in Visual Studio Code, ensure version 0.9.11 or earlier of the Azure Account extension is installed. To track progress toward supporting newer extension versions, see this GitHub issue. Once installed, open the Command Palette and run the Azure: Sign In command.</p> <ul> <li> <p>Certificate Authentication - Service Principal authentication using a certificate</p> </li> <li> <p>Client Secret Authentication - Service Principal authentication using a client id and client secret</p> </li> </ul>"},{"location":"sdk/authentication/azure/#authentication","title":"Authentication","text":"<p>The following section describes authentication using Azure Active Directory..</p> <p>Note</p> <p>If you are using the SDK directly in Databricks please note that DefaultAuth will not work.</p> <p>1. Import rtdip-sdk authentication methods with the following:</p> <pre><code>from rtdip_sdk.authentication import azure as auth\n</code></pre> <p>2. Use any of the following authentication methods. Replace tenant_id , client_id, certificate_path or client_secret with your own details.</p> Default AuthenticationCertificate AuthenticationClient Secret Authentication <pre><code>credential = auth.DefaultAuth().authenticate()\n</code></pre> <pre><code>credential = auth.CertificateAuth(tenant_id, client_id, certificate_path).authenticate()\n</code></pre> <pre><code>credential = auth.ClientSecretAuth(tenant_id, client_id, client_secret).authenticate()\n</code></pre> <p>3. The methods above will return back a Client Object. The following example will show you how to retrieve the access_token from a credential object. The access token will be used in later steps to connect to RTDIP via the three options (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect).</p>"},{"location":"sdk/authentication/azure/#tokens","title":"Tokens","text":"<p>Once authenticated, it is possible to retrieve tokens for specific Azure Resources by providing scopes when retrieving tokens. Please see below for examples of how to retrieve tokens for Azure resources regularly used in RTDIP.</p> Databricks <pre><code>access_token = credential.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n</code></pre> <p>Note</p> <p>RTDIP are continuously adding more to this list so check back regularly!</p>"},{"location":"sdk/authentication/databricks/","title":"Databricks","text":"<p>Databricks supports authentication using Personal Access Tokens (PAT) and information about this authentication method is available here.</p>"},{"location":"sdk/authentication/databricks/#authentication","title":"Authentication","text":"<p>To generate a Databricks PAT Token, follow this guide and ensure that the token is stored securely and is never used directly in code.</p> <p>Your Databricks PAT Token can be used in the RTDIP SDK to authenticate with any Databricks Workspace or Databricks SQL Warehouse and simply provided in the <code>access_token</code> fields where tokens are required in the RTDIP SDK.</p>"},{"location":"sdk/authentication/databricks/#example","title":"Example","text":"<p>Below is an example of using a Databricks PAT Token for authenticating with a Databricks SQL Warehouse.</p> <pre><code>from rtdip_sdk.connectors import DatabricksSQLConnection\n\nserver_hostname = \"server_hostname\"\nhttp_path = \"http_path\"\naccess_token = \"dbapi.......\"\n\nconnection = DatabricksSQLConnection(server_hostname, http_path, access_token)\n</code></pre> <p>Replace server_hostname, http_path with your own information and specify your Databricks PAT token for the access_token. </p>"},{"location":"sdk/code-reference/authentication/azure/","title":"Authentication","text":""},{"location":"sdk/code-reference/authentication/azure/#src.sdk.python.rtdip_sdk.authentication.azure.ClientSecretAuth","title":"<code>ClientSecretAuth</code>","text":"<p>Enables authentication to Azure Active Directory using a client secret that was generated for an App Registration.</p> <p>Parameters:</p> Name Type Description Default <code>tenant_id</code> <code>str</code> <p>The Azure Active Directory tenant (directory) Id of the service principal.</p> required <code>client_id</code> <code>str</code> <p>The client (application) ID of the service principal</p> required <code>client_secret</code> <code>str</code> <p>A client secret that was generated for the App Registration used to authenticate the client.</p> required Source code in <code>src/sdk/python/rtdip_sdk/authentication/azure.py</code> <pre><code>class ClientSecretAuth:\n    \"\"\"\n    Enables authentication to Azure Active Directory using a client secret that was generated for an App Registration.\n\n    Args:\n        tenant_id: The Azure Active Directory tenant (directory) Id of the service principal.\n        client_id: The client (application) ID of the service principal\n        client_secret: A client secret that was generated for the App Registration used to authenticate the client.\n    \"\"\"\n\n    def __init__(self, tenant_id: str, client_id: str, client_secret: str) -&gt; None:\n        self.tenant_id = tenant_id\n        self.client_id = client_id\n        self.client_secret = client_secret\n\n    def authenticate(self) -&gt; ClientSecretCredential:\n        \"\"\"\n        Authenticates as a service principal using a client secret.\n\n        Returns:\n            ClientSecretCredential: Authenticates as a service principal using a client secret.\n        \"\"\"\n        try:\n            access_token = ClientSecretCredential(\n                self.tenant_id, self.client_id, self.client_secret\n            )\n            return access_token\n        except Exception as e:\n            logging.exception(\"error returning client secret credential\")\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/authentication/azure/#src.sdk.python.rtdip_sdk.authentication.azure.ClientSecretAuth.authenticate","title":"<code>authenticate()</code>","text":"<p>Authenticates as a service principal using a client secret.</p> <p>Returns:</p> Name Type Description <code>ClientSecretCredential</code> <code>ClientSecretCredential</code> <p>Authenticates as a service principal using a client secret.</p> Source code in <code>src/sdk/python/rtdip_sdk/authentication/azure.py</code> <pre><code>def authenticate(self) -&gt; ClientSecretCredential:\n    \"\"\"\n    Authenticates as a service principal using a client secret.\n\n    Returns:\n        ClientSecretCredential: Authenticates as a service principal using a client secret.\n    \"\"\"\n    try:\n        access_token = ClientSecretCredential(\n            self.tenant_id, self.client_id, self.client_secret\n        )\n        return access_token\n    except Exception as e:\n        logging.exception(\"error returning client secret credential\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/authentication/azure/#src.sdk.python.rtdip_sdk.authentication.azure.CertificateAuth","title":"<code>CertificateAuth</code>","text":"<p>Enables authentication to Azure Active Directory using a certificate that was generated for an App Registration.</p> <p>The certificate must have an RSA private key, because this credential signs assertions using RS256</p> <p>Parameters:</p> Name Type Description Default <code>tenant_id</code> <code>str</code> <p>The Azure Active Directory tenant (directory) Id of the service principal.</p> required <code>client_id</code> <code>str</code> <p>The client (application) ID of the service principal</p> required <code>certificate_path</code> <code>str</code> <p>Optional path to a certificate file in PEM or PKCS12 format, including the private key. If not provided, certificate_data is required.</p> <code>None</code> <code>certificate_data</code> <code>bytes</code> <p>Optional bytes of a certificate in PEM or PKCS12 format, including the private key</p> <code>None</code> <code>password</code> <code>Union[str, bytes]</code> <p>Optional certificate password. If a unicode string, it will be encoded as UTF-8. If the certificate requires a different encoding, pass appropriately encoded bytes instead.</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/authentication/azure.py</code> <pre><code>class CertificateAuth:\n    \"\"\"\n    Enables authentication to Azure Active Directory using a certificate that was generated for an App Registration.\n\n    The certificate must have an RSA private key, because this credential signs assertions using RS256\n\n    Args:\n        tenant_id: The Azure Active Directory tenant (directory) Id of the service principal.\n        client_id: The client (application) ID of the service principal\n        certificate_path: Optional path to a certificate file in PEM or PKCS12 format, including the private key. If not provided, certificate_data is required.\n        certificate_data: Optional bytes of a certificate in PEM or PKCS12 format, including the private key\n        password: Optional certificate password. If a unicode string, it will be encoded as UTF-8. If the certificate requires a different encoding, pass appropriately encoded bytes instead.\n    \"\"\"\n\n    def __init__(\n        self,\n        tenant_id: str,\n        client_id: str,\n        certificate_path: str = None,\n        certificate_data: bytes = None,\n        password: Union[str, bytes] = None,\n    ) -&gt; None:\n        self.tenant_id = tenant_id\n        self.client_id = client_id\n        self.certificate_path = certificate_path\n        self.certificate_data = certificate_data\n        self.password = password\n\n    def authenticate(self) -&gt; CertificateCredential:\n        \"\"\"\n        Authenticates as a service principal using a certificate.\n\n        Returns:\n            CertificateCredential: Authenticates as a service principal using a certificate.\n        \"\"\"\n        try:\n            access_token = CertificateCredential(\n                self.tenant_id,\n                self.client_id,\n                self.certificate_path,\n                certificate_data=self.certificate_data,\n                password=self.password,\n            )\n            return access_token\n        except Exception as e:\n            logging.exception(\"error returning certificate credential\")\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/authentication/azure/#src.sdk.python.rtdip_sdk.authentication.azure.CertificateAuth.authenticate","title":"<code>authenticate()</code>","text":"<p>Authenticates as a service principal using a certificate.</p> <p>Returns:</p> Name Type Description <code>CertificateCredential</code> <code>CertificateCredential</code> <p>Authenticates as a service principal using a certificate.</p> Source code in <code>src/sdk/python/rtdip_sdk/authentication/azure.py</code> <pre><code>def authenticate(self) -&gt; CertificateCredential:\n    \"\"\"\n    Authenticates as a service principal using a certificate.\n\n    Returns:\n        CertificateCredential: Authenticates as a service principal using a certificate.\n    \"\"\"\n    try:\n        access_token = CertificateCredential(\n            self.tenant_id,\n            self.client_id,\n            self.certificate_path,\n            certificate_data=self.certificate_data,\n            password=self.password,\n        )\n        return access_token\n    except Exception as e:\n        logging.exception(\"error returning certificate credential\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/authentication/azure/#src.sdk.python.rtdip_sdk.authentication.azure.DefaultAuth","title":"<code>DefaultAuth</code>","text":"<p>A default credential capable of handling most Azure SDK authentication scenarios.</p> <p>The identity it uses depends on the environment. When an access token is needed, it requests one using these identities in turn, stopping when one provides a token:</p> <p>1) A service principal configured by environment variables.</p> <p>2) An Azure managed identity.</p> <p>3) On Windows only: a user who has signed in with a Microsoft application, such as Visual Studio. If multiple identities are in the cache, then the value of the environment variable AZURE_USERNAME is used to select which identity to use.</p> <p>4) The user currently signed in to Visual Studio Code.</p> <p>5) The identity currently logged in to the Azure CLI.</p> <p>6) The identity currently logged in to Azure PowerShell.</p> <p>Parameters:</p> Name Type Description Default <code>exclude_cli_credential</code> <code>Optional</code> <p>Whether to exclude the Azure CLI from the credential. Defaults to False.</p> <code>False</code> <code>exclude_environment_credential</code> <code>Optional</code> <p>Whether to exclude a service principal configured by environment variables from the credential. Defaults to True.</p> <code>True</code> <code>exclude_managed_identity_credential</code> <code>Optional</code> <p>Whether to exclude managed identity from the credential. Defaults to True</p> <code>True</code> <code>exclude_powershell_credential</code> <code>Optional</code> <p>Whether to exclude Azure PowerShell. Defaults to False.</p> <code>False</code> <code>exclude_visual_studio_code_credential</code> <code>Optional</code> <p>Whether to exclude stored credential from VS Code. Defaults to False</p> <code>False</code> <code>exclude_shared_token_cache_credential</code> <code>Optional</code> <p>Whether to exclude the shared token cache. Defaults to False.</p> <code>False</code> <code>exclude_interactive_browser_credential</code> <code>Optional</code> <p>Whether to exclude interactive browser authentication (see InteractiveBrowserCredential). Defaults to False</p> <code>False</code> <code>logging_enable</code> <code>Optional</code> <p>Turn on or off logging. Defaults to False.</p> <code>False</code> Source code in <code>src/sdk/python/rtdip_sdk/authentication/azure.py</code> <pre><code>class DefaultAuth:\n    \"\"\"\n    A default credential capable of handling most Azure SDK authentication scenarios.\n\n    The identity it uses depends on the environment. When an access token is needed, it requests one using these identities in turn, stopping when one provides a token:\n\n    1) A service principal configured by environment variables.\n\n    2) An Azure managed identity.\n\n    3) On Windows only: a user who has signed in with a Microsoft application, such as Visual Studio. If multiple identities are in the cache, then the value of the environment variable AZURE_USERNAME is used to select which identity to use.\n\n    4) The user currently signed in to Visual Studio Code.\n\n    5) The identity currently logged in to the Azure CLI.\n\n    6) The identity currently logged in to Azure PowerShell.\n\n    Args:\n        exclude_cli_credential (Optional): Whether to exclude the Azure CLI from the credential. Defaults to False.\n        exclude_environment_credential (Optional): Whether to exclude a service principal configured by environment variables from the credential. Defaults to True.\n        exclude_managed_identity_credential (Optional): Whether to exclude managed identity from the credential. Defaults to True\n        exclude_powershell_credential (Optional): Whether to exclude Azure PowerShell. Defaults to False.\n        exclude_visual_studio_code_credential (Optional): Whether to exclude stored credential from VS Code. Defaults to False\n        exclude_shared_token_cache_credential (Optional): Whether to exclude the shared token cache. Defaults to False.\n        exclude_interactive_browser_credential (Optional): Whether to exclude interactive browser authentication (see InteractiveBrowserCredential). Defaults to False\n        logging_enable (Optional): Turn on or off logging. Defaults to False.\n    \"\"\"\n\n    def __init__(\n        self,\n        exclude_cli_credential=False,\n        exclude_environment_credential=True,\n        exclude_managed_identity_credential=True,\n        exclude_powershell_credential=False,\n        exclude_visual_studio_code_credential=False,\n        exclude_shared_token_cache_credential=False,\n        exclude_interactive_browser_credential=False,\n        logging_enable=False,\n    ) -&gt; None:\n        self.exclude_cli_credential = exclude_cli_credential\n        self.exclude_environment_credential = exclude_environment_credential\n        self.exclude_managed_identity_credential = exclude_managed_identity_credential\n        self.exclude_powershell_credential = exclude_powershell_credential\n        self.exclude_visual_studio_code_credential = (\n            exclude_visual_studio_code_credential\n        )\n        self.exclude_shared_token_cache_credential = (\n            exclude_shared_token_cache_credential\n        )\n        self.exclude_interactive_browser_credential = (\n            exclude_interactive_browser_credential\n        )\n        self.logging_enable = logging_enable\n\n    def authenticate(self) -&gt; DefaultAzureCredential:\n        \"\"\"\n        A default credential capable of handling most Azure SDK authentication scenarios.\n\n        Returns:\n            DefaultAzureCredential: A default credential capable of handling most Azure SDK authentication scenarios.\n        \"\"\"\n        try:\n            access_token = DefaultAzureCredential(\n                exclude_cli_credential=self.exclude_cli_credential,\n                exclude_environment_credential=self.exclude_environment_credential,\n                exclude_managed_identity_credential=self.exclude_managed_identity_credential,\n                exclude_powershell_credential=self.exclude_powershell_credential,\n                exclude_visual_studio_code_credential=self.exclude_visual_studio_code_credential,\n                exclude_shared_token_cache_credential=self.exclude_shared_token_cache_credential,\n                exclude_interactive_browser_credential=self.exclude_interactive_browser_credential,\n                logging_enable=self.logging_enable,\n            )\n            return access_token\n        except Exception as e:\n            logging.exception(\"error returning default azure credential\")\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/authentication/azure/#src.sdk.python.rtdip_sdk.authentication.azure.DefaultAuth.authenticate","title":"<code>authenticate()</code>","text":"<p>A default credential capable of handling most Azure SDK authentication scenarios.</p> <p>Returns:</p> Name Type Description <code>DefaultAzureCredential</code> <code>DefaultAzureCredential</code> <p>A default credential capable of handling most Azure SDK authentication scenarios.</p> Source code in <code>src/sdk/python/rtdip_sdk/authentication/azure.py</code> <pre><code>def authenticate(self) -&gt; DefaultAzureCredential:\n    \"\"\"\n    A default credential capable of handling most Azure SDK authentication scenarios.\n\n    Returns:\n        DefaultAzureCredential: A default credential capable of handling most Azure SDK authentication scenarios.\n    \"\"\"\n    try:\n        access_token = DefaultAzureCredential(\n            exclude_cli_credential=self.exclude_cli_credential,\n            exclude_environment_credential=self.exclude_environment_credential,\n            exclude_managed_identity_credential=self.exclude_managed_identity_credential,\n            exclude_powershell_credential=self.exclude_powershell_credential,\n            exclude_visual_studio_code_credential=self.exclude_visual_studio_code_credential,\n            exclude_shared_token_cache_credential=self.exclude_shared_token_cache_credential,\n            exclude_interactive_browser_credential=self.exclude_interactive_browser_credential,\n            logging_enable=self.logging_enable,\n        )\n        return access_token\n    except Exception as e:\n        logging.exception(\"error returning default azure credential\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/","title":"Json","text":""},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.DataBricksAutoLoaderSource","title":"<code>DataBricksAutoLoaderSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>The Spark Auto Loader is used to read new data files as they arrive in cloud storage. Further information on Auto Loader is available here</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.DataBricksAutoLoaderSource--example","title":"Example","text":"ADLS Gen2AWS S3GCS <pre><code>from rtdip_sdk.pipelines.sources import DataBricksAutoLoaderSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\noptions = {}\npath = \"abfss://{FILE-SYSTEM}@{ACCOUNT-NAME}.dfs.core.windows.net/{PATH}/{FILE-NAME}\nformat = \"{DESIRED-FILE-FORMAT}\"\n\nDataBricksAutoLoaderSource(spark, options, path, format).read_stream()\n\nOR\n\nDataBricksAutoLoaderSource(spark, options, path, format).read_batch()\n</code></pre> <pre><code>from rtdip_sdk.pipelines.sources import DataBricksAutoLoaderSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\noptions = {}\npath = \"https://s3.{REGION-CODE}.amazonaws.com/{BUCKET-NAME}/{KEY-NAME}\"\nformat = \"{DESIRED-FILE-FORMAT}\"\n\nDataBricksAutoLoaderSource(spark, options, path, format).read_stream()\n\nOR\n\nDataBricksAutoLoaderSource(spark, options, path, format).read_batch()\n</code></pre> <pre><code>from rtdip_sdk.pipelines.sources import DataBricksAutoLoaderSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\noptions = {}\npath = \"gs://{BUCKET-NAME}/{FILE-PATH}\"\nformat = \"{DESIRED-FILE-FORMAT}\"\n\nDataBricksAutoLoaderSource(spark, options, path, format).read_stream()\n\nOR\n\nDataBricksAutoLoaderSource(spark, options, path, format).read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from cloud storage</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for configuring the Auto Loader. Further information on the options available are here</p> required <code>path</code> <code>str</code> <p>The cloud storage path</p> required <code>format</code> <code>str</code> <p>Specifies the file format to be read. Supported formats are available here</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/autoloader.py</code> <pre><code>class DataBricksAutoLoaderSource(SourceInterface):\n    \"\"\"\n    The Spark Auto Loader is used to read new data files as they arrive in cloud storage. Further information on Auto Loader is available [here](https://docs.databricks.com/ingestion/auto-loader/index.html)\n\n    Example\n    --------\n    === \"ADLS Gen2\"\n\n        ```python\n        from rtdip_sdk.pipelines.sources import DataBricksAutoLoaderSource\n        from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n        # Not required if using Databricks\n        spark = SparkSessionUtility(config={}).execute()\n\n        options = {}\n        path = \"abfss://{FILE-SYSTEM}@{ACCOUNT-NAME}.dfs.core.windows.net/{PATH}/{FILE-NAME}\n        format = \"{DESIRED-FILE-FORMAT}\"\n\n        DataBricksAutoLoaderSource(spark, options, path, format).read_stream()\n\n        OR\n\n        DataBricksAutoLoaderSource(spark, options, path, format).read_batch()\n        ```\n    === \"AWS S3\"\n\n        ```python\n        from rtdip_sdk.pipelines.sources import DataBricksAutoLoaderSource\n        from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n        # Not required if using Databricks\n        spark = SparkSessionUtility(config={}).execute()\n\n        options = {}\n        path = \"https://s3.{REGION-CODE}.amazonaws.com/{BUCKET-NAME}/{KEY-NAME}\"\n        format = \"{DESIRED-FILE-FORMAT}\"\n\n        DataBricksAutoLoaderSource(spark, options, path, format).read_stream()\n\n        OR\n\n        DataBricksAutoLoaderSource(spark, options, path, format).read_batch()\n        ```\n    === \"GCS\"\n\n        ```python\n        from rtdip_sdk.pipelines.sources import DataBricksAutoLoaderSource\n        from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n        # Not required if using Databricks\n        spark = SparkSessionUtility(config={}).execute()\n\n        options = {}\n        path = \"gs://{BUCKET-NAME}/{FILE-PATH}\"\n        format = \"{DESIRED-FILE-FORMAT}\"\n\n        DataBricksAutoLoaderSource(spark, options, path, format).read_stream()\n\n        OR\n\n        DataBricksAutoLoaderSource(spark, options, path, format).read_batch()\n        ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to read data from cloud storage\n        options (dict): Options that can be specified for configuring the Auto Loader. Further information on the options available are [here](https://docs.databricks.com/ingestion/auto-loader/options.html)\n        path (str): The cloud storage path\n        format (str): Specifies the file format to be read. Supported formats are available [here](https://docs.databricks.com/ingestion/auto-loader/options.html#file-format-options)\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    path: str\n\n    def __init__(\n        self, spark: SparkSession, options: dict, path: str, format: str\n    ) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.path = path\n        self.options[\"cloudFiles.format\"] = format\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK on Databricks\n        \"\"\"\n        return SystemType.PYSPARK_DATABRICKS\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_core\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self, df: DataFrame):\n        return True\n\n    def read_batch(self):\n        \"\"\"\n        Raises:\n            NotImplementedError: Auto Loader only supports streaming reads. To perform a batch read, use the read_stream method of this component and specify the Trigger on the write_stream to be `availableNow` to perform batch-like reads of cloud storage files.\n        \"\"\"\n        raise NotImplementedError(\n            \"Auto Loader only supports streaming reads. To perform a batch read, use the read_stream method and specify Trigger on the write_stream as `availableNow`\"\n        )\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        Performs streaming reads of files in cloud storage.\n        \"\"\"\n        try:\n            return (\n                self.spark.readStream.format(\"cloudFiles\")\n                .options(**self.options)\n                .load(self.path)\n            )\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.DataBricksAutoLoaderSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK on Databricks</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/autoloader.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK on Databricks\n    \"\"\"\n    return SystemType.PYSPARK_DATABRICKS\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.DataBricksAutoLoaderSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Auto Loader only supports streaming reads. To perform a batch read, use the read_stream method of this component and specify the Trigger on the write_stream to be <code>availableNow</code> to perform batch-like reads of cloud storage files.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/autoloader.py</code> <pre><code>def read_batch(self):\n    \"\"\"\n    Raises:\n        NotImplementedError: Auto Loader only supports streaming reads. To perform a batch read, use the read_stream method of this component and specify the Trigger on the write_stream to be `availableNow` to perform batch-like reads of cloud storage files.\n    \"\"\"\n    raise NotImplementedError(\n        \"Auto Loader only supports streaming reads. To perform a batch read, use the read_stream method and specify Trigger on the write_stream as `availableNow`\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.DataBricksAutoLoaderSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Performs streaming reads of files in cloud storage.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/autoloader.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    Performs streaming reads of files in cloud storage.\n    \"\"\"\n    try:\n        return (\n            self.spark.readStream.format(\"cloudFiles\")\n            .options(**self.options)\n            .load(self.path)\n        )\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkDeltaSharingSource","title":"<code>SparkDeltaSharingSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>The Spark Delta Sharing Source is used to read data from a Delta table where Delta sharing is configured</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkDeltaSharingSource--example","title":"Example","text":"<p><pre><code>#Delta Sharing Source for Streaming Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkDeltaSharingSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\ndelta_sharing_source = SparkDeltaSharingSource(\n    spark=spark,\n    options={\n        \"maxFilesPerTrigger\": 1000,\n        \"ignoreChanges: True,\n        \"startingVersion\": 0\n    },\n    table_name=\"{YOUR-DELTA-TABLE-PATH}\"\n)\n\ndelta_sharing_source.read_stream()\n</code></pre> <pre><code>#Delta Sharing Source for Batch Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkDeltaSharingSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\ndelta_sharing_source = SparkDeltaSharingSource(\n    spark=spark,\n    options={\n        \"versionAsOf\": 0,\n        \"timestampAsOf\": \"yyyy-mm-dd hh:mm:ss[.fffffffff]\"\n    },\n    table_name=\"{YOUR-DELTA-TABLE-PATH}\"\n)\n\ndelta_sharing_source.read_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from a Delta table</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available here</p> required <code>table_path</code> <code>str</code> <p>Path to credentials file and Delta table to query</p> required <p>Attributes:</p> Name Type Description <code>ignoreDeletes</code> <code>bool str</code> <p>Ignore transactions that delete data at partition boundaries. (Streaming)</p> <code>ignoreChanges</code> <code>bool str</code> <p>Pre-process updates if files had to be rewritten in the source table due to a data changing operation. (Streaming)</p> <code>startingVersion</code> <code>int str</code> <p>The Delta Lake version to start from. (Streaming)</p> <code>startingTimestamp</code> <code>datetime str</code> <p>The timestamp to start from. (Streaming)</p> <code>maxFilesPerTrigger</code> <code>int</code> <p>How many new files to be considered in every micro-batch. The default is 1000. (Streaming)</p> <code>maxBytesPerTrigger</code> <code>int</code> <p>How much data gets processed in each micro-batch. (Streaming)</p> <code>readChangeFeed</code> <code>bool str</code> <p>Stream read the change data feed of the shared table. (Batch &amp; Streaming)</p> <code>timestampAsOf</code> <code>datetime str</code> <p>Query the Delta Table from a specific point in time. (Batch)</p> <code>versionAsOf</code> <code>int str</code> <p>Query the Delta Table from a specific version. (Batch)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta_sharing.py</code> <pre><code>class SparkDeltaSharingSource(SourceInterface):\n    \"\"\"\n    The Spark Delta Sharing Source is used to read data from a Delta table where Delta sharing is configured\n\n    Example\n    --------\n    ```python\n    #Delta Sharing Source for Streaming Queries\n\n    from rtdip_sdk.pipelines.sources import SparkDeltaSharingSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    delta_sharing_source = SparkDeltaSharingSource(\n        spark=spark,\n        options={\n            \"maxFilesPerTrigger\": 1000,\n            \"ignoreChanges: True,\n            \"startingVersion\": 0\n        },\n        table_name=\"{YOUR-DELTA-TABLE-PATH}\"\n    )\n\n    delta_sharing_source.read_stream()\n    ```\n    ```python\n    #Delta Sharing Source for Batch Queries\n\n    from rtdip_sdk.pipelines.sources import SparkDeltaSharingSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    delta_sharing_source = SparkDeltaSharingSource(\n        spark=spark,\n        options={\n            \"versionAsOf\": 0,\n            \"timestampAsOf\": \"yyyy-mm-dd hh:mm:ss[.fffffffff]\"\n        },\n        table_name=\"{YOUR-DELTA-TABLE-PATH}\"\n    )\n\n    delta_sharing_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to read data from a Delta table\n        options (dict): Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available [here](https://docs.databricks.com/data-sharing/read-data-open.html#apache-spark-read-shared-data){ target=\"_blank\" }\n        table_path (str): Path to credentials file and Delta table to query\n\n    Attributes:\n        ignoreDeletes (bool str): Ignore transactions that delete data at partition boundaries. (Streaming)\n        ignoreChanges (bool str): Pre-process updates if files had to be rewritten in the source table due to a data changing operation. (Streaming)\n        startingVersion (int str): The Delta Lake version to start from. (Streaming)\n        startingTimestamp (datetime str): The timestamp to start from. (Streaming)\n        maxFilesPerTrigger (int): How many new files to be considered in every micro-batch. The default is 1000. (Streaming)\n        maxBytesPerTrigger (int): How much data gets processed in each micro-batch. (Streaming)\n        readChangeFeed (bool str): Stream read the change data feed of the shared table. (Batch &amp; Streaming)\n        timestampAsOf (datetime str): Query the Delta Table from a specific point in time. (Batch)\n        versionAsOf (int str): Query the Delta Table from a specific version. (Batch)\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    table_path: str\n\n    def __init__(self, spark: SparkSession, options: dict, table_path: str) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.table_path = table_path\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_sharing\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self):\n        return True\n\n    def read_batch(self):\n        \"\"\"\n        Reads batch data from Delta. Most of the options provided by the Apache Spark DataFrame read API are supported for performing batch reads on Delta tables.\n        \"\"\"\n        try:\n            return (\n                self.spark.read.format(\"deltaSharing\")\n                .options(**self.options)\n                .table(self.table_path)\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        Reads streaming data from Delta. All of the data in the table is processed as well as any new data that arrives after the stream started. .load() can take table name or path.\n        \"\"\"\n        try:\n            return (\n                self.spark.readStream.format(\"deltaSharing\")\n                .options(**self.options)\n                .load(self.table_path)\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkDeltaSharingSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta_sharing.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkDeltaSharingSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads batch data from Delta. Most of the options provided by the Apache Spark DataFrame read API are supported for performing batch reads on Delta tables.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta_sharing.py</code> <pre><code>def read_batch(self):\n    \"\"\"\n    Reads batch data from Delta. Most of the options provided by the Apache Spark DataFrame read API are supported for performing batch reads on Delta tables.\n    \"\"\"\n    try:\n        return (\n            self.spark.read.format(\"deltaSharing\")\n            .options(**self.options)\n            .table(self.table_path)\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkDeltaSharingSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Reads streaming data from Delta. All of the data in the table is processed as well as any new data that arrives after the stream started. .load() can take table name or path.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta_sharing.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    Reads streaming data from Delta. All of the data in the table is processed as well as any new data that arrives after the stream started. .load() can take table name or path.\n    \"\"\"\n    try:\n        return (\n            self.spark.readStream.format(\"deltaSharing\")\n            .options(**self.options)\n            .load(self.table_path)\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkEventhubSource","title":"<code>SparkEventhubSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>This Spark source class is used to read batch or streaming data from Eventhubs. Eventhub configurations need to be specified as options in a dictionary. Additionally, there are more optional configurations which can be found here. If using startingPosition or endingPosition make sure to check out the Event Position section for more details and examples.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkEventhubSource--example","title":"Example","text":"<p><pre><code>#Eventhub Source for Streaming Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkEventhubSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\nimport json\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconnectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n\nstartingEventPosition = {\n\"offset\": -1,\n\"seqNo\": -1,\n\"enqueuedTime\": None,\n\"isInclusive\": True\n}\n\neventhub_source = SparkEventhubSource(\n    spark=spark,\n    options = {\n        \"eventhubs.connectionString\": connectionString,\n        \"eventhubs.consumerGroup\": \"{YOUR-CONSUMER-GROUP}\",\n        \"eventhubs.startingPosition\": json.dumps(startingEventPosition),\n        \"maxEventsPerTrigger\" : 1000\n    }\n)\n\neventhub_source.read_stream()\n</code></pre> <pre><code> #Eventhub Source for Batch Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkEventhubSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\nimport json\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconnectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n\nstartingEventPosition = {\n    \"offset\": -1,\n    \"seqNo\": -1,\n    \"enqueuedTime\": None,\n    \"isInclusive\": True\n}\n\nendingEventPosition = {\n    \"offset\": None,\n    \"seqNo\": -1,\n    \"enqueuedTime\": endTime,\n    \"isInclusive\": True\n}\n\neventhub_source = SparkEventhubSource(\n    spark,\n    options = {\n        \"eventhubs.connectionString\": connectionString,\n        \"eventhubs.consumerGroup\": \"{YOUR-CONSUMER-GROUP}\",\n        \"eventhubs.startingPosition\": json.dumps(startingEventPosition),\n        \"eventhubs.endingPosition\": json.dumps(endingEventPosition)\n    }\n)\n\neventhub_source.read_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session</p> required <code>options</code> <code>dict</code> <p>A dictionary of Eventhub configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>eventhubs.connectionString</code> <code>str</code> <p>Eventhubs connection string is required to connect to the Eventhubs service. (Streaming and Batch)</p> <code>eventhubs.consumerGroup</code> <code>str</code> <p>A consumer group is a view of an entire eventhub. Consumer groups enable multiple consuming applications to each have a separate view of the event stream, and to read the stream independently at their own pace and with their own offsets. (Streaming and Batch)</p> <code>eventhubs.startingPosition</code> <code>JSON str</code> <p>The starting position for your Structured Streaming job. If a specific EventPosition is not set for a partition using startingPositions, then we use the EventPosition set in startingPosition. If nothing is set in either option, we will begin consuming from the end of the partition. (Streaming and Batch)</p> <code>eventhubs.endingPosition</code> <code>JSON str</code> <p>(JSON str): The ending position of a batch query. This works the same as startingPosition. (Batch)</p> <code>maxEventsPerTrigger</code> <code>long</code> <p>Rate limit on maximum number of events processed per trigger interval. The specified total number of events will be proportionally split across partitions of different volume. (Stream)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/eventhub.py</code> <pre><code>class SparkEventhubSource(SourceInterface):\n    \"\"\"\n    This Spark source class is used to read batch or streaming data from Eventhubs. Eventhub configurations need to be specified as options in a dictionary.\n    Additionally, there are more optional configurations which can be found [here.](https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/PySpark/structured-streaming-pyspark.md#event-hubs-configuration){ target=\"_blank\" }\n    If using startingPosition or endingPosition make sure to check out the **Event Position** section for more details and examples.\n\n    Example\n    --------\n    ```python\n    #Eventhub Source for Streaming Queries\n\n    from rtdip_sdk.pipelines.sources import SparkEventhubSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n    import json\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    connectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n\n    startingEventPosition = {\n    \"offset\": -1,\n    \"seqNo\": -1,\n    \"enqueuedTime\": None,\n    \"isInclusive\": True\n    }\n\n    eventhub_source = SparkEventhubSource(\n        spark=spark,\n        options = {\n            \"eventhubs.connectionString\": connectionString,\n            \"eventhubs.consumerGroup\": \"{YOUR-CONSUMER-GROUP}\",\n            \"eventhubs.startingPosition\": json.dumps(startingEventPosition),\n            \"maxEventsPerTrigger\" : 1000\n        }\n    )\n\n    eventhub_source.read_stream()\n    ```\n    ```python\n     #Eventhub Source for Batch Queries\n\n    from rtdip_sdk.pipelines.sources import SparkEventhubSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n    import json\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    connectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n\n    startingEventPosition = {\n        \"offset\": -1,\n        \"seqNo\": -1,\n        \"enqueuedTime\": None,\n        \"isInclusive\": True\n    }\n\n    endingEventPosition = {\n        \"offset\": None,\n        \"seqNo\": -1,\n        \"enqueuedTime\": endTime,\n        \"isInclusive\": True\n    }\n\n    eventhub_source = SparkEventhubSource(\n        spark,\n        options = {\n            \"eventhubs.connectionString\": connectionString,\n            \"eventhubs.consumerGroup\": \"{YOUR-CONSUMER-GROUP}\",\n            \"eventhubs.startingPosition\": json.dumps(startingEventPosition),\n            \"eventhubs.endingPosition\": json.dumps(endingEventPosition)\n        }\n    )\n\n    eventhub_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session\n        options (dict): A dictionary of Eventhub configurations (See Attributes table below)\n\n    Attributes:\n        eventhubs.connectionString (str):  Eventhubs connection string is required to connect to the Eventhubs service. (Streaming and Batch)\n        eventhubs.consumerGroup (str): A consumer group is a view of an entire eventhub. Consumer groups enable multiple consuming applications to each have a separate view of the event stream, and to read the stream independently at their own pace and with their own offsets. (Streaming and Batch)\n        eventhubs.startingPosition (JSON str): The starting position for your Structured Streaming job. If a specific EventPosition is not set for a partition using startingPositions, then we use the EventPosition set in startingPosition. If nothing is set in either option, we will begin consuming from the end of the partition. (Streaming and Batch)\n        eventhubs.endingPosition: (JSON str): The ending position of a batch query. This works the same as startingPosition. (Batch)\n        maxEventsPerTrigger (long): Rate limit on maximum number of events processed per trigger interval. The specified total number of events will be proportionally split across partitions of different volume. (Stream)\n\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.schema = EVENTHUB_SCHEMA\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        spark_libraries = Libraries()\n        spark_libraries.add_maven_library(get_default_package(\"spark_azure_eventhub\"))\n        return spark_libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self) -&gt; bool:\n        return True\n\n    def post_read_validation(self, df: DataFrame) -&gt; bool:\n        assert df.schema == self.schema\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Reads batch data from Eventhubs.\n        \"\"\"\n        eventhub_connection_string = \"eventhubs.connectionString\"\n        try:\n            if eventhub_connection_string in self.options:\n                sc = self.spark.sparkContext\n                self.options[eventhub_connection_string] = (\n                    sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                        self.options[eventhub_connection_string]\n                    )\n                )\n\n            return self.spark.read.format(\"eventhubs\").options(**self.options).load()\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        Reads streaming data from Eventhubs.\n        \"\"\"\n        eventhub_connection_string = \"eventhubs.connectionString\"\n        try:\n            if eventhub_connection_string in self.options:\n                sc = self.spark.sparkContext\n                self.options[eventhub_connection_string] = (\n                    sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                        self.options[eventhub_connection_string]\n                    )\n                )\n\n            return (\n                self.spark.readStream.format(\"eventhubs\").options(**self.options).load()\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkEventhubSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/eventhub.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkEventhubSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads batch data from Eventhubs.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/eventhub.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Reads batch data from Eventhubs.\n    \"\"\"\n    eventhub_connection_string = \"eventhubs.connectionString\"\n    try:\n        if eventhub_connection_string in self.options:\n            sc = self.spark.sparkContext\n            self.options[eventhub_connection_string] = (\n                sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                    self.options[eventhub_connection_string]\n                )\n            )\n\n        return self.spark.read.format(\"eventhubs\").options(**self.options).load()\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkEventhubSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Reads streaming data from Eventhubs.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/eventhub.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    Reads streaming data from Eventhubs.\n    \"\"\"\n    eventhub_connection_string = \"eventhubs.connectionString\"\n    try:\n        if eventhub_connection_string in self.options:\n            sc = self.spark.sparkContext\n            self.options[eventhub_connection_string] = (\n                sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                    self.options[eventhub_connection_string]\n                )\n            )\n\n        return (\n            self.spark.readStream.format(\"eventhubs\").options(**self.options).load()\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkIoThubSource","title":"<code>SparkIoThubSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>This Spark source class is used to read batch or streaming data from an IoT Hub. IoT Hub configurations need to be specified as options in a dictionary. Additionally, there are more optional configurations which can be found here. If using startingPosition or endingPosition make sure to check out the Event Position section for more details and examples.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkIoThubSource--example","title":"Example","text":"<p><pre><code>#IoT Hub Source for Streaming Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkIoThubSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\nimport json\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconnectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n\nstartingEventPosition = {\n\"offset\": -1,\n\"seqNo\": -1,\n\"enqueuedTime\": None,\n\"isInclusive\": True\n}\n\niot_hub_source = SparkIoThubSource(\n    spark=spark,\n    options = {\n        \"eventhubs.connectionString\": connectionString,\n        \"eventhubs.consumerGroup\": \"{YOUR-CONSUMER-GROUP}\",\n        \"eventhubs.startingPosition\": json.dumps(startingEventPosition),\n        \"maxEventsPerTrigger\" : 1000\n    }\n)\n\niot_hub_source.read_stream()\n</code></pre> <pre><code> #IoT Hub Source for Batch Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkIoThubSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\nimport json\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconnectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n\nstartingEventPosition = {\n    \"offset\": -1,\n    \"seqNo\": -1,\n    \"enqueuedTime\": None,\n    \"isInclusive\": True\n}\n\nendingEventPosition = {\n    \"offset\": None,\n    \"seqNo\": -1,\n    \"enqueuedTime\": endTime,\n    \"isInclusive\": True\n}\n\niot_hub_source = SparkIoThubSource(\n    spark,\n    options = {\n        \"eventhubs.connectionString\": connectionString,\n        \"eventhubs.consumerGroup\": \"{YOUR-CONSUMER-GROUP}\",\n        \"eventhubs.startingPosition\": json.dumps(startingEventPosition),\n        \"eventhubs.endingPosition\": json.dumps(endingEventPosition)\n    }\n)\n\niot_hub_source.read_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session</p> required <code>options</code> <code>dict</code> <p>A dictionary of IoT Hub configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>eventhubs.connectionString</code> <code>str</code> <p>IoT Hub connection string is required to connect to the Eventhubs service. (Streaming and Batch)</p> <code>eventhubs.consumerGroup</code> <code>str</code> <p>A consumer group is a view of an entire IoT Hub. Consumer groups enable multiple consuming applications to each have a separate view of the event stream, and to read the stream independently at their own pace and with their own offsets. (Streaming and Batch)</p> <code>eventhubs.startingPosition</code> <code>JSON str</code> <p>The starting position for your Structured Streaming job. If a specific EventPosition is not set for a partition using startingPositions, then we use the EventPosition set in startingPosition. If nothing is set in either option, we will begin consuming from the end of the partition. (Streaming and Batch)</p> <code>eventhubs.endingPosition</code> <code>JSON str</code> <p>(JSON str): The ending position of a batch query. This works the same as startingPosition. (Batch)</p> <code>maxEventsPerTrigger</code> <code>long</code> <p>Rate limit on maximum number of events processed per trigger interval. The specified total number of events will be proportionally split across partitions of different volume. (Stream)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iot_hub.py</code> <pre><code>class SparkIoThubSource(SourceInterface):\n    \"\"\"\n    This Spark source class is used to read batch or streaming data from an IoT Hub. IoT Hub configurations need to be specified as options in a dictionary.\n    Additionally, there are more optional configurations which can be found [here.](https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/PySpark/structured-streaming-pyspark.md#event-hubs-configuration){ target=\"_blank\" }\n    If using startingPosition or endingPosition make sure to check out the **Event Position** section for more details and examples.\n\n    Example\n    --------\n    ```python\n    #IoT Hub Source for Streaming Queries\n\n    from rtdip_sdk.pipelines.sources import SparkIoThubSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n    import json\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    connectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n\n    startingEventPosition = {\n    \"offset\": -1,\n    \"seqNo\": -1,\n    \"enqueuedTime\": None,\n    \"isInclusive\": True\n    }\n\n    iot_hub_source = SparkIoThubSource(\n        spark=spark,\n        options = {\n            \"eventhubs.connectionString\": connectionString,\n            \"eventhubs.consumerGroup\": \"{YOUR-CONSUMER-GROUP}\",\n            \"eventhubs.startingPosition\": json.dumps(startingEventPosition),\n            \"maxEventsPerTrigger\" : 1000\n        }\n    )\n\n    iot_hub_source.read_stream()\n    ```\n    ```python\n     #IoT Hub Source for Batch Queries\n\n    from rtdip_sdk.pipelines.sources import SparkIoThubSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n    import json\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    connectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n\n    startingEventPosition = {\n        \"offset\": -1,\n        \"seqNo\": -1,\n        \"enqueuedTime\": None,\n        \"isInclusive\": True\n    }\n\n    endingEventPosition = {\n        \"offset\": None,\n        \"seqNo\": -1,\n        \"enqueuedTime\": endTime,\n        \"isInclusive\": True\n    }\n\n    iot_hub_source = SparkIoThubSource(\n        spark,\n        options = {\n            \"eventhubs.connectionString\": connectionString,\n            \"eventhubs.consumerGroup\": \"{YOUR-CONSUMER-GROUP}\",\n            \"eventhubs.startingPosition\": json.dumps(startingEventPosition),\n            \"eventhubs.endingPosition\": json.dumps(endingEventPosition)\n        }\n    )\n\n    iot_hub_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session\n        options (dict): A dictionary of IoT Hub configurations (See Attributes table below)\n\n    Attributes:\n        eventhubs.connectionString (str):  IoT Hub connection string is required to connect to the Eventhubs service. (Streaming and Batch)\n        eventhubs.consumerGroup (str): A consumer group is a view of an entire IoT Hub. Consumer groups enable multiple consuming applications to each have a separate view of the event stream, and to read the stream independently at their own pace and with their own offsets. (Streaming and Batch)\n        eventhubs.startingPosition (JSON str): The starting position for your Structured Streaming job. If a specific EventPosition is not set for a partition using startingPositions, then we use the EventPosition set in startingPosition. If nothing is set in either option, we will begin consuming from the end of the partition. (Streaming and Batch)\n        eventhubs.endingPosition: (JSON str): The ending position of a batch query. This works the same as startingPosition. (Batch)\n        maxEventsPerTrigger (long): Rate limit on maximum number of events processed per trigger interval. The specified total number of events will be proportionally split across partitions of different volume. (Stream)\n\n    \"\"\"\n\n    options: dict\n    spark: SparkSession\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.schema = EVENTHUB_SCHEMA\n        self.options = options\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    @staticmethod\n    def libraries():\n        spark_libraries = Libraries()\n        spark_libraries.add_maven_library(get_default_package(\"spark_azure_eventhub\"))\n        return spark_libraries\n\n    def pre_read_validation(self) -&gt; bool:\n        return True\n\n    def post_read_validation(self, df: DataFrame) -&gt; bool:\n        assert df.schema == self.schema\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Reads batch data from IoT Hubs.\n        \"\"\"\n        iothub_connection_string = \"eventhubs.connectionString\"\n        try:\n            if iothub_connection_string in self.options:\n                sc = self.spark.sparkContext\n                self.options[iothub_connection_string] = (\n                    sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                        self.options[iothub_connection_string]\n                    )\n                )\n\n            return self.spark.read.format(\"eventhubs\").options(**self.options).load()\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        Reads streaming data from IoT Hubs.\n        \"\"\"\n        iothub_connection_string = \"eventhubs.connectionString\"\n        try:\n            if iothub_connection_string in self.options:\n                sc = self.spark.sparkContext\n                self.options[iothub_connection_string] = (\n                    sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                        self.options[iothub_connection_string]\n                    )\n                )\n\n            return (\n                self.spark.readStream.format(\"eventhubs\").options(**self.options).load()\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkIoThubSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iot_hub.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkIoThubSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads batch data from IoT Hubs.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iot_hub.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Reads batch data from IoT Hubs.\n    \"\"\"\n    iothub_connection_string = \"eventhubs.connectionString\"\n    try:\n        if iothub_connection_string in self.options:\n            sc = self.spark.sparkContext\n            self.options[iothub_connection_string] = (\n                sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                    self.options[iothub_connection_string]\n                )\n            )\n\n        return self.spark.read.format(\"eventhubs\").options(**self.options).load()\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkIoThubSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Reads streaming data from IoT Hubs.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iot_hub.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    Reads streaming data from IoT Hubs.\n    \"\"\"\n    iothub_connection_string = \"eventhubs.connectionString\"\n    try:\n        if iothub_connection_string in self.options:\n            sc = self.spark.sparkContext\n            self.options[iothub_connection_string] = (\n                sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                    self.options[iothub_connection_string]\n                )\n            )\n\n        return (\n            self.spark.readStream.format(\"eventhubs\").options(**self.options).load()\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKafkaSource","title":"<code>SparkKafkaSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>This Spark source class is used to read batch or streaming data from Kafka. Required and optional configurations can be found in the Attributes tables below.</p> <p>Additionally, there are more optional configurations which can be found here.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKafkaSource--example","title":"Example","text":"<p><pre><code> #Kafka Source for Streaming Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkKafkaSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nkafka_source = SparkKafkaSource(\n    spark=spark,\n    options={\n        \"kafka.bootstrap.servers\": \"{HOST_1}:{PORT_1},{HOST_2}:{PORT_2}\",\n        \"subscribe\": \"{TOPIC_1},{TOPIC_2}\",\n        \"includeHeaders\", \"true\"\n    }\n)\n\nkafka_source.read_stream()\n</code></pre> <pre><code> #Kafka Source for Batch Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkKafkaSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nkafka_source = SparkKafkaSource(\n    spark=spark,\n    options={\n        \"kafka.bootstrap.servers\": \"{HOST_1}:{PORT_1},{HOST_2}:{PORT_2}\",\n        \"subscribe\": \"{TOPIC_1},{TOPIC_2}\",\n        \"startingOffsets\": \"earliest\",\n        \"endingOffsets\": \"latest\"\n    }\n)\n\nkafka_source.read_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session</p> required <code>options</code> <code>dict</code> <p>A dictionary of Kafka configurations (See Attributes tables below). For more information on configuration options see here</p> required <p>The following attributes are the most common configurations for Kafka.</p> <p>The only configuration that must be set for the Kafka source for both batch and streaming queries is listed below.</p> <p>Attributes:</p> Name Type Description <code>kafka.bootstrap.servers</code> <code>A comma-separated list of host\ufe30port</code> <p>The Kafka \"bootstrap.servers\" configuration. (Streaming and Batch)</p> <p>There are multiple ways of specifying which topics to subscribe to. You should provide only one of these attributes:</p> <p>Attributes:</p> Name Type Description <code>assign</code> <code>json string {\"topicA\"\ufe30[0,1],\"topicB\"\ufe30[2,4]}</code> <p>Specific TopicPartitions to consume. Only one of \"assign\", \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)</p> <code>subscribe</code> <code>A comma-separated list of topics</code> <p>The topic list to subscribe. Only one of \"assign\", \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)</p> <code>subscribePattern</code> <code>Java regex string</code> <p>The pattern used to subscribe to topic(s). Only one of \"assign, \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)</p> <p>The following configurations are optional:</p> <p>Attributes:</p> Name Type Description <code>startingTimestamp</code> <code>timestamp str</code> <p>The start point of timestamp when a query is started, a string specifying a starting timestamp for all partitions in topics being subscribed. Please refer the note on starting timestamp offset options below. (Streaming and Batch)</p> <code>startingOffsetsByTimestamp</code> <code>JSON str</code> <p>The start point of timestamp when a query is started, a json string specifying a starting timestamp for each TopicPartition. Please refer the note on starting timestamp offset options below. (Streaming and Batch)</p> <code>startingOffsets</code> <code>\"earliest\", \"latest\" (streaming only), or JSON string</code> <p>The start point when a query is started, either \"earliest\" which is from the earliest offsets, \"latest\" which is just from the latest offsets, or a json string specifying a starting offset for each TopicPartition. In the json, -2 as an offset can be used to refer to earliest, -1 to latest.</p> <code>endingTimestamp</code> <code>timestamp str</code> <p>The end point when a batch query is ended, a json string specifying an ending timestamp for all partitions in topics being subscribed. Please refer the note on ending timestamp offset options below. (Batch)</p> <code>endingOffsetsByTimestamp</code> <code>JSON str</code> <p>The end point when a batch query is ended, a json string specifying an ending timestamp for each TopicPartition. Please refer the note on ending timestamp offset options below. (Batch)</p> <code>endingOffsets</code> <code>latest or JSON str</code> <p>The end point when a batch query is ended, either \"latest\" which is just referred to the latest, or a json string specifying an ending offset for each TopicPartition. In the json, -1 as an offset can be used to refer to latest, and -2 (earliest) as an offset is not allowed. (Batch)</p> <code>maxOffsetsPerTrigger</code> <code>long</code> <p>Rate limit on maximum number of offsets processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume. (Streaming)</p> <code>minOffsetsPerTrigger</code> <code>long</code> <p>Minimum number of offsets to be processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume. (Streaming)</p> <code>failOnDataLoss</code> <code>bool</code> <p>Whether to fail the query when it's possible that data is lost (e.g., topics are deleted, or offsets are out of range). This may be a false alarm. You can disable it when it doesn't work as you expected.</p> <code>minPartitions</code> <code>int</code> <p>Desired minimum number of partitions to read from Kafka. By default, Spark has a 1-1 mapping of topicPartitions to Spark partitions consuming from Kafka. (Streaming and Batch)</p> <code>includeHeaders</code> <code>bool</code> <p>Whether to include the Kafka headers in the row. (Streaming and Batch)</p> <p>Starting Timestamp Offset Note</p> <p>If Kafka doesn't return the matched offset, the behavior will follow to the value of the option <code>startingOffsetsByTimestampStrategy</code>.</p> <p><code>startingTimestamp</code> takes precedence over <code>startingOffsetsByTimestamp</code> and startingOffsets.</p> <p>For streaming queries, this only applies when a new query is started, and that resuming will always pick up from where the query left off. Newly discovered partitions during a query will start at earliest.</p> <p>Ending Timestamp Offset Note</p> <p>If Kafka doesn't return the matched offset, the offset will be set to latest.</p> <p><code>endingOffsetsByTimestamp</code> takes precedence over <code>endingOffsets</code>.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kafka.py</code> <pre><code>class SparkKafkaSource(SourceInterface):\n    \"\"\"\n    This Spark source class is used to read batch or streaming data from Kafka. Required and optional configurations can be found in the Attributes tables below.\n\n    Additionally, there are more optional configurations which can be found [here.](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html){ target=\"_blank\" }\n\n    Example\n    --------\n    ```python\n     #Kafka Source for Streaming Queries\n\n    from rtdip_sdk.pipelines.sources import SparkKafkaSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    kafka_source = SparkKafkaSource(\n        spark=spark,\n        options={\n            \"kafka.bootstrap.servers\": \"{HOST_1}:{PORT_1},{HOST_2}:{PORT_2}\",\n            \"subscribe\": \"{TOPIC_1},{TOPIC_2}\",\n            \"includeHeaders\", \"true\"\n        }\n    )\n\n    kafka_source.read_stream()\n    ```\n    ```python\n     #Kafka Source for Batch Queries\n\n    from rtdip_sdk.pipelines.sources import SparkKafkaSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    kafka_source = SparkKafkaSource(\n        spark=spark,\n        options={\n            \"kafka.bootstrap.servers\": \"{HOST_1}:{PORT_1},{HOST_2}:{PORT_2}\",\n            \"subscribe\": \"{TOPIC_1},{TOPIC_2}\",\n            \"startingOffsets\": \"earliest\",\n            \"endingOffsets\": \"latest\"\n        }\n    )\n\n    kafka_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session\n        options (dict): A dictionary of Kafka configurations (See Attributes tables below). For more information on configuration options see [here](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html){ target=\"_blank\" }\n\n    The following attributes are the most common configurations for Kafka.\n\n    The only configuration that must be set for the Kafka source for both batch and streaming queries is listed below.\n\n    Attributes:\n        kafka.bootstrap.servers (A comma-separated list of host\ufe30port):  The Kafka \"bootstrap.servers\" configuration. (Streaming and Batch)\n\n    There are multiple ways of specifying which topics to subscribe to. You should provide only one of these attributes:\n\n    Attributes:\n        assign (json string {\"topicA\"\ufe30[0,1],\"topicB\"\ufe30[2,4]}):  Specific TopicPartitions to consume. Only one of \"assign\", \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)\n        subscribe (A comma-separated list of topics): The topic list to subscribe. Only one of \"assign\", \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)\n        subscribePattern (Java regex string): The pattern used to subscribe to topic(s). Only one of \"assign, \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)\n\n    The following configurations are optional:\n\n    Attributes:\n        startingTimestamp (timestamp str): The start point of timestamp when a query is started, a string specifying a starting timestamp for all partitions in topics being subscribed. Please refer the note on starting timestamp offset options below. (Streaming and Batch)\n        startingOffsetsByTimestamp (JSON str): The start point of timestamp when a query is started, a json string specifying a starting timestamp for each TopicPartition. Please refer the note on starting timestamp offset options below. (Streaming and Batch)\n        startingOffsets (\"earliest\", \"latest\" (streaming only), or JSON string): The start point when a query is started, either \"earliest\" which is from the earliest offsets, \"latest\" which is just from the latest offsets, or a json string specifying a starting offset for each TopicPartition. In the json, -2 as an offset can be used to refer to earliest, -1 to latest.\n        endingTimestamp (timestamp str): The end point when a batch query is ended, a json string specifying an ending timestamp for all partitions in topics being subscribed. Please refer the note on ending timestamp offset options below. (Batch)\n        endingOffsetsByTimestamp (JSON str): The end point when a batch query is ended, a json string specifying an ending timestamp for each TopicPartition. Please refer the note on ending timestamp offset options below. (Batch)\n        endingOffsets (latest or JSON str): The end point when a batch query is ended, either \"latest\" which is just referred to the latest, or a json string specifying an ending offset for each TopicPartition. In the json, -1 as an offset can be used to refer to latest, and -2 (earliest) as an offset is not allowed. (Batch)\n        maxOffsetsPerTrigger (long): Rate limit on maximum number of offsets processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume. (Streaming)\n        minOffsetsPerTrigger (long): Minimum number of offsets to be processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume. (Streaming)\n        failOnDataLoss (bool): Whether to fail the query when it's possible that data is lost (e.g., topics are deleted, or offsets are out of range). This may be a false alarm. You can disable it when it doesn't work as you expected.\n        minPartitions (int): Desired minimum number of partitions to read from Kafka. By default, Spark has a 1-1 mapping of topicPartitions to Spark partitions consuming from Kafka. (Streaming and Batch)\n        includeHeaders (bool): Whether to include the Kafka headers in the row. (Streaming and Batch)\n\n    !!! note \"Starting Timestamp Offset Note\"\n        If Kafka doesn't return the matched offset, the behavior will follow to the value of the option &lt;code&gt;startingOffsetsByTimestampStrategy&lt;/code&gt;.\n\n        &lt;code&gt;startingTimestamp&lt;/code&gt; takes precedence over &lt;code&gt;startingOffsetsByTimestamp&lt;/code&gt; and &lt;/code&gt;startingOffsets&lt;/code&gt;.\n\n        For streaming queries, this only applies when a new query is started, and that resuming will always pick up from where the query left off. Newly discovered partitions during a query will start at earliest.\n\n    !!! note \"Ending Timestamp Offset Note\"\n        If Kafka doesn't return the matched offset, the offset will be set to latest.\n\n        &lt;code&gt;endingOffsetsByTimestamp&lt;/code&gt; takes precedence over &lt;code&gt;endingOffsets&lt;/code&gt;.\n\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.schema = KAFKA_SCHEMA\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        spark_libraries = Libraries()\n        spark_libraries.add_maven_library(get_default_package(\"spark_sql_kafka\"))\n        return spark_libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self) -&gt; bool:\n        return True\n\n    def post_read_validation(self, df: DataFrame) -&gt; bool:\n        assert df.schema == self.schema\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Reads batch data from Kafka.\n        \"\"\"\n        try:\n            return self.spark.read.format(\"kafka\").options(**self.options).load()\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        Reads streaming data from Kafka.\n        \"\"\"\n        try:\n            return self.spark.readStream.format(\"kafka\").options(**self.options).load()\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKafkaSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kafka.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKafkaSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads batch data from Kafka.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kafka.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Reads batch data from Kafka.\n    \"\"\"\n    try:\n        return self.spark.read.format(\"kafka\").options(**self.options).load()\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKafkaSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Reads streaming data from Kafka.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kafka.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    Reads streaming data from Kafka.\n    \"\"\"\n    try:\n        return self.spark.readStream.format(\"kafka\").options(**self.options).load()\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKafkaEventhubSource","title":"<code>SparkKafkaEventhubSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>This Spark source class is used to read batch or streaming data from an Eventhub using the Kafka protocol. This enables Eventhubs to be used as a source in applications like Delta Live Tables or Databricks Serverless Jobs as the Spark Eventhubs JAR is not supported in these scenarios.</p> <p>The dataframe returned is transformed to ensure the schema is as close to the Eventhub Spark source as possible. There are some minor differences:</p> <ul> <li><code>offset</code> is dependent on <code>x-opt-offset</code> being populated in the headers provided. If this is not found in the headers, the value will be null</li> <li><code>publisher</code> is dependent on <code>x-opt-publisher</code> being populated in the headers provided. If this is not found in the headers, the value will be null</li> <li><code>partitionKey</code> is dependent on <code>x-opt-partition-key</code> being populated in the headers provided. If this is not found in the headers, the value will be null</li> <li><code>systemProperties</code> are identified according to the list provided in the Eventhub documentation and IoT Hub documentation</li> </ul> <p>Default settings will be specified if not provided in the <code>options</code> parameter:</p> <ul> <li><code>kafka.sasl.mechanism</code> will be set to <code>PLAIN</code></li> <li><code>kafka.security.protocol</code> will be set to <code>SASL_SSL</code></li> <li><code>kafka.request.timeout.ms</code> will be set to <code>60000</code></li> <li><code>kafka.session.timeout.ms</code> will be set to <code>60000</code></li> </ul>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKafkaEventhubSource--examples","title":"Examples","text":"<p><pre><code>#Kafka Source for Streaming Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkKafkaEventhubSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconnectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\nconsumerGroup = \"{YOUR-CONSUMER-GROUP}\"\n\nkafka_eventhub_source = SparkKafkaEventhubSource(\n    spark=spark,\n    options={\n        \"startingOffsets\": \"earliest\",\n        \"maxOffsetsPerTrigger\": 10000,\n        \"failOnDataLoss\": \"false\",\n    },\n    connection_string=connectionString,\n    consumer_group=\"consumerGroup\"\n)\n\nkafka_eventhub_source.read_stream()\n</code></pre> <pre><code>#Kafka Source for Batch Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkKafkaEventhubSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconnectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\nconsumerGroup = \"{YOUR-CONSUMER-GROUP}\"\n\nkafka_eventhub_source = SparkKafkaEventhubSource(\n    spark=spark,\n    options={\n        \"startingOffsets\": \"earliest\",\n        \"endingOffsets\": \"latest\",\n        \"failOnDataLoss\": \"false\"\n    },\n    connection_string=connectionString,\n    consumer_group=\"consumerGroup\"\n)\n\nkafka_eventhub_source.read_batch()\n</code></pre></p> <p>Required and optional configurations can be found in the Attributes and Parameter tables below. Additionally, there are more optional configurations which can be found here.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session</p> required <code>options</code> <code>dict</code> <p>A dictionary of Kafka configurations (See Attributes tables below). For more information on configuration options see here</p> required <code>connection_string</code> <code>str</code> <p>Eventhubs connection string is required to connect to the Eventhubs service. This must include the Eventhub name as the <code>EntityPath</code> parameter. Example <code>\"Endpoint=sb://test.servicebus.windows.net/;SharedAccessKeyName=test;SharedAccessKey=test_key;EntityPath=test_eventhub\"</code></p> required <code>consumer_group</code> <code>str</code> <p>The Eventhub consumer group to use for the connection</p> required <code>decode_kafka_headers_to_amqp_properties</code> <code>optional bool</code> <p>Perform decoding of Kafka headers into their AMQP properties. Default is True</p> <code>True</code> <p>The only configuration that must be set for the Kafka source for both batch and streaming queries is listed below.</p> <p>Attributes:</p> Name Type Description <code>kafka.bootstrap.servers</code> <code>A comma-separated list of host\ufe30port</code> <p>The Kafka \"bootstrap.servers\" configuration. (Streaming and Batch)</p> <p>There are multiple ways of specifying which topics to subscribe to. You should provide only one of these parameters:</p> <p>Attributes:</p> Name Type Description <code>assign</code> <code>json string {\"topicA\"\ufe30[0,1],\"topicB\"\ufe30[2,4]}</code> <p>Specific TopicPartitions to consume. Only one of \"assign\", \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)</p> <code>subscribe</code> <code>A comma-separated list of topics</code> <p>The topic list to subscribe. Only one of \"assign\", \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)</p> <code>subscribePattern</code> <code>Java regex string</code> <p>The pattern used to subscribe to topic(s). Only one of \"assign, \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)</p> <p>The following configurations are optional:</p> <p>Attributes:</p> Name Type Description <code>startingTimestamp</code> <code>timestamp str</code> <p>The start point of timestamp when a query is started, a string specifying a starting timestamp for all partitions in topics being subscribed. Please refer the note on starting timestamp offset options below. (Streaming and Batch)</p> <code>startingOffsetsByTimestamp</code> <code>JSON str</code> <p>The start point of timestamp when a query is started, a json string specifying a starting timestamp for each TopicPartition. Please refer the note on starting timestamp offset options below. (Streaming and Batch)</p> <code>startingOffsets</code> <code>\"earliest\", \"latest\" (streaming only), or JSON string</code> <p>The start point when a query is started, either \"earliest\" which is from the earliest offsets, \"latest\" which is just from the latest offsets, or a json string specifying a starting offset for each TopicPartition. In the json, -2 as an offset can be used to refer to earliest, -1 to latest.</p> <code>endingTimestamp</code> <code>timestamp str</code> <p>The end point when a batch query is ended, a json string specifying an ending timestamp for all partitions in topics being subscribed. Please refer the note on ending timestamp offset options below. (Batch)</p> <code>endingOffsetsByTimestamp</code> <code>JSON str</code> <p>The end point when a batch query is ended, a json string specifying an ending timestamp for each TopicPartition. Please refer the note on ending timestamp offset options below. (Batch)</p> <code>endingOffsets</code> <code>latest or JSON str</code> <p>The end point when a batch query is ended, either \"latest\" which is just referred to the latest, or a json string specifying an ending offset for each TopicPartition. In the json, -1 as an offset can be used to refer to latest, and -2 (earliest) as an offset is not allowed. (Batch)</p> <code>maxOffsetsPerTrigger</code> <code>long</code> <p>Rate limit on maximum number of offsets processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume. (Streaming)</p> <code>minOffsetsPerTrigger</code> <code>long</code> <p>Minimum number of offsets to be processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume. (Streaming)</p> <code>failOnDataLoss</code> <code>bool</code> <p>Whether to fail the query when it's possible that data is lost (e.g., topics are deleted, or offsets are out of range). This may be a false alarm. You can disable it when it doesn't work as you expected.</p> <code>minPartitions</code> <code>int</code> <p>Desired minimum number of partitions to read from Kafka. By default, Spark has a 1-1 mapping of topicPartitions to Spark partitions consuming from Kafka. (Streaming and Batch)</p> <code>includeHeaders</code> <code>bool</code> <p>Whether to include the Kafka headers in the row. (Streaming and Batch)</p> <p>Starting Timestamp Offset Note</p> <p>If Kafka doesn't return the matched offset, the behavior will follow to the value of the option <code>startingOffsetsByTimestampStrategy</code>.</p> <p><code>startingTimestamp</code> takes precedence over <code>startingOffsetsByTimestamp</code> and startingOffsets.</p> <p>For streaming queries, this only applies when a new query is started, and that resuming will always pick up from where the query left off. Newly discovered partitions during a query will start at earliest.</p> <p>Ending Timestamp Offset Note</p> <p>If Kafka doesn't return the matched offset, the offset will be set to latest.</p> <p><code>endingOffsetsByTimestamp</code> takes precedence over <code>endingOffsets</code>.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kafka_eventhub.py</code> <pre><code>class SparkKafkaEventhubSource(SourceInterface):\n    \"\"\"\n    This Spark source class is used to read batch or streaming data from an Eventhub using the Kafka protocol. This enables Eventhubs to be used as a source in applications like Delta Live Tables or Databricks Serverless Jobs as the Spark Eventhubs JAR is not supported in these scenarios.\n\n    The dataframe returned is transformed to ensure the schema is as close to the Eventhub Spark source as possible. There are some minor differences:\n\n    - `offset` is dependent on `x-opt-offset` being populated in the headers provided. If this is not found in the headers, the value will be null\n    - `publisher` is dependent on `x-opt-publisher` being populated in the headers provided. If this is not found in the headers, the value will be null\n    - `partitionKey` is dependent on `x-opt-partition-key` being populated in the headers provided. If this is not found in the headers, the value will be null\n    - `systemProperties` are identified according to the list provided in the [Eventhub documentation](https://learn.microsoft.com/en-us/azure/data-explorer/ingest-data-event-hub-overview#event-system-properties-mapping){ target=\"_blank\" } and [IoT Hub documentation](https://learn.microsoft.com/en-us/azure/data-explorer/ingest-data-iot-hub-overview#event-system-properties-mapping){ target=\"_blank\" }\n\n    Default settings will be specified if not provided in the `options` parameter:\n\n    - `kafka.sasl.mechanism` will be set to `PLAIN`\n    - `kafka.security.protocol` will be set to `SASL_SSL`\n    - `kafka.request.timeout.ms` will be set to `60000`\n    - `kafka.session.timeout.ms` will be set to `60000`\n\n    Examples\n    --------\n    ```python\n    #Kafka Source for Streaming Queries\n\n    from rtdip_sdk.pipelines.sources import SparkKafkaEventhubSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    connectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n    consumerGroup = \"{YOUR-CONSUMER-GROUP}\"\n\n    kafka_eventhub_source = SparkKafkaEventhubSource(\n        spark=spark,\n        options={\n            \"startingOffsets\": \"earliest\",\n            \"maxOffsetsPerTrigger\": 10000,\n            \"failOnDataLoss\": \"false\",\n        },\n        connection_string=connectionString,\n        consumer_group=\"consumerGroup\"\n    )\n\n    kafka_eventhub_source.read_stream()\n    ```\n    ```python\n    #Kafka Source for Batch Queries\n\n    from rtdip_sdk.pipelines.sources import SparkKafkaEventhubSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    connectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n    consumerGroup = \"{YOUR-CONSUMER-GROUP}\"\n\n    kafka_eventhub_source = SparkKafkaEventhubSource(\n        spark=spark,\n        options={\n            \"startingOffsets\": \"earliest\",\n            \"endingOffsets\": \"latest\",\n            \"failOnDataLoss\": \"false\"\n        },\n        connection_string=connectionString,\n        consumer_group=\"consumerGroup\"\n    )\n\n    kafka_eventhub_source.read_batch()\n    ```\n\n    Required and optional configurations can be found in the Attributes and Parameter tables below.\n    Additionally, there are more optional configurations which can be found [here.](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html){ target=\"_blank\" }\n\n    Parameters:\n        spark (SparkSession): Spark Session\n        options (dict): A dictionary of Kafka configurations (See Attributes tables below). For more information on configuration options see [here](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html){ target=\"_blank\" }\n        connection_string (str): Eventhubs connection string is required to connect to the Eventhubs service. This must include the Eventhub name as the `EntityPath` parameter. Example `\"Endpoint=sb://test.servicebus.windows.net/;SharedAccessKeyName=test;SharedAccessKey=test_key;EntityPath=test_eventhub\"`\n        consumer_group (str): The Eventhub consumer group to use for the connection\n        decode_kafka_headers_to_amqp_properties (optional bool): Perform decoding of Kafka headers into their AMQP properties. Default is True\n\n    The only configuration that must be set for the Kafka source for both batch and streaming queries is listed below.\n\n    Attributes:\n        kafka.bootstrap.servers (A comma-separated list of host\ufe30port):  The Kafka \"bootstrap.servers\" configuration. (Streaming and Batch)\n\n    There are multiple ways of specifying which topics to subscribe to. You should provide only one of these parameters:\n\n    Attributes:\n        assign (json string {\"topicA\"\ufe30[0,1],\"topicB\"\ufe30[2,4]}):  Specific TopicPartitions to consume. Only one of \"assign\", \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)\n        subscribe (A comma-separated list of topics): The topic list to subscribe. Only one of \"assign\", \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)\n        subscribePattern (Java regex string): The pattern used to subscribe to topic(s). Only one of \"assign, \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)\n\n    The following configurations are optional:\n\n    Attributes:\n        startingTimestamp (timestamp str): The start point of timestamp when a query is started, a string specifying a starting timestamp for all partitions in topics being subscribed. Please refer the note on starting timestamp offset options below. (Streaming and Batch)\n        startingOffsetsByTimestamp (JSON str): The start point of timestamp when a query is started, a json string specifying a starting timestamp for each TopicPartition. Please refer the note on starting timestamp offset options below. (Streaming and Batch)\n        startingOffsets (\"earliest\", \"latest\" (streaming only), or JSON string): The start point when a query is started, either \"earliest\" which is from the earliest offsets, \"latest\" which is just from the latest offsets, or a json string specifying a starting offset for each TopicPartition. In the json, -2 as an offset can be used to refer to earliest, -1 to latest.\n        endingTimestamp (timestamp str): The end point when a batch query is ended, a json string specifying an ending timestamp for all partitions in topics being subscribed. Please refer the note on ending timestamp offset options below. (Batch)\n        endingOffsetsByTimestamp (JSON str): The end point when a batch query is ended, a json string specifying an ending timestamp for each TopicPartition. Please refer the note on ending timestamp offset options below. (Batch)\n        endingOffsets (latest or JSON str): The end point when a batch query is ended, either \"latest\" which is just referred to the latest, or a json string specifying an ending offset for each TopicPartition. In the json, -1 as an offset can be used to refer to latest, and -2 (earliest) as an offset is not allowed. (Batch)\n        maxOffsetsPerTrigger (long): Rate limit on maximum number of offsets processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume. (Streaming)\n        minOffsetsPerTrigger (long): Minimum number of offsets to be processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume. (Streaming)\n        failOnDataLoss (bool): Whether to fail the query when it's possible that data is lost (e.g., topics are deleted, or offsets are out of range). This may be a false alarm. You can disable it when it doesn't work as you expected.\n        minPartitions (int): Desired minimum number of partitions to read from Kafka. By default, Spark has a 1-1 mapping of topicPartitions to Spark partitions consuming from Kafka. (Streaming and Batch)\n        includeHeaders (bool): Whether to include the Kafka headers in the row. (Streaming and Batch)\n\n    !!! note \"Starting Timestamp Offset Note\"\n        If Kafka doesn't return the matched offset, the behavior will follow to the value of the option &lt;code&gt;startingOffsetsByTimestampStrategy&lt;/code&gt;.\n\n        &lt;code&gt;startingTimestamp&lt;/code&gt; takes precedence over &lt;code&gt;startingOffsetsByTimestamp&lt;/code&gt; and &lt;/code&gt;startingOffsets&lt;/code&gt;.\n\n        For streaming queries, this only applies when a new query is started, and that resuming will always pick up from where the query left off. Newly discovered partitions during a query will start at earliest.\n\n    !!! note \"Ending Timestamp Offset Note\"\n        If Kafka doesn't return the matched offset, the offset will be set to latest.\n\n        &lt;code&gt;endingOffsetsByTimestamp&lt;/code&gt; takes precedence over &lt;code&gt;endingOffsets&lt;/code&gt;.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        options: dict,\n        connection_string: str,\n        consumer_group: str,\n        decode_kafka_headers_to_amqp_properties: bool = True,\n    ) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.connection_string = connection_string\n        self.consumer_group = consumer_group\n        self.decode_kafka_headers_to_amqp_properties = (\n            decode_kafka_headers_to_amqp_properties\n        )\n        self.connection_string_properties = self._parse_connection_string(\n            connection_string\n        )\n        self.schema = KAFKA_EVENTHUB_SCHEMA\n        self.options = self._configure_options(options)\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        spark_libraries = Libraries()\n        spark_libraries.add_maven_library(get_default_package(\"spark_sql_kafka\"))\n        return spark_libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self) -&gt; bool:\n        return True\n\n    def post_read_validation(self, df: DataFrame) -&gt; bool:\n        assert df.schema == self.schema\n        return True\n\n    # Code is from Azure Eventhub Python SDK. Will import the package if possible with Conda in the  conda-forge channel in the future\n    def _parse_connection_string(self, connection_string: str):\n        conn_settings = [s.split(\"=\", 1) for s in connection_string.split(\";\")]\n        if any(len(tup) != 2 for tup in conn_settings):\n            raise ValueError(\"Connection string is either blank or malformed.\")\n        conn_settings = dict(conn_settings)\n        shared_access_signature = None\n        for key, value in conn_settings.items():\n            if key.lower() == \"sharedaccesssignature\":\n                shared_access_signature = value\n        shared_access_key = conn_settings.get(\"SharedAccessKey\")\n        shared_access_key_name = conn_settings.get(\"SharedAccessKeyName\")\n        if any([shared_access_key, shared_access_key_name]) and not all(\n            [shared_access_key, shared_access_key_name]\n        ):\n            raise ValueError(\n                \"Connection string must have both SharedAccessKeyName and SharedAccessKey.\"\n            )\n        if shared_access_signature is not None and shared_access_key is not None:\n            raise ValueError(\n                \"Only one of the SharedAccessKey or SharedAccessSignature must be present.\"\n            )\n        endpoint = conn_settings.get(\"Endpoint\")\n        if not endpoint:\n            raise ValueError(\"Connection string is either blank or malformed.\")\n        parsed = urlparse(endpoint.rstrip(\"/\"))\n        if not parsed.netloc:\n            raise ValueError(\"Invalid Endpoint on the Connection String.\")\n        namespace = parsed.netloc.strip()\n        properties = {\n            \"fully_qualified_namespace\": namespace,\n            \"endpoint\": endpoint,\n            \"eventhub_name\": conn_settings.get(\"EntityPath\"),\n            \"shared_access_signature\": shared_access_signature,\n            \"shared_access_key_name\": shared_access_key_name,\n            \"shared_access_key\": shared_access_key,\n        }\n        return properties\n\n    def _connection_string_builder(self, properties: dict) -&gt; str:\n        connection_string = \"Endpoint=\" + properties.get(\"endpoint\") + \";\"\n\n        if properties.get(\"shared_access_key\"):\n            connection_string += (\n                \"SharedAccessKey=\" + properties.get(\"shared_access_key\") + \";\"\n            )\n\n        if properties.get(\"shared_access_key_name\"):\n            connection_string += (\n                \"SharedAccessKeyName=\" + properties.get(\"shared_access_key_name\") + \";\"\n            )\n\n        if properties.get(\"shared_access_signature\"):\n            connection_string += (\n                \"SharedAccessSignature=\"\n                + properties.get(\"shared_access_signature\")\n                + \";\"\n            )\n        return connection_string\n\n    def _configure_options(self, options: dict) -&gt; dict:\n        if \"subscribe\" not in options:\n            options[\"subscribe\"] = self.connection_string_properties.get(\n                \"eventhub_name\"\n            )\n\n        if \"kafka.bootstrap.servers\" not in options:\n            options[\"kafka.bootstrap.servers\"] = (\n                self.connection_string_properties.get(\"fully_qualified_namespace\")\n                + \":9093\"\n            )\n\n        if \"kafka.sasl.mechanism\" not in options:\n            options[\"kafka.sasl.mechanism\"] = \"PLAIN\"\n\n        if \"kafka.security.protocol\" not in options:\n            options[\"kafka.security.protocol\"] = \"SASL_SSL\"\n\n        if \"kafka.sasl.jaas.config\" not in options:\n            kafka_package = \"org.apache.kafka.common.security.plain.PlainLoginModule\"\n            if \"DATABRICKS_RUNTIME_VERSION\" in os.environ or (\n                \"_client\" in self.spark.__dict__\n                and \"databricks\" in self.spark.client.host\n            ):\n                kafka_package = \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule\"\n            connection_string = self._connection_string_builder(\n                self.connection_string_properties\n            )\n            options[\"kafka.sasl.jaas.config\"] = (\n                '{} required username=\"$ConnectionString\" password=\"{}\";'.format(\n                    kafka_package, connection_string\n                )\n            )  # NOSONAR\n\n        if \"kafka.request.timeout.ms\" not in options:\n            options[\"kafka.request.timeout.ms\"] = \"60000\"\n\n        if \"kafka.session.timeout.ms\" not in options:\n            options[\"kafka.session.timeout.ms\"] = \"60000\"\n\n        if \"kafka.group.id\" not in options:\n            options[\"kafka.group.id\"] = self.consumer_group\n\n        options[\"includeHeaders\"] = \"true\"\n\n        return options\n\n    def _transform_to_eventhub_schema(self, df: DataFrame) -&gt; DataFrame:\n        return (\n            df.withColumn(\"headers\", map_from_entries(col(\"headers\")))\n            .select(\n                col(\"value\").alias(\"body\"),\n                col(\"partition\").cast(\"string\"),\n                col(\"offset\").alias(\"sequenceNumber\"),\n                col(\"timestamp\").alias(\"enqueuedTime\"),\n                (\n                    decode_kafka_headers_to_amqp_properties(col(\"headers\")).alias(\n                        \"properties\"\n                    )\n                    if self.decode_kafka_headers_to_amqp_properties\n                    else create_map().cast(\"map&lt;string,string&gt;\").alias(\"properties\")\n                ),\n            )\n            .withColumn(\"offset\", col(\"properties\").getItem(\"x-opt-offset\"))\n            .withColumn(\"publisher\", col(\"properties\").getItem(\"x-opt-publisher\"))\n            .withColumn(\n                \"partitionKey\", col(\"properties\").getItem(\"x-opt-partition-key\")\n            )\n            .withColumn(\n                \"systemProperties\",\n                map_filter(\n                    col(\"properties\"), lambda k, _: k.isin(eventhub_system_properties)\n                ),\n            )\n            .withColumn(\n                \"properties\",\n                map_filter(\n                    col(\"properties\"), lambda k, _: ~k.isin(eventhub_system_properties)\n                ),\n            )\n            .select(\n                col(\"body\"),\n                col(\"partition\"),\n                col(\"offset\"),\n                col(\"sequenceNumber\"),\n                col(\"enqueuedTime\"),\n                col(\"publisher\"),\n                col(\"partitionKey\"),\n                col(\"properties\"),\n                col(\"systemProperties\"),\n            )\n        )\n\n    def read_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Reads batch data from Kafka.\n        \"\"\"\n        try:\n            df = self.spark.read.format(\"kafka\").options(**self.options).load()\n            return self._transform_to_eventhub_schema(df)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        Reads streaming data from Kafka.\n        \"\"\"\n        try:\n            df = self.spark.readStream.format(\"kafka\").options(**self.options).load()\n            return self._transform_to_eventhub_schema(df)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKafkaEventhubSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kafka_eventhub.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKafkaEventhubSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads batch data from Kafka.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kafka_eventhub.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Reads batch data from Kafka.\n    \"\"\"\n    try:\n        df = self.spark.read.format(\"kafka\").options(**self.options).load()\n        return self._transform_to_eventhub_schema(df)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKafkaEventhubSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Reads streaming data from Kafka.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kafka_eventhub.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    Reads streaming data from Kafka.\n    \"\"\"\n    try:\n        df = self.spark.readStream.format(\"kafka\").options(**self.options).load()\n        return self._transform_to_eventhub_schema(df)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKinesisSource","title":"<code>SparkKinesisSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>The Spark Kinesis Source is used to read data from Kinesis in a Databricks environment. Structured streaming from Kinesis is not supported in open source Spark.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKinesisSource--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import SparkKinesisSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nkinesis_source = SparkKinesisSource(\n    spark=spark,\n    options={\n        \"awsAccessKey\": \"{AWS-ACCESS-KEY}\",\n        \"awsSecretKey\": \"{AWS-SECRET-KEY}\",\n        \"streamName\": \"{STREAM-NAME}\",\n        \"region\": \"{REGION}\",\n        \"endpoint\": \"https://kinesis.{REGION}.amazonaws.com\",\n        \"initialPosition\": \"earliest\"\n    }\n)\n\nkinesis_source.read_stream()\n\nOR\n\nkinesis_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from Kinesis</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for a Kinesis read operation (See Attributes table below). Further information on the options is available here</p> required <p>Attributes:</p> Name Type Description <code>awsAccessKey</code> <code>str</code> <p>AWS access key.</p> <code>awsSecretKey</code> <code>str</code> <p>AWS secret access key corresponding to the access key.</p> <code>streamName</code> <code>List[str]</code> <p>The stream names to subscribe to.</p> <code>region</code> <code>str</code> <p>The region the streams are defined in.</p> <code>endpoint</code> <code>str</code> <p>The regional endpoint for Kinesis Data Streams.</p> <code>initialPosition</code> <code>str</code> <p>The point to start reading from; earliest, latest, or at_timestamp.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kinesis.py</code> <pre><code>class SparkKinesisSource(SourceInterface):\n    \"\"\"\n    The Spark Kinesis Source is used to read data from Kinesis in a Databricks environment.\n    Structured streaming from Kinesis is **not** supported in open source Spark.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.sources import SparkKinesisSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    kinesis_source = SparkKinesisSource(\n        spark=spark,\n        options={\n            \"awsAccessKey\": \"{AWS-ACCESS-KEY}\",\n            \"awsSecretKey\": \"{AWS-SECRET-KEY}\",\n            \"streamName\": \"{STREAM-NAME}\",\n            \"region\": \"{REGION}\",\n            \"endpoint\": \"https://kinesis.{REGION}.amazonaws.com\",\n            \"initialPosition\": \"earliest\"\n        }\n    )\n\n    kinesis_source.read_stream()\n\n    OR\n\n    kinesis_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to read data from Kinesis\n        options (dict): Options that can be specified for a Kinesis read operation (See Attributes table below). Further information on the options is available [here](https://docs.databricks.com/structured-streaming/kinesis.html#configuration){ target=\"_blank\" }\n\n    Attributes:\n        awsAccessKey (str): AWS access key.\n        awsSecretKey (str): AWS secret access key corresponding to the access key.\n        streamName (List[str]): The stream names to subscribe to.\n        region (str): The region the streams are defined in.\n        endpoint (str): The regional endpoint for Kinesis Data Streams.\n        initialPosition (str): The point to start reading from; earliest, latest, or at_timestamp.\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.schema = KINESIS_SCHEMA\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK_DATABRICKS\n        \"\"\"\n        return SystemType.PYSPARK_DATABRICKS\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self, df: DataFrame) -&gt; bool:\n        assert df.schema == self.schema\n        return True\n\n    def read_batch(self):\n        \"\"\"\n        Raises:\n            NotImplementedError: Kinesis only supports streaming reads. To perform a batch read, use the read_stream method of this component and specify the Trigger on the write_stream to be `availableNow=True` to perform batch-like reads of cloud storage files.\n        \"\"\"\n        raise NotImplementedError(\n            \"Kinesis only supports streaming reads. To perform a batch read, use the read_stream method and specify Trigger on the write_stream as `availableNow=True`\"\n        )\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        Reads streaming data from Kinesis. All of the data in the table is processed as well as any new data that arrives after the stream started.\n        \"\"\"\n        try:\n            return (\n                self.spark.readStream.format(\"kinesis\").options(**self.options).load()\n            )\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKinesisSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK_DATABRICKS</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kinesis.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK_DATABRICKS\n    \"\"\"\n    return SystemType.PYSPARK_DATABRICKS\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKinesisSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Kinesis only supports streaming reads. To perform a batch read, use the read_stream method of this component and specify the Trigger on the write_stream to be <code>availableNow=True</code> to perform batch-like reads of cloud storage files.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kinesis.py</code> <pre><code>def read_batch(self):\n    \"\"\"\n    Raises:\n        NotImplementedError: Kinesis only supports streaming reads. To perform a batch read, use the read_stream method of this component and specify the Trigger on the write_stream to be `availableNow=True` to perform batch-like reads of cloud storage files.\n    \"\"\"\n    raise NotImplementedError(\n        \"Kinesis only supports streaming reads. To perform a batch read, use the read_stream method and specify Trigger on the write_stream as `availableNow=True`\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKinesisSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Reads streaming data from Kinesis. All of the data in the table is processed as well as any new data that arrives after the stream started.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kinesis.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    Reads streaming data from Kinesis. All of the data in the table is processed as well as any new data that arrives after the stream started.\n    \"\"\"\n    try:\n        return (\n            self.spark.readStream.format(\"kinesis\").options(**self.options).load()\n        )\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.BaseISOSource","title":"<code>BaseISOSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>class BaseISOSource(SourceInterface):\n    \"\"\"\n    Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options: list = []\n    spark_schema = StructType([StructField(\"id\", IntegerType(), True)])\n    default_query_timezone: str = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.query_timezone = pytz.timezone(\n            self.options.get(\"query_timezone\", self.default_query_timezone)\n        )\n        self.current_date = datetime.now(timezone.utc).astimezone(self.query_timezone)\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n        \"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.iso_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - {url}\")\n\n        response = requests.get(url)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n\n        return response.content\n\n    def _get_localized_datetime(self, datetime_str: str) -&gt; datetime:\n        \"\"\"\n        Converts string datetime into Python datetime object with configured format and timezone.\n        Args:\n            datetime_str: String to be converted into datetime.\n\n        Returns: Timezone aware datetime object.\n\n        \"\"\"\n        parsed_dt = datetime.strptime(datetime_str, self.query_datetime_format)\n        parsed_dt = parsed_dt.replace(tzinfo=self.query_timezone)\n        return parsed_dt\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Hits the fetch_from_url method with certain parameters to get raw data from API.\n\n        All the children ISO classes must override this method and call the fetch_url method\n        in it.\n\n        Returns:\n             Raw DataFrame from API.\n        \"\"\"\n\n        return pd.read_csv(BytesIO(self._fetch_from_url(\"\")))\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Performs all the basic transformations to prepare data for further processing.\n        All the children ISO classes must override this method.\n\n        Args:\n            df: Raw DataFrame, received from the API.\n\n        Returns:\n             Modified DataFrame, ready for basic use.\n\n        \"\"\"\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Another data transformation helper method to be called after prepare data.\n        Used for advance data processing such as cleaning, filtering, restructuring.\n        All the children ISO classes must override this method if there is any post-processing required.\n\n        Args:\n            df: Initial modified version of DataFrame, received after preparing the data.\n\n        Returns:\n             Final version of data after all the fixes and modifications.\n\n        \"\"\"\n        return df\n\n    def _get_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Entrypoint method to return the final version of DataFrame.\n\n        Returns:\n            Modified form of data for specific use case.\n\n        \"\"\"\n        df = self._pull_data()\n        df = self._prepare_data(df)\n        df = self._sanitize_data(df)\n\n        # Reorder columns to keep the data consistent\n        df = df[self.spark_schema.names]\n\n        return df\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Performs all the options checks. Raises exception in case of any invalid value.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        return True\n\n    def pre_read_validation(self) -&gt; bool:\n        \"\"\"\n        Ensures all the required options are provided and performs other validations.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        for key in self.required_options:\n            if key not in self.options:\n                raise ValueError(f\"Required option `{key}` is missing.\")\n\n        return self._validate_options()\n\n    def post_read_validation(self) -&gt; bool:\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n        Returns:\n             Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n        \"\"\"\n\n        try:\n            self.pre_read_validation()\n            pdf = self._get_data()\n            pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n            # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n            pd.DataFrame.iteritems = pd.DataFrame.items\n            df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n            return df\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n        Returns:\n             Final Spark DataFrame after all the processing.\n\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.BaseISOSource.pre_read_validation","title":"<code>pre_read_validation()</code>","text":"<p>Ensures all the required options are provided and performs other validations. Returns:      True if all checks are passed.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def pre_read_validation(self) -&gt; bool:\n    \"\"\"\n    Ensures all the required options are provided and performs other validations.\n    Returns:\n         True if all checks are passed.\n\n    \"\"\"\n    for key in self.required_options:\n        if key not in self.options:\n            raise ValueError(f\"Required option `{key}` is missing.\")\n\n    return self._validate_options()\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.BaseISOSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data. Returns:      Final Spark DataFrame converted from Pandas DataFrame post-execution.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n    Returns:\n         Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n    \"\"\"\n\n    try:\n        self.pre_read_validation()\n        pdf = self._get_data()\n        pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n        # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n        pd.DataFrame.iteritems = pd.DataFrame.items\n        df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n        return df\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.BaseISOSource.read_stream","title":"<code>read_stream()</code>","text":"<p>By default, the streaming operation is not supported but child classes can override if ISO supports streaming.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Final Spark DataFrame after all the processing.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n    Returns:\n         Final Spark DataFrame after all the processing.\n\n    \"\"\"\n\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.ERCOTDailyLoadISOSource","title":"<code>ERCOTDailyLoadISOSource</code>","text":"<p>               Bases: <code>BaseISOSource</code></p> <p>The ERCOT Daily Load ISO Source is used to read daily load data from ERCOT using WebScrapping. It supports actual and forecast data. To read more about the reports, visit the following URLs (The urls are only accessible if the requester/client is in US)-</p> <p>For load type <code>actual</code>: Actual System Load by Weather Zone  For load type <code>forecast</code>: Seven-Day Load Forecast by Weather Zone</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>load_type</code> <code>list</code> <p>Must be one of <code>actual</code> or <code>forecast</code>.</p> <code>date</code> <code>str</code> <p>Must be in <code>YYYY-MM-DD</code> format.</p> <code>certificate_pfx_key</code> <code>str</code> <p>The certificate key data or password received from ERCOT.</p> <code>certificate_pfx_key_contents</code> <code>str</code> <p>The certificate data received from ERCOT, it could be base64 encoded.</p> <p>Please check the BaseISOSource for available methods.</p> BaseISOSource Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/ercot_daily_load_iso.py</code> <pre><code>class ERCOTDailyLoadISOSource(BaseISOSource):\n    \"\"\"\n    The ERCOT Daily Load ISO Source is used to read daily load data from ERCOT using WebScrapping.\n    It supports actual and forecast data. To read more about the reports, visit the following URLs\n    (The urls are only accessible if the requester/client is in US)-\n\n    For load type `actual`: [Actual System Load by Weather Zone](https://www.ercot.com/mp/data-products/\n    data-product-details?id=NP6-345-CD)\n    &lt;br&gt;\n    For load type `forecast`: [Seven-Day Load Forecast by Weather Zone](https://www.ercot.com/mp/data-products/\n    data-product-details?id=NP3-561-CD)\n\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations (See Attributes table below)\n\n    Attributes:\n        load_type (list): Must be one of `actual` or `forecast`.\n        date (str): Must be in `YYYY-MM-DD` format.\n        certificate_pfx_key (str): The certificate key data or password received from ERCOT.\n        certificate_pfx_key_contents (str): The certificate data received from ERCOT, it could be base64 encoded.\n\n    Please check the BaseISOSource for available methods.\n\n    BaseISOSource:\n        ::: src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    url_forecast: str = \"https://mis.ercot.com/misapp/GetReports.do?reportTypeId=12312\"\n    url_actual: str = \"https://mis.ercot.com/misapp/GetReports.do?reportTypeId=13101\"\n    url_prefix: str = \"https://mis.ercot.com\"\n    query_datetime_format: str = \"%Y-%m-%d\"\n    required_options = [\n        \"load_type\",\n        \"date\",\n        \"certificate_pfx_key\",\n        \"certificate_pfx_key_contents\",\n    ]\n    spark_schema = ERCOT_SCHEMA\n    default_query_timezone = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super().__init__(spark, options)\n        self.spark = spark\n        self.options = options\n        self.load_type = self.options.get(\"load_type\", \"actual\")\n        self.date = self.options.get(\"date\", \"\").strip()\n        self.certificate_pfx_key = self.options.get(\"certificate_pfx_key\", \"\").strip()\n        self.certificate_pfx_key_contents = self.options.get(\n            \"certificate_pfx_key_contents\", \"\"\n        ).strip()\n\n    def generate_temp_client_cert_files_from_pfx(self):\n        password = self.certificate_pfx_key.encode()\n        pfx: bytes = base64.b64decode(self.certificate_pfx_key_contents)\n\n        if base64.b64encode(pfx) != self.certificate_pfx_key_contents.encode():\n            pfx = self.certificate_pfx_key_contents\n\n        key, cert, _ = pkcs12.load_key_and_certificates(data=pfx, password=password)\n        key_bytes = key.private_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PrivateFormat.TraditionalOpenSSL,\n            encryption_algorithm=serialization.NoEncryption(),\n        )\n\n        cert_bytes = cert.public_bytes(encoding=serialization.Encoding.PEM)\n        return TempCertFiles(cert_bytes, key_bytes)\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Pulls data from the ERCOT API and parses the zip files for CSV data.\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n\n        logging.info(f\"Getting {self.load_type} data for date {self.date}\")\n        url = self.url_forecast\n        req_date = datetime.strptime(self.date, self.query_datetime_format)\n\n        if self.load_type == \"actual\":\n            req_date = req_date + timedelta(days=1)\n            url = self.url_actual\n\n        url_lists, files = self.generate_urls_for_zip(url, req_date)\n        dfs = []\n        logging.info(f\"Generated {len(url_lists)} URLs - {url_lists}\")\n        logging.info(f\"Requesting files - {files}\")\n\n        for url in url_lists:\n            df = self.download_zip(url)\n            dfs.append(df)\n        final_df = pd.concat(dfs)\n        return final_df\n\n    def download_zip(self, url) -&gt; pd.DataFrame:\n        logging.info(f\"Downloading zip using {url}\")\n        with self.generate_temp_client_cert_files_from_pfx() as cert:\n            response = requests.get(url, cert=cert)\n\n        if not response.content:\n            raise HTTPError(\"Empty Response was returned\")\n\n        logging.info(\"Unzipping the file\")\n        zf = ZipFile(BytesIO(response.content))\n        csvs = [s for s in zf.namelist() if \".csv\" in s]\n\n        if len(csvs) == 0:\n            raise ValueError(\"No data was found in the specified interval\")\n\n        df = pd.read_csv(zf.open(csvs[0]))\n        return df\n\n    def generate_urls_for_zip(self, url: str, date: datetime) -&gt; (List[str], List[str]):\n        logging.info(f\"Finding urls list for date {date}\")\n        with self.generate_temp_client_cert_files_from_pfx() as cert:\n            page_response = requests.get(url, timeout=5, cert=cert)\n\n        page_content = BeautifulSoup(page_response.content, \"html.parser\")\n        zip_info = []\n        length = len(page_content.find_all(\"td\", {\"class\": \"labelOptional_ind\"}))\n\n        for i in range(0, length):\n            zip_name = page_content.find_all(\"td\", {\"class\": \"labelOptional_ind\"})[\n                i\n            ].text\n            zip_link = page_content.find_all(\"a\")[i].get(\"href\")\n            zip_info.append((zip_name, zip_link))\n\n        date_str = date.strftime(\"%Y%m%d\")\n        zip_info = list(\n            filter(\n                lambda f_info: f_info[0].endswith(\"csv.zip\") and date_str in f_info[0],\n                zip_info,\n            )\n        )\n\n        urls = []\n        files = []\n\n        if len(zip_info) == 0:\n            raise ValueError(f\"No file was found for date - {date_str}\")\n\n        # As Forecast is generated every hour, pick the latest one.\n        zip_info = sorted(zip_info, key=lambda item: item[0], reverse=True)\n        zip_info_item = zip_info[0]\n\n        file_name, file_url = zip_info_item\n        urls.append(self.url_prefix + file_url)\n        files.append(file_name)\n\n        return urls, files\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        if self.load_type == \"actual\":\n            df[\"Date\"] = pd.to_datetime(df[\"OperDay\"], format=\"%m/%d/%Y\")\n\n            df = df.rename(\n                columns={\n                    \"COAST\": \"Coast\",\n                    \"EAST\": \"East\",\n                    \"FAR_WEST\": \"FarWest\",\n                    \"NORTH\": \"North\",\n                    \"NORTH_C\": \"NorthCentral\",\n                    \"SOUTH_C\": \"SouthCentral\",\n                    \"SOUTHERN\": \"Southern\",\n                    \"WEST\": \"West\",\n                    \"TOTAL\": \"SystemTotal\",\n                    \"DSTFlag\": \"DstFlag\",\n                }\n            )\n\n        else:\n            df = df.rename(columns={\"DSTFlag\": \"DstFlag\"})\n\n            df[\"Date\"] = pd.to_datetime(df[\"DeliveryDate\"], format=\"%m/%d/%Y\")\n\n        return df\n\n    def _validate_options(self) -&gt; bool:\n        try:\n            datetime.strptime(self.date, self.query_datetime_format)\n        except ValueError:\n            raise ValueError(\n                f\"Unable to parse date. Please specify in {self.query_datetime_format} format.\"\n            )\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource","title":"<code>BaseISOSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>class BaseISOSource(SourceInterface):\n    \"\"\"\n    Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options: list = []\n    spark_schema = StructType([StructField(\"id\", IntegerType(), True)])\n    default_query_timezone: str = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.query_timezone = pytz.timezone(\n            self.options.get(\"query_timezone\", self.default_query_timezone)\n        )\n        self.current_date = datetime.now(timezone.utc).astimezone(self.query_timezone)\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n        \"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.iso_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - {url}\")\n\n        response = requests.get(url)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n\n        return response.content\n\n    def _get_localized_datetime(self, datetime_str: str) -&gt; datetime:\n        \"\"\"\n        Converts string datetime into Python datetime object with configured format and timezone.\n        Args:\n            datetime_str: String to be converted into datetime.\n\n        Returns: Timezone aware datetime object.\n\n        \"\"\"\n        parsed_dt = datetime.strptime(datetime_str, self.query_datetime_format)\n        parsed_dt = parsed_dt.replace(tzinfo=self.query_timezone)\n        return parsed_dt\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Hits the fetch_from_url method with certain parameters to get raw data from API.\n\n        All the children ISO classes must override this method and call the fetch_url method\n        in it.\n\n        Returns:\n             Raw DataFrame from API.\n        \"\"\"\n\n        return pd.read_csv(BytesIO(self._fetch_from_url(\"\")))\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Performs all the basic transformations to prepare data for further processing.\n        All the children ISO classes must override this method.\n\n        Args:\n            df: Raw DataFrame, received from the API.\n\n        Returns:\n             Modified DataFrame, ready for basic use.\n\n        \"\"\"\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Another data transformation helper method to be called after prepare data.\n        Used for advance data processing such as cleaning, filtering, restructuring.\n        All the children ISO classes must override this method if there is any post-processing required.\n\n        Args:\n            df: Initial modified version of DataFrame, received after preparing the data.\n\n        Returns:\n             Final version of data after all the fixes and modifications.\n\n        \"\"\"\n        return df\n\n    def _get_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Entrypoint method to return the final version of DataFrame.\n\n        Returns:\n            Modified form of data for specific use case.\n\n        \"\"\"\n        df = self._pull_data()\n        df = self._prepare_data(df)\n        df = self._sanitize_data(df)\n\n        # Reorder columns to keep the data consistent\n        df = df[self.spark_schema.names]\n\n        return df\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Performs all the options checks. Raises exception in case of any invalid value.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        return True\n\n    def pre_read_validation(self) -&gt; bool:\n        \"\"\"\n        Ensures all the required options are provided and performs other validations.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        for key in self.required_options:\n            if key not in self.options:\n                raise ValueError(f\"Required option `{key}` is missing.\")\n\n        return self._validate_options()\n\n    def post_read_validation(self) -&gt; bool:\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n        Returns:\n             Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n        \"\"\"\n\n        try:\n            self.pre_read_validation()\n            pdf = self._get_data()\n            pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n            # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n            pd.DataFrame.iteritems = pd.DataFrame.items\n            df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n            return df\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n        Returns:\n             Final Spark DataFrame after all the processing.\n\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.pre_read_validation","title":"<code>pre_read_validation()</code>","text":"<p>Ensures all the required options are provided and performs other validations. Returns:      True if all checks are passed.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def pre_read_validation(self) -&gt; bool:\n    \"\"\"\n    Ensures all the required options are provided and performs other validations.\n    Returns:\n         True if all checks are passed.\n\n    \"\"\"\n    for key in self.required_options:\n        if key not in self.options:\n            raise ValueError(f\"Required option `{key}` is missing.\")\n\n    return self._validate_options()\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data. Returns:      Final Spark DataFrame converted from Pandas DataFrame post-execution.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n    Returns:\n         Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n    \"\"\"\n\n    try:\n        self.pre_read_validation()\n        pdf = self._get_data()\n        pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n        # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n        pd.DataFrame.iteritems = pd.DataFrame.items\n        df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n        return df\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_stream","title":"<code>read_stream()</code>","text":"<p>By default, the streaming operation is not supported but child classes can override if ISO supports streaming.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Final Spark DataFrame after all the processing.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n    Returns:\n         Final Spark DataFrame after all the processing.\n\n    \"\"\"\n\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.MISODailyLoadISOSource","title":"<code>MISODailyLoadISOSource</code>","text":"<p>               Bases: <code>BaseISOSource</code></p> <p>The MISO Daily Load ISO Source is used to read daily load data from MISO API. It supports both Actual and Forecast data.</p> <p>To read more about the available reports from MISO API, download the file - Market Reports</p> <p>From the list of reports in the file, it pulls the report named <code>Daily Forecast and Actual Load by Local Resource Zone</code>.</p> <p>Actual data is available for one day minus from the given date.</p> <p>Forecast data is available for next 6 day (inclusive of given date).</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.MISODailyLoadISOSource--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import MISODailyLoadISOSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nmiso_source = MISODailyLoadISOSource(\n    spark=spark,\n    options={\n        \"load_type\": \"actual\",\n        \"date\": \"20230520\",\n    }\n)\n\nmiso_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>load_type</code> <code>str</code> <p>Must be one of <code>actual</code> or <code>forecast</code></p> <code>date</code> <code>str</code> <p>Must be in <code>YYYYMMDD</code> format.</p> <p>Please check the BaseISOSource for available methods.</p> BaseISOSource Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/miso_daily_load_iso.py</code> <pre><code>class MISODailyLoadISOSource(BaseISOSource):\n    \"\"\"\n    The MISO Daily Load ISO Source is used to read daily load data from MISO API. It supports both Actual and Forecast data.\n\n    To read more about the available reports from MISO API, download the file -\n    [Market Reports](https://cdn.misoenergy.org/Market%20Reports%20Directory115139.xlsx)\n\n    From the list of reports in the file, it pulls the report named\n    `Daily Forecast and Actual Load by Local Resource Zone`.\n\n    Actual data is available for one day minus from the given date.\n\n    Forecast data is available for next 6 day (inclusive of given date).\n\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.sources import MISODailyLoadISOSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    miso_source = MISODailyLoadISOSource(\n        spark=spark,\n        options={\n            \"load_type\": \"actual\",\n            \"date\": \"20230520\",\n        }\n    )\n\n    miso_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations (See Attributes table below)\n\n    Attributes:\n        load_type (str): Must be one of `actual` or `forecast`\n        date (str): Must be in `YYYYMMDD` format.\n\n    Please check the BaseISOSource for available methods.\n\n    BaseISOSource:\n        ::: src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://docs.misoenergy.org/marketreports/\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options = [\"load_type\", \"date\"]\n    spark_schema = MISO_SCHEMA\n    default_query_timezone = \"US/Central\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super().__init__(spark, options)\n        self.spark = spark\n        self.options = options\n        self.load_type = self.options.get(\"load_type\", \"actual\")\n        self.date = self.options.get(\"date\", \"\").strip()\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Pulls data from the MISO API and parses the Excel file.\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n\n        logging.info(f\"Getting {self.load_type} data for date {self.date}\")\n        df = pd.read_excel(self._fetch_from_url(f\"{self.date}_df_al.xls\"), skiprows=4)\n\n        return df\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Creates a new `date_time` column and removes null values.\n\n        Args:\n            df: Raw form of data received from the API.\n\n        Returns:\n            Data after basic transformations.\n\n        \"\"\"\n\n        df.drop(\n            df.index[(df[\"HourEnding\"] == \"HourEnding\") | df[\"MISO MTLF (MWh)\"].isna()],\n            inplace=True,\n        )\n        df.rename(columns={\"Market Day\": \"date\"}, inplace=True)\n\n        df[\"date_time\"] = pd.to_datetime(df[\"date\"]) + pd.to_timedelta(\n            df[\"HourEnding\"].astype(int) - 1, \"h\"\n        )\n        df.drop([\"HourEnding\", \"date\"], axis=1, inplace=True)\n\n        data_cols = df.columns[df.columns != \"date_time\"]\n        df[data_cols] = df[data_cols].astype(float)\n\n        df.reset_index(inplace=True, drop=True)\n\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Filter outs Actual or Forecast data based on `load_type`.\n        Args:\n            df: Data received after preparation.\n\n        Returns:\n            Final data either containing Actual or Forecast values.\n\n        \"\"\"\n\n        skip_col_suffix = \"\"\n\n        if self.load_type == \"actual\":\n            skip_col_suffix = \"MTLF (MWh)\"\n\n        elif self.load_type == \"forecast\":\n            skip_col_suffix = \"ActualLoad (MWh)\"\n\n        df = df[[x for x in df.columns if not x.endswith(skip_col_suffix)]]\n        df = df.dropna()\n        df.columns = [str(x.split(\" \")[0]).upper() for x in df.columns]\n\n        rename_cols = {\n            \"LRZ1\": \"Lrz1\",\n            \"LRZ2_7\": \"Lrz2_7\",\n            \"LRZ3_5\": \"Lrz3_5\",\n            \"LRZ4\": \"Lrz4\",\n            \"LRZ6\": \"Lrz6\",\n            \"LRZ8_9_10\": \"Lrz8_9_10\",\n            \"MISO\": \"Miso\",\n            \"DATE_TIME\": \"Datetime\",\n        }\n\n        df = df.rename(columns=rename_cols)\n\n        return df\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Validates the following options:\n            - `date` must be in the correct format.\n            - `load_type` must be valid.\n\n        Returns:\n            True if all looks good otherwise raises Exception.\n\n        \"\"\"\n\n        try:\n            date = self._get_localized_datetime(self.date)\n        except ValueError:\n            raise ValueError(\"Unable to parse Date. Please specify in YYYYMMDD format.\")\n\n        if date &gt; self.current_date:\n            raise ValueError(\"Query date can't be in future.\")\n\n        valid_load_types = [\"actual\", \"forecast\"]\n\n        if self.load_type not in valid_load_types:\n            raise ValueError(\n                f\"Invalid load_type `{self.load_type}` given. Supported values are {valid_load_types}.\"\n            )\n\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource","title":"<code>BaseISOSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>class BaseISOSource(SourceInterface):\n    \"\"\"\n    Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options: list = []\n    spark_schema = StructType([StructField(\"id\", IntegerType(), True)])\n    default_query_timezone: str = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.query_timezone = pytz.timezone(\n            self.options.get(\"query_timezone\", self.default_query_timezone)\n        )\n        self.current_date = datetime.now(timezone.utc).astimezone(self.query_timezone)\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n        \"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.iso_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - {url}\")\n\n        response = requests.get(url)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n\n        return response.content\n\n    def _get_localized_datetime(self, datetime_str: str) -&gt; datetime:\n        \"\"\"\n        Converts string datetime into Python datetime object with configured format and timezone.\n        Args:\n            datetime_str: String to be converted into datetime.\n\n        Returns: Timezone aware datetime object.\n\n        \"\"\"\n        parsed_dt = datetime.strptime(datetime_str, self.query_datetime_format)\n        parsed_dt = parsed_dt.replace(tzinfo=self.query_timezone)\n        return parsed_dt\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Hits the fetch_from_url method with certain parameters to get raw data from API.\n\n        All the children ISO classes must override this method and call the fetch_url method\n        in it.\n\n        Returns:\n             Raw DataFrame from API.\n        \"\"\"\n\n        return pd.read_csv(BytesIO(self._fetch_from_url(\"\")))\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Performs all the basic transformations to prepare data for further processing.\n        All the children ISO classes must override this method.\n\n        Args:\n            df: Raw DataFrame, received from the API.\n\n        Returns:\n             Modified DataFrame, ready for basic use.\n\n        \"\"\"\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Another data transformation helper method to be called after prepare data.\n        Used for advance data processing such as cleaning, filtering, restructuring.\n        All the children ISO classes must override this method if there is any post-processing required.\n\n        Args:\n            df: Initial modified version of DataFrame, received after preparing the data.\n\n        Returns:\n             Final version of data after all the fixes and modifications.\n\n        \"\"\"\n        return df\n\n    def _get_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Entrypoint method to return the final version of DataFrame.\n\n        Returns:\n            Modified form of data for specific use case.\n\n        \"\"\"\n        df = self._pull_data()\n        df = self._prepare_data(df)\n        df = self._sanitize_data(df)\n\n        # Reorder columns to keep the data consistent\n        df = df[self.spark_schema.names]\n\n        return df\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Performs all the options checks. Raises exception in case of any invalid value.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        return True\n\n    def pre_read_validation(self) -&gt; bool:\n        \"\"\"\n        Ensures all the required options are provided and performs other validations.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        for key in self.required_options:\n            if key not in self.options:\n                raise ValueError(f\"Required option `{key}` is missing.\")\n\n        return self._validate_options()\n\n    def post_read_validation(self) -&gt; bool:\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n        Returns:\n             Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n        \"\"\"\n\n        try:\n            self.pre_read_validation()\n            pdf = self._get_data()\n            pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n            # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n            pd.DataFrame.iteritems = pd.DataFrame.items\n            df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n            return df\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n        Returns:\n             Final Spark DataFrame after all the processing.\n\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.pre_read_validation","title":"<code>pre_read_validation()</code>","text":"<p>Ensures all the required options are provided and performs other validations. Returns:      True if all checks are passed.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def pre_read_validation(self) -&gt; bool:\n    \"\"\"\n    Ensures all the required options are provided and performs other validations.\n    Returns:\n         True if all checks are passed.\n\n    \"\"\"\n    for key in self.required_options:\n        if key not in self.options:\n            raise ValueError(f\"Required option `{key}` is missing.\")\n\n    return self._validate_options()\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data. Returns:      Final Spark DataFrame converted from Pandas DataFrame post-execution.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n    Returns:\n         Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n    \"\"\"\n\n    try:\n        self.pre_read_validation()\n        pdf = self._get_data()\n        pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n        # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n        pd.DataFrame.iteritems = pd.DataFrame.items\n        df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n        return df\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_stream","title":"<code>read_stream()</code>","text":"<p>By default, the streaming operation is not supported but child classes can override if ISO supports streaming.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Final Spark DataFrame after all the processing.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n    Returns:\n         Final Spark DataFrame after all the processing.\n\n    \"\"\"\n\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.MISOHistoricalLoadISOSource","title":"<code>MISOHistoricalLoadISOSource</code>","text":"<p>               Bases: <code>MISODailyLoadISOSource</code></p> <p>The MISO Historical Load ISO Source is used to read historical load data from MISO API.</p> <p>To read more about the available reports from MISO API, download the file -  Market Reports</p> <p>From the list of reports in the file, it pulls the report named  <code>Historical Daily Forecast and Actual Load by Local Resource Zone</code>.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.MISOHistoricalLoadISOSource--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import MISOHistoricalLoadISOSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nmiso_source = MISOHistoricalLoadISOSource(\n    spark=spark,\n    options={\n        \"start_date\": \"20230510\",\n        \"end_date\": \"20230520\",\n    }\n)\n\nmiso_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>start_date</code> <code>str</code> <p>Must be in <code>YYYYMMDD</code> format.</p> <code>end_date</code> <code>str</code> <p>Must be in <code>YYYYMMDD</code> format.</p> <code>fill_missing</code> <code>str</code> <p>Set to <code>\"true\"</code> to fill missing Actual load with Forecast load. Default - <code>true</code>.</p> <p>Please check the BaseISOSource for available methods.</p> BaseISOSource Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/miso_historical_load_iso.py</code> <pre><code>class MISOHistoricalLoadISOSource(MISODailyLoadISOSource):\n    \"\"\"\n    The MISO Historical Load ISO Source is used to read historical load data from MISO API.\n\n    To read more about the available reports from MISO API, download the file -\n     [Market Reports](https://cdn.misoenergy.org/Market%20Reports%20Directory115139.xlsx)\n\n    From the list of reports in the file, it pulls the report named\n     `Historical Daily Forecast and Actual Load by Local Resource Zone`.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.sources import MISOHistoricalLoadISOSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    miso_source = MISOHistoricalLoadISOSource(\n        spark=spark,\n        options={\n            \"start_date\": \"20230510\",\n            \"end_date\": \"20230520\",\n        }\n    )\n\n    miso_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations (See Attributes table below)\n\n    Attributes:\n        start_date (str): Must be in `YYYYMMDD` format.\n        end_date (str): Must be in `YYYYMMDD` format.\n        fill_missing (str): Set to `\"true\"` to fill missing Actual load with Forecast load. Default - `true`.\n\n    Please check the BaseISOSource for available methods.\n\n    BaseISOSource:\n        ::: src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    required_options = [\"start_date\", \"end_date\"]\n\n    def __init__(self, spark: SparkSession, options: dict):\n        super().__init__(spark, options)\n        self.start_date = self.options.get(\"start_date\", \"\")\n        self.end_date = self.options.get(\"end_date\", \"\")\n        self.fill_missing = bool(self.options.get(\"fill_missing\", \"true\") == \"true\")\n\n    def _get_historical_data_for_date(self, date: datetime) -&gt; pd.DataFrame:\n        logging.info(f\"Getting historical data for date {date}\")\n        df = pd.read_excel(\n            self._fetch_from_url(\n                f\"{date.strftime(self.query_datetime_format)}_dfal_HIST.xls\"\n            ),\n            skiprows=5,\n        )\n\n        if date.month == 12 and date.day == 31:\n            expected_year_rows = (\n                pd.Timestamp(date.year, 12, 31).dayofyear * 24 * 7\n            )  # Every hour has 7 zones.\n            received_year_rows = (\n                len(df[df[\"MarketDay\"] != \"MarketDay\"]) - 2\n            )  # Last 2 rows are invalid.\n\n            if expected_year_rows != received_year_rows:\n                logging.warning(\n                    f\"Didn't receive full year historical data for year {date.year}.\"\n                    f\" Expected {expected_year_rows} but Received {received_year_rows}\"\n                )\n\n        return df\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Pulls data from the MISO API and parses the Excel file.\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n\n        logging.info(\n            f\"Historical load requested from {self.start_date} to {self.end_date}\"\n        )\n\n        start_date = self._get_localized_datetime(self.start_date)\n        end_date = self._get_localized_datetime(self.end_date)\n\n        dates = pd.date_range(\n            start_date, end_date + timedelta(days=365), freq=\"Y\", inclusive=\"left\"\n        )\n        logging.info(f\"Generated date ranges are - {dates}\")\n\n        # Collect all historical data on yearly basis.\n        df = pd.concat(\n            [\n                self._get_historical_data_for_date(min(date, self.current_date))\n                for date in dates\n            ],\n            sort=False,\n        )\n\n        return df\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Creates a new `Datetime` column, removes null values and pivots the data.\n\n        Args:\n            df: Raw form of data received from the API.\n\n        Returns:\n            Data after basic transformations and pivoting.\n\n        \"\"\"\n\n        df = df[df[\"MarketDay\"] != \"MarketDay\"]\n\n        # Fill missing actual values with the forecast values to avoid gaps.\n        if self.fill_missing:\n            df = df.fillna({\"ActualLoad (MWh)\": df[\"MTLF (MWh)\"]})\n\n        df = df.rename(\n            columns={\n                \"MarketDay\": \"date\",\n                \"HourEnding\": \"hour\",\n                \"ActualLoad (MWh)\": \"load\",\n                \"LoadResource Zone\": \"zone\",\n            }\n        )\n        df = df.dropna()\n\n        df[\"date_time\"] = pd.to_datetime(df[\"date\"]) + pd.to_timedelta(\n            df[\"hour\"].astype(int) - 1, \"h\"\n        )\n\n        df.drop([\"hour\", \"date\"], axis=1, inplace=True)\n        df[\"load\"] = df[\"load\"].astype(float)\n\n        df = df.pivot_table(\n            index=\"date_time\", values=\"load\", columns=\"zone\"\n        ).reset_index()\n\n        df.columns = [str(x.split(\" \")[0]).upper() for x in df.columns]\n\n        rename_cols = {\n            \"LRZ1\": \"Lrz1\",\n            \"LRZ2_7\": \"Lrz2_7\",\n            \"LRZ3_5\": \"Lrz3_5\",\n            \"LRZ4\": \"Lrz4\",\n            \"LRZ6\": \"Lrz6\",\n            \"LRZ8_9_10\": \"Lrz8_9_10\",\n            \"MISO\": \"Miso\",\n            \"DATE_TIME\": \"Datetime\",\n        }\n\n        df = df.rename(columns=rename_cols)\n\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Filter outs data outside the requested date range.\n\n        Args:\n            df: Data received after preparation.\n\n        Returns:\n            Final data after all the transformations.\n\n        \"\"\"\n\n        start_date = self._get_localized_datetime(self.start_date)\n        end_date = self._get_localized_datetime(self.end_date).replace(\n            hour=23, minute=59, second=59\n        )\n\n        df = df[\n            (df[\"Datetime\"] &gt;= start_date.replace(tzinfo=None))\n            &amp; (df[\"Datetime\"] &lt;= end_date.replace(tzinfo=None))\n        ]\n\n        df = df.sort_values(by=\"Datetime\", ascending=True).reset_index(drop=True)\n\n        expected_rows = ((min(end_date, self.current_date) - start_date).days + 1) * 24\n\n        actual_rows = len(df)\n\n        logging.info(f\"Rows Expected = {expected_rows}, Rows Found = {actual_rows}\")\n\n        return df\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Validates the following options:\n            - `start_date` &amp; `end_data` must be in the correct format.\n            - `start_date` must be behind `end_data`.\n            - `start_date` must not be in the future (UTC).\n\n        Returns:\n            True if all looks good otherwise raises Exception.\n\n        \"\"\"\n\n        try:\n            start_date = self._get_localized_datetime(self.start_date)\n        except ValueError:\n            raise ValueError(\n                \"Unable to parse Start date. Please specify in YYYYMMDD format.\"\n            )\n\n        try:\n            end_date = self._get_localized_datetime(self.end_date)\n        except ValueError:\n            raise ValueError(\n                \"Unable to parse End date. Please specify in YYYYMMDD format.\"\n            )\n\n        if start_date &gt; self.current_date:\n            raise ValueError(\"Start date can't be in future.\")\n\n        if start_date &gt; end_date:\n            raise ValueError(\"Start date can't be ahead of End date.\")\n\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource","title":"<code>BaseISOSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>class BaseISOSource(SourceInterface):\n    \"\"\"\n    Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options: list = []\n    spark_schema = StructType([StructField(\"id\", IntegerType(), True)])\n    default_query_timezone: str = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.query_timezone = pytz.timezone(\n            self.options.get(\"query_timezone\", self.default_query_timezone)\n        )\n        self.current_date = datetime.now(timezone.utc).astimezone(self.query_timezone)\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n        \"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.iso_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - {url}\")\n\n        response = requests.get(url)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n\n        return response.content\n\n    def _get_localized_datetime(self, datetime_str: str) -&gt; datetime:\n        \"\"\"\n        Converts string datetime into Python datetime object with configured format and timezone.\n        Args:\n            datetime_str: String to be converted into datetime.\n\n        Returns: Timezone aware datetime object.\n\n        \"\"\"\n        parsed_dt = datetime.strptime(datetime_str, self.query_datetime_format)\n        parsed_dt = parsed_dt.replace(tzinfo=self.query_timezone)\n        return parsed_dt\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Hits the fetch_from_url method with certain parameters to get raw data from API.\n\n        All the children ISO classes must override this method and call the fetch_url method\n        in it.\n\n        Returns:\n             Raw DataFrame from API.\n        \"\"\"\n\n        return pd.read_csv(BytesIO(self._fetch_from_url(\"\")))\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Performs all the basic transformations to prepare data for further processing.\n        All the children ISO classes must override this method.\n\n        Args:\n            df: Raw DataFrame, received from the API.\n\n        Returns:\n             Modified DataFrame, ready for basic use.\n\n        \"\"\"\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Another data transformation helper method to be called after prepare data.\n        Used for advance data processing such as cleaning, filtering, restructuring.\n        All the children ISO classes must override this method if there is any post-processing required.\n\n        Args:\n            df: Initial modified version of DataFrame, received after preparing the data.\n\n        Returns:\n             Final version of data after all the fixes and modifications.\n\n        \"\"\"\n        return df\n\n    def _get_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Entrypoint method to return the final version of DataFrame.\n\n        Returns:\n            Modified form of data for specific use case.\n\n        \"\"\"\n        df = self._pull_data()\n        df = self._prepare_data(df)\n        df = self._sanitize_data(df)\n\n        # Reorder columns to keep the data consistent\n        df = df[self.spark_schema.names]\n\n        return df\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Performs all the options checks. Raises exception in case of any invalid value.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        return True\n\n    def pre_read_validation(self) -&gt; bool:\n        \"\"\"\n        Ensures all the required options are provided and performs other validations.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        for key in self.required_options:\n            if key not in self.options:\n                raise ValueError(f\"Required option `{key}` is missing.\")\n\n        return self._validate_options()\n\n    def post_read_validation(self) -&gt; bool:\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n        Returns:\n             Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n        \"\"\"\n\n        try:\n            self.pre_read_validation()\n            pdf = self._get_data()\n            pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n            # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n            pd.DataFrame.iteritems = pd.DataFrame.items\n            df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n            return df\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n        Returns:\n             Final Spark DataFrame after all the processing.\n\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.pre_read_validation","title":"<code>pre_read_validation()</code>","text":"<p>Ensures all the required options are provided and performs other validations. Returns:      True if all checks are passed.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def pre_read_validation(self) -&gt; bool:\n    \"\"\"\n    Ensures all the required options are provided and performs other validations.\n    Returns:\n         True if all checks are passed.\n\n    \"\"\"\n    for key in self.required_options:\n        if key not in self.options:\n            raise ValueError(f\"Required option `{key}` is missing.\")\n\n    return self._validate_options()\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data. Returns:      Final Spark DataFrame converted from Pandas DataFrame post-execution.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n    Returns:\n         Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n    \"\"\"\n\n    try:\n        self.pre_read_validation()\n        pdf = self._get_data()\n        pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n        # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n        pd.DataFrame.iteritems = pd.DataFrame.items\n        df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n        return df\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_stream","title":"<code>read_stream()</code>","text":"<p>By default, the streaming operation is not supported but child classes can override if ISO supports streaming.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Final Spark DataFrame after all the processing.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n    Returns:\n         Final Spark DataFrame after all the processing.\n\n    \"\"\"\n\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PJMDailyLoadISOSource","title":"<code>PJMDailyLoadISOSource</code>","text":"<p>               Bases: <code>BaseISOSource</code></p> <p>The PJM Daily Load ISO Source is used to read daily load data from PJM API. It supports both Actual and Forecast data. Actual will return 1 day, Forecast will return 7 days.</p> <p>To read more about the reports, visit the following URLs -  Actual doc:    ops_sum_prev_period  Forecast doc:  load_frcstd_7_day</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PJMDailyLoadISOSource--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import PJMDailyLoadISOSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\npjm_source = PJMDailyLoadISOSource(\n    spark=spark,\n    options={\n        \"api_key\": \"{api_key}\",\n        \"load_type\": \"actual\"\n    }\n)\n\npjm_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>Must be a valid key from PJM, see api url</p> <code>load_type</code> <code>str</code> <p>Must be one of <code>actual</code> or <code>forecast</code></p> <p>Please check the BaseISOSource for available methods.</p> BaseISOSource Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/pjm_daily_load_iso.py</code> <pre><code>class PJMDailyLoadISOSource(BaseISOSource):\n    \"\"\"\n    The PJM Daily Load ISO Source is used to read daily load data from PJM API.\n    It supports both Actual and Forecast data. Actual will return 1 day, Forecast will return 7 days.\n\n    To read more about the reports, visit the following URLs -\n    &lt;br&gt;\n    Actual doc:    [ops_sum_prev_period](https://dataminer2.pjm.com/feed/ops_sum_prev_period/definition)\n    &lt;br&gt;\n    Forecast doc:  [load_frcstd_7_day](https://dataminer2.pjm.com/feed/load_frcstd_7_day/definition)\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.sources import PJMDailyLoadISOSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    pjm_source = PJMDailyLoadISOSource(\n        spark=spark,\n        options={\n            \"api_key\": \"{api_key}\",\n            \"load_type\": \"actual\"\n        }\n    )\n\n    pjm_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations (See Attributes table below)\n\n    Attributes:\n        api_key (str): Must be a valid key from PJM, see api url\n        load_type (str): Must be one of `actual` or `forecast`\n\n    Please check the BaseISOSource for available methods.\n\n    BaseISOSource:\n        ::: src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso\n    \"\"\"\n\n    spark: SparkSession\n    spark_schema = PJM_SCHEMA\n    options: dict\n    iso_url: str = \"https://api.pjm.com/api/v1/\"\n    query_datetime_format: str = \"%Y-%m-%d %H:%M\"\n    required_options = [\"api_key\", \"load_type\"]\n    default_query_timezone = \"US/Eastern\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super().__init__(spark, options)\n        self.spark: SparkSession = spark\n        self.options: dict = options\n        self.load_type: str = self.options.get(\"load_type\", \"\").strip()\n        self.api_key: str = self.options.get(\"api_key\", \"\").strip()\n        self.days: int = self.options.get(\"days\", 7)\n\n    def _fetch_from_url(self, url_suffix: str, start_date: str, end_date: str) -&gt; bytes:\n        \"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n        \"\"\"\n\n        url = f\"{self.iso_url}{url_suffix}\"\n        headers = {\"Ocp-Apim-Subscription-Key\": self.api_key}\n        logging.info(\n            f\"Requesting URL - {url}, start_date={start_date}, end_date={end_date}, load_type={self.load_type}\"\n        )\n        load_key = (\n            \"datetime_beginning_ept\"\n            if self.load_type != \"forecast\"\n            else \"forecast_datetime_beginning_ept\"\n        )\n        feed = (\n            \"ops_sum_prev_period\"\n            if self.load_type != \"forecast\"\n            else \"load_frcstd_7_day\"\n        )\n        query = {\n            \"startRow\": \"1\",\n            load_key: f\"{start_date}to{end_date}\",\n            \"format\": \"csv\",\n            \"download\": \"true\",\n        }\n        query_s = \"&amp;\".join([\"=\".join([k, v]) for k, v in query.items()])\n        new_url = f\"{url}{feed}?{query_s}\"\n        response = requests.get(new_url, headers=headers)\n        code = response.status_code\n\n        if code != 200:\n            raise requests.HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n        return response.content\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Pulls data from the PJM API and parses the return.\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n        start_date = self.current_date - timedelta(days=1)\n        start_date = start_date.replace(hour=0, minute=0)\n        end_date = (start_date + timedelta(days=self.days)).replace(hour=23)\n        start_date_str = start_date.strftime(self.query_datetime_format)\n        end_date_str = end_date.strftime(self.query_datetime_format)\n        df = pd.read_csv(\n            BytesIO(self._fetch_from_url(\"\", start_date_str, end_date_str))\n        )\n\n        return df\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Creates a new date time column and removes null values. Renames columns\n\n        Args:\n            df: Raw form of data received from the API.\n\n        Returns:\n            Data after basic transformations.\n\n        \"\"\"\n\n        if self.load_type == \"forecast\":\n            df = df.rename(\n                columns={\n                    \"forecast_datetime_beginning_utc\": \"start_time\",\n                    \"forecast_area\": \"zone\",\n                    \"forecast_datetime_ending_utc\": \"end_time\",\n                    \"forecast_load_mw\": \"load\",\n                }\n            )\n        else:\n            df = df.rename(\n                columns={\n                    \"datetime_beginning_utc\": \"start_time\",\n                    \"area\": \"zone\",\n                    \"datetime_ending_utc\": \"end_time\",\n                    \"actual_load\": \"load\",\n                }\n            )\n\n        df = df[[\"start_time\", \"end_time\", \"zone\", \"load\"]]\n        df = df.replace({np.nan: None, \"\": None})\n\n        date_cols = [\"start_time\", \"end_time\"]\n        for col in date_cols:\n            df[col] = pd.to_datetime(df[col], format=\"%m/%d/%Y %I:%M:%S %p\")\n\n        df[\"load\"] = df[\"load\"].astype(float)\n        df = df.replace({np.nan: None, \"\": None})\n        df.columns = list(map(lambda x: x.upper(), df.columns))\n\n        rename_cols = {\n            \"START_TIME\": \"StartTime\",\n            \"END_TIME\": \"EndTime\",\n            \"ZONE\": \"Zone\",\n            \"LOAD\": \"Load\",\n        }\n\n        df = df.rename(columns=rename_cols)\n\n        df.reset_index(inplace=True, drop=True)\n\n        return df\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Validates the following options:\n            - `load_type` must be valid.\n\n        Returns:\n            True if all looks good otherwise raises Exception.\n        \"\"\"\n\n        valid_load_types = [\"actual\", \"forecast\"]\n\n        if self.load_type not in valid_load_types:\n            raise ValueError(\n                f\"Invalid load_type `{self.load_type}` given. Supported values are {valid_load_types}.\"\n            )\n\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource","title":"<code>BaseISOSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>class BaseISOSource(SourceInterface):\n    \"\"\"\n    Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options: list = []\n    spark_schema = StructType([StructField(\"id\", IntegerType(), True)])\n    default_query_timezone: str = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.query_timezone = pytz.timezone(\n            self.options.get(\"query_timezone\", self.default_query_timezone)\n        )\n        self.current_date = datetime.now(timezone.utc).astimezone(self.query_timezone)\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n        \"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.iso_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - {url}\")\n\n        response = requests.get(url)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n\n        return response.content\n\n    def _get_localized_datetime(self, datetime_str: str) -&gt; datetime:\n        \"\"\"\n        Converts string datetime into Python datetime object with configured format and timezone.\n        Args:\n            datetime_str: String to be converted into datetime.\n\n        Returns: Timezone aware datetime object.\n\n        \"\"\"\n        parsed_dt = datetime.strptime(datetime_str, self.query_datetime_format)\n        parsed_dt = parsed_dt.replace(tzinfo=self.query_timezone)\n        return parsed_dt\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Hits the fetch_from_url method with certain parameters to get raw data from API.\n\n        All the children ISO classes must override this method and call the fetch_url method\n        in it.\n\n        Returns:\n             Raw DataFrame from API.\n        \"\"\"\n\n        return pd.read_csv(BytesIO(self._fetch_from_url(\"\")))\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Performs all the basic transformations to prepare data for further processing.\n        All the children ISO classes must override this method.\n\n        Args:\n            df: Raw DataFrame, received from the API.\n\n        Returns:\n             Modified DataFrame, ready for basic use.\n\n        \"\"\"\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Another data transformation helper method to be called after prepare data.\n        Used for advance data processing such as cleaning, filtering, restructuring.\n        All the children ISO classes must override this method if there is any post-processing required.\n\n        Args:\n            df: Initial modified version of DataFrame, received after preparing the data.\n\n        Returns:\n             Final version of data after all the fixes and modifications.\n\n        \"\"\"\n        return df\n\n    def _get_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Entrypoint method to return the final version of DataFrame.\n\n        Returns:\n            Modified form of data for specific use case.\n\n        \"\"\"\n        df = self._pull_data()\n        df = self._prepare_data(df)\n        df = self._sanitize_data(df)\n\n        # Reorder columns to keep the data consistent\n        df = df[self.spark_schema.names]\n\n        return df\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Performs all the options checks. Raises exception in case of any invalid value.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        return True\n\n    def pre_read_validation(self) -&gt; bool:\n        \"\"\"\n        Ensures all the required options are provided and performs other validations.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        for key in self.required_options:\n            if key not in self.options:\n                raise ValueError(f\"Required option `{key}` is missing.\")\n\n        return self._validate_options()\n\n    def post_read_validation(self) -&gt; bool:\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n        Returns:\n             Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n        \"\"\"\n\n        try:\n            self.pre_read_validation()\n            pdf = self._get_data()\n            pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n            # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n            pd.DataFrame.iteritems = pd.DataFrame.items\n            df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n            return df\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n        Returns:\n             Final Spark DataFrame after all the processing.\n\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.pre_read_validation","title":"<code>pre_read_validation()</code>","text":"<p>Ensures all the required options are provided and performs other validations. Returns:      True if all checks are passed.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def pre_read_validation(self) -&gt; bool:\n    \"\"\"\n    Ensures all the required options are provided and performs other validations.\n    Returns:\n         True if all checks are passed.\n\n    \"\"\"\n    for key in self.required_options:\n        if key not in self.options:\n            raise ValueError(f\"Required option `{key}` is missing.\")\n\n    return self._validate_options()\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data. Returns:      Final Spark DataFrame converted from Pandas DataFrame post-execution.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n    Returns:\n         Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n    \"\"\"\n\n    try:\n        self.pre_read_validation()\n        pdf = self._get_data()\n        pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n        # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n        pd.DataFrame.iteritems = pd.DataFrame.items\n        df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n        return df\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_stream","title":"<code>read_stream()</code>","text":"<p>By default, the streaming operation is not supported but child classes can override if ISO supports streaming.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Final Spark DataFrame after all the processing.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n    Returns:\n         Final Spark DataFrame after all the processing.\n\n    \"\"\"\n\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PJMDailyPricingISOSource","title":"<code>PJMDailyPricingISOSource</code>","text":"<p>               Bases: <code>BaseISOSource</code></p> <p>The PJM Daily Pricing ISO Source is used to retrieve Real-Time and Day-Ahead hourly data from PJM API. Real-Time will return data for T - 3 to T days and Day-Ahead will return T - 3 to T + 1 days data.</p> <p>API:             https://api.pjm.com/api/v1/  (must be a valid apy key from PJM)</p> <p>Real-Time doc:    https://dataminer2.pjm.com/feed/rt_hrl_lmps/definition</p> <p>Day-Ahead doc:    https://dataminer2.pjm.com/feed/da_hrl_lmps/definition</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PJMDailyPricingISOSource--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import PJMDailyPricingISOSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\npjm_source = PJMDailyPricingISOSource(\n    spark=spark,\n    options={\n        \"api_key\": \"{api_key}\",\n        \"load_type\": \"real_time\"\n    }\n)\n\npjm_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>Must be a valid key from PJM, see api url</p> <code>load_type</code> <code>str</code> <p>Must be one of <code>real_time</code> or <code>day_ahead</code></p> <p>Please check the BaseISOSource for available methods.</p> BaseISOSource Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/pjm_daily_pricing_iso.py</code> <pre><code>class PJMDailyPricingISOSource(BaseISOSource):\n    \"\"\"\n    The PJM Daily Pricing ISO Source is used to retrieve Real-Time and Day-Ahead hourly data from PJM API.\n    Real-Time will return data for T - 3 to T days and Day-Ahead will return T - 3 to T + 1 days data.\n\n    API:             &lt;a href=\"https://api.pjm.com/api/v1/\"&gt;https://api.pjm.com/api/v1/&lt;/a&gt;  (must be a valid apy key from PJM)\n\n    Real-Time doc:    &lt;a href=\"https://dataminer2.pjm.com/feed/rt_hrl_lmps/definition\"&gt;https://dataminer2.pjm.com/feed/rt_hrl_lmps/definition&lt;/a&gt;\n\n    Day-Ahead doc:    &lt;a href=\"https://dataminer2.pjm.com/feed/da_hrl_lmps/definition\"&gt;https://dataminer2.pjm.com/feed/da_hrl_lmps/definition&lt;/a&gt;\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.sources import PJMDailyPricingISOSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    pjm_source = PJMDailyPricingISOSource(\n        spark=spark,\n        options={\n            \"api_key\": \"{api_key}\",\n            \"load_type\": \"real_time\"\n        }\n    )\n\n    pjm_source.read_batch()\n    ```\n\n    Parameters:\n       spark (SparkSession): Spark Session instance\n       options (dict): A dictionary of ISO Source specific configurations (See Attributes table below)\n\n    Attributes:\n        api_key (str): Must be a valid key from PJM, see api url\n        load_type (str): Must be one of `real_time` or `day_ahead`\n\n    Please check the BaseISOSource for available methods.\n\n    BaseISOSource:\n        ::: src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso\n    \"\"\"\n\n    spark: SparkSession\n    spark_schema = PJM_PRICING_SCHEMA\n    options: dict\n    iso_url: str = \"https://api.pjm.com/api/v1/\"\n    query_datetime_format: str = \"%Y-%m-%d %H:%M\"\n    required_options = [\"api_key\", \"load_type\"]\n    default_query_timezone = \"US/Eastern\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super().__init__(spark, options)\n        self.spark: SparkSession = spark\n        self.options: dict = options\n        self.load_type: str = self.options.get(\"load_type\", \"\").strip()\n        self.api_key: str = self.options.get(\"api_key\", \"\").strip()\n        self.days: int = self.options.get(\"days\", 3)\n\n    def _fetch_paginated_data(\n        self, url_suffix: str, start_date: str, end_date: str\n    ) -&gt; bytes:\n        \"\"\"\n        Fetches data from the PJM API with pagination support.\n\n        Args:\n            url_suffix: String to be used as suffix to ISO URL.\n            start_date: Start date for the data retrieval.\n            end_date: End date for the data retrieval.\n\n        Returns:\n            Raw content of the data received.\n        \"\"\"\n        headers = {\"Ocp-Apim-Subscription-Key\": self.api_key}\n        items = []\n        query = {\n            \"startRow\": \"1\",\n            \"rowCount\": \"5\",\n            \"datetime_beginning_ept\": f\"{start_date}to{end_date}\",\n        }\n        query_s = \"&amp;\".join([\"=\".join([k, v]) for k, v in query.items()])\n        base_url = f\"{self.iso_url}{url_suffix}?{query_s}\"\n\n        next_page = base_url\n\n        logging.info(\n            f\"Requesting URL - {base_url}, start_date={start_date}, end_date={end_date}, load_type={self.load_type}\"\n        )\n\n        while next_page:\n            now = datetime.now()\n            logging.info(f\"Timestamp: {now}\")\n            response = requests.get(next_page, headers=headers)\n            code = response.status_code\n\n            if code != 200:\n                raise requests.HTTPError(\n                    f\"Unable to access URL `{next_page}`.\"\n                    f\" Received status code {code} with message {response.content}\"\n                )\n\n            data = response.json()\n\n            logging.info(f\"Data for page {next_page}:\")\n            items.extend(data[\"items\"])\n            next_urls = list(filter(lambda item: item[\"rel\"] == \"next\", data[\"links\"]))\n            next_page = next_urls[0][\"href\"] if next_urls else None\n            time.sleep(10)\n\n        return items\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Pulls data from the PJM API and parses the return.\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n        start_date = self.current_date - timedelta(self.days)\n        start_date = start_date.replace(hour=0, minute=0)\n        end_date = (start_date + timedelta(days=self.days)).replace(hour=23)\n        start_date_str = start_date.strftime(self.query_datetime_format)\n        end_date_str = end_date.strftime(self.query_datetime_format)\n\n        if self.load_type == \"day_ahead\":\n            url_suffix = \"da_hrl_lmps\"\n        else:\n            url_suffix = \"rt_hrl_lmps\"\n\n        data = self._fetch_paginated_data(url_suffix, start_date_str, end_date_str)\n\n        df = pd.DataFrame(data)\n        logging.info(f\"Data fetched successfully: {len(df)} rows\")\n\n        return df\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Creates a new date time column and removes null values. Renames columns\n\n        Args:\n            df: Raw form of data received from the API.\n\n        Returns:\n            Data after basic transformations.\n\n        \"\"\"\n\n        if self.load_type == \"day_ahead\":\n            df = df.rename(\n                columns={\n                    \"datetime_beginning_utc\": \"StartTime\",\n                    \"pnode_id\": \"PnodeId\",\n                    \"pnode_name\": \"PnodeName\",\n                    \"voltage\": \"Voltage\",\n                    \"equipment\": \"Equipment\",\n                    \"type\": \"Type\",\n                    \"zone\": \"Zone\",\n                    \"system_energy_price_da\": \"SystemEnergyPrice\",\n                    \"total_lmp_da\": \"TotalLmp\",\n                    \"congestion_price_da\": \"CongestionPrice\",\n                    \"marginal_loss_price_da\": \"MarginalLossPrice\",\n                    \"version_nbr\": \"VersionNbr\",\n                }\n            )\n        else:\n            df = df.rename(\n                columns={\n                    \"datetime_beginning_utc\": \"StartTime\",\n                    \"pnode_id\": \"PnodeId\",\n                    \"pnode_name\": \"PnodeName\",\n                    \"voltage\": \"Voltage\",\n                    \"equipment\": \"Equipment\",\n                    \"type\": \"Type\",\n                    \"zone\": \"Zone\",\n                    \"system_energy_price_rt\": \"SystemEnergyPrice\",\n                    \"total_lmp_rt\": \"TotalLmp\",\n                    \"congestion_price_rt\": \"CongestionPrice\",\n                    \"marginal_loss_price_rt\": \"MarginalLossPrice\",\n                    \"version_nbr\": \"VersionNbr\",\n                }\n            )\n\n        df = df[\n            [\n                \"StartTime\",\n                \"PnodeId\",\n                \"PnodeName\",\n                \"Voltage\",\n                \"Equipment\",\n                \"Type\",\n                \"Zone\",\n                \"SystemEnergyPrice\",\n                \"TotalLmp\",\n                \"CongestionPrice\",\n                \"MarginalLossPrice\",\n                \"VersionNbr\",\n            ]\n        ]\n\n        df = df.replace({np.nan: None, \"\": None})\n\n        df[\"StartTime\"] = pd.to_datetime(df[\"StartTime\"])\n        df = df.replace({np.nan: None, \"\": None})\n\n        df.reset_index(inplace=True, drop=True)\n\n        return df\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Validates the following options:\n            - `load_type` must be valid.\n\n        Returns:\n            True if all looks good otherwise raises Exception.\n        \"\"\"\n\n        valid_load_types = [\"real_time\", \"day_ahead\"]\n\n        if self.load_type not in valid_load_types:\n            raise ValueError(\n                f\"Invalid load_type `{self.load_type}` given. Supported values are {valid_load_types}.\"\n            )\n\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource","title":"<code>BaseISOSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>class BaseISOSource(SourceInterface):\n    \"\"\"\n    Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options: list = []\n    spark_schema = StructType([StructField(\"id\", IntegerType(), True)])\n    default_query_timezone: str = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.query_timezone = pytz.timezone(\n            self.options.get(\"query_timezone\", self.default_query_timezone)\n        )\n        self.current_date = datetime.now(timezone.utc).astimezone(self.query_timezone)\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n        \"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.iso_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - {url}\")\n\n        response = requests.get(url)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n\n        return response.content\n\n    def _get_localized_datetime(self, datetime_str: str) -&gt; datetime:\n        \"\"\"\n        Converts string datetime into Python datetime object with configured format and timezone.\n        Args:\n            datetime_str: String to be converted into datetime.\n\n        Returns: Timezone aware datetime object.\n\n        \"\"\"\n        parsed_dt = datetime.strptime(datetime_str, self.query_datetime_format)\n        parsed_dt = parsed_dt.replace(tzinfo=self.query_timezone)\n        return parsed_dt\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Hits the fetch_from_url method with certain parameters to get raw data from API.\n\n        All the children ISO classes must override this method and call the fetch_url method\n        in it.\n\n        Returns:\n             Raw DataFrame from API.\n        \"\"\"\n\n        return pd.read_csv(BytesIO(self._fetch_from_url(\"\")))\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Performs all the basic transformations to prepare data for further processing.\n        All the children ISO classes must override this method.\n\n        Args:\n            df: Raw DataFrame, received from the API.\n\n        Returns:\n             Modified DataFrame, ready for basic use.\n\n        \"\"\"\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Another data transformation helper method to be called after prepare data.\n        Used for advance data processing such as cleaning, filtering, restructuring.\n        All the children ISO classes must override this method if there is any post-processing required.\n\n        Args:\n            df: Initial modified version of DataFrame, received after preparing the data.\n\n        Returns:\n             Final version of data after all the fixes and modifications.\n\n        \"\"\"\n        return df\n\n    def _get_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Entrypoint method to return the final version of DataFrame.\n\n        Returns:\n            Modified form of data for specific use case.\n\n        \"\"\"\n        df = self._pull_data()\n        df = self._prepare_data(df)\n        df = self._sanitize_data(df)\n\n        # Reorder columns to keep the data consistent\n        df = df[self.spark_schema.names]\n\n        return df\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Performs all the options checks. Raises exception in case of any invalid value.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        return True\n\n    def pre_read_validation(self) -&gt; bool:\n        \"\"\"\n        Ensures all the required options are provided and performs other validations.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        for key in self.required_options:\n            if key not in self.options:\n                raise ValueError(f\"Required option `{key}` is missing.\")\n\n        return self._validate_options()\n\n    def post_read_validation(self) -&gt; bool:\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n        Returns:\n             Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n        \"\"\"\n\n        try:\n            self.pre_read_validation()\n            pdf = self._get_data()\n            pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n            # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n            pd.DataFrame.iteritems = pd.DataFrame.items\n            df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n            return df\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n        Returns:\n             Final Spark DataFrame after all the processing.\n\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.pre_read_validation","title":"<code>pre_read_validation()</code>","text":"<p>Ensures all the required options are provided and performs other validations. Returns:      True if all checks are passed.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def pre_read_validation(self) -&gt; bool:\n    \"\"\"\n    Ensures all the required options are provided and performs other validations.\n    Returns:\n         True if all checks are passed.\n\n    \"\"\"\n    for key in self.required_options:\n        if key not in self.options:\n            raise ValueError(f\"Required option `{key}` is missing.\")\n\n    return self._validate_options()\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data. Returns:      Final Spark DataFrame converted from Pandas DataFrame post-execution.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n    Returns:\n         Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n    \"\"\"\n\n    try:\n        self.pre_read_validation()\n        pdf = self._get_data()\n        pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n        # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n        pd.DataFrame.iteritems = pd.DataFrame.items\n        df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n        return df\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_stream","title":"<code>read_stream()</code>","text":"<p>By default, the streaming operation is not supported but child classes can override if ISO supports streaming.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Final Spark DataFrame after all the processing.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n    Returns:\n         Final Spark DataFrame after all the processing.\n\n    \"\"\"\n\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PJMHistoricalPricingISOSource","title":"<code>PJMHistoricalPricingISOSource</code>","text":"<p>               Bases: <code>PJMDailyPricingISOSource</code></p> <p>The PJM Historical Pricing ISO Source is used to retrieve historical Real-Time and Day-Ahead hourly data from the PJM API.</p> <p>API:             https://api.pjm.com/api/v1/  (must be a valid apy key from PJM)</p> <p>Real-Time doc:    https://dataminer2.pjm.com/feed/rt_hrl_lmps/definition</p> <p>Day-Ahead doc:    https://dataminer2.pjm.com/feed/da_hrl_lmps/definition</p> <p>The PJM Historical Pricing ISO Source accesses the same PJM endpoints as the daily pricing source but is tailored for retrieving data within a specified historical range defined by the <code>start_date</code> and <code>end_date</code> attributes.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PJMHistoricalPricingISOSource--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import PJMHistoricalPricingISOSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\npjm_source = PJMHistoricalPricingISOSource(\n    spark=spark,\n    options={\n        \"api_key\": \"{api_key}\",\n        \"start_date\": \"2023-05-10\",\n        \"end_date\": \"2023-05-20\",\n    }\n)\n\npjm_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>The Spark Session instance.</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations.</p> required <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>A valid key from PJM required for authentication.</p> <code>load_type</code> <code>str</code> <p>The type of data to retrieve, either <code>real_time</code> or <code>day_ahead</code>.</p> <code>start_date</code> <code>str</code> <p>Must be in <code>YYYY-MM-DD</code> format.</p> <code>end_date</code> <code>str</code> <p>Must be in <code>YYYY-MM-DD</code> format.</p> <p>Please refer to the BaseISOSource for available methods and further details.</p> <p>BaseISOSource: ::: src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/pjm_historical_pricing_iso.py</code> <pre><code>class PJMHistoricalPricingISOSource(PJMDailyPricingISOSource):\n    \"\"\"\n    The PJM Historical Pricing ISO Source is used to retrieve historical Real-Time and Day-Ahead hourly data from the PJM API.\n\n    API:             &lt;a href=\"https://api.pjm.com/api/v1/\"&gt;https://api.pjm.com/api/v1/&lt;/a&gt;  (must be a valid apy key from PJM)\n\n    Real-Time doc:    &lt;a href=\"https://dataminer2.pjm.com/feed/rt_hrl_lmps/definition\"&gt;https://dataminer2.pjm.com/feed/rt_hrl_lmps/definition&lt;/a&gt;\n\n    Day-Ahead doc:    &lt;a href=\"https://dataminer2.pjm.com/feed/da_hrl_lmps/definition\"&gt;https://dataminer2.pjm.com/feed/da_hrl_lmps/definition&lt;/a&gt;\n\n    The PJM Historical Pricing ISO Source accesses the same PJM endpoints as the daily pricing source but is tailored for retrieving data within a specified historical range defined by the `start_date` and `end_date` attributes.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.sources import PJMHistoricalPricingISOSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    pjm_source = PJMHistoricalPricingISOSource(\n        spark=spark,\n        options={\n            \"api_key\": \"{api_key}\",\n            \"start_date\": \"2023-05-10\",\n            \"end_date\": \"2023-05-20\",\n        }\n    )\n\n    pjm_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): The Spark Session instance.\n        options (dict): A dictionary of ISO Source specific configurations.\n\n    Attributes:\n        api_key (str): A valid key from PJM required for authentication.\n        load_type (str): The type of data to retrieve, either `real_time` or `day_ahead`.\n        start_date (str): Must be in `YYYY-MM-DD` format.\n        end_date (str): Must be in `YYYY-MM-DD` format.\n\n    Please refer to the BaseISOSource for available methods and further details.\n\n    BaseISOSource: ::: src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso\"\"\"\n\n    spark: SparkSession\n    options: dict\n    required_options = [\"api_key\", \"load_type\", \"start_date\", \"end_date\"]\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super().__init__(spark, options)\n        self.spark: SparkSession = spark\n        self.options: dict = options\n        self.start_date: str = self.options.get(\"start_date\", \"\")\n        self.end_date: str = self.options.get(\"end_date\", \"\")\n        self.user_datetime_format = \"%Y-%m-%d\"\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Pulls historical pricing data from the PJM API within the specified date range.\n\n        Returns:\n            pd.DataFrame: A DataFrame containing the raw historical pricing data retrieved from the PJM API.\n        \"\"\"\n\n        logging.info(\n            f\"Historical data requested from {self.start_date} to {self.end_date}\"\n        )\n\n        start_date_str = datetime.strptime(\n            self.start_date, self.user_datetime_format\n        ).replace(hour=0, minute=0)\n        end_date_str = datetime.strptime(\n            self.end_date, self.user_datetime_format\n        ).replace(hour=23)\n\n        if self.load_type == \"day_ahead\":\n            url_suffix = \"da_hrl_lmps\"\n        else:\n            url_suffix = \"rt_hrl_lmps\"\n\n        data = self._fetch_paginated_data(url_suffix, start_date_str, end_date_str)\n\n        df = pd.DataFrame(data)\n        logging.info(f\"Data fetched successfully: {len(df)} rows\")\n\n        return df\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Validates all parameters including the following examples:\n            - `start_date` &amp; `end_data` must be in the correct format.\n            - `start_date` must be behind `end_data`.\n            - `start_date` must not be in the future (UTC).\n\n        Returns:\n            True if all looks good otherwise raises Exception.\n\n        \"\"\"\n        super()._validate_options()\n        try:\n            start_date = datetime.strptime(self.start_date, self.user_datetime_format)\n        except ValueError:\n            raise ValueError(\n                f\"Unable to parse Start date. Please specify in {self.user_datetime_format} format.\"\n            )\n\n        try:\n            end_date = datetime.strptime(self.end_date, self.user_datetime_format)\n        except ValueError:\n            raise ValueError(\n                f\"Unable to parse End date. Please specify in {self.user_datetime_format} format.\"\n            )\n\n        if start_date &gt; datetime.now(timezone.utc).replace(tzinfo=None) - timedelta(\n            days=1\n        ):\n            raise ValueError(\"Start date can't be in future.\")\n\n        if start_date &gt; end_date:\n            raise ValueError(\"Start date can't be ahead of End date.\")\n\n        if end_date &gt; datetime.now(timezone.utc).replace(tzinfo=None) - timedelta(\n            days=1\n        ):\n            raise ValueError(\"End date can't be in future.\")\n\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PJMHistoricalLoadISOSource","title":"<code>PJMHistoricalLoadISOSource</code>","text":"<p>               Bases: <code>PJMDailyLoadISOSource</code></p> <p>The PJM Historical Load ISO Source is used to read historical load data from PJM API.</p> <p>To read more about the reports, visit the following URLs -  Actual doc:    ops_sum_prev_period  Forecast doc:  load_frcstd_7_day</p> <p>Historical is the same PJM endpoint as Actual, but is called repeatedly within a range established by the start_date &amp; end_date attributes</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PJMHistoricalLoadISOSource--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import PJMHistoricalLoadISOSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\npjm_source = PJMHistoricalLoadISOSource(\n    spark=spark,\n    options={\n        \"api_key\": \"{api_key}\",\n        \"start_date\": \"20230510\",\n        \"end_date\": \"20230520\",\n    }\n)\n\npjm_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>Must be a valid key from PJM, see PJM documentation</p> <code>start_date</code> <code>str</code> <p>Must be in <code>YYYY-MM-DD</code> format.</p> <code>end_date</code> <code>str</code> <p>Must be in <code>YYYY-MM-DD</code> format.</p> <code>query_batch_days</code> <code>int</code> <p>(optional) Number of days must be &lt; 160 as per PJM &amp; is defaulted to <code>120</code></p> <code>sleep_duration</code> <code>int</code> <p>(optional) Number of seconds to sleep between request, defaulted to <code>5</code> seconds, used to manage requests to PJM endpoint</p> <code>request_count</code> <code>int</code> <p>(optional) Number of requests made to PJM endpoint before sleep_duration, currently defaulted to <code>1</code></p> <p>Please check the BaseISOSource for available methods.</p> BaseISOSource Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/pjm_historical_load_iso.py</code> <pre><code>class PJMHistoricalLoadISOSource(PJMDailyLoadISOSource):\n    \"\"\"\n    The PJM Historical Load ISO Source is used to read historical load data from PJM API.\n\n    To read more about the reports, visit the following URLs -\n    &lt;br&gt;\n    Actual doc:    [ops_sum_prev_period](https://dataminer2.pjm.com/feed/ops_sum_prev_period/definition)\n    &lt;br&gt;\n    Forecast doc:  [load_frcstd_7_day](https://dataminer2.pjm.com/feed/load_frcstd_7_day/definition)\n\n    Historical is the same PJM endpoint as Actual, but is called repeatedly within a range established by the\n    start_date &amp; end_date attributes\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.sources import PJMHistoricalLoadISOSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    pjm_source = PJMHistoricalLoadISOSource(\n        spark=spark,\n        options={\n            \"api_key\": \"{api_key}\",\n            \"start_date\": \"20230510\",\n            \"end_date\": \"20230520\",\n        }\n    )\n\n    pjm_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations (See Attributes table below)\n\n    Attributes:\n        api_key (str): Must be a valid key from PJM, see PJM documentation\n        start_date (str): Must be in `YYYY-MM-DD` format.\n        end_date (str): Must be in `YYYY-MM-DD` format.\n\n        query_batch_days (int): (optional) Number of days must be &lt; 160 as per PJM &amp; is defaulted to `120`\n        sleep_duration (int): (optional) Number of seconds to sleep between request, defaulted to `5` seconds, used to manage requests to PJM endpoint\n        request_count (int): (optional) Number of requests made to PJM endpoint before sleep_duration, currently defaulted to `1`\n\n    Please check the BaseISOSource for available methods.\n\n    BaseISOSource:\n        ::: src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso\"\"\"\n\n    spark: SparkSession\n    options: dict\n    required_options = [\"api_key\", \"start_date\", \"end_date\"]\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super().__init__(spark, options)\n        self.spark: SparkSession = spark\n        self.options: dict = options\n        self.api_key: str = self.options.get(\"api_key\", \"\").strip()\n        self.start_date: str = self.options.get(\"start_date\", \"\")\n        self.end_date: str = self.options.get(\"end_date\", \"\")\n        self.query_batch_days: int = self.options.get(\"query_batch_days\", 120)\n        self.sleep_duration: int = self.options.get(\"sleep_duration\", 5)\n        self.request_count: int = self.options.get(\"request_count\", 1)\n        self.load_type: str = \"actual\"\n        self.user_datetime_format = \"%Y-%m-%d\"\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Pulls data from the PJM API and parses the return including date ranges.\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n\n        logging.info(\n            f\"Historical load requested from {self.start_date} to {self.end_date}\"\n        )\n        start_date = datetime.strptime(self.start_date, self.user_datetime_format)\n        end_date = datetime.strptime(self.end_date, self.user_datetime_format).replace(\n            hour=23\n        )\n\n        days_diff = (end_date - start_date).days\n        logging.info(f\"Expected hours for a single zone = {(days_diff + 1) * 24}\")\n        generated_days_ranges = []\n        dates = pd.date_range(\n            start_date, end_date, freq=pd.DateOffset(days=self.query_batch_days)\n        )\n\n        for date in dates:\n            py_date = date.to_pydatetime()\n            date_last = (py_date + timedelta(days=self.query_batch_days - 1)).replace(\n                hour=23\n            )\n            date_last = min(date_last, end_date)\n            generated_days_ranges.append((py_date, date_last))\n\n        logging.info(\n            f\"Generated date ranges for batch days {self.query_batch_days} are {generated_days_ranges}\"\n        )\n\n        # Collect all historical data on yearly basis.\n        dfs = []\n        for idx, date_range in enumerate(generated_days_ranges):\n            start_date_str = date_range[0].strftime(self.query_datetime_format)\n            end_date_str = date_range[1].strftime(self.query_datetime_format)\n\n            df = pd.read_csv(\n                BytesIO(self._fetch_from_url(\"\", start_date_str, end_date_str))\n            )\n            dfs.append(df)\n\n            if idx &gt; 0 and idx % self.request_count == 0:\n                logging.info(f\"Going to sleep for {self.sleep_duration} seconds\")\n                time.sleep(self.sleep_duration)\n\n        df = pd.concat(dfs, sort=False)\n        df = df.reset_index(drop=True)\n        return df\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Validates all parameters including the following examples:\n            - `start_date` &amp; `end_data` must be in the correct format.\n            - `start_date` must be behind `end_data`.\n            - `start_date` must not be in the future (UTC).\n\n        Returns:\n            True if all looks good otherwise raises Exception.\n\n        \"\"\"\n\n        try:\n            start_date = datetime.strptime(self.start_date, self.user_datetime_format)\n        except ValueError:\n            raise ValueError(\n                f\"Unable to parse Start date. Please specify in {self.user_datetime_format} format.\"\n            )\n\n        try:\n            end_date = datetime.strptime(self.end_date, self.user_datetime_format)\n        except ValueError:\n            raise ValueError(\n                f\"Unable to parse End date. Please specify in {self.user_datetime_format} format.\"\n            )\n\n        if start_date &gt; datetime.now(timezone.utc).replace(tzinfo=None) - timedelta(\n            days=1\n        ):\n            raise ValueError(\"Start date can't be in future.\")\n\n        if start_date &gt; end_date:\n            raise ValueError(\"Start date can't be ahead of End date.\")\n\n        if end_date &gt; datetime.now(timezone.utc).replace(tzinfo=None) - timedelta(\n            days=1\n        ):\n            raise ValueError(\"End date can't be in future.\")\n\n        if self.sleep_duration &lt; 0:\n            raise ValueError(\"Sleep duration can't be negative.\")\n\n        if self.request_count &lt; 0:\n            raise ValueError(\"Request count can't be negative.\")\n\n        if self.query_batch_days &lt; 0:\n            raise ValueError(\"Query batch days count can't be negative.\")\n\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource","title":"<code>BaseISOSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>class BaseISOSource(SourceInterface):\n    \"\"\"\n    Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options: list = []\n    spark_schema = StructType([StructField(\"id\", IntegerType(), True)])\n    default_query_timezone: str = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.query_timezone = pytz.timezone(\n            self.options.get(\"query_timezone\", self.default_query_timezone)\n        )\n        self.current_date = datetime.now(timezone.utc).astimezone(self.query_timezone)\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n        \"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.iso_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - {url}\")\n\n        response = requests.get(url)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n\n        return response.content\n\n    def _get_localized_datetime(self, datetime_str: str) -&gt; datetime:\n        \"\"\"\n        Converts string datetime into Python datetime object with configured format and timezone.\n        Args:\n            datetime_str: String to be converted into datetime.\n\n        Returns: Timezone aware datetime object.\n\n        \"\"\"\n        parsed_dt = datetime.strptime(datetime_str, self.query_datetime_format)\n        parsed_dt = parsed_dt.replace(tzinfo=self.query_timezone)\n        return parsed_dt\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Hits the fetch_from_url method with certain parameters to get raw data from API.\n\n        All the children ISO classes must override this method and call the fetch_url method\n        in it.\n\n        Returns:\n             Raw DataFrame from API.\n        \"\"\"\n\n        return pd.read_csv(BytesIO(self._fetch_from_url(\"\")))\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Performs all the basic transformations to prepare data for further processing.\n        All the children ISO classes must override this method.\n\n        Args:\n            df: Raw DataFrame, received from the API.\n\n        Returns:\n             Modified DataFrame, ready for basic use.\n\n        \"\"\"\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Another data transformation helper method to be called after prepare data.\n        Used for advance data processing such as cleaning, filtering, restructuring.\n        All the children ISO classes must override this method if there is any post-processing required.\n\n        Args:\n            df: Initial modified version of DataFrame, received after preparing the data.\n\n        Returns:\n             Final version of data after all the fixes and modifications.\n\n        \"\"\"\n        return df\n\n    def _get_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Entrypoint method to return the final version of DataFrame.\n\n        Returns:\n            Modified form of data for specific use case.\n\n        \"\"\"\n        df = self._pull_data()\n        df = self._prepare_data(df)\n        df = self._sanitize_data(df)\n\n        # Reorder columns to keep the data consistent\n        df = df[self.spark_schema.names]\n\n        return df\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Performs all the options checks. Raises exception in case of any invalid value.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        return True\n\n    def pre_read_validation(self) -&gt; bool:\n        \"\"\"\n        Ensures all the required options are provided and performs other validations.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        for key in self.required_options:\n            if key not in self.options:\n                raise ValueError(f\"Required option `{key}` is missing.\")\n\n        return self._validate_options()\n\n    def post_read_validation(self) -&gt; bool:\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n        Returns:\n             Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n        \"\"\"\n\n        try:\n            self.pre_read_validation()\n            pdf = self._get_data()\n            pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n            # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n            pd.DataFrame.iteritems = pd.DataFrame.items\n            df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n            return df\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n        Returns:\n             Final Spark DataFrame after all the processing.\n\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.pre_read_validation","title":"<code>pre_read_validation()</code>","text":"<p>Ensures all the required options are provided and performs other validations. Returns:      True if all checks are passed.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def pre_read_validation(self) -&gt; bool:\n    \"\"\"\n    Ensures all the required options are provided and performs other validations.\n    Returns:\n         True if all checks are passed.\n\n    \"\"\"\n    for key in self.required_options:\n        if key not in self.options:\n            raise ValueError(f\"Required option `{key}` is missing.\")\n\n    return self._validate_options()\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data. Returns:      Final Spark DataFrame converted from Pandas DataFrame post-execution.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n    Returns:\n         Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n    \"\"\"\n\n    try:\n        self.pre_read_validation()\n        pdf = self._get_data()\n        pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n        # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n        pd.DataFrame.iteritems = pd.DataFrame.items\n        df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n        return df\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_stream","title":"<code>read_stream()</code>","text":"<p>By default, the streaming operation is not supported but child classes can override if ISO supports streaming.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Final Spark DataFrame after all the processing.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n    Returns:\n         Final Spark DataFrame after all the processing.\n\n    \"\"\"\n\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.CAISODailyLoadISOSource","title":"<code>CAISODailyLoadISOSource</code>","text":"<p>               Bases: <code>BaseISOSource</code></p> <p>The CAISO Daily Load ISO Source is used to read daily load data from CAISO API. It supports multiple types of data. Check the <code>load_types</code> attribute.</p> <p>To read more about the available reports from CAISO API, download the file -  Interface Specification</p> <p>From the list of reports in the file, it pulls the report named <code>CAISO Demand Forecast</code> in the file.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>load_types</code> <code>list</code> <p>Must be a subset of [<code>Demand Forecast 7-Day Ahead</code>, <code>Demand Forecast 2-Day Ahead</code>, <code>Demand Forecast Day Ahead</code>, <code>RTM 15Min Load Forecast</code>, <code>RTM 5Min Load Forecast</code>, <code>Total Actual Hourly Integrated Load</code>].  Default Value - <code>[Total Actual Hourly Integrated Load]</code>.</p> <code>date</code> <code>str</code> <p>Must be in <code>YYYY-MM-DD</code> format.</p> <p>Please check the BaseISOSource for available methods.</p> BaseISOSource Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/caiso_daily_load_iso.py</code> <pre><code>class CAISODailyLoadISOSource(BaseISOSource):\n    \"\"\"\n    The CAISO Daily Load ISO Source is used to read daily load data from CAISO API.\n    It supports multiple types of data. Check the `load_types` attribute.\n\n    To read more about the available reports from CAISO API, download the file -\n     [Interface Specification](https://www.caiso.com/Documents/OASISAPISpecification.pdf)\n\n    From the list of reports in the file, it pulls the report named `CAISO Demand Forecast` in the file.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations (See Attributes table below)\n\n    Attributes:\n        load_types (list): Must be a subset of [`Demand Forecast 7-Day Ahead`, `Demand Forecast 2-Day Ahead`, `Demand Forecast Day Ahead`, `RTM 15Min Load Forecast`, `RTM 5Min Load Forecast`, `Total Actual Hourly Integrated Load`]. &lt;br&gt; Default Value - `[Total Actual Hourly Integrated Load]`.\n        date (str): Must be in `YYYY-MM-DD` format.\n\n    Please check the BaseISOSource for available methods.\n\n    BaseISOSource:\n        ::: src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://oasis.caiso.com/oasisapi/SingleZip\"\n    query_datetime_format: str = \"%Y%m%dT00:00-0000\"\n    required_options = [\"load_types\", \"date\"]\n    spark_schema = CAISO_SCHEMA\n    default_query_timezone = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super().__init__(spark, options)\n        self.spark = spark\n        self.options = options\n        self.load_types = self.options.get(\n            \"load_types\", [\"Total Actual Hourly Integrated Load\"]\n        )\n        self.date = self.options.get(\"date\", \"\").strip()\n        self.user_datetime_format = \"%Y-%m-%d\"\n\n        # The following to fix the Security Check Error as the CAISO API is timing out with HTTPS protocol.\n        self.iso_url = self.iso_url.replace(\"s://\", \"://\")\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Pulls data from the CAISO API and parses the zip files for CSV data.\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n\n        logging.info(f\"Getting {self.load_types} data for date {self.date}\")\n        start_date = datetime.strptime(self.date, self.user_datetime_format)\n        end_date = start_date + timedelta(days=1)\n        return self._fetch_and_parse_zip(start_date, end_date)\n\n    def _fetch_and_parse_zip(\n        self, start_date: datetime, end_date: datetime\n    ) -&gt; pd.DataFrame:\n        suffix = (\n            f\"?resultformat=6&amp;\"\n            f\"queryname=SLD_FCST&amp;\"\n            \"version=1&amp;\"\n            f\"startdatetime={start_date.strftime(self.query_datetime_format)}&amp;\"\n            f\"enddatetime={end_date.strftime(self.query_datetime_format)}\"\n        )\n\n        content = self._fetch_from_url(suffix)\n        if not content:\n            raise HTTPError(\"Empty Response was returned\")\n        logging.info(\"Unzipping the file\")\n\n        zf = ZipFile(BytesIO(content))\n\n        csvs = list(filter(lambda name: \".csv\" in name, zf.namelist()))\n        if len(csvs) == 0:\n            raise ValueError(\"No data was found in the specified interval\")\n\n        df = pd.read_csv(zf.open(csvs[0]))\n        return df\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        date_cols = [\"INTERVALSTARTTIME_GMT\", \"INTERVALENDTIME_GMT\"]\n        for date_col in date_cols:\n            df[date_col] = df[date_col].apply(\n                lambda data: datetime.strptime(str(data)[:19], \"%Y-%m-%dT%H:%M:%S\")\n            )\n\n        df = df.rename(\n            columns={\n                \"INTERVALSTARTTIME_GMT\": \"StartTime\",\n                \"INTERVALENDTIME_GMT\": \"EndTime\",\n                \"LOAD_TYPE\": \"LoadType\",\n                \"OPR_DT\": \"OprDt\",\n                \"OPR_HR\": \"OprHr\",\n                \"OPR_INTERVAL\": \"OprInterval\",\n                \"MARKET_RUN_ID\": \"MarketRunId\",\n                \"TAC_AREA_NAME\": \"TacAreaName\",\n                \"LABEL\": \"Label\",\n                \"XML_DATA_ITEM\": \"XmlDataItem\",\n                \"POS\": \"Pos\",\n                \"MW\": \"Load\",\n                \"EXECUTION_TYPE\": \"ExecutionType\",\n                \"GROUP\": \"Group\",\n            }\n        )\n\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        df = df[df[\"Label\"].isin(self.load_types)]\n        return df\n\n    def _validate_options(self) -&gt; bool:\n        try:\n            datetime.strptime(self.date, self.user_datetime_format)\n        except ValueError:\n            raise ValueError(\n                f\"Unable to parse date. Please specify in {self.user_datetime_format} format.\"\n            )\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource","title":"<code>BaseISOSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>class BaseISOSource(SourceInterface):\n    \"\"\"\n    Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options: list = []\n    spark_schema = StructType([StructField(\"id\", IntegerType(), True)])\n    default_query_timezone: str = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.query_timezone = pytz.timezone(\n            self.options.get(\"query_timezone\", self.default_query_timezone)\n        )\n        self.current_date = datetime.now(timezone.utc).astimezone(self.query_timezone)\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n        \"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.iso_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - {url}\")\n\n        response = requests.get(url)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n\n        return response.content\n\n    def _get_localized_datetime(self, datetime_str: str) -&gt; datetime:\n        \"\"\"\n        Converts string datetime into Python datetime object with configured format and timezone.\n        Args:\n            datetime_str: String to be converted into datetime.\n\n        Returns: Timezone aware datetime object.\n\n        \"\"\"\n        parsed_dt = datetime.strptime(datetime_str, self.query_datetime_format)\n        parsed_dt = parsed_dt.replace(tzinfo=self.query_timezone)\n        return parsed_dt\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Hits the fetch_from_url method with certain parameters to get raw data from API.\n\n        All the children ISO classes must override this method and call the fetch_url method\n        in it.\n\n        Returns:\n             Raw DataFrame from API.\n        \"\"\"\n\n        return pd.read_csv(BytesIO(self._fetch_from_url(\"\")))\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Performs all the basic transformations to prepare data for further processing.\n        All the children ISO classes must override this method.\n\n        Args:\n            df: Raw DataFrame, received from the API.\n\n        Returns:\n             Modified DataFrame, ready for basic use.\n\n        \"\"\"\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Another data transformation helper method to be called after prepare data.\n        Used for advance data processing such as cleaning, filtering, restructuring.\n        All the children ISO classes must override this method if there is any post-processing required.\n\n        Args:\n            df: Initial modified version of DataFrame, received after preparing the data.\n\n        Returns:\n             Final version of data after all the fixes and modifications.\n\n        \"\"\"\n        return df\n\n    def _get_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Entrypoint method to return the final version of DataFrame.\n\n        Returns:\n            Modified form of data for specific use case.\n\n        \"\"\"\n        df = self._pull_data()\n        df = self._prepare_data(df)\n        df = self._sanitize_data(df)\n\n        # Reorder columns to keep the data consistent\n        df = df[self.spark_schema.names]\n\n        return df\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Performs all the options checks. Raises exception in case of any invalid value.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        return True\n\n    def pre_read_validation(self) -&gt; bool:\n        \"\"\"\n        Ensures all the required options are provided and performs other validations.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        for key in self.required_options:\n            if key not in self.options:\n                raise ValueError(f\"Required option `{key}` is missing.\")\n\n        return self._validate_options()\n\n    def post_read_validation(self) -&gt; bool:\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n        Returns:\n             Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n        \"\"\"\n\n        try:\n            self.pre_read_validation()\n            pdf = self._get_data()\n            pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n            # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n            pd.DataFrame.iteritems = pd.DataFrame.items\n            df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n            return df\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n        Returns:\n             Final Spark DataFrame after all the processing.\n\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.pre_read_validation","title":"<code>pre_read_validation()</code>","text":"<p>Ensures all the required options are provided and performs other validations. Returns:      True if all checks are passed.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def pre_read_validation(self) -&gt; bool:\n    \"\"\"\n    Ensures all the required options are provided and performs other validations.\n    Returns:\n         True if all checks are passed.\n\n    \"\"\"\n    for key in self.required_options:\n        if key not in self.options:\n            raise ValueError(f\"Required option `{key}` is missing.\")\n\n    return self._validate_options()\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data. Returns:      Final Spark DataFrame converted from Pandas DataFrame post-execution.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n    Returns:\n         Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n    \"\"\"\n\n    try:\n        self.pre_read_validation()\n        pdf = self._get_data()\n        pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n        # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n        pd.DataFrame.iteritems = pd.DataFrame.items\n        df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n        return df\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_stream","title":"<code>read_stream()</code>","text":"<p>By default, the streaming operation is not supported but child classes can override if ISO supports streaming.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Final Spark DataFrame after all the processing.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n    Returns:\n         Final Spark DataFrame after all the processing.\n\n    \"\"\"\n\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.CAISOHistoricalLoadISOSource","title":"<code>CAISOHistoricalLoadISOSource</code>","text":"<p>               Bases: <code>CAISODailyLoadISOSource</code></p> <p>The CAISO Historical Load ISO Source is used to read load data for an interval of dates  between start_date and end_date inclusive from CAISO API. It supports multiple types of data. Check the <code>load_types</code> attribute.</p> <p>To read more about the available reports from CAISO API, download the file -  Interface Specification</p> <p>From the list of reports in the file, it pulls the report named <code>CAISO Demand Forecast</code> in the file.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>load_types</code> <code>list</code> <p>Must be a subset of [<code>Demand Forecast 7-Day Ahead</code>, <code>Demand Forecast 2-Day Ahead</code>, <code>Demand Forecast Day Ahead</code>, <code>RTM 15Min Load Forecast</code>, <code>RTM 5Min Load Forecast</code>, <code>Total Actual Hourly Integrated Load</code>].  Default Value - <code>[Total Actual Hourly Integrated Load]</code>.</p> <code>start_date</code> <code>str</code> <p>Must be in <code>YYYY-MM-DD</code> format.</p> <code>end_date</code> <code>str</code> <p>Must be in <code>YYYY-MM-DD</code> format.</p> <p>Please check the BaseISOSource for available methods.</p> BaseISOSource Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/caiso_historical_load_iso.py</code> <pre><code>class CAISOHistoricalLoadISOSource(CAISODailyLoadISOSource):\n    \"\"\"\n    The CAISO Historical Load ISO Source is used to read load data for an interval of dates\n     between start_date and end_date inclusive from CAISO API.\n    It supports multiple types of data. Check the `load_types` attribute.\n\n    To read more about the available reports from CAISO API, download the file -\n     [Interface Specification](https://www.caiso.com/Documents/OASISAPISpecification.pdf)\n\n    From the list of reports in the file, it pulls the report named `CAISO Demand Forecast` in the file.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations (See Attributes table below)\n\n    Attributes:\n        load_types (list): Must be a subset of [`Demand Forecast 7-Day Ahead`, `Demand Forecast 2-Day Ahead`, `Demand Forecast Day Ahead`, `RTM 15Min Load Forecast`, `RTM 5Min Load Forecast`, `Total Actual Hourly Integrated Load`]. &lt;br&gt; Default Value - `[Total Actual Hourly Integrated Load]`.\n        start_date (str): Must be in `YYYY-MM-DD` format.\n        end_date (str): Must be in `YYYY-MM-DD` format.\n\n    Please check the BaseISOSource for available methods.\n\n    BaseISOSource:\n        ::: src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    required_options = [\"load_types\", \"start_date\", \"end_date\"]\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super().__init__(spark, options)\n        self.spark = spark\n        self.options = options\n        self.load_types = self.options.get(\n            \"load_types\", [\"Total Actual Hourly Integrated Load\"]\n        )\n        self.start_date = self.options.get(\"start_date\", \"\").strip()\n        self.end_date = self.options.get(\"end_date\", \"\").strip()\n        self.user_datetime_format = \"%Y-%m-%d\"\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Pulls data from the CAISO API and parses the zip files for CSV data.\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n\n        logging.info(\n            f\"Getting {self.load_types} data from {self.start_date} to {self.end_date}\"\n        )\n        start_date = datetime.strptime(self.start_date, self.user_datetime_format)\n        end_date = datetime.strptime(self.end_date, self.user_datetime_format)\n        end_date = end_date + timedelta(days=1)\n        generated_days_ranges = []\n        dates = pd.date_range(start_date, end_date, freq=\"30D\", inclusive=\"left\")\n\n        for date in dates:\n            py_date = date.to_pydatetime()\n            date_last = py_date + timedelta(days=30)\n            date_last = min(date_last, end_date)\n            generated_days_ranges.append((py_date, date_last))\n\n        logging.info(f\"Generated date ranges are {generated_days_ranges}\")\n\n        dfs = []\n        for idx, date_range in enumerate(generated_days_ranges):\n            start_date_str, end_date_str = date_range\n            df = self._fetch_and_parse_zip(start_date_str, end_date_str)\n\n            dfs.append(df)\n\n        return pd.concat(dfs)\n\n    def _validate_options(self) -&gt; bool:\n        try:\n            datetime.strptime(self.start_date, self.user_datetime_format)\n        except ValueError:\n            raise ValueError(\n                f\"Unable to parse start_date. Please specify in {self.user_datetime_format} format.\"\n            )\n\n        try:\n            datetime.strptime(self.end_date, self.user_datetime_format)\n        except ValueError:\n            raise ValueError(\n                f\"Unable to parse end_date. Please specify in {self.user_datetime_format} format.\"\n            )\n\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource","title":"<code>BaseISOSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>class BaseISOSource(SourceInterface):\n    \"\"\"\n    Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options: list = []\n    spark_schema = StructType([StructField(\"id\", IntegerType(), True)])\n    default_query_timezone: str = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.query_timezone = pytz.timezone(\n            self.options.get(\"query_timezone\", self.default_query_timezone)\n        )\n        self.current_date = datetime.now(timezone.utc).astimezone(self.query_timezone)\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n        \"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.iso_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - {url}\")\n\n        response = requests.get(url)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n\n        return response.content\n\n    def _get_localized_datetime(self, datetime_str: str) -&gt; datetime:\n        \"\"\"\n        Converts string datetime into Python datetime object with configured format and timezone.\n        Args:\n            datetime_str: String to be converted into datetime.\n\n        Returns: Timezone aware datetime object.\n\n        \"\"\"\n        parsed_dt = datetime.strptime(datetime_str, self.query_datetime_format)\n        parsed_dt = parsed_dt.replace(tzinfo=self.query_timezone)\n        return parsed_dt\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Hits the fetch_from_url method with certain parameters to get raw data from API.\n\n        All the children ISO classes must override this method and call the fetch_url method\n        in it.\n\n        Returns:\n             Raw DataFrame from API.\n        \"\"\"\n\n        return pd.read_csv(BytesIO(self._fetch_from_url(\"\")))\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Performs all the basic transformations to prepare data for further processing.\n        All the children ISO classes must override this method.\n\n        Args:\n            df: Raw DataFrame, received from the API.\n\n        Returns:\n             Modified DataFrame, ready for basic use.\n\n        \"\"\"\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Another data transformation helper method to be called after prepare data.\n        Used for advance data processing such as cleaning, filtering, restructuring.\n        All the children ISO classes must override this method if there is any post-processing required.\n\n        Args:\n            df: Initial modified version of DataFrame, received after preparing the data.\n\n        Returns:\n             Final version of data after all the fixes and modifications.\n\n        \"\"\"\n        return df\n\n    def _get_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Entrypoint method to return the final version of DataFrame.\n\n        Returns:\n            Modified form of data for specific use case.\n\n        \"\"\"\n        df = self._pull_data()\n        df = self._prepare_data(df)\n        df = self._sanitize_data(df)\n\n        # Reorder columns to keep the data consistent\n        df = df[self.spark_schema.names]\n\n        return df\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Performs all the options checks. Raises exception in case of any invalid value.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        return True\n\n    def pre_read_validation(self) -&gt; bool:\n        \"\"\"\n        Ensures all the required options are provided and performs other validations.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        for key in self.required_options:\n            if key not in self.options:\n                raise ValueError(f\"Required option `{key}` is missing.\")\n\n        return self._validate_options()\n\n    def post_read_validation(self) -&gt; bool:\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n        Returns:\n             Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n        \"\"\"\n\n        try:\n            self.pre_read_validation()\n            pdf = self._get_data()\n            pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n            # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n            pd.DataFrame.iteritems = pd.DataFrame.items\n            df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n            return df\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n        Returns:\n             Final Spark DataFrame after all the processing.\n\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.pre_read_validation","title":"<code>pre_read_validation()</code>","text":"<p>Ensures all the required options are provided and performs other validations. Returns:      True if all checks are passed.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def pre_read_validation(self) -&gt; bool:\n    \"\"\"\n    Ensures all the required options are provided and performs other validations.\n    Returns:\n         True if all checks are passed.\n\n    \"\"\"\n    for key in self.required_options:\n        if key not in self.options:\n            raise ValueError(f\"Required option `{key}` is missing.\")\n\n    return self._validate_options()\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data. Returns:      Final Spark DataFrame converted from Pandas DataFrame post-execution.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n    Returns:\n         Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n    \"\"\"\n\n    try:\n        self.pre_read_validation()\n        pdf = self._get_data()\n        pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n        # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n        pd.DataFrame.iteritems = pd.DataFrame.items\n        df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n        return df\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_stream","title":"<code>read_stream()</code>","text":"<p>By default, the streaming operation is not supported but child classes can override if ISO supports streaming.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Final Spark DataFrame after all the processing.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n    Returns:\n         Final Spark DataFrame after all the processing.\n\n    \"\"\"\n\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkWeatherCompanyBaseWeatherSource","title":"<code>SparkWeatherCompanyBaseWeatherSource</code>","text":"<p>               Bases: <code>BaseISOSource</code></p> <p>Base class for all the Weather related sources. Provides common functionality.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of Weather Source specific configurations.</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/the_weather_company/base_weather.py</code> <pre><code>class SparkWeatherCompanyBaseWeatherSource(BaseISOSource):\n    \"\"\"\n    Base class for all the Weather related sources. Provides common functionality.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of Weather Source specific configurations.\n\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    weather_url: str = \"https://\"\n    api_params: dict = {}\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super(SparkWeatherCompanyBaseWeatherSource, self).__init__(spark, options)\n        self.spark = spark\n        self.options = options\n\n    def _get_api_params(self) -&gt; dict:\n        return self.api_params\n\n    def _fetch_weather_from_url(self, url_suffix: str, params: dict) -&gt; bytes:\n        \"\"\"\n        Gets data from external Weather Forecast API.\n\n        Args:\n            url_suffix: String to be used as suffix to weather url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.weather_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - `{url}` with params - {params}\")\n\n        response = requests.get(url, params)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n        return response.content\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n        return self._fetch_weather_from_url(url_suffix, self._get_api_params())\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkWeatherCompanyForecastAPIV1Source","title":"<code>SparkWeatherCompanyForecastAPIV1Source</code>","text":"<p>               Bases: <code>SparkWeatherCompanyBaseWeatherSource</code></p> <p>The Weather Forecast API V1 Source is used to read 15 days forecast from the Weather API.</p> <p>URL:  https://api.weather.com/v1/geocode/32.3667/-95.4/forecast/hourly/360hour.json</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below).</p> required <p>Attributes:</p> Name Type Description <code>lat</code> <code>str</code> <p>Latitude of the Weather Station.</p> <code>lon</code> <code>str</code> <p>Longitude of the Weather Station.</p> <code>api_key</code> <code>str</code> <p>Weather API key.</p> <code>language</code> <code>str</code> <p>API response language. Defaults to <code>en-US</code>.</p> <code>units</code> <code>str</code> <p>Unit of measurements. Defaults to <code>e</code>.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/the_weather_company/weather_forecast_api_v1.py</code> <pre><code>class SparkWeatherCompanyForecastAPIV1Source(SparkWeatherCompanyBaseWeatherSource):\n    \"\"\"\n    The Weather Forecast API V1 Source is used to read 15 days forecast from the Weather API.\n\n    URL: &lt;a href=\"https://api.weather.com/v1/geocode/32.3667/-95.4/forecast/hourly/360hour.json\"&gt;\n    https://api.weather.com/v1/geocode/32.3667/-95.4/forecast/hourly/360hour.json&lt;/a&gt;\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations (See Attributes table below).\n\n    Attributes:\n        lat (str): Latitude of the Weather Station.\n        lon (str): Longitude of the Weather Station.\n        api_key (str): Weather API key.\n        language (str): API response language. Defaults to `en-US`.\n        units (str): Unit of measurements. Defaults to `e`.\n    \"\"\"\n\n    spark: SparkSession\n    spark_schema = WEATHER_FORECAST_SCHEMA\n    options: dict\n    weather_url: str = \"https://api.weather.com/v1/geocode/\"\n    required_options = [\"lat\", \"lon\", \"api_key\"]\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super(SparkWeatherCompanyForecastAPIV1Source, self).__init__(spark, options)\n        self.spark = spark\n        self.options = options\n        self.lat = self.options.get(\"lat\", \"\").strip()\n        self.lon = self.options.get(\"lon\", \"\").strip()\n        self.api_key = self.options.get(\"api_key\", \"\").strip()\n        self.language = self.options.get(\"language\", \"en-US\").strip()\n        self.units = self.options.get(\"units\", \"e\").strip()\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Prepares weather data for the use.\n\n        Args:\n            df: Data received after preparation.\n\n        Returns:\n            Final data after all the transformations.\n\n        \"\"\"\n\n        rename_cols = {\n            \"latitude\": \"Latitude\",\n            \"longitude\": \"Longitude\",\n            \"class\": \"Class\",\n            \"expire_time_gmt\": \"ExpireTimeGmt\",\n            \"fcst_valid\": \"FcstValid\",\n            \"fcst_valid_local\": \"FcstValidLocal\",\n            \"num\": \"Num\",\n            \"day_ind\": \"DayInd\",\n            \"temp\": \"Temp\",\n            \"dewpt\": \"Dewpt\",\n            \"hi\": \"Hi\",\n            \"wc\": \"Wc\",\n            \"feels_like\": \"FeelsLike\",\n            \"icon_extd\": \"IconExtd\",\n            \"wxman\": \"Wxman\",\n            \"icon_code\": \"IconCode\",\n            \"dow\": \"Dow\",\n            \"phrase_12char\": \"Phrase12Char\",\n            \"phrase_22char\": \"Phrase22Char\",\n            \"phrase_32char\": \"Phrase32Char\",\n            \"subphrase_pt1\": \"SubphrasePt1\",\n            \"subphrase_pt2\": \"SubphrasePt2\",\n            \"subphrase_pt3\": \"SubphrasePt3\",\n            \"pop\": \"Pop\",\n            \"precip_type\": \"PrecipType\",\n            \"qpf\": \"Qpf\",\n            \"snow_qpf\": \"SnowQpf\",\n            \"rh\": \"Rh\",\n            \"wspd\": \"Wspd\",\n            \"wdir\": \"Wdir\",\n            \"wdir_cardinal\": \"WdirCardinal\",\n            \"gust\": \"Gust\",\n            \"clds\": \"Clds\",\n            \"vis\": \"Vis\",\n            \"mslp\": \"Mslp\",\n            \"uv_index_raw\": \"UvIndexRaw\",\n            \"uv_index\": \"UvIndex\",\n            \"uv_warning\": \"UvWarning\",\n            \"uv_desc\": \"UvDesc\",\n            \"golf_index\": \"GolfIndex\",\n            \"golf_category\": \"GolfCategory\",\n            \"severity\": \"Severity\",\n        }\n\n        df = df.rename(columns=rename_cols)\n\n        fields = self.spark_schema.fields\n\n        str_cols = list(\n            map(\n                lambda x: x.name,\n                filter(lambda x: isinstance(x.dataType, StringType), fields),\n            )\n        )\n        double_cols = list(\n            map(\n                lambda x: x.name,\n                filter(lambda x: isinstance(x.dataType, DoubleType), fields),\n            )\n        )\n        int_cols = list(\n            map(\n                lambda x: x.name,\n                filter(lambda x: isinstance(x.dataType, IntegerType), fields),\n            )\n        )\n\n        df[str_cols] = df[str_cols].astype(str)\n        df[double_cols] = df[double_cols].astype(float)\n        df[int_cols] = df[int_cols].astype(int)\n\n        df.reset_index(inplace=True, drop=True)\n\n        return df\n\n    def _get_api_params(self):\n        params = {\n            \"language\": self.language,\n            \"units\": self.units,\n            \"apiKey\": self.api_key,\n        }\n        return params\n\n    def _pull_for_weather_station(self, lat: str, lon: str) -&gt; pd.DataFrame:\n        response = json.loads(\n            self._fetch_from_url(f\"{lat}/{lon}/forecast/hourly/360hour.json\").decode(\n                \"utf-8\"\n            )\n        )\n        return pd.DataFrame(response[\"forecasts\"])\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Pulls data from the Weather API and parses the JSON file.\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n\n        df = self._pull_for_weather_station(self.lat, self.lon)\n        df[\"latitude\"] = self.lat\n        df[\"longitude\"] = self.lon\n\n        return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkWeatherCompanyForecastAPIV1MultiSource","title":"<code>SparkWeatherCompanyForecastAPIV1MultiSource</code>","text":"<p>               Bases: <code>SparkWeatherCompanyForecastAPIV1Source</code></p> <p>The Weather Forecast API V1 Multi Source is used to read 15 days forecast from the Weather API. It allows to pull weather data for multiple stations and returns all of them in a single DataFrame.</p> <p>URL for one station:  https://api.weather.com/v1/geocode/32.3667/-95.4/forecast/hourly/360hour.json</p> <p>It takes a list of Weather Stations. Each station item must contain comma separated Latitude &amp; Longitude.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkWeatherCompanyForecastAPIV1MultiSource--examples","title":"Examples","text":"<p><code>[\"32.3667,-95.4\", \"51.52,-0.11\"]</code></p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below).</p> required <p>Attributes:</p> Name Type Description <code>stations</code> <code>list[str]</code> <p>List of Weather Stations.</p> <code>api_key</code> <code>str</code> <p>Weather API key.</p> <code>language</code> <code>str</code> <p>API response language. Defaults to <code>en-US</code>.</p> <code>units</code> <code>str</code> <p>Unit of measurements. Defaults to <code>e</code>.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/the_weather_company/weather_forecast_api_v1_multi.py</code> <pre><code>class SparkWeatherCompanyForecastAPIV1MultiSource(\n    SparkWeatherCompanyForecastAPIV1Source\n):\n    \"\"\"\n    The Weather Forecast API V1 Multi Source is used to read 15 days forecast from the Weather API. It allows to\n    pull weather data for multiple stations and returns all of them in a single DataFrame.\n\n    URL for one station: &lt;a href=\"https://api.weather.com/v1/geocode/32.3667/-95.4/forecast/hourly/360hour.json\"&gt;\n    https://api.weather.com/v1/geocode/32.3667/-95.4/forecast/hourly/360hour.json&lt;/a&gt;\n\n    It takes a list of Weather Stations. Each station item must contain comma separated Latitude &amp; Longitude.\n\n    Examples\n    --------\n    `[\"32.3667,-95.4\", \"51.52,-0.11\"]`\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations (See Attributes table below).\n\n    Attributes:\n        stations (list[str]): List of Weather Stations.\n        api_key (str): Weather API key.\n        language (str): API response language. Defaults to `en-US`.\n        units (str): Unit of measurements. Defaults to `e`.\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    spark_schema = WEATHER_FORECAST_SCHEMA\n    required_options = [\"stations\", \"api_key\"]\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super(SparkWeatherCompanyForecastAPIV1MultiSource, self).__init__(\n            spark, options\n        )\n        self.spark = spark\n        self.options = options\n        self.stations = self.options.get(\"stations\", [])\n        self.api_key = self.options.get(\"api_key\", \"\").strip()\n        self.language = self.options.get(\"language\", \"en-US\").strip()\n        self.units = self.options.get(\"units\", \"e\").strip()\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Pulls data from the Weather API and parses the JSON file for multiple stations\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n\n        result_df = None\n        for station in self.stations:\n            parts = station.split(\",\")\n            lat, lon = parts\n\n            df = self._pull_for_weather_station(lat, lon)\n            df[\"latitude\"] = lat\n            df[\"longitude\"] = lon\n\n            if result_df is not None:\n                result_df = pd.concat([result_df, df])\n            else:\n                result_df = df\n\n        return result_df\n\n    def _validate_options(self) -&gt; bool:\n        for station in self.stations:\n            parts = station.split(\",\")\n\n            if len(parts) != 2 or parts[0].strip() == \"\" or parts[1].strip() == \"\":\n                raise ValueError(\n                    f\"Each station item must contain comma separated Latitude &amp; Longitude. Eg: 10.23:45.2\"\n                )\n\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PythonDeltaSource","title":"<code>PythonDeltaSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>The Python Delta Source is used to read data from a Delta table without using Apache Spark, returning a Polars LazyFrame.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PythonDeltaSource--example","title":"Example","text":"AzureAWS <pre><code>from rtdip_sdk.pipelines.sources import PythonDeltaSource\n\npath = \"abfss://{FILE-SYSTEM}@{ACCOUNT-NAME}.dfs.core.windows.net/{PATH}/{FILE-NAME}\n\npython_delta_source = PythonDeltaSource(\n    path=path,\n    version=None,\n    storage_options={\n        \"azure_storage_account_name\": \"{AZURE-STORAGE-ACCOUNT-NAME}\",\n        \"azure_storage_account_key\": \"{AZURE-STORAGE-ACCOUNT-KEY}\"\n    },\n    pyarrow_options=None,\n    without_files=False\n)\n\npython_delta_source.read_batch()\n</code></pre> <pre><code>from rtdip_sdk.pipelines.sources import PythonDeltaSource\n\npath = \"https://s3.{REGION-CODE}.amazonaws.com/{BUCKET-NAME}/{KEY-NAME}\"\n\npython_delta_source = PythonDeltaSource(\n    path=path,\n    version=None,\n    storage_options={\n        \"aws_access_key_id\": \"{AWS-ACCESS-KEY-ID}\",\n        \"aws_secret_access_key\": \"{AWS-SECRET-ACCESS-KEY}\"\n    },\n    pyarrow_options=None,\n    without_files=False\n)\n\npython_delta_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the Delta table. Can be local or in S3/Azure storage</p> required <code>version</code> <code>optional int</code> <p>Specify the Delta table version to read from. Defaults to the latest version</p> <code>None</code> <code>storage_options</code> <code>optional dict</code> <p>Used to read from AWS/Azure storage. For AWS use format {\"aws_access_key_id\": \"&lt;&gt;\", \"aws_secret_access_key\":\"&lt;&gt;\"}. For Azure use format {\"azure_storage_account_name\": \"&lt;&gt;\", \"azure_storage_account_key\": \"&lt;&gt;\"}.</p> <code>None</code> <code>pyarrow_options</code> <code>optional dict</code> <p>Data Access and Efficiency options when reading from Delta. See to_pyarrow_dataset.</p> <code>None</code> <code>without_files</code> <code>optional bool</code> <p>If True loads the table without tracking files</p> <code>False</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/delta.py</code> <pre><code>class PythonDeltaSource(SourceInterface):\n    \"\"\"\n    The Python Delta Source is used to read data from a Delta table without using Apache Spark, returning a Polars LazyFrame.\n\n     Example\n    --------\n    === \"Azure\"\n\n        ```python\n        from rtdip_sdk.pipelines.sources import PythonDeltaSource\n\n        path = \"abfss://{FILE-SYSTEM}@{ACCOUNT-NAME}.dfs.core.windows.net/{PATH}/{FILE-NAME}\n\n        python_delta_source = PythonDeltaSource(\n            path=path,\n            version=None,\n            storage_options={\n                \"azure_storage_account_name\": \"{AZURE-STORAGE-ACCOUNT-NAME}\",\n                \"azure_storage_account_key\": \"{AZURE-STORAGE-ACCOUNT-KEY}\"\n            },\n            pyarrow_options=None,\n            without_files=False\n        )\n\n        python_delta_source.read_batch()\n        ```\n    === \"AWS\"\n\n        ```python\n        from rtdip_sdk.pipelines.sources import PythonDeltaSource\n\n        path = \"https://s3.{REGION-CODE}.amazonaws.com/{BUCKET-NAME}/{KEY-NAME}\"\n\n        python_delta_source = PythonDeltaSource(\n            path=path,\n            version=None,\n            storage_options={\n                \"aws_access_key_id\": \"{AWS-ACCESS-KEY-ID}\",\n                \"aws_secret_access_key\": \"{AWS-SECRET-ACCESS-KEY}\"\n            },\n            pyarrow_options=None,\n            without_files=False\n        )\n\n        python_delta_source.read_batch()\n        ```\n\n    Parameters:\n        path (str): Path to the Delta table. Can be local or in S3/Azure storage\n        version (optional int): Specify the Delta table version to read from. Defaults to the latest version\n        storage_options (optional dict): Used to read from AWS/Azure storage. For AWS use format {\"aws_access_key_id\": \"&lt;&gt;\", \"aws_secret_access_key\":\"&lt;&gt;\"}. For Azure use format {\"azure_storage_account_name\": \"&lt;&gt;\", \"azure_storage_account_key\": \"&lt;&gt;\"}.\n        pyarrow_options (optional dict): Data Access and Efficiency options when reading from Delta. See [to_pyarrow_dataset](https://delta-io.github.io/delta-rs/python/api_reference.html#deltalake.table.DeltaTable.to_pyarrow_dataset){ target=\"_blank\" }.\n        without_files (optional bool): If True loads the table without tracking files\n    \"\"\"\n\n    path: str\n    version: int\n    storage_options: dict\n    pyarrow_options: dict\n    without_files: bool\n\n    def __init__(\n        self,\n        path: str,\n        version: int = None,\n        storage_options: dict = None,\n        pyarrow_options: dict = None,\n        without_files: bool = False,\n    ):\n        self.path = path\n        self.version = version\n        self.storage_options = storage_options\n        self.pyarrow_options = pyarrow_options\n        self.without_files = without_files\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYTHON\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self):\n        return True\n\n    def read_batch(self) -&gt; LazyFrame:\n        \"\"\"\n        Reads data from a Delta table into a Polars LazyFrame\n        \"\"\"\n        without_files_dict = {\"without_files\": self.without_files}\n        lf = pl.scan_delta(\n            source=self.path,\n            version=self.version,\n            storage_options=self.storage_options,\n            delta_table_options=without_files_dict,\n            pyarrow_options=self.pyarrow_options,\n        )\n        return lf\n\n    def read_stream(self):\n        \"\"\"\n        Raises:\n            NotImplementedError: Reading from a Delta table using Python is only possible for batch reads. To perform a streaming read, use the read_stream method of the SparkDeltaSource component.\n        \"\"\"\n        raise NotImplementedError(\n            \"Reading from a Delta table using Python is only possible for batch reads. To perform a streaming read, use the read_stream method of the SparkDeltaSource component\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PythonDeltaSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYTHON</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/delta.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYTHON\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PythonDeltaSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads data from a Delta table into a Polars LazyFrame</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/delta.py</code> <pre><code>def read_batch(self) -&gt; LazyFrame:\n    \"\"\"\n    Reads data from a Delta table into a Polars LazyFrame\n    \"\"\"\n    without_files_dict = {\"without_files\": self.without_files}\n    lf = pl.scan_delta(\n        source=self.path,\n        version=self.version,\n        storage_options=self.storage_options,\n        delta_table_options=without_files_dict,\n        pyarrow_options=self.pyarrow_options,\n    )\n    return lf\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PythonDeltaSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Reading from a Delta table using Python is only possible for batch reads. To perform a streaming read, use the read_stream method of the SparkDeltaSource component.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/delta.py</code> <pre><code>def read_stream(self):\n    \"\"\"\n    Raises:\n        NotImplementedError: Reading from a Delta table using Python is only possible for batch reads. To perform a streaming read, use the read_stream method of the SparkDeltaSource component.\n    \"\"\"\n    raise NotImplementedError(\n        \"Reading from a Delta table using Python is only possible for batch reads. To perform a streaming read, use the read_stream method of the SparkDeltaSource component\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PythonDeltaSharingSource","title":"<code>PythonDeltaSharingSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>The Python Delta Sharing Source is used to read data from a Delta table with Delta Sharing configured, without using Apache Spark.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PythonDeltaSharingSource--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import PythonDeltaSharingSource\n\npython_delta_sharing_source = PythonDeltaSharingSource(\n    profile_path=\"{CREDENTIAL-FILE-LOCATION}\",\n    share_name=\"{SHARE-NAME}\",\n    schema_name=\"{SCHEMA-NAME}\",\n    table_name=\"{TABLE-NAME}\"\n)\n\npython_delta_sharing_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>profile_path</code> <code>str</code> <p>Location of the credential file. Can be any URL supported by FSSPEC</p> required <code>share_name</code> <code>str</code> <p>The value of 'share=' for the table</p> required <code>schema_name</code> <code>str</code> <p>The value of 'schema=' for the table</p> required <code>table_name</code> <code>str</code> <p>The value of 'name=' for the table</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/delta_sharing.py</code> <pre><code>class PythonDeltaSharingSource(SourceInterface):\n    \"\"\"\n    The Python Delta Sharing Source is used to read data from a Delta table with Delta Sharing configured, without using Apache Spark.\n\n    Example\n    -------\n    ```python\n    from rtdip_sdk.pipelines.sources import PythonDeltaSharingSource\n\n    python_delta_sharing_source = PythonDeltaSharingSource(\n        profile_path=\"{CREDENTIAL-FILE-LOCATION}\",\n        share_name=\"{SHARE-NAME}\",\n        schema_name=\"{SCHEMA-NAME}\",\n        table_name=\"{TABLE-NAME}\"\n    )\n\n    python_delta_sharing_source.read_batch()\n    ```\n\n    Parameters:\n        profile_path (str): Location of the credential file. Can be any URL supported by [FSSPEC](https://filesystem-spec.readthedocs.io/en/latest/index.html){ target=\"_blank\" }\n        share_name (str): The value of 'share=' for the table\n        schema_name (str): The value of 'schema=' for the table\n        table_name (str): The value of 'name=' for the table\n    \"\"\"\n\n    profile_path: str\n    share_name: str\n    schema_name: str\n    table_name: str\n\n    def __init__(\n        self, profile_path: str, share_name: str, schema_name: str, table_name: str\n    ):\n        self.profile_path = profile_path\n        self.share_name = share_name\n        self.schema_name = schema_name\n        self.table_name = table_name\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYTHON\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self):\n        return True\n\n    def read_batch(self) -&gt; LazyFrame:\n        \"\"\"\n        Reads data from a Delta table with Delta Sharing into a Polars LazyFrame.\n        \"\"\"\n        pandas_df = delta_sharing.load_as_pandas(\n            f\"{self.profile_path}#{self.share_name}.{self.schema_name}.{self.table_name}\"\n        )\n        polars_lazyframe = pl.from_pandas(pandas_df).lazy()\n        return polars_lazyframe\n\n    def read_stream(self):\n        \"\"\"\n        Raises:\n            NotImplementedError: Reading from a Delta table with Delta Sharing using Python is only possible for batch reads.\n        \"\"\"\n        raise NotImplementedError(\n            \"Reading from a Delta table with Delta Sharing using Python is only possible for batch reads.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PythonDeltaSharingSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYTHON</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/delta_sharing.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYTHON\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PythonDeltaSharingSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads data from a Delta table with Delta Sharing into a Polars LazyFrame.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/delta_sharing.py</code> <pre><code>def read_batch(self) -&gt; LazyFrame:\n    \"\"\"\n    Reads data from a Delta table with Delta Sharing into a Polars LazyFrame.\n    \"\"\"\n    pandas_df = delta_sharing.load_as_pandas(\n        f\"{self.profile_path}#{self.share_name}.{self.schema_name}.{self.table_name}\"\n    )\n    polars_lazyframe = pl.from_pandas(pandas_df).lazy()\n    return polars_lazyframe\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PythonDeltaSharingSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Reading from a Delta table with Delta Sharing using Python is only possible for batch reads.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/delta_sharing.py</code> <pre><code>def read_stream(self):\n    \"\"\"\n    Raises:\n        NotImplementedError: Reading from a Delta table with Delta Sharing using Python is only possible for batch reads.\n    \"\"\"\n    raise NotImplementedError(\n        \"Reading from a Delta table with Delta Sharing using Python is only possible for batch reads.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkECMWFBaseMarsSource","title":"<code>SparkECMWFBaseMarsSource</code>","text":"<p>Download nc files from ECMWF MARS server using the ECMWF python API. Data is downloaded in parallel using joblib from ECMWF MARS server using the ECMWF python API.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str</code> <p>Path to local directory where the nc files will be stored, in format \"yyyy-mm-dd_HH.nc\"</p> required <code>date_start</code> <code>str</code> <p>Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format</p> required <code>date_end</code> <code>str</code> <p>End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format</p> required <code>ecmwf_api_key</code> <code>str</code> <p>API key for ECMWF MARS server</p> required <code>ecmwf_api_email</code> <code>str</code> <p>Email for ECMWF MARS server</p> required <code>ecmwf_api_url</code> <code>str</code> <p>URL for ECMWF MARS server</p> <code>'https://api.ecmwf.int/v1'</code> <code>run_frequency</code> <code>str</code> <p>Frequency format of runs to download, e.g. \"H\"</p> <code>'H'</code> <code>run_interval</code> <code>str</code> <p>Interval of runs, e.g. a run_frequency of \"H\" and run_interval of \"12\" will extract the data of the 00 and 12 run for each day.</p> <code>'12'</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/ecmwf/base_mars.py</code> <pre><code>class SparkECMWFBaseMarsSource:\n    \"\"\"\n    Download nc files from ECMWF MARS server using the ECMWF python API.\n    Data is downloaded in parallel using joblib from ECMWF MARS server using the ECMWF python API.\n\n    Parameters:\n        save_path (str): Path to local directory where the nc files will be stored, in format \"yyyy-mm-dd_HH.nc\"\n        date_start (str): Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format\n        date_end (str): End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format\n        ecmwf_api_key (str): API key for ECMWF MARS server\n        ecmwf_api_email (str): Email for ECMWF MARS server\n        ecmwf_api_url (str): URL for ECMWF MARS server\n        run_frequency (str):Frequency format of runs to download, e.g. \"H\"\n        run_interval (str): Interval of runs, e.g. a run_frequency of \"H\" and run_interval of \"12\" will extract the data of the 00 and 12 run for each day.\n    \"\"\"\n\n    def __init__(\n        self,\n        date_start: str,\n        date_end: str,\n        save_path: str,\n        ecmwf_api_key: str,\n        ecmwf_api_email: str,\n        ecmwf_api_url: str = \"https://api.ecmwf.int/v1\",\n        run_interval: str = \"12\",\n        run_frequency: str = \"H\",\n    ):\n        self.retrieve_ran = False\n        self.date_start = date_start\n        self.date_end = date_end\n        self.save_path = save_path\n        self.format = format\n        self.run_interval = run_interval\n        self.run_frequency = run_frequency\n        self.ecmwf_api_key = ecmwf_api_key\n        self.ecmwf_api_url = ecmwf_api_url\n        self.ecmwf_api_email = ecmwf_api_email\n\n        # Pandas date_list (info best retrieved per forecast day)\n        self.dates = pd.date_range(\n            start=date_start, end=date_end, freq=run_interval + run_frequency\n        )\n\n    def retrieve(\n        self,\n        mars_dict: dict,\n        n_jobs=None,\n        backend=\"loky\",\n        tries=5,\n        cost=False,\n    ):\n        \"\"\"Retrieve the data from the server.\n\n        Function will use the ecmwf api to download the data from the server.\n        Note that mars has a max of two active requests per user and 20 queued\n        requests.\n        Data is downloaded in parallel using joblib from ECMWF MARS server using the ECMWF python API.\n\n\n        Parameters:\n            mars_dict (dict): Dictionary of mars parameters.\n            n_jobs (int, optional): Download in parallel? by default None, i.e. no parallelization\n            backend (str, optional): Specify the parallelization backend implementation in joblib, by default \"loky\"\n            tries (int, optional): Number of tries for each request if it fails, by default 5\n            cost (bool, optional):  Pass a cost request to mars to estimate the size and efficiency of your request, but not actually download the data. Can be useful for defining requests, by default False.\n        \"\"\"\n        chk = [\"date\", \"target\", \"time\", \"format\", \"output\"]\n        for i in chk:\n            if i in mars_dict.keys():\n                raise ValueError(f\"don't include {i} in the mars_dict\")\n\n        parallel = Parallel(n_jobs=n_jobs, backend=backend)\n\n        def _retrieve_datetime(i, j, cost=cost):\n            i_dict = {\"date\": i, \"time\": j}\n\n            if cost:\n                filename = f\"{i}_{j}.txt\"  # NOSONAR\n            else:\n                filename = f\"{i}_{j}.nc\"\n                i_dict[\"format\"] = \"netcdf\"  # NOSONAR\n\n            target = os.path.join(self.save_path, filename)\n            msg = f\"retrieving mars data --- {filename}\"\n\n            req_dict = {**i_dict, **mars_dict}\n            for k, v in req_dict.items():\n                if isinstance(v, (list, tuple)):\n                    req_dict[k] = \"/\".join([str(x) for x in v])  # NOSONAR\n\n            req_dict = [\"{}={}\".format(k, v) for k, v in req_dict.items()]\n            if cost:\n                req_dict = \"list,output=cost,{}\".format(\",\".join(req_dict))  # NOSONAR\n            else:\n                req_dict = \"retrieve,{}\".format(\",\".join(req_dict))  # NOSONAR\n\n            for j in range(tries):\n                try:\n                    print(msg)\n                    server = ECMWFService(\n                        \"mars\",\n                        url=self.ecmwf_api_url,\n                        email=self.ecmwf_api_email,\n                        key=self.ecmwf_api_key,\n                    )\n                    server.execute(req_dict, target)\n                    return 1  # NOSONAR\n                except:  # NOSONAR\n                    if j &lt; tries - 1:\n                        continue  # NOSONAR\n                    else:\n                        return 0  # NOSONAR\n\n        self.success = parallel(\n            delayed(_retrieve_datetime)(str(k.date()), f\"{k.hour:02}\")\n            for k in self.dates\n        )\n        self.retrieve_ran = True\n\n        return self\n\n    def info(self) -&gt; pd.Series:\n        \"\"\"\n        Return info on each ECMWF request.\n\n        Returns:\n            pd.Series: Successful request for each run == 1.\n        \"\"\"\n        if not self.retrieve_ran:\n            raise ValueError(\n                \"Before using self.info(), prepare the request using \"\n                + \"self.retrieve()\"\n            )\n        y = pd.Series(self.success, index=self.dates, name=\"success\", dtype=bool)\n\n        return y\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkECMWFBaseMarsSource.retrieve","title":"<code>retrieve(mars_dict, n_jobs=None, backend='loky', tries=5, cost=False)</code>","text":"<p>Retrieve the data from the server.</p> <p>Function will use the ecmwf api to download the data from the server. Note that mars has a max of two active requests per user and 20 queued requests. Data is downloaded in parallel using joblib from ECMWF MARS server using the ECMWF python API.</p> <p>Parameters:</p> Name Type Description Default <code>mars_dict</code> <code>dict</code> <p>Dictionary of mars parameters.</p> required <code>n_jobs</code> <code>int</code> <p>Download in parallel? by default None, i.e. no parallelization</p> <code>None</code> <code>backend</code> <code>str</code> <p>Specify the parallelization backend implementation in joblib, by default \"loky\"</p> <code>'loky'</code> <code>tries</code> <code>int</code> <p>Number of tries for each request if it fails, by default 5</p> <code>5</code> <code>cost</code> <code>bool</code> <p>Pass a cost request to mars to estimate the size and efficiency of your request, but not actually download the data. Can be useful for defining requests, by default False.</p> <code>False</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/ecmwf/base_mars.py</code> <pre><code>def retrieve(\n    self,\n    mars_dict: dict,\n    n_jobs=None,\n    backend=\"loky\",\n    tries=5,\n    cost=False,\n):\n    \"\"\"Retrieve the data from the server.\n\n    Function will use the ecmwf api to download the data from the server.\n    Note that mars has a max of two active requests per user and 20 queued\n    requests.\n    Data is downloaded in parallel using joblib from ECMWF MARS server using the ECMWF python API.\n\n\n    Parameters:\n        mars_dict (dict): Dictionary of mars parameters.\n        n_jobs (int, optional): Download in parallel? by default None, i.e. no parallelization\n        backend (str, optional): Specify the parallelization backend implementation in joblib, by default \"loky\"\n        tries (int, optional): Number of tries for each request if it fails, by default 5\n        cost (bool, optional):  Pass a cost request to mars to estimate the size and efficiency of your request, but not actually download the data. Can be useful for defining requests, by default False.\n    \"\"\"\n    chk = [\"date\", \"target\", \"time\", \"format\", \"output\"]\n    for i in chk:\n        if i in mars_dict.keys():\n            raise ValueError(f\"don't include {i} in the mars_dict\")\n\n    parallel = Parallel(n_jobs=n_jobs, backend=backend)\n\n    def _retrieve_datetime(i, j, cost=cost):\n        i_dict = {\"date\": i, \"time\": j}\n\n        if cost:\n            filename = f\"{i}_{j}.txt\"  # NOSONAR\n        else:\n            filename = f\"{i}_{j}.nc\"\n            i_dict[\"format\"] = \"netcdf\"  # NOSONAR\n\n        target = os.path.join(self.save_path, filename)\n        msg = f\"retrieving mars data --- {filename}\"\n\n        req_dict = {**i_dict, **mars_dict}\n        for k, v in req_dict.items():\n            if isinstance(v, (list, tuple)):\n                req_dict[k] = \"/\".join([str(x) for x in v])  # NOSONAR\n\n        req_dict = [\"{}={}\".format(k, v) for k, v in req_dict.items()]\n        if cost:\n            req_dict = \"list,output=cost,{}\".format(\",\".join(req_dict))  # NOSONAR\n        else:\n            req_dict = \"retrieve,{}\".format(\",\".join(req_dict))  # NOSONAR\n\n        for j in range(tries):\n            try:\n                print(msg)\n                server = ECMWFService(\n                    \"mars\",\n                    url=self.ecmwf_api_url,\n                    email=self.ecmwf_api_email,\n                    key=self.ecmwf_api_key,\n                )\n                server.execute(req_dict, target)\n                return 1  # NOSONAR\n            except:  # NOSONAR\n                if j &lt; tries - 1:\n                    continue  # NOSONAR\n                else:\n                    return 0  # NOSONAR\n\n    self.success = parallel(\n        delayed(_retrieve_datetime)(str(k.date()), f\"{k.hour:02}\")\n        for k in self.dates\n    )\n    self.retrieve_ran = True\n\n    return self\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkECMWFBaseMarsSource.info","title":"<code>info()</code>","text":"<p>Return info on each ECMWF request.</p> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Successful request for each run == 1.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/ecmwf/base_mars.py</code> <pre><code>def info(self) -&gt; pd.Series:\n    \"\"\"\n    Return info on each ECMWF request.\n\n    Returns:\n        pd.Series: Successful request for each run == 1.\n    \"\"\"\n    if not self.retrieve_ran:\n        raise ValueError(\n            \"Before using self.info(), prepare the request using \"\n            + \"self.retrieve()\"\n        )\n    y = pd.Series(self.success, index=self.dates, name=\"success\", dtype=bool)\n\n    return y\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkECMWFWeatherForecastSource","title":"<code>SparkECMWFWeatherForecastSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>The Weather Forecast API V1 Source class to doownload nc files from ECMWF MARS server using the ECMWF python API.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>save_path</code> <code>str</code> <p>Path to local directory where the nc files will be stored, in format \"yyyy-mm-dd_HH.nc\"</p> required <code>date_start</code> <code>str</code> <p>Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format    date_end:str,</p> required <code>date_end</code> <code>str</code> <p>End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format</p> required <code>ecmwf_class</code> <code>str</code> <p>ecmwf classification of data</p> required <code>stream</code> <code>str</code> <p>Operational model stream</p> required <code>expver</code> <code>str</code> <p>Version of data</p> required <code>leveltype</code> <code>str</code> <p>Surface level forecasts</p> required <code>ec_vars</code> <code>list</code> <p>Variables of forecast measurements.</p> required <code>forecast_area</code> <code>list</code> <p>N/W/S/E coordinates of the forecast area</p> required <code>ecmwf_api_key</code> <code>str</code> <p>API key for ECMWF API</p> required <code>ecmwf_api_email</code> <code>str</code> <p>Email for ECMWF API</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/ecmwf/weather_forecast.py</code> <pre><code>class SparkECMWFWeatherForecastSource(SourceInterface):\n    \"\"\"\n    The Weather Forecast API V1 Source class to doownload nc files from ECMWF MARS server using the ECMWF python API.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        save_path (str): Path to local directory where the nc files will be stored, in format \"yyyy-mm-dd_HH.nc\"\n        date_start (str): Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format    date_end:str,\n        date_end (str): End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format\n        ecmwf_class (str): ecmwf classification of data\n        stream (str): Operational model stream\n        expver (str): Version of data\n        leveltype (str): Surface level forecasts\n        ec_vars (list): Variables of forecast measurements.\n        forecast_area (list): N/W/S/E coordinates of the forecast area\n        ecmwf_api_key (str): API key for ECMWF API\n        ecmwf_api_email (str): Email for ECMWF API\n    \"\"\"\n\n    spark: SparkSession\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        save_path: str,\n        date_start: str,\n        date_end: str,\n        ecmwf_class: str,\n        stream: str,\n        expver: str,\n        leveltype: str,\n        ec_vars: list,\n        forecast_area: list,\n        ecmwf_api_key: str,\n        ecmwf_api_email: str,\n    ) -&gt; None:\n        self.spark = spark\n        self.save_path = save_path\n        self.date_start = date_start\n        self.date_end = date_end\n        self.ecmwf_class = ecmwf_class\n        self.stream = stream  # operational model\n        self.expver = expver  # experiment version of data\n        self.leveltype = leveltype  # surface level forecasts\n        self.ec_vars = ec_vars  # variables\n        self.forecast_area = forecast_area  # N/W/S/E\n        self.ecmwf_api_key = ecmwf_api_key\n        self.ecmwf_api_email = ecmwf_api_email\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self):\n        return True\n\n    def read_stream(self):\n        return True\n\n    @classmethod\n    def _get_lead_time(cls):\n        \"\"\"\n        Lead time for the forecast data.\n        90 hours - 1 Hour Interval\n        90-146 - 3 Hour interval\n        146 -246 - 6 Hour interval\n\n        Returns:\n            lead_times: Lead times in an array format.\n        \"\"\"\n        lead_times = [*range(91), *range(93, 146, 3), *range(150, 246, 6)]\n        np.array(lead_times)\n\n        return lead_times\n\n    def _get_api_params(self, lead_times):\n        \"\"\"\n        API parameters for the forecast data.\n\n        Returns:\n            params (dict): API parameters for the forecast data.\n        \"\"\"\n\n        params = {\n            \"class\": self.ecmwf_class,  # ecmwf classification of data\n            \"stream\": self.stream,  # operational model\n            \"expver\": self.expver,  # experiment version of data\n            \"levtype\": self.leveltype,  # surface level forecasts\n            \"type\": \"fc\",  # forecasts\n            \"param\": self.ec_vars,  # variables\n            \"step\": lead_times,  # which lead times to download\n            \"area\": self.forecast_area,  # N/W/S/E\n            \"grid\": [0.1, 0.1],  # grid res of output\n        }\n\n        return params\n\n    def read_batch(self):\n        \"\"\"\n        Pulls data from the Weather API and returns as .nc files.\n\n        \"\"\"\n        lead_times = self._get_lead_time()\n        para = self._get_api_params(lead_times=lead_times)\n\n        ec_conn = SparkECMWFBaseMarsSource(\n            date_start=self.date_start,\n            date_end=self.date_end,\n            save_path=self.save_path,\n            run_interval=\"12\",\n            run_frequency=\"H\",\n            ecmwf_api_key=self.ecmwf_api_key,\n            ecmwf_api_email=self.ecmwf_api_email,\n            ecmwf_api_url=\"https://api.ecmwf.int/v1\",\n        )\n\n        ec_conn.retrieve(\n            mars_dict=para,\n            tries=5,\n            n_jobs=-1,  # maximum of 20 queued requests per user (only two allowed active)\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkECMWFWeatherForecastSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/ecmwf/weather_forecast.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkECMWFWeatherForecastSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Pulls data from the Weather API and returns as .nc files.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/ecmwf/weather_forecast.py</code> <pre><code>def read_batch(self):\n    \"\"\"\n    Pulls data from the Weather API and returns as .nc files.\n\n    \"\"\"\n    lead_times = self._get_lead_time()\n    para = self._get_api_params(lead_times=lead_times)\n\n    ec_conn = SparkECMWFBaseMarsSource(\n        date_start=self.date_start,\n        date_end=self.date_end,\n        save_path=self.save_path,\n        run_interval=\"12\",\n        run_frequency=\"H\",\n        ecmwf_api_key=self.ecmwf_api_key,\n        ecmwf_api_email=self.ecmwf_api_email,\n        ecmwf_api_url=\"https://api.ecmwf.int/v1\",\n    )\n\n    ec_conn.retrieve(\n        mars_dict=para,\n        tries=5,\n        n_jobs=-1,  # maximum of 20 queued requests per user (only two allowed active)\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkDeltaSource","title":"<code>SparkDeltaSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>The Spark Delta Source is used to read data from a Delta table.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkDeltaSource--example","title":"Example","text":"<p><pre><code>#Delta Source for Streaming Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkDeltaSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\ndelta_source = SparkDeltaSource(\n    spark=spark,\n    options={\n        \"maxFilesPerTrigger\": 1000,\n        \"ignoreChanges: True,\n        \"startingVersion\": 0\n    },\n    table_name=\"{YOUR-DELTA-TABLE-PATH}\"\n)\n\ndelta_source.read_stream()\n</code></pre> <pre><code>#Delta Source for Batch Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkDeltaSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\ndelta_source = SparkDeltaSource(\n    spark=spark,\n    options={\n        \"versionAsOf\": 0,\n        \"timestampAsOf\": \"yyyy-mm-dd hh:mm:ss[.fffffffff]\"\n    },\n    table_name=\"{YOUR-DELTA-TABLE-PATH}\"\n)\n\ndelta_source.read_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from a Delta table.</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for batch and streaming.</p> required <code>table_name</code> <code>str</code> <p>Name of the Hive Metastore or Unity Catalog Delta Table</p> required <p>Attributes:</p> Name Type Description <code>maxFilesPerTrigger</code> <code>int</code> <p>How many new files to be considered in every micro-batch. The default is 1000. (Streaming)</p> <code>maxBytesPerTrigger</code> <code>int</code> <p>How much data gets processed in each micro-batch. (Streaming)</p> <code>ignoreDeletes</code> <code>bool str</code> <p>Ignore transactions that delete data at partition boundaries. (Streaming)</p> <code>ignoreChanges</code> <code>bool str</code> <p>Pre-process updates if files had to be rewritten in the source table due to a data changing operation. (Streaming)</p> <code>startingVersion</code> <code>int str</code> <p>The Delta Lake version to start from. (Streaming)</p> <code>startingTimestamp</code> <code>datetime str</code> <p>The timestamp to start from. (Streaming)</p> <code>withEventTimeOrder</code> <code>bool str</code> <p>Whether the initial snapshot should be processed with event time order. (Streaming)</p> <code>timestampAsOf</code> <code>datetime str</code> <p>Query the Delta Table from a specific point in time. (Batch)</p> <code>versionAsOf</code> <code>int str</code> <p>Query the Delta Table from a specific version. (Batch)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta.py</code> <pre><code>class SparkDeltaSource(SourceInterface):\n    \"\"\"\n    The Spark Delta Source is used to read data from a Delta table.\n\n    Example\n    --------\n    ```python\n    #Delta Source for Streaming Queries\n\n    from rtdip_sdk.pipelines.sources import SparkDeltaSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    delta_source = SparkDeltaSource(\n        spark=spark,\n        options={\n            \"maxFilesPerTrigger\": 1000,\n            \"ignoreChanges: True,\n            \"startingVersion\": 0\n        },\n        table_name=\"{YOUR-DELTA-TABLE-PATH}\"\n    )\n\n    delta_source.read_stream()\n    ```\n    ```python\n    #Delta Source for Batch Queries\n\n    from rtdip_sdk.pipelines.sources import SparkDeltaSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    delta_source = SparkDeltaSource(\n        spark=spark,\n        options={\n            \"versionAsOf\": 0,\n            \"timestampAsOf\": \"yyyy-mm-dd hh:mm:ss[.fffffffff]\"\n        },\n        table_name=\"{YOUR-DELTA-TABLE-PATH}\"\n    )\n\n    delta_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to read data from a Delta table.\n        options (dict): Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for [batch](https://docs.delta.io/latest/delta-batch.html#read-a-table){ target=\"_blank\" } and [streaming](https://docs.delta.io/latest/delta-streaming.html#delta-table-as-a-source){ target=\"_blank\" }.\n        table_name (str): Name of the Hive Metastore or Unity Catalog Delta Table\n\n    Attributes:\n        maxFilesPerTrigger (int): How many new files to be considered in every micro-batch. The default is 1000. (Streaming)\n        maxBytesPerTrigger (int): How much data gets processed in each micro-batch. (Streaming)\n        ignoreDeletes (bool str): Ignore transactions that delete data at partition boundaries. (Streaming)\n        ignoreChanges (bool str): Pre-process updates if files had to be rewritten in the source table due to a data changing operation. (Streaming)\n        startingVersion (int str): The Delta Lake version to start from. (Streaming)\n        startingTimestamp (datetime str): The timestamp to start from. (Streaming)\n        withEventTimeOrder (bool str): Whether the initial snapshot should be processed with event time order. (Streaming)\n        timestampAsOf (datetime str): Query the Delta Table from a specific point in time. (Batch)\n        versionAsOf (int str): Query the Delta Table from a specific version. (Batch)\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    table_name: str\n\n    def __init__(self, spark: SparkSession, options: dict, table_name: str) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.table_name = table_name\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_core\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self):\n        return True\n\n    def read_batch(self):\n        \"\"\"\n        Reads batch data from Delta. Most of the options provided by the Apache Spark DataFrame read API are supported for performing batch reads on Delta tables.\n        \"\"\"\n        try:\n            return (\n                self.spark.read.format(\"delta\")\n                .options(**self.options)\n                .table(self.table_name)\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        Reads streaming data from Delta. All of the data in the table is processed as well as any new data that arrives after the stream started. .load() can take table name or path.\n        \"\"\"\n        try:\n            return (\n                self.spark.readStream.format(\"delta\")\n                .options(**self.options)\n                .load(self.table_name)\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkDeltaSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkDeltaSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads batch data from Delta. Most of the options provided by the Apache Spark DataFrame read API are supported for performing batch reads on Delta tables.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta.py</code> <pre><code>def read_batch(self):\n    \"\"\"\n    Reads batch data from Delta. Most of the options provided by the Apache Spark DataFrame read API are supported for performing batch reads on Delta tables.\n    \"\"\"\n    try:\n        return (\n            self.spark.read.format(\"delta\")\n            .options(**self.options)\n            .table(self.table_name)\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkDeltaSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Reads streaming data from Delta. All of the data in the table is processed as well as any new data that arrives after the stream started. .load() can take table name or path.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    Reads streaming data from Delta. All of the data in the table is processed as well as any new data that arrives after the stream started. .load() can take table name or path.\n    \"\"\"\n    try:\n        return (\n            self.spark.readStream.format(\"delta\")\n            .options(**self.options)\n            .load(self.table_name)\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.BinaryToStringTransformer","title":"<code>BinaryToStringTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a dataframe body column from a binary to a string.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.BinaryToStringTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import BinaryToStringTransformer\n\nbinary_to_string_transformer = BinaryToStringTransformer(\n    data=df,\n    souce_column_name=\"body\",\n    target_column_name=\"body\"\n)\n\nresult = binary_to_string_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to be transformed</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the Binary data</p> required <code>target_column_name</code> <code>str</code> <p>Spark Dataframe column name to be used for the String data</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/binary_to_string.py</code> <pre><code>class BinaryToStringTransformer(TransformerInterface):\n    \"\"\"\n    Converts a dataframe body column from a binary to a string.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import BinaryToStringTransformer\n\n    binary_to_string_transformer = BinaryToStringTransformer(\n        data=df,\n        souce_column_name=\"body\",\n        target_column_name=\"body\"\n    )\n\n    result = binary_to_string_transformer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe to be transformed\n        source_column_name (str): Spark Dataframe column containing the Binary data\n        target_column_name (str): Spark Dataframe column name to be used for the String data\n    \"\"\"\n\n    data: DataFrame\n    source_column_name: str\n    target_column_name: str\n\n    def __init__(\n        self, data: DataFrame, source_column_name: str, target_column_name: str\n    ) -&gt; None:\n        self.data = data\n        self.source_column_name = source_column_name\n        self.target_column_name = target_column_name\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the body column converted to string.\n        \"\"\"\n        return self.data.withColumn(\n            self.target_column_name, self.data[self.source_column_name].cast(\"string\")\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.BinaryToStringTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/binary_to_string.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.BinaryToStringTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the body column converted to string.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/binary_to_string.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the body column converted to string.\n    \"\"\"\n    return self.data.withColumn(\n        self.target_column_name, self.data[self.source_column_name].cast(\"string\")\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.OPCPublisherOPCUAJsonToPCDMTransformer","title":"<code>OPCPublisherOPCUAJsonToPCDMTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a Spark Dataframe column containing a json string created by OPC Publisher to the Process Control Data Model.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.OPCPublisherOPCUAJsonToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import OPCPublisherOPCUAJsonToPCDMTransformer\n\nopc_publisher_opcua_json_to_pcdm_transformer = OPCPublisherOPCUAJsonToPCDMTransformer(\n    data=df,\n    souce_column_name=\"body\",\n    multiple_rows_per_message=True,\n    status_null_value=\"Good\",\n    change_type_value=\"insert\",\n    timestamp_formats=[\n        \"yyyy-MM-dd'T'HH:mm:ss.SSSX\",\n        \"yyyy-MM-dd'T'HH:mm:ssX\"\n    ],\n    filter=None\n)\n\nresult = opc_publisher_opcua_json_to_pcdm_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe containing the column with Json OPC UA data</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the OPC Publisher Json OPC UA data</p> required <code>multiple_rows_per_message</code> <code>optional bool</code> <p>Each Dataframe Row contains an array of/multiple OPC UA messages. The list of Json will be exploded into rows in the Dataframe.</p> <code>True</code> <code>status_null_value</code> <code>optional str</code> <p>If populated, will replace null values in the Status column with the specified value.</p> <code>None</code> <code>change_type_value</code> <code>optional str</code> <p>If populated, will replace 'insert' in the ChangeType column with the specified value.</p> <code>'insert'</code> <code>timestamp_formats</code> <code>optional list[str]</code> <p>Specifies the timestamp formats to be used for converting the timestamp string to a Timestamp Type. For more information on formats, refer to this documentation.</p> <code>[\"yyyy-MM-dd'T'HH:mm:ss.SSSX\", \"yyyy-MM-dd'T'HH:mm:ssX\"]</code> <code>filter</code> <code>optional str</code> <p>Enables providing a filter to the data which can be required in certain scenarios. For example, it would be possible to filter on IoT Hub Device Id and Module by providing a filter in SQL format such as <code>systemProperties.iothub-connection-device-id = \"&lt;Device Id&gt;\" AND systemProperties.iothub-connection-module-id = \"&lt;Module&gt;\"</code></p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/opc_publisher_opcua_json_to_pcdm.py</code> <pre><code>class OPCPublisherOPCUAJsonToPCDMTransformer(TransformerInterface):\n    \"\"\"\n    Converts a Spark Dataframe column containing a json string created by OPC Publisher to the Process Control Data Model.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import OPCPublisherOPCUAJsonToPCDMTransformer\n\n    opc_publisher_opcua_json_to_pcdm_transformer = OPCPublisherOPCUAJsonToPCDMTransformer(\n        data=df,\n        souce_column_name=\"body\",\n        multiple_rows_per_message=True,\n        status_null_value=\"Good\",\n        change_type_value=\"insert\",\n        timestamp_formats=[\n            \"yyyy-MM-dd'T'HH:mm:ss.SSSX\",\n            \"yyyy-MM-dd'T'HH:mm:ssX\"\n        ],\n        filter=None\n    )\n\n    result = opc_publisher_opcua_json_to_pcdm_transformer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe containing the column with Json OPC UA data\n        source_column_name (str): Spark Dataframe column containing the OPC Publisher Json OPC UA data\n        multiple_rows_per_message (optional bool): Each Dataframe Row contains an array of/multiple OPC UA messages. The list of Json will be exploded into rows in the Dataframe.\n        status_null_value (optional str): If populated, will replace null values in the Status column with the specified value.\n        change_type_value (optional str): If populated, will replace 'insert' in the ChangeType column with the specified value.\n        timestamp_formats (optional list[str]): Specifies the timestamp formats to be used for converting the timestamp string to a Timestamp Type. For more information on formats, refer to this [documentation.](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html)\n        filter (optional str): Enables providing a filter to the data which can be required in certain scenarios. For example, it would be possible to filter on IoT Hub Device Id and Module by providing a filter in SQL format such as `systemProperties.iothub-connection-device-id = \"&lt;Device Id&gt;\" AND systemProperties.iothub-connection-module-id = \"&lt;Module&gt;\"`\n    \"\"\"\n\n    data: DataFrame\n    source_column_name: str\n    multiple_rows_per_message: bool\n    tagname_field: str\n    status_null_value: str\n    change_type_value: str\n    timestamp_formats: list\n    filter: str\n\n    def __init__(\n        self,\n        data: DataFrame,\n        source_column_name: str,\n        multiple_rows_per_message: bool = True,\n        tagname_field: str = \"DisplayName\",\n        status_null_value: str = None,\n        change_type_value: str = \"insert\",\n        timestamp_formats: list = [\n            \"yyyy-MM-dd'T'HH:mm:ss.SSSX\",\n            \"yyyy-MM-dd'T'HH:mm:ssX\",\n        ],\n        filter: str = None,\n    ) -&gt; None:  # NOSONAR\n        self.data = data\n        self.source_column_name = source_column_name\n        self.multiple_rows_per_message = multiple_rows_per_message\n        self.tagname_field = tagname_field\n        self.status_null_value = status_null_value\n        self.change_type_value = change_type_value\n        self.timestamp_formats = timestamp_formats\n        self.filter = filter\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the specified column converted to PCDM\n        \"\"\"\n        if self.multiple_rows_per_message:\n            df = self.data.withColumn(\n                self.source_column_name,\n                from_json(col(self.source_column_name), ArrayType(StringType())),\n            ).withColumn(self.source_column_name, explode(self.source_column_name))\n        else:\n            df = self.data.withColumn(\n                self.source_column_name,\n                from_json(col(self.source_column_name), StringType()),\n            )\n\n        if self.filter != None:\n            df = df.where(self.filter)\n\n        df = (\n            df.withColumn(\n                \"OPCUA\", from_json(col(self.source_column_name), OPC_PUBLISHER_SCHEMA)\n            )\n            .withColumn(\"TagName\", (col(\"OPCUA.{}\".format(self.tagname_field))))\n            .withColumn(\n                \"EventTime\",\n                coalesce(\n                    *[\n                        to_timestamp(col(\"OPCUA.Value.SourceTimestamp\"), f)\n                        for f in self.timestamp_formats\n                    ]\n                ),\n            )\n            .withColumn(\"Value\", col(\"OPCUA.Value.Value\"))\n            .withColumn(\n                \"ValueType\",\n                when(col(\"Value\").cast(\"float\").isNotNull(), \"float\")\n                .when(col(\"Value\").cast(\"float\").isNull(), \"string\")\n                .otherwise(\"unknown\"),\n            )\n            .withColumn(\"ChangeType\", lit(self.change_type_value))\n        )\n\n        status_col_name = \"OPCUA.Value.StatusCode.Symbol\"\n        if self.status_null_value != None:\n            df = df.withColumn(\n                \"Status\",\n                when(col(status_col_name).isNotNull(), col(status_col_name)).otherwise(\n                    lit(self.status_null_value)\n                ),\n            )\n        else:\n            df = df.withColumn(\"Status\", col(status_col_name))\n\n        return df.select(\n            \"TagName\", \"EventTime\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.OPCPublisherOPCUAJsonToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/opc_publisher_opcua_json_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.OPCPublisherOPCUAJsonToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the specified column converted to PCDM</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/opc_publisher_opcua_json_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the specified column converted to PCDM\n    \"\"\"\n    if self.multiple_rows_per_message:\n        df = self.data.withColumn(\n            self.source_column_name,\n            from_json(col(self.source_column_name), ArrayType(StringType())),\n        ).withColumn(self.source_column_name, explode(self.source_column_name))\n    else:\n        df = self.data.withColumn(\n            self.source_column_name,\n            from_json(col(self.source_column_name), StringType()),\n        )\n\n    if self.filter != None:\n        df = df.where(self.filter)\n\n    df = (\n        df.withColumn(\n            \"OPCUA\", from_json(col(self.source_column_name), OPC_PUBLISHER_SCHEMA)\n        )\n        .withColumn(\"TagName\", (col(\"OPCUA.{}\".format(self.tagname_field))))\n        .withColumn(\n            \"EventTime\",\n            coalesce(\n                *[\n                    to_timestamp(col(\"OPCUA.Value.SourceTimestamp\"), f)\n                    for f in self.timestamp_formats\n                ]\n            ),\n        )\n        .withColumn(\"Value\", col(\"OPCUA.Value.Value\"))\n        .withColumn(\n            \"ValueType\",\n            when(col(\"Value\").cast(\"float\").isNotNull(), \"float\")\n            .when(col(\"Value\").cast(\"float\").isNull(), \"string\")\n            .otherwise(\"unknown\"),\n        )\n        .withColumn(\"ChangeType\", lit(self.change_type_value))\n    )\n\n    status_col_name = \"OPCUA.Value.StatusCode.Symbol\"\n    if self.status_null_value != None:\n        df = df.withColumn(\n            \"Status\",\n            when(col(status_col_name).isNotNull(), col(status_col_name)).otherwise(\n                lit(self.status_null_value)\n            ),\n        )\n    else:\n        df = df.withColumn(\"Status\", col(status_col_name))\n\n    return df.select(\n        \"TagName\", \"EventTime\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.OPCPublisherOPCAEJsonToPCDMTransformer","title":"<code>OPCPublisherOPCAEJsonToPCDMTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a Spark Dataframe column containing a json string created by OPC Publisher for A&amp;E(Alarm &amp;Events) data to the Process Control Data Model.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.OPCPublisherOPCAEJsonToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import OPCPublisherOPCAEJsonToPCDMTransformer\n\nopc_publisher_opcae_json_to_pcdm_transformer = OPCPublisherOPCAEJsonToPCDMTransformer(\n    data=df,\n    souce_column_name=\"body\",\n    timestamp_formats=[\n        \"yyyy-MM-dd'T'HH:mm:ss.SSSX\",\n        \"yyyy-MM-dd'T'HH:mm:ssX\"\n    ],\n    filter=None\n)\n\nresult = opc_publisher_opcae_json_to_pcdm_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe containing the column with Json OPC AE data</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the OPC Publisher Json OPC AE data</p> required <code>timestamp_formats</code> <code>optional list[str]</code> <p>Specifies the timestamp formats to be used for converting the timestamp string to a Timestamp Type. For more information on formats, refer to this documentation.</p> <code>None</code> <code>filter</code> <code>optional str</code> <p>Enables providing a filter to the data which can be required in certain scenarios. For example, it would be possible to filter on IoT Hub Device Id and Module by providing a filter in SQL format such as <code>systemProperties.iothub-connection-device-id = \"&lt;Device Id&gt;\" AND systemProperties.iothub-connection-module-id = \"&lt;Module&gt;\"</code></p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/opc_publisher_opcae_json_to_pcdm.py</code> <pre><code>class OPCPublisherOPCAEJsonToPCDMTransformer(TransformerInterface):\n    \"\"\"\n    Converts a Spark Dataframe column containing a json string created by OPC Publisher for A&amp;E(Alarm &amp;Events) data to the Process Control Data Model.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import OPCPublisherOPCAEJsonToPCDMTransformer\n\n    opc_publisher_opcae_json_to_pcdm_transformer = OPCPublisherOPCAEJsonToPCDMTransformer(\n        data=df,\n        souce_column_name=\"body\",\n        timestamp_formats=[\n            \"yyyy-MM-dd'T'HH:mm:ss.SSSX\",\n            \"yyyy-MM-dd'T'HH:mm:ssX\"\n        ],\n        filter=None\n    )\n\n    result = opc_publisher_opcae_json_to_pcdm_transformer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe containing the column with Json OPC AE data\n        source_column_name (str): Spark Dataframe column containing the OPC Publisher Json OPC AE data\n        timestamp_formats (optional list[str]): Specifies the timestamp formats to be used for converting the timestamp string to a Timestamp Type. For more information on formats, refer to this [documentation.](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html)\n        filter (optional str): Enables providing a filter to the data which can be required in certain scenarios. For example, it would be possible to filter on IoT Hub Device Id and Module by providing a filter in SQL format such as `systemProperties.iothub-connection-device-id = \"&lt;Device Id&gt;\" AND systemProperties.iothub-connection-module-id = \"&lt;Module&gt;\"`\n    \"\"\"\n\n    data: DataFrame\n    source_column_name: str\n    timestamp_formats: list\n    filter: str\n\n    def __init__(\n        self,\n        data: DataFrame,\n        source_column_name: str,\n        timestamp_formats=None,\n        filter: str = None,\n    ) -&gt; None:  # NOSONAR\n        self.data = data\n        self.source_column_name = source_column_name\n        self.timestamp_formats = timestamp_formats or [\n            \"yyyy-MM-dd'T'HH:mm:ss.SSSX\",\n            \"yyyy-MM-dd'T'HH:mm:ssX\",\n        ]\n        self.filter = filter\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the OPC Publisher A&amp;E data converted to the Process Control Data Model\n        \"\"\"\n\n        df = self.data.withColumn(\n            self.source_column_name,\n            from_json(col(self.source_column_name), ArrayType(StringType())),\n        ).withColumn(self.source_column_name, explode(self.source_column_name))\n\n        if self.filter != None:\n            df = df.where(self.filter)\n\n        df = df.withColumn(\n            \"OPCAE\", from_json(col(self.source_column_name), OPC_PUBLISHER_AE_SCHEMA)\n        )\n\n        df = df.select(\n            col(\"OPCAE.NodeId\"),\n            col(\"OPCAE.DisplayName\"),\n            col(\"OPCAE.Value.ConditionId.Value\").alias(\"ConditionId\"),\n            col(\"OPCAE.Value.AckedState.Value\").alias(\"AckedState\"),\n            col(\"OPCAE.Value.AckedState/FalseState.Value\").alias(\n                \"AckedState/FalseState\"\n            ),\n            col(\"OPCAE.Value.AckedState/Id.Value\").alias(\"AckedState/Id\"),\n            col(\"OPCAE.Value.AckedState/TrueState.Value\").alias(\"AckedState/TrueState\"),\n            col(\"OPCAE.Value.ActiveState.Value\").alias(\"ActiveState\"),\n            col(\"OPCAE.Value.ActiveState/FalseState.Value\").alias(\n                \"ActiveState/FalseState\"\n            ),\n            col(\"OPCAE.Value.ActiveState/Id.Value\").alias(\"ActiveState/Id\"),\n            col(\"OPCAE.Value.ActiveState/TrueState.Value\").alias(\n                \"ActiveState/TrueState\"\n            ),\n            col(\"OPCAE.Value.EnabledState.Value\").alias(\"EnabledState\"),\n            col(\"OPCAE.Value.EnabledState/FalseState.Value\").alias(\n                \"EnabledState/FalseState\"\n            ),\n            col(\"OPCAE.Value.EnabledState/Id.Value\").alias(\"EnabledState/Id\"),\n            col(\"OPCAE.Value.EnabledState/TrueState.Value\").alias(\n                \"EnabledState/TrueState\"\n            ),\n            col(\"OPCAE.Value.EventId.Value\").alias(\"EventId\"),\n            col(\"OPCAE.Value.EventType.Value\").alias(\"EventType\"),\n            col(\"OPCAE.Value.HighHighLimit.Value\").alias(\"HighHighLimit\"),\n            col(\"OPCAE.Value.HighLimit.Value\").alias(\"HighLimit\"),\n            col(\"OPCAE.Value.InputNode.Value\").alias(\"InputNode\"),\n            col(\"OPCAE.Value.LowLimit.Value\").alias(\"LowLimit\"),\n            col(\"OPCAE.Value.LowLowLimit.Value\").alias(\"LowLowLimit\"),\n            col(\"OPCAE.Value.Message.Value\").alias(\"Message\"),\n            col(\"OPCAE.Value.Quality.Value\").alias(\"Quality\"),\n            col(\"OPCAE.Value.ReceiveTime.Value\").alias(\"ReceiveTime\"),\n            col(\"OPCAE.Value.Retain.Value\").alias(\"Retain\"),\n            col(\"OPCAE.Value.Severity.Value\").alias(\"Severity\"),\n            col(\"OPCAE.Value.SourceName.Value\").alias(\"SourceName\"),\n            col(\"OPCAE.Value.SourceNode.Value\").alias(\"SourceNode\"),\n            col(\"OPCAE.Value.Time.Value\").alias(\"EventTime\"),\n        )\n\n        df = df.withColumn(\n            \"EventTime\",\n            coalesce(\n                *[to_timestamp(col(\"EventTime\"), f) for f in self.timestamp_formats]\n            ),\n        )\n\n        return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.OPCPublisherOPCAEJsonToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/opc_publisher_opcae_json_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.OPCPublisherOPCAEJsonToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the OPC Publisher A&amp;E data converted to the Process Control Data Model</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/opc_publisher_opcae_json_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the OPC Publisher A&amp;E data converted to the Process Control Data Model\n    \"\"\"\n\n    df = self.data.withColumn(\n        self.source_column_name,\n        from_json(col(self.source_column_name), ArrayType(StringType())),\n    ).withColumn(self.source_column_name, explode(self.source_column_name))\n\n    if self.filter != None:\n        df = df.where(self.filter)\n\n    df = df.withColumn(\n        \"OPCAE\", from_json(col(self.source_column_name), OPC_PUBLISHER_AE_SCHEMA)\n    )\n\n    df = df.select(\n        col(\"OPCAE.NodeId\"),\n        col(\"OPCAE.DisplayName\"),\n        col(\"OPCAE.Value.ConditionId.Value\").alias(\"ConditionId\"),\n        col(\"OPCAE.Value.AckedState.Value\").alias(\"AckedState\"),\n        col(\"OPCAE.Value.AckedState/FalseState.Value\").alias(\n            \"AckedState/FalseState\"\n        ),\n        col(\"OPCAE.Value.AckedState/Id.Value\").alias(\"AckedState/Id\"),\n        col(\"OPCAE.Value.AckedState/TrueState.Value\").alias(\"AckedState/TrueState\"),\n        col(\"OPCAE.Value.ActiveState.Value\").alias(\"ActiveState\"),\n        col(\"OPCAE.Value.ActiveState/FalseState.Value\").alias(\n            \"ActiveState/FalseState\"\n        ),\n        col(\"OPCAE.Value.ActiveState/Id.Value\").alias(\"ActiveState/Id\"),\n        col(\"OPCAE.Value.ActiveState/TrueState.Value\").alias(\n            \"ActiveState/TrueState\"\n        ),\n        col(\"OPCAE.Value.EnabledState.Value\").alias(\"EnabledState\"),\n        col(\"OPCAE.Value.EnabledState/FalseState.Value\").alias(\n            \"EnabledState/FalseState\"\n        ),\n        col(\"OPCAE.Value.EnabledState/Id.Value\").alias(\"EnabledState/Id\"),\n        col(\"OPCAE.Value.EnabledState/TrueState.Value\").alias(\n            \"EnabledState/TrueState\"\n        ),\n        col(\"OPCAE.Value.EventId.Value\").alias(\"EventId\"),\n        col(\"OPCAE.Value.EventType.Value\").alias(\"EventType\"),\n        col(\"OPCAE.Value.HighHighLimit.Value\").alias(\"HighHighLimit\"),\n        col(\"OPCAE.Value.HighLimit.Value\").alias(\"HighLimit\"),\n        col(\"OPCAE.Value.InputNode.Value\").alias(\"InputNode\"),\n        col(\"OPCAE.Value.LowLimit.Value\").alias(\"LowLimit\"),\n        col(\"OPCAE.Value.LowLowLimit.Value\").alias(\"LowLowLimit\"),\n        col(\"OPCAE.Value.Message.Value\").alias(\"Message\"),\n        col(\"OPCAE.Value.Quality.Value\").alias(\"Quality\"),\n        col(\"OPCAE.Value.ReceiveTime.Value\").alias(\"ReceiveTime\"),\n        col(\"OPCAE.Value.Retain.Value\").alias(\"Retain\"),\n        col(\"OPCAE.Value.Severity.Value\").alias(\"Severity\"),\n        col(\"OPCAE.Value.SourceName.Value\").alias(\"SourceName\"),\n        col(\"OPCAE.Value.SourceNode.Value\").alias(\"SourceNode\"),\n        col(\"OPCAE.Value.Time.Value\").alias(\"EventTime\"),\n    )\n\n    df = df.withColumn(\n        \"EventTime\",\n        coalesce(\n            *[to_timestamp(col(\"EventTime\"), f) for f in self.timestamp_formats]\n        ),\n    )\n\n    return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.FledgeOPCUAJsonToPCDMTransformer","title":"<code>FledgeOPCUAJsonToPCDMTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a Spark Dataframe column containing a json string created by Fledge to the Process Control Data Model.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.FledgeOPCUAJsonToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import FledgeOPCUAJsonToPCDMTransformer\n\nfledge_opcua_json_to_pcdm_transfromer = FledgeOPCUAJsonToPCDMTransformer(\n    data=df,\n    souce_column_name=\"body\",\n    status_null_value=\"Good\",\n    change_type_value=\"insert\",\n    timestamp_formats=[\n        \"yyyy-MM-dd'T'HH:mm:ss.SSSX\",\n        \"yyyy-MM-dd'T'HH:mm:ssX\",\n    ]\n)\n\nresult = fledge_opcua_json_to_pcdm_transfromer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe containing the column with Json Fledge data</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the OPC Publisher Json OPC UA data</p> required <code>status_null_value</code> <code>str</code> <p>If populated, will replace 'Good' in the Status column with the specified value.</p> <code>'Good'</code> <code>change_type_value</code> <code>optional str</code> <p>If populated, will replace 'insert' in the ChangeType column with the specified value.</p> <code>'insert'</code> <code>timestamp_formats</code> <code>list[str]</code> <p>Specifies the timestamp formats to be used for converting the timestamp string to a Timestamp Type. For more information on formats, refer to this documentation.</p> <code>[\"yyyy-MM-dd'T'HH:mm:ss.SSSX\", \"yyyy-MM-dd'T'HH:mm:ssX\"]</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/fledge_opcua_json_to_pcdm.py</code> <pre><code>class FledgeOPCUAJsonToPCDMTransformer(TransformerInterface):\n    \"\"\"\n    Converts a Spark Dataframe column containing a json string created by Fledge to the Process Control Data Model.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import FledgeOPCUAJsonToPCDMTransformer\n\n    fledge_opcua_json_to_pcdm_transfromer = FledgeOPCUAJsonToPCDMTransformer(\n        data=df,\n        souce_column_name=\"body\",\n        status_null_value=\"Good\",\n        change_type_value=\"insert\",\n        timestamp_formats=[\n            \"yyyy-MM-dd'T'HH:mm:ss.SSSX\",\n            \"yyyy-MM-dd'T'HH:mm:ssX\",\n        ]\n    )\n\n    result = fledge_opcua_json_to_pcdm_transfromer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe containing the column with Json Fledge data\n        source_column_name (str): Spark Dataframe column containing the OPC Publisher Json OPC UA data\n        status_null_value (str): If populated, will replace 'Good' in the Status column with the specified value.\n        change_type_value (optional str): If populated, will replace 'insert' in the ChangeType column with the specified value.\n        timestamp_formats (list[str]): Specifies the timestamp formats to be used for converting the timestamp string to a Timestamp Type. For more information on formats, refer to this [documentation.](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html)\n    \"\"\"\n\n    data: DataFrame\n    source_column_name: str\n    status_null_value: str\n    change_type_value: str\n    timestamp_formats: list\n\n    def __init__(\n        self,\n        data: DataFrame,\n        source_column_name: str,\n        status_null_value: str = \"Good\",\n        change_type_value: str = \"insert\",\n        timestamp_formats: list = [\n            \"yyyy-MM-dd'T'HH:mm:ss.SSSX\",\n            \"yyyy-MM-dd'T'HH:mm:ssX\",\n        ],\n    ) -&gt; None:  # NOSONAR\n        self.data = data\n        self.source_column_name = source_column_name\n        self.status_null_value = status_null_value\n        self.change_type_value = change_type_value\n        self.timestamp_formats = timestamp_formats\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the specified column converted to PCDM\n        \"\"\"\n        df = (\n            self.data.withColumn(\n                self.source_column_name,\n                from_json(self.source_column_name, FLEDGE_SCHEMA),\n            )\n            .selectExpr(\"inline({})\".format(self.source_column_name))\n            .select(explode(\"readings\"), \"timestamp\")\n            .withColumn(\n                \"EventTime\",\n                coalesce(\n                    *[to_timestamp(col(\"timestamp\"), f) for f in self.timestamp_formats]\n                ),\n            )\n            .withColumnRenamed(\"key\", \"TagName\")\n            .withColumnRenamed(\"value\", \"Value\")\n            .withColumn(\"Status\", lit(self.status_null_value))\n            .withColumn(\n                \"ValueType\",\n                when(col(\"value\").cast(\"float\").isNotNull(), \"float\").when(\n                    col(\"value\").cast(\"float\").isNull(), \"string\"\n                ),\n            )\n            .withColumn(\"ChangeType\", lit(self.change_type_value))\n        )\n\n        return df.select(\n            \"TagName\", \"EventTime\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.FledgeOPCUAJsonToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/fledge_opcua_json_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.FledgeOPCUAJsonToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the specified column converted to PCDM</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/fledge_opcua_json_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the specified column converted to PCDM\n    \"\"\"\n    df = (\n        self.data.withColumn(\n            self.source_column_name,\n            from_json(self.source_column_name, FLEDGE_SCHEMA),\n        )\n        .selectExpr(\"inline({})\".format(self.source_column_name))\n        .select(explode(\"readings\"), \"timestamp\")\n        .withColumn(\n            \"EventTime\",\n            coalesce(\n                *[to_timestamp(col(\"timestamp\"), f) for f in self.timestamp_formats]\n            ),\n        )\n        .withColumnRenamed(\"key\", \"TagName\")\n        .withColumnRenamed(\"value\", \"Value\")\n        .withColumn(\"Status\", lit(self.status_null_value))\n        .withColumn(\n            \"ValueType\",\n            when(col(\"value\").cast(\"float\").isNotNull(), \"float\").when(\n                col(\"value\").cast(\"float\").isNull(), \"string\"\n            ),\n        )\n        .withColumn(\"ChangeType\", lit(self.change_type_value))\n    )\n\n    return df.select(\n        \"TagName\", \"EventTime\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SSIPPIBinaryFileToPCDMTransformer","title":"<code>SSIPPIBinaryFileToPCDMTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a Spark DataFrame column containing binaryFile parquet data to the Process Control Data Model.</p> <p>This DataFrame should contain a path and the binary data. Typically this can be done using the Autoloader source component and specify \"binaryFile\" as the format.</p> <p>For more information about the SSIP PI Batch Connector, please see here.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SSIPPIBinaryFileToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import SSIPPIBinaryFileToPCDMTransformer\n\nssip_pi_binary_file_to_pcdm_transformer = SSIPPIBinaryFileToPCDMTransformer(\n    data=df\n)\n\nresult = ssip_pi_binary_file_to_pcdm_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame containing the path and binaryFile data</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ssip_pi_binary_file_to_pcdm.py</code> <pre><code>class SSIPPIBinaryFileToPCDMTransformer(TransformerInterface):\n    \"\"\"\n    Converts a Spark DataFrame column containing binaryFile parquet data to the Process Control Data Model.\n\n    This DataFrame should contain a path and the binary data. Typically this can be done using the Autoloader source component and specify \"binaryFile\" as the format.\n\n    For more information about the SSIP PI Batch Connector, please see [here.](https://bakerhughesc3.ai/oai-solution/shell-sensor-intelligence-platform/)\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import SSIPPIBinaryFileToPCDMTransformer\n\n    ssip_pi_binary_file_to_pcdm_transformer = SSIPPIBinaryFileToPCDMTransformer(\n        data=df\n    )\n\n    result = ssip_pi_binary_file_to_pcdm_transformer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): DataFrame containing the path and binaryFile data\n    \"\"\"\n\n    data: DataFrame\n\n    def __init__(self, data: DataFrame) -&gt; None:\n        self.data = data\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_pypi_library(get_default_package(\"pyarrow\"))\n        libraries.add_pypi_library(get_default_package(\"pandas\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    @staticmethod\n    def _convert_binary_to_pandas(pdf):\n        try:\n            binary_list = pdf.values.tolist()\n            binary_data = binary_list[0][3]\n            buf = pa.py_buffer(binary_data)\n            table = pq.read_table(buf)\n        except Exception as e:\n            print(str(e))\n            return pd.DataFrame(\n                {\n                    \"EventDate\": pd.Series([], dtype=\"datetime64[ns]\"),\n                    \"TagName\": pd.Series([], dtype=\"str\"),\n                    \"EventTime\": pd.Series([], dtype=\"datetime64[ns]\"),\n                    \"Status\": pd.Series([], dtype=\"str\"),\n                    \"Value\": pd.Series([], dtype=\"str\"),\n                    \"ValueType\": pd.Series([], dtype=\"str\"),\n                    \"ChangeType\": pd.Series([], dtype=\"str\"),\n                }\n            )\n\n        output_pdf = table.to_pandas()\n\n        if \"ValueType\" not in output_pdf.columns:\n            value_type = str(table.schema.field(\"Value\").type)\n            if value_type == \"int16\" or value_type == \"int32\":\n                value_type = \"integer\"\n            output_pdf[\"ValueType\"] = value_type\n\n        if \"ChangeType\" not in output_pdf.columns:\n            output_pdf[\"ChangeType\"] = \"insert\"\n\n        output_pdf[\"EventDate\"] = output_pdf[\"EventTime\"].dt.date\n        output_pdf[\"Value\"] = output_pdf[\"Value\"].astype(str)\n        output_pdf = output_pdf[\n            [\n                \"EventDate\",\n                \"TagName\",\n                \"EventTime\",\n                \"Status\",\n                \"Value\",\n                \"ValueType\",\n                \"ChangeType\",\n            ]\n        ]\n        return output_pdf\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the provided Binary data convert to PCDM\n        \"\"\"\n        return self.data.groupBy(\"path\").applyInPandas(\n            SSIPPIBinaryFileToPCDMTransformer._convert_binary_to_pandas,\n            schema=\"EventDate DATE, TagName STRING, EventTime TIMESTAMP, Status STRING, Value STRING, ValueType STRING, ChangeType STRING\",\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SSIPPIBinaryFileToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ssip_pi_binary_file_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SSIPPIBinaryFileToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the provided Binary data convert to PCDM</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ssip_pi_binary_file_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the provided Binary data convert to PCDM\n    \"\"\"\n    return self.data.groupBy(\"path\").applyInPandas(\n        SSIPPIBinaryFileToPCDMTransformer._convert_binary_to_pandas,\n        schema=\"EventDate DATE, TagName STRING, EventTime TIMESTAMP, Status STRING, Value STRING, ValueType STRING, ChangeType STRING\",\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SSIPPIJsonStreamToPCDMTransformer","title":"<code>SSIPPIJsonStreamToPCDMTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a Spark DataFrame containing Binary JSON data and related Properties to the Process Control Data Model</p> <p>For more information about the SSIP PI Streaming Connector, please see here.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SSIPPIJsonStreamToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import SSIPPIJsonStreamToPCDMTransformer\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nssip_pi_json_stream_to_pcdm_transformer = SSIPPIJsonStreamToPCDMTransformer(\n    spark=spark,\n    data=df,\n    source_column_name=\"body\",\n    properties_column_name=\"\",\n    metadata_delta_table=None\n)\n\nresult = ssip_pi_json_stream_to_pcdm_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session</p> required <code>data</code> <code>DataFrame</code> <p>DataFrame containing the path and binaryFile data</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the Binary json data</p> required <code>properties_column_name</code> <code>str</code> <p>Spark Dataframe struct typed column containing an element with the PointType</p> required <code>metadata_delta_table</code> <code>(optional, str)</code> <p>Name of a metadata table that can be used for PointType mappings</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ssip_pi_binary_json_to_pcdm.py</code> <pre><code>class SSIPPIJsonStreamToPCDMTransformer(TransformerInterface):\n    \"\"\"\n    Converts a Spark DataFrame containing Binary JSON data and related Properties to the Process Control Data Model\n\n    For more information about the SSIP PI Streaming Connector, please see [here.](https://bakerhughesc3.ai/oai-solution/shell-sensor-intelligence-platform/)\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import SSIPPIJsonStreamToPCDMTransformer\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    ssip_pi_json_stream_to_pcdm_transformer = SSIPPIJsonStreamToPCDMTransformer(\n        spark=spark,\n        data=df,\n        source_column_name=\"body\",\n        properties_column_name=\"\",\n        metadata_delta_table=None\n    )\n\n    result = ssip_pi_json_stream_to_pcdm_transformer.transform()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session\n        data (DataFrame): DataFrame containing the path and binaryFile data\n        source_column_name (str): Spark Dataframe column containing the Binary json data\n        properties_column_name (str): Spark Dataframe struct typed column containing an element with the PointType\n        metadata_delta_table (optional, str): Name of a metadata table that can be used for PointType mappings\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    source_column_name: str\n    properties_column_name: str\n    metadata_delta_table: str\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n        source_column_name: str,\n        properties_column_name: str,\n        metadata_delta_table: str = None,\n    ) -&gt; None:\n        self.spark = spark\n        self.data = data\n        self.source_column_name = source_column_name\n        self.properties_column_name = properties_column_name\n        self.metadata_delta_table = metadata_delta_table\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the provided Binary data converted to PCDM\n        \"\"\"\n        df = (\n            self.data.withColumn(\n                self.source_column_name, col(self.source_column_name).cast(\"string\")\n            )\n            .withColumn(\n                \"EventDate\",\n                get_json_object(col(self.source_column_name), \"$.EventTime\").cast(\n                    \"date\"\n                ),\n            )\n            .withColumn(\n                \"TagName\",\n                get_json_object(col(self.source_column_name), \"$.TagName\").cast(\n                    \"string\"\n                ),\n            )\n            .withColumn(\n                \"EventTime\",\n                get_json_object(col(self.source_column_name), \"$.EventTime\").cast(\n                    \"timestamp\"\n                ),\n            )\n            .withColumn(\n                \"Status\",\n                get_json_object(col(self.source_column_name), \"$.Quality\").cast(\n                    \"string\"\n                ),\n            )\n            .withColumn(\n                \"Value\",\n                get_json_object(col(self.source_column_name), \"$.Value\").cast(\"string\"),\n            )\n            .withColumn(\n                \"PointType\", element_at(col(self.properties_column_name), \"PointType\")\n            )\n            .withColumn(\n                \"Action\",\n                element_at(col(self.properties_column_name), \"Action\").cast(\"string\"),\n            )\n        )\n\n        if self.metadata_delta_table != None:\n            metadata_df = SparkDeltaSource(\n                self.spark, {}, self.metadata_delta_table\n            ).read_batch()\n            metadata_df = metadata_df.select(\n                \"TagName\", col(\"PointType\").alias(\"MetadataPointType\")\n            )\n            df = df.join(metadata_df, (df.TagName == metadata_df.TagName), \"left\")\n            df = df.withColumn(\n                \"PointType\",\n                (when(col(\"PointType\").isNull(), col(\"MetadataPointType\"))).otherwise(\n                    col(\"PointType\")\n                ),\n            )\n\n        return (\n            df.withColumn(\n                \"ValueType\",\n                (\n                    when(col(\"PointType\") == \"Digital\", \"string\")\n                    .when(col(\"PointType\") == \"String\", \"string\")\n                    .when(col(\"PointType\") == \"Float16\", \"float\")\n                    .when(col(\"PointType\") == \"Float32\", \"float\")\n                    .when(col(\"PointType\") == \"Float64\", \"float\")\n                    .when(col(\"PointType\") == \"Int16\", \"integer\")\n                    .when(col(\"PointType\") == \"Int32\", \"integer\")\n                    .otherwise(\"string\")\n                ),\n            )\n            .selectExpr(\n                \"*\",\n                \"CASE WHEN ValueType = 'integer' THEN try_cast(Value as integer) END as Value_Integer\",\n                \"CASE WHEN ValueType = 'float' THEN try_cast(Value as float) END as Value_Float\",\n            )\n            .withColumn(\n                \"ValueType\",\n                when(\n                    (col(\"Value_Integer\").isNull()) &amp; (col(\"ValueType\") == \"integer\"),\n                    \"string\",\n                )\n                .when(\n                    (col(\"Value_Float\").isNull()) &amp; (col(\"ValueType\") == \"float\"),\n                    \"string\",\n                )\n                .otherwise(col(\"ValueType\")),\n            )\n            .withColumn(\n                \"ChangeType\",\n                (\n                    when(col(\"Action\") == \"Insert\", \"insert\")\n                    .when(col(\"Action\") == \"Add\", \"insert\")\n                    .when(col(\"Action\") == \"Delete\", \"delete\")\n                    .when(col(\"Action\") == \"Update\", \"update\")\n                    .when(col(\"Action\") == \"Refresh\", \"update\")\n                ),\n            )\n            .select(\n                col(\"EventDate\"),\n                col(\"TagName\"),\n                col(\"EventTime\"),\n                col(\"Status\"),\n                col(\"Value\"),\n                col(\"ValueType\"),\n                col(\"ChangeType\"),\n            )\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SSIPPIJsonStreamToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ssip_pi_binary_json_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SSIPPIJsonStreamToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the provided Binary data converted to PCDM</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ssip_pi_binary_json_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the provided Binary data converted to PCDM\n    \"\"\"\n    df = (\n        self.data.withColumn(\n            self.source_column_name, col(self.source_column_name).cast(\"string\")\n        )\n        .withColumn(\n            \"EventDate\",\n            get_json_object(col(self.source_column_name), \"$.EventTime\").cast(\n                \"date\"\n            ),\n        )\n        .withColumn(\n            \"TagName\",\n            get_json_object(col(self.source_column_name), \"$.TagName\").cast(\n                \"string\"\n            ),\n        )\n        .withColumn(\n            \"EventTime\",\n            get_json_object(col(self.source_column_name), \"$.EventTime\").cast(\n                \"timestamp\"\n            ),\n        )\n        .withColumn(\n            \"Status\",\n            get_json_object(col(self.source_column_name), \"$.Quality\").cast(\n                \"string\"\n            ),\n        )\n        .withColumn(\n            \"Value\",\n            get_json_object(col(self.source_column_name), \"$.Value\").cast(\"string\"),\n        )\n        .withColumn(\n            \"PointType\", element_at(col(self.properties_column_name), \"PointType\")\n        )\n        .withColumn(\n            \"Action\",\n            element_at(col(self.properties_column_name), \"Action\").cast(\"string\"),\n        )\n    )\n\n    if self.metadata_delta_table != None:\n        metadata_df = SparkDeltaSource(\n            self.spark, {}, self.metadata_delta_table\n        ).read_batch()\n        metadata_df = metadata_df.select(\n            \"TagName\", col(\"PointType\").alias(\"MetadataPointType\")\n        )\n        df = df.join(metadata_df, (df.TagName == metadata_df.TagName), \"left\")\n        df = df.withColumn(\n            \"PointType\",\n            (when(col(\"PointType\").isNull(), col(\"MetadataPointType\"))).otherwise(\n                col(\"PointType\")\n            ),\n        )\n\n    return (\n        df.withColumn(\n            \"ValueType\",\n            (\n                when(col(\"PointType\") == \"Digital\", \"string\")\n                .when(col(\"PointType\") == \"String\", \"string\")\n                .when(col(\"PointType\") == \"Float16\", \"float\")\n                .when(col(\"PointType\") == \"Float32\", \"float\")\n                .when(col(\"PointType\") == \"Float64\", \"float\")\n                .when(col(\"PointType\") == \"Int16\", \"integer\")\n                .when(col(\"PointType\") == \"Int32\", \"integer\")\n                .otherwise(\"string\")\n            ),\n        )\n        .selectExpr(\n            \"*\",\n            \"CASE WHEN ValueType = 'integer' THEN try_cast(Value as integer) END as Value_Integer\",\n            \"CASE WHEN ValueType = 'float' THEN try_cast(Value as float) END as Value_Float\",\n        )\n        .withColumn(\n            \"ValueType\",\n            when(\n                (col(\"Value_Integer\").isNull()) &amp; (col(\"ValueType\") == \"integer\"),\n                \"string\",\n            )\n            .when(\n                (col(\"Value_Float\").isNull()) &amp; (col(\"ValueType\") == \"float\"),\n                \"string\",\n            )\n            .otherwise(col(\"ValueType\")),\n        )\n        .withColumn(\n            \"ChangeType\",\n            (\n                when(col(\"Action\") == \"Insert\", \"insert\")\n                .when(col(\"Action\") == \"Add\", \"insert\")\n                .when(col(\"Action\") == \"Delete\", \"delete\")\n                .when(col(\"Action\") == \"Update\", \"update\")\n                .when(col(\"Action\") == \"Refresh\", \"update\")\n            ),\n        )\n        .select(\n            col(\"EventDate\"),\n            col(\"TagName\"),\n            col(\"EventTime\"),\n            col(\"Status\"),\n            col(\"Value\"),\n            col(\"ValueType\"),\n            col(\"ChangeType\"),\n        )\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.AIOJsonToPCDMTransformer","title":"<code>AIOJsonToPCDMTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a Spark Dataframe column containing a json string created by AIO to the Process Control Data Model.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.AIOJsonToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import AIOJsonToPCDMTransformer\n\naio_json_to_pcdm_transfromer = AIOJsonToPCDMTransformer(\n    data=df,\n    souce_column_name=\"body\",\n    status_null_value=\"Good\",\n    change_type_value=\"insert\"\n)\n\nresult = aio_json_to_pcdm_transfromer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe containing the column with Json AIO data</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the Json AIO data</p> required <code>status_null_value</code> <code>str</code> <p>If populated, will replace 'Good' in the Status column with the specified value.</p> <code>'Good'</code> <code>change_type_value</code> <code>optional str</code> <p>If populated, will replace 'insert' in the ChangeType column with the specified value.</p> <code>'insert'</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/aio_json_to_pcdm.py</code> <pre><code>class AIOJsonToPCDMTransformer(TransformerInterface):\n    \"\"\"\n    Converts a Spark Dataframe column containing a json string created by AIO to the Process Control Data Model.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import AIOJsonToPCDMTransformer\n\n    aio_json_to_pcdm_transfromer = AIOJsonToPCDMTransformer(\n        data=df,\n        souce_column_name=\"body\",\n        status_null_value=\"Good\",\n        change_type_value=\"insert\"\n    )\n\n    result = aio_json_to_pcdm_transfromer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe containing the column with Json AIO data\n        source_column_name (str): Spark Dataframe column containing the Json AIO data\n        status_null_value (str): If populated, will replace 'Good' in the Status column with the specified value.\n        change_type_value (optional str): If populated, will replace 'insert' in the ChangeType column with the specified value.\n    \"\"\"\n\n    data: DataFrame\n    source_column_name: str\n    status_null_value: str\n    change_type_value: str\n\n    def __init__(\n        self,\n        data: DataFrame,\n        source_column_name: str,\n        status_null_value: str = \"Good\",\n        change_type_value: str = \"insert\",\n    ) -&gt; None:  # NOSONAR\n        self.data = data\n        self.source_column_name = source_column_name\n        self.status_null_value = status_null_value\n        self.change_type_value = change_type_value\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the specified column converted to PCDM\n        \"\"\"\n        df = (\n            self.data.select(\n                from_json(col(self.source_column_name), \"Payload STRING\").alias(\"body\")\n            )\n            .select(from_json(expr(\"body.Payload\"), AIO_SCHEMA).alias(\"body\"))\n            .select(explode(\"body\"))\n            .select(col(\"key\").alias(\"TagName\"), \"value.*\")\n            .select(col(\"SourceTimestamp\").alias(\"EventTime\"), \"TagName\", \"Value\")\n            .withColumn(\"Status\", lit(self.status_null_value))\n            .withColumn(\n                \"ValueType\",\n                when(col(\"Value\").cast(\"float\").isNotNull(), \"float\").otherwise(\n                    \"string\"\n                ),\n            )\n            .withColumn(\"ChangeType\", lit(self.change_type_value))\n        )\n\n        return df.select(\n            \"EventTime\", \"TagName\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.AIOJsonToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/aio_json_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.AIOJsonToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the specified column converted to PCDM</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/aio_json_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the specified column converted to PCDM\n    \"\"\"\n    df = (\n        self.data.select(\n            from_json(col(self.source_column_name), \"Payload STRING\").alias(\"body\")\n        )\n        .select(from_json(expr(\"body.Payload\"), AIO_SCHEMA).alias(\"body\"))\n        .select(explode(\"body\"))\n        .select(col(\"key\").alias(\"TagName\"), \"value.*\")\n        .select(col(\"SourceTimestamp\").alias(\"EventTime\"), \"TagName\", \"Value\")\n        .withColumn(\"Status\", lit(self.status_null_value))\n        .withColumn(\n            \"ValueType\",\n            when(col(\"Value\").cast(\"float\").isNotNull(), \"float\").otherwise(\n                \"string\"\n            ),\n        )\n        .withColumn(\"ChangeType\", lit(self.change_type_value))\n    )\n\n    return df.select(\n        \"EventTime\", \"TagName\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.OPCUAJsonToPCDMTransformer","title":"<code>OPCUAJsonToPCDMTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a Spark Dataframe column containing a json string created by Open Source OPC UA to the Process Control Data Model.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.OPCUAJsonToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import OPCUAJsonToPCDMTransformer\n\nopcua_json_to_pcdm_transfromer = OPCUAJsonToPCDMTransformer(\n    data=df,\n    souce_column_name=\"body\",\n    status_null_value=\"Good\",\n    change_type_value=\"insert\"\n)\n\nresult = opcua_json_to_pcdm_transfromer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe containing the column with Json OPC UA data</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the OPC Publisher Json OPC UA data</p> required <code>status_null_value</code> <code>str</code> <p>If populated, will replace 'Good' in the Status column with the specified value.</p> <code>'Good'</code> <code>change_type_value</code> <code>optional str</code> <p>If populated, will replace 'insert' in the ChangeType column with the specified value.</p> <code>'insert'</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/opcua_json_to_pcdm.py</code> <pre><code>class OPCUAJsonToPCDMTransformer(TransformerInterface):\n    \"\"\"\n    Converts a Spark Dataframe column containing a json string created by Open Source OPC UA to the Process Control Data Model.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import OPCUAJsonToPCDMTransformer\n\n    opcua_json_to_pcdm_transfromer = OPCUAJsonToPCDMTransformer(\n        data=df,\n        souce_column_name=\"body\",\n        status_null_value=\"Good\",\n        change_type_value=\"insert\"\n    )\n\n    result = opcua_json_to_pcdm_transfromer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe containing the column with Json OPC UA data\n        source_column_name (str): Spark Dataframe column containing the OPC Publisher Json OPC UA data\n        status_null_value (str): If populated, will replace 'Good' in the Status column with the specified value.\n        change_type_value (optional str): If populated, will replace 'insert' in the ChangeType column with the specified value.\n    \"\"\"\n\n    data: DataFrame\n    source_column_name: str\n    status_null_value: str\n    change_type_value: str\n\n    def __init__(\n        self,\n        data: DataFrame,\n        source_column_name: str,\n        status_null_value: str = \"Good\",\n        change_type_value: str = \"insert\",\n    ) -&gt; None:  # NOSONAR\n        self.data = data\n        self.source_column_name = source_column_name\n        self.status_null_value = status_null_value\n        self.change_type_value = change_type_value\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the specified column converted to PCDM\n        \"\"\"\n        df = (\n            self.data.select(\n                from_json(col(self.source_column_name), \"Messages STRING\").alias(\"body\")\n            )\n            .select(from_json(expr(\"body.Messages\"), OPCUA_SCHEMA).alias(\"body\"))\n            .selectExpr(\"inline(body)\")\n            .select(col(\"Timestamp\").alias(\"EventTime\"), explode(\"Payload\"))\n            .select(\"EventTime\", col(\"key\").alias(\"TagName\"), \"value.*\")\n            .withColumn(\"Status\", lit(self.status_null_value))\n            .withColumn(\n                \"ValueType\",\n                when(col(\"Value\").cast(\"float\").isNotNull(), \"float\").otherwise(\n                    \"string\"\n                ),\n            )\n            .withColumn(\"ChangeType\", lit(self.change_type_value))\n        )\n\n        return df.select(\n            \"EventTime\", \"TagName\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.OPCUAJsonToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/opcua_json_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.OPCUAJsonToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the specified column converted to PCDM</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/opcua_json_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the specified column converted to PCDM\n    \"\"\"\n    df = (\n        self.data.select(\n            from_json(col(self.source_column_name), \"Messages STRING\").alias(\"body\")\n        )\n        .select(from_json(expr(\"body.Messages\"), OPCUA_SCHEMA).alias(\"body\"))\n        .selectExpr(\"inline(body)\")\n        .select(col(\"Timestamp\").alias(\"EventTime\"), explode(\"Payload\"))\n        .select(\"EventTime\", col(\"key\").alias(\"TagName\"), \"value.*\")\n        .withColumn(\"Status\", lit(self.status_null_value))\n        .withColumn(\n            \"ValueType\",\n            when(col(\"Value\").cast(\"float\").isNotNull(), \"float\").otherwise(\n                \"string\"\n            ),\n        )\n        .withColumn(\"ChangeType\", lit(self.change_type_value))\n    )\n\n    return df.select(\n        \"EventTime\", \"TagName\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.CAISOToMDMTransformer","title":"<code>CAISOToMDMTransformer</code>","text":"<p>               Bases: <code>BaseRawToMDMTransformer</code></p> <p>Converts CAISO Raw data into Meters Data Model.</p> <p>Please check the BaseRawToMDMTransformer for the required arguments and methods.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.CAISOToMDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import CAISOToMDMTransformer\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\ncaiso_to_mdm_transformer = CAISOToMDMTransformer(\n    spark=spark,\n    data=df,\n    output_type=\"usage\",\n    name=None,\n    description=None,\n    value_type=None,\n    version=None,\n    series_id=None,\n    series_parent_id=None\n)\n\nresult = caiso_to_mdm_transformer.transform()\n</code></pre> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/iso/caiso_to_mdm.py</code> <pre><code>class CAISOToMDMTransformer(BaseRawToMDMTransformer):\n    \"\"\"\n    Converts CAISO Raw data into Meters Data Model.\n\n    Please check the BaseRawToMDMTransformer for the required arguments and methods.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import CAISOToMDMTransformer\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    caiso_to_mdm_transformer = CAISOToMDMTransformer(\n        spark=spark,\n        data=df,\n        output_type=\"usage\",\n        name=None,\n        description=None,\n        value_type=None,\n        version=None,\n        series_id=None,\n        series_parent_id=None\n    )\n\n    result = caiso_to_mdm_transformer.transform()\n    ```\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    input_schema = CAISO_SCHEMA\n    uid_col = \"TacAreaName\"\n    series_id_col = \"'series_std_001'\"\n    timestamp_col = \"to_timestamp(StartTime)\"\n    interval_timestamp_col = \"Timestamp + INTERVAL 1 HOURS\"\n    value_col = \"Load\"\n    series_parent_id_col = \"'series_parent_std_001'\"\n    name_col = \"'CAISO API'\"\n    uom_col = \"'mwh'\"\n    description_col = \"'CAISO data pulled from CAISO ISO API'\"\n    timestamp_start_col = \"StartTime\"\n    timestamp_end_col = \"StartTime + INTERVAL 1 HOURS\"\n    time_zone_col = \"'PST'\"\n    version_col = \"'1'\"\n    series_type = SeriesType.Hour\n    model_type = ModelType.Default\n    value_type = ValueType.Usage\n    properties_col = \"null\"\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.ERCOTToMDMTransformer","title":"<code>ERCOTToMDMTransformer</code>","text":"<p>               Bases: <code>BaseRawToMDMTransformer</code></p> <p>Converts ERCOT Raw data into Meters Data Model.</p> <p>Please check the BaseRawToMDMTransformer for the required arguments and methods.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.ERCOTToMDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import ERCOTToMDMTransformer\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nercot_to_mdm_transformer = ERCOTToMDMTransformer(\n    spark=spark,\n    data=df,\n    output_type=\"usage\",\n    name=None,\n    description=None,\n    value_type=None,\n    version=None,\n    series_id=None,\n    series_parent_id=None\n)\n\nresult = ercot_to_mdm_transformer.transform()\n</code></pre> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/iso/ercot_to_mdm.py</code> <pre><code>class ERCOTToMDMTransformer(BaseRawToMDMTransformer):\n    \"\"\"\n    Converts ERCOT Raw data into Meters Data Model.\n\n    Please check the BaseRawToMDMTransformer for the required arguments and methods.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import ERCOTToMDMTransformer\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    ercot_to_mdm_transformer = ERCOTToMDMTransformer(\n        spark=spark,\n        data=df,\n        output_type=\"usage\",\n        name=None,\n        description=None,\n        value_type=None,\n        version=None,\n        series_id=None,\n        series_parent_id=None\n    )\n\n    result = ercot_to_mdm_transformer.transform()\n    ```\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    input_schema = ERCOT_SCHEMA\n    uid_col = \"variable\"\n    series_id_col = \"'series_std_001'\"\n    timestamp_col = \"to_utc_timestamp(StartTime, 'America/Chicago')\"\n    interval_timestamp_col = \"Timestamp + INTERVAL 1 HOURS\"\n    value_col = \"value\"\n    series_parent_id_col = \"'series_parent_std_001'\"\n    name_col = \"'ERCOT API'\"\n    uom_col = \"'mwh'\"\n    description_col = \"'ERCOT data pulled from ERCOT ISO API'\"\n    timestamp_start_col = \"StartTime\"\n    timestamp_end_col = \"StartTime + INTERVAL 1 HOURS\"\n    time_zone_col = \"'America/Chicago'\"\n    version_col = \"'1'\"\n    series_type = SeriesType.Hour\n    model_type = ModelType.Default\n    value_type = ValueType.Usage\n    properties_col = \"null\"\n\n    def _pre_process(self) -&gt; DataFrame:\n        df: DataFrame = super(ERCOTToMDMTransformer, self)._pre_process()\n        df = melt(\n            df,\n            id_vars=[\"Date\", \"HourEnding\", \"DstFlag\"],\n            value_vars=[\n                \"Coast\",\n                \"East\",\n                \"FarWest\",\n                \"North\",\n                \"NorthCentral\",\n                \"SouthCentral\",\n                \"Southern\",\n                \"West\",\n                \"SystemTotal\",\n            ],\n        )\n        df = df.withColumn(\n            \"StartTime\",\n            F.expr(\n                \"Date + MAKE_INTERVAL(0,0,0,0,cast(split(HourEnding,':')[0] as integer),0,0)\"\n            ),\n        )\n        return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.MISOToMDMTransformer","title":"<code>MISOToMDMTransformer</code>","text":"<p>               Bases: <code>BaseRawToMDMTransformer</code></p> <p>Converts MISO Raw data into Meters Data Model.</p> <p>Please check the BaseRawToMDMTransformer for the required arguments and methods.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.MISOToMDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import MISOToMDMTransformer\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nmiso_to_mdm_transformer = MISOToMDMTransformer(\n    spark=spark,\n    data=df,\n    output_type=\"usage\",\n    name=None,\n    description=None,\n    value_type=None,\n    version=None,\n    series_id=None,\n    series_parent_id=None\n)\n\nresult = miso_to_mdm_transformer.transform()\n</code></pre> BaseRawToMDMTransformer Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/iso/miso_to_mdm.py</code> <pre><code>class MISOToMDMTransformer(BaseRawToMDMTransformer):\n    \"\"\"\n    Converts MISO Raw data into Meters Data Model.\n\n    Please check the BaseRawToMDMTransformer for the required arguments and methods.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import MISOToMDMTransformer\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    miso_to_mdm_transformer = MISOToMDMTransformer(\n        spark=spark,\n        data=df,\n        output_type=\"usage\",\n        name=None,\n        description=None,\n        value_type=None,\n        version=None,\n        series_id=None,\n        series_parent_id=None\n    )\n\n    result = miso_to_mdm_transformer.transform()\n    ```\n\n    BaseRawToMDMTransformer:\n        ::: src.sdk.python.rtdip_sdk.pipelines.transformers.spark.base_raw_to_mdm\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    input_schema = MISO_SCHEMA\n    uid_col = \"variable\"\n    series_id_col = \"'series_std_001'\"\n    timestamp_col = \"to_utc_timestamp(Datetime, 'US/Central')\"\n    interval_timestamp_col = \"Timestamp + INTERVAL 1 HOURS\"\n    value_col = \"bround(value, 2)\"\n    series_parent_id_col = \"'series_parent_std_001'\"\n    name_col = \"'Miso API'\"\n    uom_col = \"'mwh'\"\n    description_col = \"'Miso data pulled from Miso ISO API'\"\n    timestamp_start_col = \"Datetime\"\n    timestamp_end_col = \"Datetime + INTERVAL 1 HOURS\"\n    time_zone_col = \"'US/Central'\"\n    version_col = \"'1'\"\n    series_type = SeriesType.Hour\n    model_type = ModelType.Default\n    value_type = ValueType.Usage\n    properties_col = \"null\"\n\n    def _pre_process(self) -&gt; DataFrame:\n        df: DataFrame = super(MISOToMDMTransformer, self)._pre_process()\n        df = melt(\n            df,\n            id_vars=[\"Datetime\"],\n            value_vars=[\n                \"Lrz1\",\n                \"Lrz2_7\",\n                \"Lrz3_5\",\n                \"Lrz4\",\n                \"Lrz6\",\n                \"Lrz8_9_10\",\n                \"Miso\",\n            ],\n        )\n        return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.base_raw_to_mdm.BaseRawToMDMTransformer","title":"<code>BaseRawToMDMTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Base class for all the Raw to Meters Data Model Transformers.</p> Meters Data Model requires two outputs <ul> <li><code>UsageData</code> : To store measurement(value) as timeseries data.</li> <li><code>MetaData</code> : To store meters related meta information.</li> </ul> <p>It supports the generation of both the outputs as they share some common properties.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance.</p> required <code>data</code> <code>DataFrame</code> <p>Dataframe containing the raw MISO data.</p> required <code>output_type</code> <code>str</code> <p>Must be one of <code>usage</code> or <code>meta</code>.</p> required <code>name</code> <code>str</code> <p>Set this to override default <code>name</code> column.</p> <code>None</code> <code>description</code> <code>str</code> <p>Set this to override default <code>description</code> column.</p> <code>None</code> <code>value_type</code> <code>ValueType</code> <p>Set this to override default <code>value_type</code> column.</p> <code>None</code> <code>version</code> <code>str</code> <p>Set this to override default <code>version</code> column.</p> <code>None</code> <code>series_id</code> <code>str</code> <p>Set this to override default <code>series_id</code> column.</p> <code>None</code> <code>series_parent_id</code> <code>str</code> <p>Set this to override default <code>series_parent_id</code> column.</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/base_raw_to_mdm.py</code> <pre><code>class BaseRawToMDMTransformer(TransformerInterface):\n    \"\"\"\n    Base class for all the Raw to Meters Data Model Transformers.\n\n    Meters Data Model requires two outputs:\n        - `UsageData` : To store measurement(value) as timeseries data.\n        - `MetaData` : To store meters related meta information.\n\n    It supports the generation of both the outputs as they share some common properties.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance.\n        data (DataFrame): Dataframe containing the raw MISO data.\n        output_type (str): Must be one of `usage` or `meta`.\n        name (str): Set this to override default `name` column.\n        description (str): Set this to override default `description` column.\n        value_type (ValueType): Set this to override default `value_type` column.\n        version (str): Set this to override default `version` column.\n        series_id (str): Set this to override default `series_id` column.\n        series_parent_id (str): Set this to override default `series_parent_id` column.\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    output_type: str\n    input_schema: StructType\n    target_schema: StructType\n    uid_col: str\n    series_id_col: str\n    timestamp_col: str\n    interval_timestamp_col: str\n    value_col: str\n    series_parent_id_col: str\n    name_col: str\n    uom_col: str\n    description_col: str\n    timestamp_start_col: str\n    timestamp_end_col: str\n    time_zone_col: str\n    version_col: str\n    series_type: SeriesType\n    model_type: ModelType\n    value_type: ValueType\n    properties_col: str\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n        output_type: str,\n        name: str = None,\n        description: str = None,\n        value_type: ValueType = None,\n        version: str = None,\n        series_id: str = None,\n        series_parent_id: str = None,\n    ):\n        self.spark = spark\n        self.data = data\n        self.output_type = output_type\n        self.name = name if name is not None else self.name_col\n        self.description = (\n            description if description is not None else self.description_col\n        )\n        self.value_type = value_type if value_type is not None else self.value_type\n        self.version = version if version is not None else self.version_col\n        self.series_id = series_id if series_id is not None else self.series_id_col\n        self.series_parent_id = (\n            series_parent_id\n            if series_parent_id is not None\n            else self.series_parent_id_col\n        )\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self) -&gt; bool:\n        valid_output_types = [\"usage\", \"meta\"]\n        if self.output_type not in valid_output_types:\n            raise ValueError(\n                f\"Invalid output_type `{self.output_type}` given. Must be one of {valid_output_types}\"\n            )\n\n        assert str(self.data.schema) == str(self.input_schema)\n        assert type(self.series_type).__name__ == SeriesType.__name__\n        assert type(self.model_type).__name__ == ModelType.__name__\n        assert type(self.value_type).__name__ == ValueType.__name__\n        return True\n\n    def post_transform_validation(self) -&gt; bool:\n        assert str(self.data.schema) == str(self.target_schema)\n        return True\n\n    def _get_transformed_df(self) -&gt; DataFrame:\n        if self.output_type == \"usage\":\n            self.target_schema = MDM_USAGE_SCHEMA\n            return self._get_usage_transformed_df()\n        else:\n            self.target_schema = MDM_META_SCHEMA\n            return self._get_meta_transformed_df()\n\n    def _convert_into_target_schema(self) -&gt; None:\n        \"\"\"\n        Converts a Spark DataFrame structure into new structure based on the Target Schema.\n\n        Returns: Nothing.\n\n        \"\"\"\n\n        df: DataFrame = self.data\n        df = df.select(self.target_schema.names)\n\n        for field in self.target_schema.fields:\n            df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n\n        self.data = self.spark.createDataFrame(df.rdd, self.target_schema)\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the raw data converted into MDM.\n        \"\"\"\n\n        self.pre_transform_validation()\n        self.data = self._get_transformed_df()\n        self._convert_into_target_schema()\n        self.post_transform_validation()\n\n        return self.data\n\n    def _add_uid_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Uid\", expr(self.uid_col))\n\n    def _add_series_id_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"SeriesId\", expr(self.series_id))\n\n    def _add_timestamp_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Timestamp\", expr(self.timestamp_col))\n\n    def _add_interval_timestamp_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"IntervalTimestamp\", expr(self.interval_timestamp_col))\n\n    def _add_value_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Value\", expr(self.value_col))\n\n    def _add_series_parent_id_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"SeriesParentId\", expr(self.series_parent_id))\n\n    def _add_name_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Name\", expr(self.name))\n\n    def _add_uom_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Uom\", expr(self.uom_col))\n\n    def _add_description_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Description\", expr(self.description))\n\n    def _add_timestamp_start_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"TimestampStart\", expr(self.timestamp_start_col))\n\n    def _add_timestamp_end_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"TimestampEnd\", expr(self.timestamp_end_col))\n\n    def _add_time_zone_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Timezone\", expr(self.time_zone_col))\n\n    def _add_version_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Version\", expr(self.version))\n\n    def _add_series_type_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"SeriesType\", lit(self.series_type.value))\n\n    def _add_model_type_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"ModelType\", lit(self.model_type.value))\n\n    def _add_value_type_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"ValueType\", lit(self.value_type.value))\n\n    def _add_properties_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Properties\", expr(self.properties_col))\n\n    def _pre_process(self) -&gt; DataFrame:\n        return self.data\n\n    @staticmethod\n    def _post_process(df: DataFrame) -&gt; DataFrame:\n        return df\n\n    def _get_usage_transformed_df(self) -&gt; DataFrame:\n        df = self._pre_process()\n\n        df = self._add_uid_column(df)\n        df = self._add_series_id_column(df)\n        df = self._add_timestamp_column(df)\n        df = self._add_interval_timestamp_column(df)\n        df = self._add_value_column(df)\n\n        df = self._post_process(df)\n\n        return df\n\n    def _get_meta_transformed_df(self) -&gt; DataFrame:\n        df = self._pre_process()\n\n        df = self._add_uid_column(df)\n        df = self._add_series_id_column(df)\n        df = self._add_series_parent_id_column(df)\n        df = self._add_name_column(df)\n        df = self._add_uom_column(df)\n        df = self._add_description_column(df)\n        df = self._add_timestamp_start_column(df)\n        df = self._add_timestamp_end_column(df)\n        df = self._add_time_zone_column(df)\n        df = self._add_version_column(df)\n        df = self._add_series_type_column(df)\n        df = self._add_model_type_column(df)\n        df = self._add_value_type_column(df)\n        df = self._add_properties_column(df)\n\n        df = self._post_process(df)\n\n        return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.base_raw_to_mdm.BaseRawToMDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the raw data converted into MDM.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/base_raw_to_mdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the raw data converted into MDM.\n    \"\"\"\n\n    self.pre_transform_validation()\n    self.data = self._get_transformed_df()\n    self._convert_into_target_schema()\n    self.post_transform_validation()\n\n    return self.data\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PJMToMDMTransformer","title":"<code>PJMToMDMTransformer</code>","text":"<p>               Bases: <code>BaseRawToMDMTransformer</code></p> <p>Converts PJM Raw data into Meters Data Model.</p> <p>Please check the BaseRawToMDMTransformer for the required arguments and methods.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PJMToMDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import PJMToMDMTransformer\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\npjm_to_mdm_transformer = PJMToMDMTransformer(\n    spark=spark,\n    data=df,\n    output_type=\"usage\",\n    name=None,\n    description=None,\n    value_type=None,\n    version=None,\n    series_id=None,\n    series_parent_id=None\n)\n\nresult = pjm_to_mdm_transformer.transform()\n</code></pre> BaseRawToMDMTransformer Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/iso/pjm_to_mdm.py</code> <pre><code>class PJMToMDMTransformer(BaseRawToMDMTransformer):\n    \"\"\"\n    Converts PJM Raw data into Meters Data Model.\n\n    Please check the BaseRawToMDMTransformer for the required arguments and methods.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import PJMToMDMTransformer\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    pjm_to_mdm_transformer = PJMToMDMTransformer(\n        spark=spark,\n        data=df,\n        output_type=\"usage\",\n        name=None,\n        description=None,\n        value_type=None,\n        version=None,\n        series_id=None,\n        series_parent_id=None\n    )\n\n    result = pjm_to_mdm_transformer.transform()\n    ```\n\n    BaseRawToMDMTransformer:\n        ::: src.sdk.python.rtdip_sdk.pipelines.transformers.spark.base_raw_to_mdm\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    input_schema = PJM_SCHEMA\n    uid_col = \"Zone\"\n    series_id_col = \"'series_std_001'\"\n    timestamp_col = \"to_utc_timestamp(StartTime, 'America/New_York')\"\n    interval_timestamp_col = \"Timestamp + INTERVAL 1 HOURS\"\n    value_col = \"bround(Load, 2)\"\n    series_parent_id_col = \"'series_parent_std_001'\"\n    name_col = \"'PJM API'\"\n    uom_col = \"'mwh'\"\n    description_col = \"'PJM data pulled from PJM ISO API'\"\n    timestamp_start_col = \"StartTime\"\n    timestamp_end_col = \"StartTime + INTERVAL 1 HOURS\"\n    time_zone_col = \"'America/New_York'\"\n    version_col = \"'1'\"\n    series_type = SeriesType.Hour\n    model_type = ModelType.Default\n    value_type = ValueType.Usage\n    properties_col = \"null\"\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.base_raw_to_mdm.BaseRawToMDMTransformer","title":"<code>BaseRawToMDMTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Base class for all the Raw to Meters Data Model Transformers.</p> Meters Data Model requires two outputs <ul> <li><code>UsageData</code> : To store measurement(value) as timeseries data.</li> <li><code>MetaData</code> : To store meters related meta information.</li> </ul> <p>It supports the generation of both the outputs as they share some common properties.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance.</p> required <code>data</code> <code>DataFrame</code> <p>Dataframe containing the raw MISO data.</p> required <code>output_type</code> <code>str</code> <p>Must be one of <code>usage</code> or <code>meta</code>.</p> required <code>name</code> <code>str</code> <p>Set this to override default <code>name</code> column.</p> <code>None</code> <code>description</code> <code>str</code> <p>Set this to override default <code>description</code> column.</p> <code>None</code> <code>value_type</code> <code>ValueType</code> <p>Set this to override default <code>value_type</code> column.</p> <code>None</code> <code>version</code> <code>str</code> <p>Set this to override default <code>version</code> column.</p> <code>None</code> <code>series_id</code> <code>str</code> <p>Set this to override default <code>series_id</code> column.</p> <code>None</code> <code>series_parent_id</code> <code>str</code> <p>Set this to override default <code>series_parent_id</code> column.</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/base_raw_to_mdm.py</code> <pre><code>class BaseRawToMDMTransformer(TransformerInterface):\n    \"\"\"\n    Base class for all the Raw to Meters Data Model Transformers.\n\n    Meters Data Model requires two outputs:\n        - `UsageData` : To store measurement(value) as timeseries data.\n        - `MetaData` : To store meters related meta information.\n\n    It supports the generation of both the outputs as they share some common properties.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance.\n        data (DataFrame): Dataframe containing the raw MISO data.\n        output_type (str): Must be one of `usage` or `meta`.\n        name (str): Set this to override default `name` column.\n        description (str): Set this to override default `description` column.\n        value_type (ValueType): Set this to override default `value_type` column.\n        version (str): Set this to override default `version` column.\n        series_id (str): Set this to override default `series_id` column.\n        series_parent_id (str): Set this to override default `series_parent_id` column.\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    output_type: str\n    input_schema: StructType\n    target_schema: StructType\n    uid_col: str\n    series_id_col: str\n    timestamp_col: str\n    interval_timestamp_col: str\n    value_col: str\n    series_parent_id_col: str\n    name_col: str\n    uom_col: str\n    description_col: str\n    timestamp_start_col: str\n    timestamp_end_col: str\n    time_zone_col: str\n    version_col: str\n    series_type: SeriesType\n    model_type: ModelType\n    value_type: ValueType\n    properties_col: str\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n        output_type: str,\n        name: str = None,\n        description: str = None,\n        value_type: ValueType = None,\n        version: str = None,\n        series_id: str = None,\n        series_parent_id: str = None,\n    ):\n        self.spark = spark\n        self.data = data\n        self.output_type = output_type\n        self.name = name if name is not None else self.name_col\n        self.description = (\n            description if description is not None else self.description_col\n        )\n        self.value_type = value_type if value_type is not None else self.value_type\n        self.version = version if version is not None else self.version_col\n        self.series_id = series_id if series_id is not None else self.series_id_col\n        self.series_parent_id = (\n            series_parent_id\n            if series_parent_id is not None\n            else self.series_parent_id_col\n        )\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self) -&gt; bool:\n        valid_output_types = [\"usage\", \"meta\"]\n        if self.output_type not in valid_output_types:\n            raise ValueError(\n                f\"Invalid output_type `{self.output_type}` given. Must be one of {valid_output_types}\"\n            )\n\n        assert str(self.data.schema) == str(self.input_schema)\n        assert type(self.series_type).__name__ == SeriesType.__name__\n        assert type(self.model_type).__name__ == ModelType.__name__\n        assert type(self.value_type).__name__ == ValueType.__name__\n        return True\n\n    def post_transform_validation(self) -&gt; bool:\n        assert str(self.data.schema) == str(self.target_schema)\n        return True\n\n    def _get_transformed_df(self) -&gt; DataFrame:\n        if self.output_type == \"usage\":\n            self.target_schema = MDM_USAGE_SCHEMA\n            return self._get_usage_transformed_df()\n        else:\n            self.target_schema = MDM_META_SCHEMA\n            return self._get_meta_transformed_df()\n\n    def _convert_into_target_schema(self) -&gt; None:\n        \"\"\"\n        Converts a Spark DataFrame structure into new structure based on the Target Schema.\n\n        Returns: Nothing.\n\n        \"\"\"\n\n        df: DataFrame = self.data\n        df = df.select(self.target_schema.names)\n\n        for field in self.target_schema.fields:\n            df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n\n        self.data = self.spark.createDataFrame(df.rdd, self.target_schema)\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the raw data converted into MDM.\n        \"\"\"\n\n        self.pre_transform_validation()\n        self.data = self._get_transformed_df()\n        self._convert_into_target_schema()\n        self.post_transform_validation()\n\n        return self.data\n\n    def _add_uid_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Uid\", expr(self.uid_col))\n\n    def _add_series_id_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"SeriesId\", expr(self.series_id))\n\n    def _add_timestamp_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Timestamp\", expr(self.timestamp_col))\n\n    def _add_interval_timestamp_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"IntervalTimestamp\", expr(self.interval_timestamp_col))\n\n    def _add_value_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Value\", expr(self.value_col))\n\n    def _add_series_parent_id_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"SeriesParentId\", expr(self.series_parent_id))\n\n    def _add_name_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Name\", expr(self.name))\n\n    def _add_uom_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Uom\", expr(self.uom_col))\n\n    def _add_description_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Description\", expr(self.description))\n\n    def _add_timestamp_start_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"TimestampStart\", expr(self.timestamp_start_col))\n\n    def _add_timestamp_end_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"TimestampEnd\", expr(self.timestamp_end_col))\n\n    def _add_time_zone_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Timezone\", expr(self.time_zone_col))\n\n    def _add_version_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Version\", expr(self.version))\n\n    def _add_series_type_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"SeriesType\", lit(self.series_type.value))\n\n    def _add_model_type_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"ModelType\", lit(self.model_type.value))\n\n    def _add_value_type_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"ValueType\", lit(self.value_type.value))\n\n    def _add_properties_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Properties\", expr(self.properties_col))\n\n    def _pre_process(self) -&gt; DataFrame:\n        return self.data\n\n    @staticmethod\n    def _post_process(df: DataFrame) -&gt; DataFrame:\n        return df\n\n    def _get_usage_transformed_df(self) -&gt; DataFrame:\n        df = self._pre_process()\n\n        df = self._add_uid_column(df)\n        df = self._add_series_id_column(df)\n        df = self._add_timestamp_column(df)\n        df = self._add_interval_timestamp_column(df)\n        df = self._add_value_column(df)\n\n        df = self._post_process(df)\n\n        return df\n\n    def _get_meta_transformed_df(self) -&gt; DataFrame:\n        df = self._pre_process()\n\n        df = self._add_uid_column(df)\n        df = self._add_series_id_column(df)\n        df = self._add_series_parent_id_column(df)\n        df = self._add_name_column(df)\n        df = self._add_uom_column(df)\n        df = self._add_description_column(df)\n        df = self._add_timestamp_start_column(df)\n        df = self._add_timestamp_end_column(df)\n        df = self._add_time_zone_column(df)\n        df = self._add_version_column(df)\n        df = self._add_series_type_column(df)\n        df = self._add_model_type_column(df)\n        df = self._add_value_type_column(df)\n        df = self._add_properties_column(df)\n\n        df = self._post_process(df)\n\n        return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.base_raw_to_mdm.BaseRawToMDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the raw data converted into MDM.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/base_raw_to_mdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the raw data converted into MDM.\n    \"\"\"\n\n    self.pre_transform_validation()\n    self.data = self._get_transformed_df()\n    self._convert_into_target_schema()\n    self.post_transform_validation()\n\n    return self.data\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.EdgeXOPCUAJsonToPCDMTransformer","title":"<code>EdgeXOPCUAJsonToPCDMTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a Spark Dataframe column containing a json string created by EdgeX to the Process Control Data Model.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.EdgeXOPCUAJsonToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import EdgeXOPCUAJsonToPCDMTransformer\n\nedge_opcua_json_to_pcdm_transformer = EdgeXOPCUAJsonToPCDMTransformer(\n    data=df,\n    souce_column_name=\"body\",\n    status_null_value=\"Good\",\n    change_type_value=\"insert\"\n)\n\nresult = edge_opcua_json_to_pcdm_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe containing the column with EdgeX data</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the OPC Publisher Json OPC UA data</p> required <code>status_null_value</code> <code>optional str</code> <p>If populated, will replace 'Good' in the Status column with the specified value.</p> <code>'Good'</code> <code>change_type_value</code> <code>optional str</code> <p>If populated, will replace 'insert' in the ChangeType column with the specified value.</p> <code>'insert'</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/edgex_opcua_json_to_pcdm.py</code> <pre><code>class EdgeXOPCUAJsonToPCDMTransformer(TransformerInterface):\n    \"\"\"\n    Converts a Spark Dataframe column containing a json string created by EdgeX to the Process Control Data Model.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import EdgeXOPCUAJsonToPCDMTransformer\n\n    edge_opcua_json_to_pcdm_transformer = EdgeXOPCUAJsonToPCDMTransformer(\n        data=df,\n        souce_column_name=\"body\",\n        status_null_value=\"Good\",\n        change_type_value=\"insert\"\n    )\n\n    result = edge_opcua_json_to_pcdm_transformer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe containing the column with EdgeX data\n        source_column_name (str): Spark Dataframe column containing the OPC Publisher Json OPC UA data\n        status_null_value (optional str): If populated, will replace 'Good' in the Status column with the specified value.\n        change_type_value (optional str): If populated, will replace 'insert' in the ChangeType column with the specified value.\n    \"\"\"\n\n    data: DataFrame\n    source_column_name: str\n    status_null_value: str\n    change_type_value: str\n    tagname_field: str\n\n    def __init__(\n        self,\n        data: DataFrame,\n        source_column_name: str,\n        status_null_value: str = \"Good\",\n        change_type_value: str = \"insert\",\n        tagname_field=\"resourceName\",\n    ) -&gt; None:\n        self.data = data\n        self.source_column_name = source_column_name\n        self.status_null_value = status_null_value\n        self.change_type_value = change_type_value\n        self.tagname_field = tagname_field\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the specified column converted to PCDM\n        \"\"\"\n        df = (\n            self.data.withColumn(\n                self.source_column_name,\n                from_json(self.source_column_name, EDGEX_SCHEMA),\n            )\n            .select(\"*\", explode(\"{}.readings\".format(self.source_column_name)))\n            .selectExpr(\n                \"explode({}.readings.{}) as TagName\".format(\n                    self.source_column_name, self.tagname_field\n                ),\n                \"to_utc_timestamp(to_timestamp((col.origin / 1000000000)), current_timezone()) as EventTime\",\n                \"col.value as Value\",\n                \"col.valueType as ValueType\",\n            )\n            .withColumn(\"Status\", lit(self.status_null_value))\n            .withColumn(\"ChangeType\", lit(self.change_type_value))\n            .withColumn(\n                \"ValueType\",\n                (\n                    when(col(\"ValueType\") == \"Int8\", \"integer\")\n                    .when(col(\"ValueType\") == \"Int16\", \"integer\")\n                    .when(col(\"ValueType\") == \"Int32\", \"integer\")\n                    .when(col(\"ValueType\") == \"Int64\", \"integer\")\n                    .when(col(\"ValueType\") == \"Uint8\", \"integer\")\n                    .when(col(\"ValueType\") == \"Uint16\", \"integer\")\n                    .when(col(\"ValueType\") == \"Uint32\", \"integer\")\n                    .when(col(\"ValueType\") == \"Uint64\", \"integer\")\n                    .when(col(\"ValueType\") == \"Float32\", \"float\")\n                    .when(col(\"ValueType\") == \"Float64\", \"float\")\n                    .when(col(\"ValueType\") == \"Bool\", \"bool\")\n                    .otherwise(\"string\")\n                ),\n            )\n        )\n\n        return df.select(\n            \"TagName\", \"EventTime\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.EdgeXOPCUAJsonToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/edgex_opcua_json_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.EdgeXOPCUAJsonToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the specified column converted to PCDM</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/edgex_opcua_json_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the specified column converted to PCDM\n    \"\"\"\n    df = (\n        self.data.withColumn(\n            self.source_column_name,\n            from_json(self.source_column_name, EDGEX_SCHEMA),\n        )\n        .select(\"*\", explode(\"{}.readings\".format(self.source_column_name)))\n        .selectExpr(\n            \"explode({}.readings.{}) as TagName\".format(\n                self.source_column_name, self.tagname_field\n            ),\n            \"to_utc_timestamp(to_timestamp((col.origin / 1000000000)), current_timezone()) as EventTime\",\n            \"col.value as Value\",\n            \"col.valueType as ValueType\",\n        )\n        .withColumn(\"Status\", lit(self.status_null_value))\n        .withColumn(\"ChangeType\", lit(self.change_type_value))\n        .withColumn(\n            \"ValueType\",\n            (\n                when(col(\"ValueType\") == \"Int8\", \"integer\")\n                .when(col(\"ValueType\") == \"Int16\", \"integer\")\n                .when(col(\"ValueType\") == \"Int32\", \"integer\")\n                .when(col(\"ValueType\") == \"Int64\", \"integer\")\n                .when(col(\"ValueType\") == \"Uint8\", \"integer\")\n                .when(col(\"ValueType\") == \"Uint16\", \"integer\")\n                .when(col(\"ValueType\") == \"Uint32\", \"integer\")\n                .when(col(\"ValueType\") == \"Uint64\", \"integer\")\n                .when(col(\"ValueType\") == \"Float32\", \"float\")\n                .when(col(\"ValueType\") == \"Float64\", \"float\")\n                .when(col(\"ValueType\") == \"Bool\", \"bool\")\n                .otherwise(\"string\")\n            ),\n        )\n    )\n\n    return df.select(\n        \"TagName\", \"EventTime\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.ECMWFExtractBaseToWeatherDataModel","title":"<code>ECMWFExtractBaseToWeatherDataModel</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Base class for extracting forecast data downloaded in .nc format from ECMWF MARS Server.</p> <p>Parameters:</p> Name Type Description Default <code>load_path</code> <code>str</code> <p>Path to local directory where the nc files will be stored, in format \"yyyy-mm-dd_HH.nc\"</p> required <code>date_start</code> <code>str</code> <p>Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format</p> required <code>date_end</code> <code>str</code> <p>End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format</p> required <code>run_frequency</code> <code>str</code> <p>Frequency format of runs to download, e.g. \"H\"</p> required <code>run_interval</code> <code>str</code> <p>Interval of runs, e.g. a run_frequency of \"H\" and run_interval of \"12\" will extract the data of the 00 and 12 run for each day.</p> required <code>lat</code> <code>DataArray</code> <p>Latitude values to extract from nc files</p> required <code>lon</code> <code>DataArray</code> <p>Longitude values to extract from nc files</p> required <code>utc</code> <code>bool = True</code> <p>Whether to convert the time to UTC or not</p> <code>True</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ecmwf/nc_extractbase_to_weather_data_model.py</code> <pre><code>class ECMWFExtractBaseToWeatherDataModel(TransformerInterface):\n    \"\"\"\n    Base class for extracting forecast data downloaded in .nc format from ECMWF MARS Server.\n\n    Args:\n        load_path (str): Path to local directory where the nc files will be stored, in format \"yyyy-mm-dd_HH.nc\"\n        date_start (str): Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format\n        date_end (str): End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format\n        run_frequency (str):Frequency format of runs to download, e.g. \"H\"\n        run_interval (str): Interval of runs, e.g. a run_frequency of \"H\" and run_interval of \"12\" will extract the data of the 00 and 12 run for each day.\n        lat (DataArray): Latitude values to extract from nc files\n        lon (DataArray): Longitude values to extract from nc files\n        utc (bool = True): Whether to convert the time to UTC or not\n    \"\"\"\n\n    def __init__(\n        self,\n        load_path: str,\n        date_start: str,\n        date_end: str,\n        run_interval: str,\n        run_frequency: str,\n        lat: xr.DataArray,\n        lon: xr.DataArray,\n        utc: bool = True,\n    ):\n        self.load_path = load_path\n        self.lat = lat\n        self.lon = lon\n        self.date_start = date_start\n        self.date_end = date_end\n        self.run_frequency = run_frequency\n        self.run_interval = run_interval\n        self.utc = utc\n        self.dates = pd.date_range(\n            start=self.date_start,\n            end=self.date_end,\n            freq=self.run_interval + self.run_frequency,\n        )\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    @staticmethod\n    def _convert_ws_tag_names(x: list):\n        \"\"\"\n        Converts the tag names of wind speed from the format used in the nc files to the format used in the weather data model.\n\n        Args:\n            x (list): List of variable names of raw tags to be extracted from the nc files\n\n        Returns:\n            new_tags(list): List of variable names of raw tags to be extracted from the nc files, converted to the format used in the weather data model.\n        \"\"\"\n        convert_dict = {\n            \"10u\": \"u10\",\n            \"100u\": \"u100\",\n            \"200u\": \"u200\",\n            \"10v\": \"v10\",\n            \"100v\": \"v100\",\n            \"200v\": \"v200\",\n        }\n        new_tags = [convert_dict[i] if i in convert_dict.keys() else i for i in x]\n        return new_tags\n\n    def transform(\n        self, tag_prefix: str, variables: list, method: str = \"nearest\"\n    ) -&gt; pd.DataFrame:\n        \"\"\"Extract raw data from stored nc filed downloaded via ECMWF MARS.\n\n        Args:\n            tag_prefix (str): Prefix of the tag names of raw tags to be added to the dataframe\n            variables (list): List of variable names of raw tags to be extracted from the nc files\n            method (str, optional): The method used to match latitude/longitude in xarray using .sel(), by default \"nearest\"\n\n        Returns:\n            df (pd.DataFrame): Raw data extracted with lat, lon, run_time, target_time as a pd.multiindex and variables as columns.\n        \"\"\"\n        df = []\n        # e.g. 10u variable is saved as u10 in the file...\n        vars_processed = self._convert_ws_tag_names(variables)\n\n        for i in self.dates:\n            filename = f\"{str(i.date())}_{i.hour:02}.nc\"\n            fullpath = os.path.join(self.load_path, filename)\n            ds = xr.open_dataset(fullpath)\n            tmp = (\n                ds[vars_processed]\n                .sel(latitude=self.lat, longitude=self.lon, method=method)\n                .to_dataframe()\n            )\n            tmp[\"run_time\"] = i\n            df.append(tmp)\n            ds.close()\n\n        df = pd.concat(df, axis=0)\n\n        df = df.rename_axis(\n            index={\n                \"time\": \"target_time\",\n                \"latitude\": \"lat\",\n                \"longitude\": \"lon\",\n            }\n        )\n\n        df = df.reset_index([\"lat\", \"lon\"])\n        df[[\"lat\", \"lon\"]] = df[[\"lat\", \"lon\"]].apply(\n            lambda x: np.round(x.astype(float), 5)\n        )\n\n        if \"level\" in df.index.names:\n            index_names = [\"lat\", \"lon\", \"level\", \"run_time\", \"target_time\"]\n        else:\n            index_names = [\"lat\", \"lon\", \"run_time\", \"target_time\"]\n        df = df.reset_index().set_index(index_names)\n\n        if self.utc:\n            df = df.tz_localize(\"UTC\", level=\"target_time\")\n            df = df.tz_localize(\"UTC\", level=\"run_time\")\n\n        df = df[~(df.index.duplicated(keep=\"first\"))]\n        df = df.sort_index(axis=0)\n        df = df.sort_index(axis=1)\n\n        df_new = df.reset_index()\n\n        df_new = df_new.rename(\n            columns={\n                \"lat\": \"Latitude\",\n                \"lon\": \"Longitude\",\n                \"run_time\": \"EnqueuedTime\",\n                \"target_time\": \"EventTime\",\n            }\n        )\n\n        df_new = (\n            df_new.set_index([\"Latitude\", \"Longitude\", \"EnqueuedTime\", \"EventTime\"])[\n                vars_processed\n            ]\n            .rename_axis(\"Measure\", axis=1)\n            .stack()\n            .reset_index(name=\"Value\")\n        )\n\n        df_new[\"Source\"] = \"ECMWF_MARS\"\n        df_new[\"Status\"] = \"Good\"\n        df_new[\"Latest\"] = True\n        df_new[\"EventDate\"] = pd.to_datetime(df_new[\"EventTime\"]).dt.date\n        df_new[\"TagName\"] = (\n            tag_prefix\n            + df_new[\"Latitude\"].astype(str)\n            + \"_\"\n            + df_new[\"Longitude\"].astype(str)\n            + \"_\"\n            + df_new[\"Source\"]\n            + \"_\"\n            + df_new[\"Measure\"]\n        )\n        df_final = df_new.drop(\"Measure\", axis=1)\n\n        return df_final\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.ECMWFExtractBaseToWeatherDataModel.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ecmwf/nc_extractbase_to_weather_data_model.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.ECMWFExtractBaseToWeatherDataModel.transform","title":"<code>transform(tag_prefix, variables, method='nearest')</code>","text":"<p>Extract raw data from stored nc filed downloaded via ECMWF MARS.</p> <p>Parameters:</p> Name Type Description Default <code>tag_prefix</code> <code>str</code> <p>Prefix of the tag names of raw tags to be added to the dataframe</p> required <code>variables</code> <code>list</code> <p>List of variable names of raw tags to be extracted from the nc files</p> required <code>method</code> <code>str</code> <p>The method used to match latitude/longitude in xarray using .sel(), by default \"nearest\"</p> <code>'nearest'</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Raw data extracted with lat, lon, run_time, target_time as a pd.multiindex and variables as columns.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ecmwf/nc_extractbase_to_weather_data_model.py</code> <pre><code>def transform(\n    self, tag_prefix: str, variables: list, method: str = \"nearest\"\n) -&gt; pd.DataFrame:\n    \"\"\"Extract raw data from stored nc filed downloaded via ECMWF MARS.\n\n    Args:\n        tag_prefix (str): Prefix of the tag names of raw tags to be added to the dataframe\n        variables (list): List of variable names of raw tags to be extracted from the nc files\n        method (str, optional): The method used to match latitude/longitude in xarray using .sel(), by default \"nearest\"\n\n    Returns:\n        df (pd.DataFrame): Raw data extracted with lat, lon, run_time, target_time as a pd.multiindex and variables as columns.\n    \"\"\"\n    df = []\n    # e.g. 10u variable is saved as u10 in the file...\n    vars_processed = self._convert_ws_tag_names(variables)\n\n    for i in self.dates:\n        filename = f\"{str(i.date())}_{i.hour:02}.nc\"\n        fullpath = os.path.join(self.load_path, filename)\n        ds = xr.open_dataset(fullpath)\n        tmp = (\n            ds[vars_processed]\n            .sel(latitude=self.lat, longitude=self.lon, method=method)\n            .to_dataframe()\n        )\n        tmp[\"run_time\"] = i\n        df.append(tmp)\n        ds.close()\n\n    df = pd.concat(df, axis=0)\n\n    df = df.rename_axis(\n        index={\n            \"time\": \"target_time\",\n            \"latitude\": \"lat\",\n            \"longitude\": \"lon\",\n        }\n    )\n\n    df = df.reset_index([\"lat\", \"lon\"])\n    df[[\"lat\", \"lon\"]] = df[[\"lat\", \"lon\"]].apply(\n        lambda x: np.round(x.astype(float), 5)\n    )\n\n    if \"level\" in df.index.names:\n        index_names = [\"lat\", \"lon\", \"level\", \"run_time\", \"target_time\"]\n    else:\n        index_names = [\"lat\", \"lon\", \"run_time\", \"target_time\"]\n    df = df.reset_index().set_index(index_names)\n\n    if self.utc:\n        df = df.tz_localize(\"UTC\", level=\"target_time\")\n        df = df.tz_localize(\"UTC\", level=\"run_time\")\n\n    df = df[~(df.index.duplicated(keep=\"first\"))]\n    df = df.sort_index(axis=0)\n    df = df.sort_index(axis=1)\n\n    df_new = df.reset_index()\n\n    df_new = df_new.rename(\n        columns={\n            \"lat\": \"Latitude\",\n            \"lon\": \"Longitude\",\n            \"run_time\": \"EnqueuedTime\",\n            \"target_time\": \"EventTime\",\n        }\n    )\n\n    df_new = (\n        df_new.set_index([\"Latitude\", \"Longitude\", \"EnqueuedTime\", \"EventTime\"])[\n            vars_processed\n        ]\n        .rename_axis(\"Measure\", axis=1)\n        .stack()\n        .reset_index(name=\"Value\")\n    )\n\n    df_new[\"Source\"] = \"ECMWF_MARS\"\n    df_new[\"Status\"] = \"Good\"\n    df_new[\"Latest\"] = True\n    df_new[\"EventDate\"] = pd.to_datetime(df_new[\"EventTime\"]).dt.date\n    df_new[\"TagName\"] = (\n        tag_prefix\n        + df_new[\"Latitude\"].astype(str)\n        + \"_\"\n        + df_new[\"Longitude\"].astype(str)\n        + \"_\"\n        + df_new[\"Source\"]\n        + \"_\"\n        + df_new[\"Measure\"]\n    )\n    df_final = df_new.drop(\"Measure\", axis=1)\n\n    return df_final\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.ECMWFExtractGridToWeatherDataModel","title":"<code>ECMWFExtractGridToWeatherDataModel</code>","text":"<p>               Bases: <code>ECMWFExtractBaseToWeatherDataModel</code></p> <p>Extract a grid from a local .nc file downloaded from ECMWF via MARS</p> <p>Parameters:</p> Name Type Description Default <code>lat_min</code> <code>float</code> <p>Minimum latitude of grid to extract</p> required <code>lat_max</code> <code>float</code> <p>Maximum latitude of grid to extract</p> required <code>lon_min</code> <code>float</code> <p>Minimum longitude of grid to extract</p> required <code>lon_max</code> <code>float</code> <p>Maximum longitude of grid to extract</p> required <code>grid_step</code> <code>float</code> <p>The grid length to use to define the grid, e.g. 0.1.</p> required <code>load_path</code> <code>str</code> <p>Path to local directory with nc files downloaded in format \"yyyy-mm-dd_HH.nc\"</p> required <code>date_start</code> <code>str</code> <p>Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format</p> required <code>date_end</code> <code>str</code> <p>End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format</p> required <code>run_frequency</code> <code>str</code> <p>Frequency format of runs to download, e.g. \"H\"</p> required <code>run_interval</code> <code>str</code> <p>Interval of runs, e.g. a run_frequency of \"H\" and run_interval of \"12\" will extract the data of the 00 and 12 run for each day.</p> required <code>utc</code> <code>bool</code> <p>Add utc to the datetime indexes? Defaults to True.</p> <code>True</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ecmwf/nc_extractgrid_to_weather_data_model.py</code> <pre><code>class ECMWFExtractGridToWeatherDataModel(ECMWFExtractBaseToWeatherDataModel):\n    \"\"\"Extract a grid from a local .nc file downloaded from ECMWF via MARS\n\n    Args:\n        lat_min (float): Minimum latitude of grid to extract\n        lat_max (float): Maximum latitude of grid to extract\n        lon_min (float): Minimum longitude of grid to extract\n        lon_max (float): Maximum longitude of grid to extract\n        grid_step (float): The grid length to use to define the grid, e.g. 0.1.\n        load_path (str): Path to local directory with nc files downloaded in format \"yyyy-mm-dd_HH.nc\"\n        date_start (str): Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format\n        date_end (str): End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format\n        run_frequency (str): Frequency format of runs to download, e.g. \"H\"\n        run_interval (str): Interval of runs, e.g. a run_frequency of \"H\" and run_interval of \"12\" will extract the data of the 00 and 12 run for each day.\n        utc (bool, optional): Add utc to the datetime indexes? Defaults to True.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        lat_min: float,\n        lat_max: float,\n        lon_min: float,\n        lon_max: float,\n        grid_step: float,\n        load_path: str,\n        date_start: str,\n        date_end: str,\n        run_interval: str,\n        run_frequency: str,\n        utc: bool = True,\n    ):\n        # hmm careful with floating points, this seems to work ok...\n        lat_xr = xr.DataArray(\n            np.linspace(\n                lat_min, lat_max, int(np.round((lat_max - lat_min) / grid_step)) + 1\n            ),\n            dims=[\"latitude\"],\n        )\n        lon_xr = xr.DataArray(\n            np.linspace(\n                lon_min, lon_max, int(np.round((lon_max - lon_min) / grid_step)) + 1\n            ),\n            dims=[\"longitude\"],\n        )\n\n        self.load_path = load_path\n        self.lat_min = lat_min\n        self.lat_max = lat_max\n        self.lon_min = lon_min\n        self.lon_max = lon_max\n        self.grid_step = grid_step\n        self.lat = lat_xr\n        self.lon = lon_xr\n        self.date_start = date_start\n        self.date_end = date_end\n        self.run_frequency = run_frequency\n        self.run_interval = run_interval\n        self.utc = utc\n\n        super(ECMWFExtractGridToWeatherDataModel, self).__init__(\n            lat=lat_xr,\n            lon=lon_xr,\n            load_path=load_path,\n            date_start=date_start,\n            date_end=date_end,\n            run_interval=run_interval,\n            run_frequency=run_frequency,\n            utc=utc,\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.ECMWFExtractPointToWeatherDataModel","title":"<code>ECMWFExtractPointToWeatherDataModel</code>","text":"<p>               Bases: <code>ECMWFExtractBaseToWeatherDataModel</code></p> <p>Extract a single point from a local .nc file downloaded from ECMWF via MARS</p> <p>Parameters:</p> Name Type Description Default <code>lat</code> <code>float</code> <p>Latitude of point to extract</p> required <code>lon</code> <code>float</code> <p>Longitude of point to extract</p> required <code>load_path</code> <code>str</code> <p>Path to local directory with nc files downloaded in format \"yyyy-mm-dd_HH.nc\"</p> required <code>date_start</code> <code>str</code> <p>Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format</p> required <code>date_end</code> <code>str</code> <p>End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format</p> required <code>run_frequency</code> <code>str</code> <p>Frequency format of runs to download, e.g. \"H\"</p> required <code>run_interval</code> <code>str</code> <p>Interval of runs, e.g. a run_frequency of \"H\" and run_interval of \"12\" will extract the data of the 00 and 12 run for each day.</p> required <code>utc</code> <code>bool</code> <p>Add utc to the datetime indexes? Defaults to True.</p> <code>True</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ecmwf/nc_extractpoint_to_weather_data_model.py</code> <pre><code>class ECMWFExtractPointToWeatherDataModel(ECMWFExtractBaseToWeatherDataModel):\n    \"\"\"\n    Extract a single point from a local .nc file downloaded from ECMWF via MARS\n\n    Args:\n        lat (float): Latitude of point to extract\n        lon (float): Longitude of point to extract\n        load_path (str): Path to local directory with nc files downloaded in format \"yyyy-mm-dd_HH.nc\"\n        date_start (str): Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format\n        date_end (str): End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format\n        run_frequency (str): Frequency format of runs to download, e.g. \"H\"\n        run_interval (str): Interval of runs, e.g. a run_frequency of \"H\" and run_interval of \"12\" will extract the data of the 00 and 12 run for each day.\n        utc (bool, optional): Add utc to the datetime indexes? Defaults to True.\n    \"\"\"\n\n    def __init__(\n        self,\n        lat: float,\n        lon: float,\n        load_path: str,\n        date_start: str,\n        date_end: str,\n        run_interval: str,\n        run_frequency: str,\n        utc: bool = True,\n    ):\n        lat_xr = xr.DataArray([lat], dims=[\"latitude\"])\n        lon_xr = xr.DataArray([lon], dims=[\"longitude\"])\n\n        self.lat = lat_xr\n        self.lon = lon_xr\n        self.load_path = load_path\n        self.date_start = date_start\n        self.date_end = date_end\n        self.run_frequency = run_frequency\n        self.run_interval = run_interval\n        self.utc = utc\n\n        super(ECMWFExtractPointToWeatherDataModel, self).__init__(\n            lat=lat_xr,\n            lon=lon_xr,\n            load_path=load_path,\n            date_start=date_start,\n            date_end=date_end,\n            run_interval=run_interval,\n            run_frequency=run_frequency,\n            utc=utc,\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.RawForecastToWeatherDataModel","title":"<code>RawForecastToWeatherDataModel</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a raw forecast into weather data model.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance.</p> required <code>data</code> <code>DataFrame</code> <p>Dataframe to be transformed</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/the_weather_company/raw_forecast_to_weather_data_model.py</code> <pre><code>class RawForecastToWeatherDataModel(TransformerInterface):\n    \"\"\"\n    Converts a raw forecast into weather data model.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance.\n        data (DataFrame): Dataframe to be transformed\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n    ) -&gt; None:\n        self.spark = spark\n        self.data = data\n        self.target_schema = WEATHER_DATA_MODEL\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self) -&gt; bool:\n        assert str(self.data.schema) == str(self.target_schema)\n        return True\n\n    def _convert_into_target_schema(self) -&gt; None:\n        \"\"\"\n        Converts a Spark DataFrame structure into new structure based on the Target Schema.\n\n        Returns: Nothing.\n\n        \"\"\"\n\n        df: DataFrame = self.data\n        df = df.select(self.target_schema.names)\n\n        for field in self.target_schema.fields:\n            df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n\n        self.data = self.spark.createDataFrame(df.rdd, self.target_schema)\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A Forecast dataframe converted into Weather Data Model\n        \"\"\"\n\n        self.pre_transform_validation()\n\n        processed_date = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        df = (\n            self.data.withColumn(\"WeatherDay\", substring(\"FcstValidLocal\", 0, 10))\n            .withColumn(\n                \"WeatherHour\",\n                (substring(\"FcstValidLocal\", 12, 2).cast(IntegerType()) + 1),\n            )\n            .withColumn(\"WeatherTimezoneOffset\", substring(\"FcstValidLocal\", 20, 5))\n            .withColumn(\"WeatherType\", lit(\"F\"))\n            .withColumn(\"ProcessedDate\", lit(processed_date))\n            .withColumnRenamed(\"Temp\", \"Temperature\")\n            .withColumnRenamed(\"Dewpt\", \"DewPoint\")\n            .withColumnRenamed(\"Rh\", \"Humidity\")\n            .withColumnRenamed(\"Hi\", \"HeatIndex\")\n            .withColumnRenamed(\"Wc\", \"WindChill\")\n            .withColumnRenamed(\"Wdir\", \"WindDirection\")\n            .withColumnRenamed(\"Wspd\", \"WindSpeed\")\n            .withColumnRenamed(\"Clds\", \"CloudCover\")\n            .withColumn(\"WetBulbTemp\", lit(\"\"))\n            .withColumn(\"SolarIrradiance\", lit(\"\"))\n            .withColumnRenamed(\"Qpf\", \"Precipitation\")\n            .withColumnRenamed(\"DayInd\", \"DayOrNight\")\n            .withColumnRenamed(\"Dow\", \"DayOfWeek\")\n            .withColumnRenamed(\"Gust\", \"WindGust\")\n            .withColumnRenamed(\"Mslp\", \"MslPressure\")\n            .withColumnRenamed(\"Num\", \"ForecastDayNum\")\n            .withColumnRenamed(\"Pop\", \"PropOfPrecip\")\n            .withColumnRenamed(\"PrecipType\", \"PrecipType\")\n            .withColumnRenamed(\"SnowQpf\", \"SnowAccumulation\")\n            .withColumnRenamed(\"UvIndex\", \"UvIndex\")\n            .withColumnRenamed(\"Vis\", \"Visibility\")\n        )\n\n        columns = df.columns\n        for column in columns:\n            df = df.withColumn(\n                column, when(col(column) == \"\", lit(None)).otherwise(col(column))\n            )\n\n        self.data = df\n        self._convert_into_target_schema()\n        self.post_transform_validation()\n\n        return self.data\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.RawForecastToWeatherDataModel.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/the_weather_company/raw_forecast_to_weather_data_model.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.RawForecastToWeatherDataModel.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A Forecast dataframe converted into Weather Data Model</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/the_weather_company/raw_forecast_to_weather_data_model.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A Forecast dataframe converted into Weather Data Model\n    \"\"\"\n\n    self.pre_transform_validation()\n\n    processed_date = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    df = (\n        self.data.withColumn(\"WeatherDay\", substring(\"FcstValidLocal\", 0, 10))\n        .withColumn(\n            \"WeatherHour\",\n            (substring(\"FcstValidLocal\", 12, 2).cast(IntegerType()) + 1),\n        )\n        .withColumn(\"WeatherTimezoneOffset\", substring(\"FcstValidLocal\", 20, 5))\n        .withColumn(\"WeatherType\", lit(\"F\"))\n        .withColumn(\"ProcessedDate\", lit(processed_date))\n        .withColumnRenamed(\"Temp\", \"Temperature\")\n        .withColumnRenamed(\"Dewpt\", \"DewPoint\")\n        .withColumnRenamed(\"Rh\", \"Humidity\")\n        .withColumnRenamed(\"Hi\", \"HeatIndex\")\n        .withColumnRenamed(\"Wc\", \"WindChill\")\n        .withColumnRenamed(\"Wdir\", \"WindDirection\")\n        .withColumnRenamed(\"Wspd\", \"WindSpeed\")\n        .withColumnRenamed(\"Clds\", \"CloudCover\")\n        .withColumn(\"WetBulbTemp\", lit(\"\"))\n        .withColumn(\"SolarIrradiance\", lit(\"\"))\n        .withColumnRenamed(\"Qpf\", \"Precipitation\")\n        .withColumnRenamed(\"DayInd\", \"DayOrNight\")\n        .withColumnRenamed(\"Dow\", \"DayOfWeek\")\n        .withColumnRenamed(\"Gust\", \"WindGust\")\n        .withColumnRenamed(\"Mslp\", \"MslPressure\")\n        .withColumnRenamed(\"Num\", \"ForecastDayNum\")\n        .withColumnRenamed(\"Pop\", \"PropOfPrecip\")\n        .withColumnRenamed(\"PrecipType\", \"PrecipType\")\n        .withColumnRenamed(\"SnowQpf\", \"SnowAccumulation\")\n        .withColumnRenamed(\"UvIndex\", \"UvIndex\")\n        .withColumnRenamed(\"Vis\", \"Visibility\")\n    )\n\n    columns = df.columns\n    for column in columns:\n        df = df.withColumn(\n            column, when(col(column) == \"\", lit(None)).otherwise(col(column))\n        )\n\n    self.data = df\n    self._convert_into_target_schema()\n    self.post_transform_validation()\n\n    return self.data\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PCDMToHoneywellAPMTransformer","title":"<code>PCDMToHoneywellAPMTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a Spark Dataframe in PCDM format to Honeywell APM format.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PCDMToHoneywellAPMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import PCDMToHoneywellAPMTransformer\n\npcdm_to_honeywell_apm_transformer = PCDMToHoneywellAPMTransformer(\n    data=df,\n    quality=\"Good\",\n    history_samples_per_message=1,\n    compress_payload=True\n)\n\nresult = pcdm_to_honeywell_apm_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dataframe</code> <p>Spark Dataframe in PCDM format</p> required <code>quality</code> <code>str</code> <p>Value for quality inside HistorySamples</p> <code>'Good'</code> <code>history_samples_per_message</code> <code>int</code> <p>The number of HistorySamples for each row in the DataFrame (Batch Only)</p> <code>1</code> <code>compress_payload</code> <code>bool</code> <p>If True compresses CloudPlatformEvent with gzip compression</p> <code>True</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/pcdm_to_honeywell_apm.py</code> <pre><code>class PCDMToHoneywellAPMTransformer(TransformerInterface):\n    \"\"\"\n    Converts a Spark Dataframe in PCDM format to Honeywell APM format.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import PCDMToHoneywellAPMTransformer\n\n    pcdm_to_honeywell_apm_transformer = PCDMToHoneywellAPMTransformer(\n        data=df,\n        quality=\"Good\",\n        history_samples_per_message=1,\n        compress_payload=True\n    )\n\n    result = pcdm_to_honeywell_apm_transformer.transform()\n    ```\n\n    Parameters:\n        data (Dataframe): Spark Dataframe in PCDM format\n        quality (str): Value for quality inside HistorySamples\n        history_samples_per_message (int): The number of HistorySamples for each row in the DataFrame (Batch Only)\n        compress_payload (bool): If True compresses CloudPlatformEvent with gzip compression\n    \"\"\"\n\n    data: DataFrame\n    quality: str\n    history_samples_per_message: int\n    compress_payload: bool\n\n    def __init__(\n        self,\n        data: DataFrame,\n        quality: str = \"Good\",\n        history_samples_per_message: int = 1,\n        compress_payload: bool = True,\n    ) -&gt; None:\n        self.data = data\n        self.quality = quality\n        self.history_samples_per_message = history_samples_per_message\n        self.compress_payload = compress_payload\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with with rows in Honeywell APM format\n        \"\"\"\n\n        @udf(\"string\")\n        def _compress_payload(data):\n            compressed_data = gzip.compress(data.encode(\"utf-8\"))\n            encoded_data = base64.b64encode(compressed_data).decode(\"utf-8\")\n            return encoded_data\n\n        if self.data.isStreaming == False and self.history_samples_per_message &gt; 1:\n            w = Window.partitionBy(\"TagName\").orderBy(\"TagName\")\n            cleaned_pcdm_df = (\n                self.data.withColumn(\n                    \"index\",\n                    floor(\n                        (row_number().over(w) - 0.01) / self.history_samples_per_message\n                    ),\n                )\n                .withColumn(\n                    \"HistorySamples\",\n                    struct(\n                        col(\"TagName\").alias(\"ItemName\"),\n                        lit(self.quality).alias(\"Quality\"),\n                        col(\"EventTime\").alias(\"Time\"),\n                        col(\"Value\").alias(\"Value\"),\n                    ).alias(\"HistorySamples\"),\n                )\n                .groupBy(\"TagName\", \"index\")\n                .agg(collect_list(\"HistorySamples\").alias(\"HistorySamples\"))\n                .withColumn(\"guid\", sha2(col(\"TagName\"), 256).cast(\"string\"))\n                .withColumn(\n                    \"value\",\n                    struct(\n                        col(\"guid\").alias(\"SystemGuid\"), col(\"HistorySamples\")\n                    ).alias(\"value\"),\n                )\n            )\n        else:\n            cleaned_pcdm_df = self.data.withColumn(\n                \"guid\", sha2(col(\"TagName\"), 256).cast(\"string\")\n            ).withColumn(\n                \"value\",\n                struct(\n                    col(\"guid\").alias(\"SystemGuid\"),\n                    array(\n                        struct(\n                            col(\"TagName\").alias(\"ItemName\"),\n                            lit(self.quality).alias(\"Quality\"),\n                            col(\"EventTime\").alias(\"Time\"),\n                            col(\"Value\").alias(\"Value\"),\n                        ),\n                    ).alias(\"HistorySamples\"),\n                ),\n            )\n\n        df = (\n            cleaned_pcdm_df.withColumn(\n                \"CloudPlatformEvent\",\n                struct(\n                    lit(datetime.now(tz=pytz.UTC)).alias(\"CreatedTime\"),\n                    lit(expr(\"uuid()\")).alias(\"Id\"),\n                    col(\"guid\").alias(\"CreatorId\"),\n                    lit(\"CloudPlatformSystem\").alias(\"CreatorType\"),\n                    lit(None).alias(\"GeneratorId\"),\n                    lit(\"CloudPlatformTenant\").alias(\"GeneratorType\"),\n                    col(\"guid\").alias(\"TargetId\"),\n                    lit(\"CloudPlatformTenant\").alias(\"TargetType\"),\n                    lit(None).alias(\"TargetContext\"),\n                    struct(\n                        lit(\"TextualBody\").alias(\"type\"),\n                        to_json(col(\"value\")).alias(\"value\"),\n                        lit(\"application/json\").alias(\"format\"),\n                    ).alias(\"Body\"),\n                    array(\n                        struct(\n                            lit(\"SystemType\").alias(\"Key\"),\n                            lit(\"apm-system\").alias(\"Value\"),\n                        ),\n                        struct(\n                            lit(\"SystemGuid\").alias(\"Key\"), col(\"guid\").alias(\"Value\")\n                        ),\n                    ).alias(\"BodyProperties\"),\n                    lit(\"DataChange.Update\").alias(\"EventType\"),\n                ),\n            )\n            .withColumn(\"AnnotationStreamIds\", lit(\",\"))\n            .withColumn(\"partitionKey\", col(\"guid\"))\n        )\n        if self.compress_payload:\n            return df.select(\n                _compress_payload(to_json(\"CloudPlatformEvent\")).alias(\n                    \"CloudPlatformEvent\"\n                ),\n                \"AnnotationStreamIds\",\n                \"partitionKey\",\n            )\n        else:\n            return df.select(\n                \"CloudPlatformEvent\", \"AnnotationStreamIds\", \"partitionKey\"\n            )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PCDMToHoneywellAPMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/pcdm_to_honeywell_apm.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PCDMToHoneywellAPMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with with rows in Honeywell APM format</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/pcdm_to_honeywell_apm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with with rows in Honeywell APM format\n    \"\"\"\n\n    @udf(\"string\")\n    def _compress_payload(data):\n        compressed_data = gzip.compress(data.encode(\"utf-8\"))\n        encoded_data = base64.b64encode(compressed_data).decode(\"utf-8\")\n        return encoded_data\n\n    if self.data.isStreaming == False and self.history_samples_per_message &gt; 1:\n        w = Window.partitionBy(\"TagName\").orderBy(\"TagName\")\n        cleaned_pcdm_df = (\n            self.data.withColumn(\n                \"index\",\n                floor(\n                    (row_number().over(w) - 0.01) / self.history_samples_per_message\n                ),\n            )\n            .withColumn(\n                \"HistorySamples\",\n                struct(\n                    col(\"TagName\").alias(\"ItemName\"),\n                    lit(self.quality).alias(\"Quality\"),\n                    col(\"EventTime\").alias(\"Time\"),\n                    col(\"Value\").alias(\"Value\"),\n                ).alias(\"HistorySamples\"),\n            )\n            .groupBy(\"TagName\", \"index\")\n            .agg(collect_list(\"HistorySamples\").alias(\"HistorySamples\"))\n            .withColumn(\"guid\", sha2(col(\"TagName\"), 256).cast(\"string\"))\n            .withColumn(\n                \"value\",\n                struct(\n                    col(\"guid\").alias(\"SystemGuid\"), col(\"HistorySamples\")\n                ).alias(\"value\"),\n            )\n        )\n    else:\n        cleaned_pcdm_df = self.data.withColumn(\n            \"guid\", sha2(col(\"TagName\"), 256).cast(\"string\")\n        ).withColumn(\n            \"value\",\n            struct(\n                col(\"guid\").alias(\"SystemGuid\"),\n                array(\n                    struct(\n                        col(\"TagName\").alias(\"ItemName\"),\n                        lit(self.quality).alias(\"Quality\"),\n                        col(\"EventTime\").alias(\"Time\"),\n                        col(\"Value\").alias(\"Value\"),\n                    ),\n                ).alias(\"HistorySamples\"),\n            ),\n        )\n\n    df = (\n        cleaned_pcdm_df.withColumn(\n            \"CloudPlatformEvent\",\n            struct(\n                lit(datetime.now(tz=pytz.UTC)).alias(\"CreatedTime\"),\n                lit(expr(\"uuid()\")).alias(\"Id\"),\n                col(\"guid\").alias(\"CreatorId\"),\n                lit(\"CloudPlatformSystem\").alias(\"CreatorType\"),\n                lit(None).alias(\"GeneratorId\"),\n                lit(\"CloudPlatformTenant\").alias(\"GeneratorType\"),\n                col(\"guid\").alias(\"TargetId\"),\n                lit(\"CloudPlatformTenant\").alias(\"TargetType\"),\n                lit(None).alias(\"TargetContext\"),\n                struct(\n                    lit(\"TextualBody\").alias(\"type\"),\n                    to_json(col(\"value\")).alias(\"value\"),\n                    lit(\"application/json\").alias(\"format\"),\n                ).alias(\"Body\"),\n                array(\n                    struct(\n                        lit(\"SystemType\").alias(\"Key\"),\n                        lit(\"apm-system\").alias(\"Value\"),\n                    ),\n                    struct(\n                        lit(\"SystemGuid\").alias(\"Key\"), col(\"guid\").alias(\"Value\")\n                    ),\n                ).alias(\"BodyProperties\"),\n                lit(\"DataChange.Update\").alias(\"EventType\"),\n            ),\n        )\n        .withColumn(\"AnnotationStreamIds\", lit(\",\"))\n        .withColumn(\"partitionKey\", col(\"guid\"))\n    )\n    if self.compress_payload:\n        return df.select(\n            _compress_payload(to_json(\"CloudPlatformEvent\")).alias(\n                \"CloudPlatformEvent\"\n            ),\n            \"AnnotationStreamIds\",\n            \"partitionKey\",\n        )\n    else:\n        return df.select(\n            \"CloudPlatformEvent\", \"AnnotationStreamIds\", \"partitionKey\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.HoneywellAPMJsonToPCDMTransformer","title":"<code>HoneywellAPMJsonToPCDMTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a Spark Dataframe column containing a json string created by Honeywell APM to the Process Control Data Model.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.HoneywellAPMJsonToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import HoneywellAPMJsonToPCDMTransformer\n\nhoneywell_apm_json_to_pcdm_transformer = HoneywellAPMJsonToPCDMTransformer(\n    data=df,\n    souce_column_name=\"body\",\n    status_null_value=\"Good\",\n    change_type_value=\"insert\"\n)\n\nresult = honeywell_apm_json_to_pcdm_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe containing the column with EdgeX data</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the OPC Publisher Json OPC UA data</p> required <code>status_null_value</code> <code>optional str</code> <p>If populated, will replace 'Good' in the Status column with the specified value.</p> <code>'Good'</code> <code>change_type_value</code> <code>optional str</code> <p>If populated, will replace 'insert' in the ChangeType column with the specified value.</p> <code>'insert'</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/honeywell_apm_to_pcdm.py</code> <pre><code>class HoneywellAPMJsonToPCDMTransformer(TransformerInterface):\n    \"\"\"\n    Converts a Spark Dataframe column containing a json string created by Honeywell APM to the Process Control Data Model.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import HoneywellAPMJsonToPCDMTransformer\n\n    honeywell_apm_json_to_pcdm_transformer = HoneywellAPMJsonToPCDMTransformer(\n        data=df,\n        souce_column_name=\"body\",\n        status_null_value=\"Good\",\n        change_type_value=\"insert\"\n    )\n\n    result = honeywell_apm_json_to_pcdm_transformer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe containing the column with EdgeX data\n        source_column_name (str): Spark Dataframe column containing the OPC Publisher Json OPC UA data\n        status_null_value (optional str): If populated, will replace 'Good' in the Status column with the specified value.\n        change_type_value (optional str): If populated, will replace 'insert' in the ChangeType column with the specified value.\n    \"\"\"\n\n    data: DataFrame\n    source_column_name: str\n    status_null_value: str\n    change_type_value: str\n\n    def __init__(\n        self,\n        data: DataFrame,\n        source_column_name: str,\n        status_null_value: str = \"Good\",\n        change_type_value: str = \"insert\",\n    ) -&gt; None:\n        self.data = data\n        self.source_column_name = source_column_name\n        self.status_null_value = status_null_value\n        self.change_type_value = change_type_value\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the specified column converted to PCDM\n        \"\"\"\n        df = (\n            self.data.withColumn(\"body\", from_json(self.source_column_name, APM_SCHEMA))\n            .select(explode(\"body.SystemTimeSeries.Samples\"))\n            .selectExpr(\"*\", \"to_timestamp(col.Time) as EventTime\")\n            .withColumn(\"TagName\", col(\"col.Itemname\"))\n            .withColumn(\"Status\", lit(self.status_null_value))\n            .withColumn(\"Value\", col(\"col.Value\"))\n            .withColumn(\n                \"ValueType\",\n                when(col(\"value\").cast(\"float\").isNotNull(), \"float\").when(\n                    col(\"value\").cast(\"float\").isNull(), \"string\"\n                ),\n            )\n            .withColumn(\"ChangeType\", lit(self.change_type_value))\n        )\n\n        return df.select(\n            \"TagName\", \"EventTime\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.HoneywellAPMJsonToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/honeywell_apm_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.HoneywellAPMJsonToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the specified column converted to PCDM</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/honeywell_apm_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the specified column converted to PCDM\n    \"\"\"\n    df = (\n        self.data.withColumn(\"body\", from_json(self.source_column_name, APM_SCHEMA))\n        .select(explode(\"body.SystemTimeSeries.Samples\"))\n        .selectExpr(\"*\", \"to_timestamp(col.Time) as EventTime\")\n        .withColumn(\"TagName\", col(\"col.Itemname\"))\n        .withColumn(\"Status\", lit(self.status_null_value))\n        .withColumn(\"Value\", col(\"col.Value\"))\n        .withColumn(\n            \"ValueType\",\n            when(col(\"value\").cast(\"float\").isNotNull(), \"float\").when(\n                col(\"value\").cast(\"float\").isNull(), \"string\"\n            ),\n        )\n        .withColumn(\"ChangeType\", lit(self.change_type_value))\n    )\n\n    return df.select(\n        \"TagName\", \"EventTime\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SEMJsonToPCDMTransformer","title":"<code>SEMJsonToPCDMTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a Spark Dataframe column containing a json string created by SEM to the Process Control Data Model.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SEMJsonToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import SEMJsonToPCDMTransformer\n\nsem_json_to_pcdm_transformer = SEMJsonToPCDMTransformer(\n    data=df\n    source_column_name=\"body\",\n    version=10,\n    status_null_value=\"Good\",\n    change_type_value=\"insert\"\n)\n\nresult = sem_json_to_pcdm_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe containing the column with SEM data</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the Json SEM data</p> required <code>version</code> <code>int</code> <p>The version for the OBC field mappings. The latest version is 10.</p> required <code>status_null_value</code> <code>optional str</code> <p>If populated, will replace 'Good' in the Status column with the specified value.</p> <code>'Good'</code> <code>change_type_value</code> <code>optional str</code> <p>If populated, will replace 'insert' in the ChangeType column with the specified value.</p> <code>'insert'</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/sem_json_to_pcdm.py</code> <pre><code>class SEMJsonToPCDMTransformer(TransformerInterface):\n    \"\"\"\n    Converts a Spark Dataframe column containing a json string created by SEM to the Process Control Data Model.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import SEMJsonToPCDMTransformer\n\n    sem_json_to_pcdm_transformer = SEMJsonToPCDMTransformer(\n        data=df\n        source_column_name=\"body\",\n        version=10,\n        status_null_value=\"Good\",\n        change_type_value=\"insert\"\n    )\n\n    result = sem_json_to_pcdm_transformer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe containing the column with SEM data\n        source_column_name (str): Spark Dataframe column containing the Json SEM data\n        version (int): The version for the OBC field mappings. The latest version is 10.\n        status_null_value (optional str): If populated, will replace 'Good' in the Status column with the specified value.\n        change_type_value (optional str): If populated, will replace 'insert' in the ChangeType column with the specified value.\n    \"\"\"\n\n    data: DataFrame\n    source_column_name: str\n    version: int\n    status_null_value: str\n    change_type_value: str\n\n    def __init__(\n        self,\n        data: DataFrame,\n        source_column_name: str,\n        version: int,\n        status_null_value: str = \"Good\",\n        change_type_value: str = \"insert\",\n    ) -&gt; None:\n        _package_version_meets_minimum(\"pyspark\", \"3.4.0\")\n        self.data = data\n        self.source_column_name = source_column_name\n        self.version = version\n        self.status_null_value = status_null_value\n        self.change_type_value = change_type_value\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the specified column converted to PCDM\n        \"\"\"\n        if self.version == 10:\n            mapping = obc_field_mappings.OBC_FIELD_MAPPINGS_V10\n            df = (\n                self.data.withColumn(\n                    self.source_column_name,\n                    from_json(self.source_column_name, SEM_SCHEMA),\n                )\n                .select(self.source_column_name + \".readings\")\n                .melt(\n                    ids=[\"readings.resourceName\"],\n                    values=[\"readings.value\"],\n                    variableColumnName=\"var\",\n                    valueColumnName=\"value\",\n                )\n                .drop(\"var\")\n                .select(map_from_arrays(\"resourceName\", \"value\").alias(\"resourceName\"))\n                .select(\"resourceName.dID\", \"resourceName.d\", \"resourceName.t\")\n                .select(\n                    regexp_replace(col(\"t\").cast(\"string\"), \"(\\d{10})(\\d+)\", \"$1.$2\")\n                    .cast(\"double\")\n                    .alias(\"timestamp\"),\n                    \"dID\",\n                    posexplode(split(expr(\"substring(d, 2, length(d)-2)\"), \",\")),\n                )\n                .select(\n                    to_timestamp(\"timestamp\").alias(\"EventTime\"),\n                    col(\"dID\"),\n                    col(\"pos\").cast(\"string\"),\n                    col(\"col\").alias(\"Value\"),\n                )\n                .withColumn(\n                    \"TagName\",\n                    concat(\n                        col(\"dID\"),\n                        lit(\":\"),\n                        udf(lambda row: mapping[row][\"TagName\"])(col(\"pos\")),\n                    ),\n                )\n                .withColumn(\n                    \"ValueType\", udf(lambda row: mapping[row][\"ValueType\"])(col(\"pos\"))\n                )\n                .withColumn(\"Status\", lit(self.status_null_value))\n                .withColumn(\"ChangeType\", lit(self.change_type_value))\n            )\n            return df.select(\n                \"EventTime\", \"TagName\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n            )\n        else:\n            return logging.exception(\n                \"The wrong version was specified. Please use the latest version\"\n            )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SEMJsonToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/sem_json_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SEMJsonToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the specified column converted to PCDM</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/sem_json_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the specified column converted to PCDM\n    \"\"\"\n    if self.version == 10:\n        mapping = obc_field_mappings.OBC_FIELD_MAPPINGS_V10\n        df = (\n            self.data.withColumn(\n                self.source_column_name,\n                from_json(self.source_column_name, SEM_SCHEMA),\n            )\n            .select(self.source_column_name + \".readings\")\n            .melt(\n                ids=[\"readings.resourceName\"],\n                values=[\"readings.value\"],\n                variableColumnName=\"var\",\n                valueColumnName=\"value\",\n            )\n            .drop(\"var\")\n            .select(map_from_arrays(\"resourceName\", \"value\").alias(\"resourceName\"))\n            .select(\"resourceName.dID\", \"resourceName.d\", \"resourceName.t\")\n            .select(\n                regexp_replace(col(\"t\").cast(\"string\"), \"(\\d{10})(\\d+)\", \"$1.$2\")\n                .cast(\"double\")\n                .alias(\"timestamp\"),\n                \"dID\",\n                posexplode(split(expr(\"substring(d, 2, length(d)-2)\"), \",\")),\n            )\n            .select(\n                to_timestamp(\"timestamp\").alias(\"EventTime\"),\n                col(\"dID\"),\n                col(\"pos\").cast(\"string\"),\n                col(\"col\").alias(\"Value\"),\n            )\n            .withColumn(\n                \"TagName\",\n                concat(\n                    col(\"dID\"),\n                    lit(\":\"),\n                    udf(lambda row: mapping[row][\"TagName\"])(col(\"pos\")),\n                ),\n            )\n            .withColumn(\n                \"ValueType\", udf(lambda row: mapping[row][\"ValueType\"])(col(\"pos\"))\n            )\n            .withColumn(\"Status\", lit(self.status_null_value))\n            .withColumn(\"ChangeType\", lit(self.change_type_value))\n        )\n        return df.select(\n            \"EventTime\", \"TagName\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n        )\n    else:\n        return logging.exception(\n            \"The wrong version was specified. Please use the latest version\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.MiricoJsonToPCDMTransformer","title":"<code>MiricoJsonToPCDMTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a Spark Dataframe column containing a json string created from Mirico to the Process Control Data Model.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.MiricoJsonToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import MiricoJsonToPCDMTransformer\n\nmirico_json_to_pcdm_transformer = MiricoJsonToPCDMTransformer(\n    data=df\n    source_column_name=\"body\",\n    status_null_value=\"Good\",\n    change_type_value=\"insert\"\n    tagname_field=\"test\"\n)\n\nresult = mirico_json_to_pcdm_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe containing the column with Mirico data</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the Json Mirico data</p> required <code>status_null_value</code> <code>optional str</code> <p>If populated, will replace 'Good' in the Status column with the specified value.</p> <code>'Good'</code> <code>change_type_value</code> <code>optional str</code> <p>If populated, will replace 'insert' in the ChangeType column with the specified value.</p> <code>'insert'</code> <code>tagname_field</code> <code>optional str</code> <p>If populated, will add the specified field to the TagName column.</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/mirico_json_to_pcdm.py</code> <pre><code>class MiricoJsonToPCDMTransformer(TransformerInterface):\n    \"\"\"\n    Converts a Spark Dataframe column containing a json string created from Mirico to the Process Control Data Model.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import MiricoJsonToPCDMTransformer\n\n    mirico_json_to_pcdm_transformer = MiricoJsonToPCDMTransformer(\n        data=df\n        source_column_name=\"body\",\n        status_null_value=\"Good\",\n        change_type_value=\"insert\"\n        tagname_field=\"test\"\n    )\n\n    result = mirico_json_to_pcdm_transformer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe containing the column with Mirico data\n        source_column_name (str): Spark Dataframe column containing the Json Mirico data\n        status_null_value (optional str): If populated, will replace 'Good' in the Status column with the specified value.\n        change_type_value (optional str): If populated, will replace 'insert' in the ChangeType column with the specified value.\n        tagname_field (optional str): If populated, will add the specified field to the TagName column.\n    \"\"\"\n\n    data: DataFrame\n    source_column_name: str\n    status_null_value: str\n    change_type_value: str\n\n    def __init__(\n        self,\n        data: DataFrame,\n        source_column_name: str,\n        status_null_value: str = \"Good\",\n        change_type_value: str = \"insert\",\n        tagname_field: str = None,\n    ) -&gt; None:\n        _package_version_meets_minimum(\"pyspark\", \"3.4.0\")\n        self.data = data\n        self.source_column_name = source_column_name\n        self.status_null_value = status_null_value\n        self.change_type_value = change_type_value\n        self.tagname_field = tagname_field\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the specified column converted to PCDM\n        \"\"\"\n\n        mapping = mirico_field_mappings.MIRICO_FIELD_MAPPINGS\n        df = (\n            self.data.withColumn(\n                self.source_column_name,\n                from_json(self.source_column_name, \"map&lt;string,string&gt;\"),\n            )\n            .withColumn(\"TagName\", map_keys(\"body\"))\n            .withColumn(\"Value\", map_values(\"body\"))\n            .select(\n                map_from_arrays(\"TagName\", \"Value\").alias(\"x\"),\n                to_timestamp(col(\"x.timeStamp\")).alias(\"EventTime\"),\n                col(\"x.siteName\").alias(\"siteName\"),\n                col(\"x.gasType\").alias(\"gasType\"),\n                col(\"x.retroName\").alias(\"retroName\"),\n            )\n            .select(\"EventTime\", \"siteName\", \"gasType\", \"retroName\", posexplode(\"x\"))\n            .withColumn(\n                \"ValueType\", udf(lambda row: mapping[row][\"ValueType\"])(col(\"pos\"))\n            )\n            .withColumn(\"Status\", lit(\"Good\"))\n            .withColumn(\"ChangeType\", lit(\"insert\"))\n            .withColumn(\n                \"TagName\",\n                when(\n                    lit(self.tagname_field).isNotNull(),\n                    concat_ws(\n                        \":\",\n                        *[\n                            upper(lit(self.tagname_field)),\n                            concat_ws(\n                                \"_\",\n                                *[\n                                    upper(col(\"siteName\")),\n                                    upper(col(\"retroName\")),\n                                    when(\n                                        upper(col(\"key\")) == \"GASPPM\",\n                                        concat_ws(\n                                            \"_\",\n                                            *[upper(col(\"key\")), upper(col(\"gasType\"))]\n                                        ),\n                                    ).otherwise(upper(col(\"key\"))),\n                                ]\n                            ),\n                        ]\n                    ),\n                ).otherwise(\n                    concat_ws(\n                        \"_\",\n                        *[\n                            upper(col(\"siteName\")),\n                            upper(col(\"retroName\")),\n                            when(\n                                upper(col(\"key\")) == \"GASPPM\",\n                                concat_ws(\n                                    \"_\", *[upper(col(\"key\")), upper(col(\"gasType\"))]\n                                ),\n                            ).otherwise(upper(col(\"key\"))),\n                        ]\n                    )\n                ),\n            )\n            .filter(\n                ~col(\"key\").isin(\n                    \"timeStamp\",\n                    \"gasType\",\n                    \"retroLongitude\",\n                    \"retroLatitude\",\n                    \"retroAltitude\",\n                    \"sensorLongitude\",\n                    \"sensorLatitude\",\n                    \"sensorAltitude\",\n                    \"siteName\",\n                    \"siteKey\",\n                    \"retroName\",\n                )\n            )\n        )\n        return df.select(\n            \"EventTime\", \"TagName\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.MiricoJsonToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/mirico_json_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.MiricoJsonToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the specified column converted to PCDM</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/mirico_json_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the specified column converted to PCDM\n    \"\"\"\n\n    mapping = mirico_field_mappings.MIRICO_FIELD_MAPPINGS\n    df = (\n        self.data.withColumn(\n            self.source_column_name,\n            from_json(self.source_column_name, \"map&lt;string,string&gt;\"),\n        )\n        .withColumn(\"TagName\", map_keys(\"body\"))\n        .withColumn(\"Value\", map_values(\"body\"))\n        .select(\n            map_from_arrays(\"TagName\", \"Value\").alias(\"x\"),\n            to_timestamp(col(\"x.timeStamp\")).alias(\"EventTime\"),\n            col(\"x.siteName\").alias(\"siteName\"),\n            col(\"x.gasType\").alias(\"gasType\"),\n            col(\"x.retroName\").alias(\"retroName\"),\n        )\n        .select(\"EventTime\", \"siteName\", \"gasType\", \"retroName\", posexplode(\"x\"))\n        .withColumn(\n            \"ValueType\", udf(lambda row: mapping[row][\"ValueType\"])(col(\"pos\"))\n        )\n        .withColumn(\"Status\", lit(\"Good\"))\n        .withColumn(\"ChangeType\", lit(\"insert\"))\n        .withColumn(\n            \"TagName\",\n            when(\n                lit(self.tagname_field).isNotNull(),\n                concat_ws(\n                    \":\",\n                    *[\n                        upper(lit(self.tagname_field)),\n                        concat_ws(\n                            \"_\",\n                            *[\n                                upper(col(\"siteName\")),\n                                upper(col(\"retroName\")),\n                                when(\n                                    upper(col(\"key\")) == \"GASPPM\",\n                                    concat_ws(\n                                        \"_\",\n                                        *[upper(col(\"key\")), upper(col(\"gasType\"))]\n                                    ),\n                                ).otherwise(upper(col(\"key\"))),\n                            ]\n                        ),\n                    ]\n                ),\n            ).otherwise(\n                concat_ws(\n                    \"_\",\n                    *[\n                        upper(col(\"siteName\")),\n                        upper(col(\"retroName\")),\n                        when(\n                            upper(col(\"key\")) == \"GASPPM\",\n                            concat_ws(\n                                \"_\", *[upper(col(\"key\")), upper(col(\"gasType\"))]\n                            ),\n                        ).otherwise(upper(col(\"key\"))),\n                    ]\n                )\n            ),\n        )\n        .filter(\n            ~col(\"key\").isin(\n                \"timeStamp\",\n                \"gasType\",\n                \"retroLongitude\",\n                \"retroLatitude\",\n                \"retroAltitude\",\n                \"sensorLongitude\",\n                \"sensorLatitude\",\n                \"sensorAltitude\",\n                \"siteName\",\n                \"siteKey\",\n                \"retroName\",\n            )\n        )\n    )\n    return df.select(\n        \"EventTime\", \"TagName\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.MiricoJsonToMetadataTransformer","title":"<code>MiricoJsonToMetadataTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a Spark Dataframe column containing a json string created from Mirico to the Metadata Model.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.MiricoJsonToMetadataTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import MiricoJsonToMetadataTransformer\n\nmirico_json_to_metadata_transformer = MiricoJsonToMetadataTransformer(\n    data=df\n    source_column_name=\"body\"\n)\n\nresult = mirico_json_to_metadata_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe containing the column with Mirico data</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the Json Mirico data</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/mirico_json_to_metadata.py</code> <pre><code>class MiricoJsonToMetadataTransformer(TransformerInterface):\n    \"\"\"\n    Converts a Spark Dataframe column containing a json string created from Mirico to the Metadata Model.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import MiricoJsonToMetadataTransformer\n\n    mirico_json_to_metadata_transformer = MiricoJsonToMetadataTransformer(\n        data=df\n        source_column_name=\"body\"\n    )\n\n    result = mirico_json_to_metadata_transformer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe containing the column with Mirico data\n        source_column_name (str): Spark Dataframe column containing the Json Mirico data\n    \"\"\"\n\n    data: DataFrame\n    source_column_name: str\n\n    def __init__(self, data: DataFrame, source_column_name: str) -&gt; None:\n        _package_version_meets_minimum(\"pyspark\", \"3.4.0\")\n        self.data = data\n        self.source_column_name = source_column_name\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the specified column converted to Metadata model\n        \"\"\"\n\n        df = self.data.select(\n            from_json(self.source_column_name, MIRICO_METADATA_SCHEMA).alias(\"body\"),\n        )\n\n        tag_name_expr = concat_ws(\n            \"_\",\n            *[\n                upper(col(\"body.siteName\")),\n                upper(col(\"body.retroName\")),\n                upper(col(\"body.gasType\")),\n            ]\n        )\n\n        df = df.select(\n            tag_name_expr.alias(\"TagName\"),\n            lit(\"\").alias(\"Description\"),\n            lit(\"\").alias(\"UoM\"),\n            expr(\n                \"\"\"struct(\n                body.retroAltitude,\n                body.retroLongitude,\n                body.retroLatitude,\n                body.sensorAltitude,\n                body.sensorLongitude,\n                body.sensorLatitude)\"\"\"\n            ).alias(\"Properties\"),\n        ).dropDuplicates([\"TagName\"])\n\n        return df.select(\"TagName\", \"Description\", \"UoM\", \"Properties\")\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.MiricoJsonToMetadataTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/mirico_json_to_metadata.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.MiricoJsonToMetadataTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the specified column converted to Metadata model</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/mirico_json_to_metadata.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the specified column converted to Metadata model\n    \"\"\"\n\n    df = self.data.select(\n        from_json(self.source_column_name, MIRICO_METADATA_SCHEMA).alias(\"body\"),\n    )\n\n    tag_name_expr = concat_ws(\n        \"_\",\n        *[\n            upper(col(\"body.siteName\")),\n            upper(col(\"body.retroName\")),\n            upper(col(\"body.gasType\")),\n        ]\n    )\n\n    df = df.select(\n        tag_name_expr.alias(\"TagName\"),\n        lit(\"\").alias(\"Description\"),\n        lit(\"\").alias(\"UoM\"),\n        expr(\n            \"\"\"struct(\n            body.retroAltitude,\n            body.retroLongitude,\n            body.retroLatitude,\n            body.sensorAltitude,\n            body.sensorLongitude,\n            body.sensorLatitude)\"\"\"\n        ).alias(\"Properties\"),\n    ).dropDuplicates([\"TagName\"])\n\n    return df.select(\"TagName\", \"Description\", \"UoM\", \"Properties\")\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PandasToPySparkTransformer","title":"<code>PandasToPySparkTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a Pandas DataFrame to a PySpark DataFrame.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PandasToPySparkTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import PandasToPySparkTransformer\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\npandas_to_pyspark = PandasToPySparkTransformer(\n    spark=spark,\n    df=df,\n)\n\nresult = pandas_to_pyspark.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to convert DataFrame</p> required <code>df</code> <code>DataFrame</code> <p>Pandas DataFrame to be converted</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/pandas_to_pyspark.py</code> <pre><code>class PandasToPySparkTransformer(TransformerInterface):\n    \"\"\"\n    Converts a Pandas DataFrame to a PySpark DataFrame.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import PandasToPySparkTransformer\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    pandas_to_pyspark = PandasToPySparkTransformer(\n        spark=spark,\n        df=df,\n    )\n\n    result = pandas_to_pyspark.transform()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to convert DataFrame\n        df (DataFrame): Pandas DataFrame to be converted\n    \"\"\"\n\n    spark: SparkSession\n    df: PandasDataFrame\n\n    def __init__(self, spark: SparkSession, df: PandasDataFrame) -&gt; None:\n        self.spark = spark\n        self.df = df\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; PySparkDataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A PySpark dataframe converted from a Pandas DataFrame.\n        \"\"\"\n\n        self.df = _prepare_pandas_to_convert_to_spark(self.df)\n        df = self.spark.createDataFrame(self.df)\n\n        return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PandasToPySparkTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/pandas_to_pyspark.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PandasToPySparkTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A PySpark dataframe converted from a Pandas DataFrame.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/pandas_to_pyspark.py</code> <pre><code>def transform(self) -&gt; PySparkDataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A PySpark dataframe converted from a Pandas DataFrame.\n    \"\"\"\n\n    self.df = _prepare_pandas_to_convert_to_spark(self.df)\n    df = self.spark.createDataFrame(self.df)\n\n    return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PySparkToPandasTransformer","title":"<code>PySparkToPandasTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a PySpark DataFrame to a Pandas DataFrame.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PySparkToPandasTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import PySparkToPandasTransformer\n\npyspark_to_pandas = PySparkToPandasTransformer(\n    df=df\n)\n\nresult = pyspark_to_pandas.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>PySpark DataFrame to be converted</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/pyspark_to_pandas.py</code> <pre><code>class PySparkToPandasTransformer(TransformerInterface):\n    \"\"\"\n    Converts a PySpark DataFrame to a Pandas DataFrame.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import PySparkToPandasTransformer\n\n    pyspark_to_pandas = PySparkToPandasTransformer(\n        df=df\n    )\n\n    result = pyspark_to_pandas.transform()\n    ```\n\n    Parameters:\n        df (DataFrame): PySpark DataFrame to be converted\n    \"\"\"\n\n    df: PySparkDataFrame\n\n    def __init__(self, df: PySparkDataFrame) -&gt; None:\n        self.df = df\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; PandasDataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A Pandas dataframe converted from a PySpark DataFrame.\n        \"\"\"\n        df = self.df.toPandas()\n        return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PySparkToPandasTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/pyspark_to_pandas.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PySparkToPandasTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A Pandas dataframe converted from a PySpark DataFrame.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/pyspark_to_pandas.py</code> <pre><code>def transform(self) -&gt; PandasDataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A Pandas dataframe converted from a PySpark DataFrame.\n    \"\"\"\n    df = self.df.toPandas()\n    return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkDeltaDestination","title":"<code>SparkDeltaDestination</code>","text":"<p>               Bases: <code>DestinationInterface</code></p> <p>The Spark Delta Destination is used to write data to a Delta table.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkDeltaDestination--examples","title":"Examples","text":"<p><pre><code>#Delta Destination for Streaming Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkDeltaDestination\n\ndelta_destination = SparkDeltaDestination(\n    data=df,\n    options={\n        \"checkpointLocation\": \"/{CHECKPOINT-LOCATION}/\"\n    },\n    destination=\"DELTA-TABLE-PATH\",\n    mode=\"append\",\n    trigger=\"10 seconds\",\n    query_name=\"DeltaDestination\",\n    query_wait_interval=None\n)\n\ndelta_destination.write_stream()\n</code></pre> <pre><code>#Delta Destination for Batch Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkDeltaDestination\n\ndelta_destination = SparkDeltaDestination(\n    data=df,\n    options={\n        \"overwriteSchema\": True\n    },\n    destination=\"DELTA-TABLE-PATH\",\n    mode=\"append\",\n    trigger=\"10 seconds\",\n    query_name=\"DeltaDestination\",\n    query_wait_interval=None\n)\n\ndelta_destination.write_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to be written to Delta</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for a Delta Table write operation (See Attributes table below). Further information on the options is available for batch and streaming.</p> required <code>destination</code> <code>str</code> <p>Either the name of the Hive Metastore or Unity Catalog Delta Table or the path to the Delta table</p> required <code>mode</code> <code>optional str</code> <p>Method of writing to Delta Table - append/overwrite (batch), append/update/complete (stream). Default is append</p> <code>'append'</code> <code>trigger</code> <code>optional str</code> <p>Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds</p> <code>'10 seconds'</code> <code>query_name</code> <code>optional str</code> <p>Unique name for the query in associated SparkSession. (stream) Default is DeltaDestination</p> <code>'DeltaDestination'</code> <code>query_wait_interval</code> <code>optional int</code> <p>If set, waits for the streaming query to complete before returning. (stream) Default is None</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>checkpointLocation</code> <code>str</code> <p>Path to checkpoint files. (Streaming)</p> <code>txnAppId</code> <code>str</code> <p>A unique string that you can pass on each DataFrame write. (Batch &amp; Streaming)</p> <code>txnVersion</code> <code>str</code> <p>A monotonically increasing number that acts as transaction version. (Batch &amp; Streaming)</p> <code>maxRecordsPerFile</code> <code>int str</code> <p>Specify the maximum number of records to write to a single file for a Delta Lake table. (Batch)</p> <code>replaceWhere</code> <code>str</code> <p>Condition(s) for overwriting. (Batch)</p> <code>partitionOverwriteMode</code> <code>str</code> <p>When set to dynamic, overwrites all existing data in each logical partition for which the write will commit new data. Default is static. (Batch)</p> <code>overwriteSchema</code> <code>bool str</code> <p>If True, overwrites the schema as well as the table data. (Batch)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta.py</code> <pre><code>class SparkDeltaDestination(DestinationInterface):\n    \"\"\"\n    The Spark Delta Destination is used to write data to a Delta table.\n\n    Examples\n    --------\n    ```python\n    #Delta Destination for Streaming Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkDeltaDestination\n\n    delta_destination = SparkDeltaDestination(\n        data=df,\n        options={\n            \"checkpointLocation\": \"/{CHECKPOINT-LOCATION}/\"\n        },\n        destination=\"DELTA-TABLE-PATH\",\n        mode=\"append\",\n        trigger=\"10 seconds\",\n        query_name=\"DeltaDestination\",\n        query_wait_interval=None\n    )\n\n    delta_destination.write_stream()\n    ```\n    ```python\n    #Delta Destination for Batch Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkDeltaDestination\n\n    delta_destination = SparkDeltaDestination(\n        data=df,\n        options={\n            \"overwriteSchema\": True\n        },\n        destination=\"DELTA-TABLE-PATH\",\n        mode=\"append\",\n        trigger=\"10 seconds\",\n        query_name=\"DeltaDestination\",\n        query_wait_interval=None\n    )\n\n    delta_destination.write_batch()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe to be written to Delta\n        options (dict): Options that can be specified for a Delta Table write operation (See Attributes table below). Further information on the options is available for [batch](https://docs.delta.io/latest/delta-batch.html#write-to-a-table){ target=\"_blank\" } and [streaming](https://docs.delta.io/latest/delta-streaming.html#delta-table-as-a-sink){ target=\"_blank\" }.\n        destination (str): Either the name of the Hive Metastore or Unity Catalog Delta Table **or** the path to the Delta table\n        mode (optional str): Method of writing to Delta Table - append/overwrite (batch), append/update/complete (stream). Default is append\n        trigger (optional str): Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds\n        query_name (optional str): Unique name for the query in associated SparkSession. (stream) Default is DeltaDestination\n        query_wait_interval (optional int): If set, waits for the streaming query to complete before returning. (stream) Default is None\n\n    Attributes:\n        checkpointLocation (str): Path to checkpoint files. (Streaming)\n        txnAppId (str): A unique string that you can pass on each DataFrame write. (Batch &amp; Streaming)\n        txnVersion (str): A monotonically increasing number that acts as transaction version. (Batch &amp; Streaming)\n        maxRecordsPerFile (int str): Specify the maximum number of records to write to a single file for a Delta Lake table. (Batch)\n        replaceWhere (str): Condition(s) for overwriting. (Batch)\n        partitionOverwriteMode (str): When set to dynamic, overwrites all existing data in each logical partition for which the write will commit new data. Default is static. (Batch)\n        overwriteSchema (bool str): If True, overwrites the schema as well as the table data. (Batch)\n    \"\"\"\n\n    data: DataFrame\n    options: dict\n    destination: str\n    mode: str\n    trigger: str\n    query_name: str\n    query_wait_interval: int\n\n    def __init__(\n        self,\n        data: DataFrame,\n        options: dict,\n        destination: str,\n        mode: str = \"append\",\n        trigger: str = \"10 seconds\",\n        query_name: str = \"DeltaDestination\",\n        query_wait_interval: int = None,\n    ) -&gt; None:\n        self.data = data\n        self.options = options\n        self.destination = destination\n        self.mode = mode\n        self.trigger = trigger\n        self.query_name = query_name\n        self.query_wait_interval = query_wait_interval\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_core\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {\n            \"spark.sql.extensions\": \"io.delta.sql.DeltaSparkSessionExtension\",\n            \"spark.sql.catalog.spark_catalog\": \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n        }\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def write_batch(self):\n        \"\"\"\n        Writes batch data to Delta. Most of the options provided by the Apache Spark DataFrame write API are supported for performing batch writes on tables.\n        \"\"\"\n        try:\n            if \"/\" in self.destination:\n                return (\n                    self.data.write.format(\"delta\")\n                    .mode(self.mode)\n                    .options(**self.options)\n                    .save(self.destination)\n                )\n            else:\n                return (\n                    self.data.write.format(\"delta\")\n                    .mode(self.mode)\n                    .options(**self.options)\n                    .saveAsTable(self.destination)\n                )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self):\n        \"\"\"\n        Writes streaming data to Delta. Exactly-once processing is guaranteed\n        \"\"\"\n        TRIGGER_OPTION = (\n            {\"availableNow\": True}\n            if self.trigger == \"availableNow\"\n            else {\"processingTime\": self.trigger}\n        )\n        try:\n            if \"/\" in self.destination:\n                query = (\n                    self.data.writeStream.trigger(**TRIGGER_OPTION)\n                    .format(\"delta\")\n                    .queryName(self.query_name)\n                    .outputMode(self.mode)\n                    .options(**self.options)\n                    .start(self.destination)\n                )\n            else:\n                query = (\n                    self.data.writeStream.trigger(**TRIGGER_OPTION)\n                    .format(\"delta\")\n                    .queryName(self.query_name)\n                    .outputMode(self.mode)\n                    .options(**self.options)\n                    .toTable(self.destination)\n                )\n\n            if self.query_wait_interval:\n                while query.isActive:\n                    if query.lastProgress:\n                        logging.info(query.lastProgress)\n                    time.sleep(self.query_wait_interval)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkDeltaDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkDeltaDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Writes batch data to Delta. Most of the options provided by the Apache Spark DataFrame write API are supported for performing batch writes on tables.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta.py</code> <pre><code>def write_batch(self):\n    \"\"\"\n    Writes batch data to Delta. Most of the options provided by the Apache Spark DataFrame write API are supported for performing batch writes on tables.\n    \"\"\"\n    try:\n        if \"/\" in self.destination:\n            return (\n                self.data.write.format(\"delta\")\n                .mode(self.mode)\n                .options(**self.options)\n                .save(self.destination)\n            )\n        else:\n            return (\n                self.data.write.format(\"delta\")\n                .mode(self.mode)\n                .options(**self.options)\n                .saveAsTable(self.destination)\n            )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkDeltaDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Writes streaming data to Delta. Exactly-once processing is guaranteed</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta.py</code> <pre><code>def write_stream(self):\n    \"\"\"\n    Writes streaming data to Delta. Exactly-once processing is guaranteed\n    \"\"\"\n    TRIGGER_OPTION = (\n        {\"availableNow\": True}\n        if self.trigger == \"availableNow\"\n        else {\"processingTime\": self.trigger}\n    )\n    try:\n        if \"/\" in self.destination:\n            query = (\n                self.data.writeStream.trigger(**TRIGGER_OPTION)\n                .format(\"delta\")\n                .queryName(self.query_name)\n                .outputMode(self.mode)\n                .options(**self.options)\n                .start(self.destination)\n            )\n        else:\n            query = (\n                self.data.writeStream.trigger(**TRIGGER_OPTION)\n                .format(\"delta\")\n                .queryName(self.query_name)\n                .outputMode(self.mode)\n                .options(**self.options)\n                .toTable(self.destination)\n            )\n\n        if self.query_wait_interval:\n            while query.isActive:\n                if query.lastProgress:\n                    logging.info(query.lastProgress)\n                time.sleep(self.query_wait_interval)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkDeltaMergeDestination","title":"<code>SparkDeltaMergeDestination</code>","text":"<p>               Bases: <code>DestinationInterface</code></p> <p>The Spark Delta Merge Destination is used to merge data into a Delta table. Refer to this documentation for more information about Delta Merge.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkDeltaMergeDestination--examples","title":"Examples","text":"<p><pre><code>#Delta Merge Destination for Streaming Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkDeltaMergeDestination\n\ndelta_merge_destination = SparkDeltaMergeDestination(\n    data=df,\n    destination=\"DELTA-TABLE-PATH\",\n    options={\n        \"checkpointLocation\": \"/{CHECKPOINT-LOCATION}/\"\n    },\n    merge_condition=\"`source.id = target.id`\"\n    when_matched_update_list=None\n    when_matched_delete_list=None\n    when_not_matched_insert_list=None\n    when_not_matched_by_source_update_list=None\n    when_not_matched_by_source_delete_list=None\n    try_broadcast_join=False\n    trigger=\"10 seconds\",\n    query_name=\"DeltaDestination\"\n    query_wait_interval=None\n)\n\ndelta_merge_destination.write_stream()\n</code></pre> <pre><code>#Delta Merge Destination for Batch Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkDeltaMergeDestination\n\ndelta_merge_destination = SparkDeltaMergeDestination(\n    data=df,\n    destination=\"DELTA-TABLE-PATH\",\n    options={},\n    merge_condition=\"`source.id = target.id`\",\n    when_matched_update_list=None,\n    when_matched_delete_list=None,\n    when_not_matched_insert_list=None,\n    when_not_matched_by_source_update_list=None,\n    when_not_matched_by_source_delete_list=None,\n    try_broadcast_join=False,\n    trigger=\"10 seconds\",\n    query_name=\"DeltaDestination\"\n    query_wait_interval=None\n)\n\ndelta_merge_destination.write_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to be merged into a Delta Table</p> required <code>destination</code> <code>str</code> <p>Either the name of the Hive Metastore or Unity Catalog Delta Table or the path to the Delta table</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for batch and streaming.</p> required <code>merge_condition</code> <code>str</code> <p>Condition for matching records between dataframe and delta table. Reference Dataframe columns as <code>source</code> and Delta Table columns as <code>target</code>. For example <code>source.id = target.id</code>.</p> required <code>when_matched_update_list</code> <code>optional list[DeltaMergeConditionValues]</code> <p>Conditions(optional) and values to be used when updating rows that match the <code>merge_condition</code>. Specify <code>*</code> for Values if all columns from Dataframe should be inserted.</p> <code>None</code> <code>when_matched_delete_list</code> <code>optional list[DeltaMergeCondition]</code> <p>Conditions(optional) to be used when deleting rows that match the <code>merge_condition</code>.</p> <code>None</code> <code>when_not_matched_insert_list</code> <code>optional list[DeltaMergeConditionValues]</code> <p>Conditions(optional) and values to be used when inserting rows that do not match the <code>merge_condition</code>. Specify <code>*</code> for Values if all columns from Dataframe should be inserted.</p> <code>None</code> <code>when_not_matched_by_source_update_list</code> <code>optional list[DeltaMergeConditionValues]</code> <p>Conditions(optional) and values to be used when updating rows that do not match the <code>merge_condition</code>.</p> <code>None</code> <code>when_not_matched_by_source_delete_list</code> <code>optional list[DeltaMergeCondition]</code> <p>Conditions(optional) to be used when deleting rows that do not match the <code>merge_condition</code>.</p> <code>None</code> <code>try_broadcast_join</code> <code>optional bool</code> <p>Attempts to perform a broadcast join in the merge which can leverage data skipping using partition pruning and file pruning automatically. Can fail if dataframe being merged is large and therefore more suitable for streaming merges than batch merges</p> <code>False</code> <code>trigger</code> <code>optional str</code> <p>Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds</p> <code>'10 seconds'</code> <code>query_name</code> <code>optional str</code> <p>Unique name for the query in associated SparkSession</p> <code>'DeltaMergeDestination'</code> <code>query_wait_interval</code> <code>optional int</code> <p>If set, waits for the streaming query to complete before returning. (stream) Default is None</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>checkpointLocation</code> <code>str</code> <p>Path to checkpoint files. (Streaming)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta_merge.py</code> <pre><code>class SparkDeltaMergeDestination(DestinationInterface):\n    \"\"\"\n    The Spark Delta Merge Destination is used to merge data into a Delta table. Refer to this [documentation](https://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge&amp;language-python) for more information about Delta Merge.\n\n    Examples\n    --------\n    ```python\n    #Delta Merge Destination for Streaming Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkDeltaMergeDestination\n\n    delta_merge_destination = SparkDeltaMergeDestination(\n        data=df,\n        destination=\"DELTA-TABLE-PATH\",\n        options={\n            \"checkpointLocation\": \"/{CHECKPOINT-LOCATION}/\"\n        },\n        merge_condition=\"`source.id = target.id`\"\n        when_matched_update_list=None\n        when_matched_delete_list=None\n        when_not_matched_insert_list=None\n        when_not_matched_by_source_update_list=None\n        when_not_matched_by_source_delete_list=None\n        try_broadcast_join=False\n        trigger=\"10 seconds\",\n        query_name=\"DeltaDestination\"\n        query_wait_interval=None\n    )\n\n    delta_merge_destination.write_stream()\n    ```\n    ```python\n    #Delta Merge Destination for Batch Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkDeltaMergeDestination\n\n    delta_merge_destination = SparkDeltaMergeDestination(\n        data=df,\n        destination=\"DELTA-TABLE-PATH\",\n        options={},\n        merge_condition=\"`source.id = target.id`\",\n        when_matched_update_list=None,\n        when_matched_delete_list=None,\n        when_not_matched_insert_list=None,\n        when_not_matched_by_source_update_list=None,\n        when_not_matched_by_source_delete_list=None,\n        try_broadcast_join=False,\n        trigger=\"10 seconds\",\n        query_name=\"DeltaDestination\"\n        query_wait_interval=None\n    )\n\n    delta_merge_destination.write_batch()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe to be merged into a Delta Table\n        destination (str): Either the name of the Hive Metastore or Unity Catalog Delta Table **or** the path to the Delta table\n        options (dict): Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for [batch](https://docs.delta.io/latest/delta-batch.html#write-to-a-table){ target=\"_blank\" } and [streaming](https://docs.delta.io/latest/delta-streaming.html#delta-table-as-a-sink){ target=\"_blank\" }.\n        merge_condition (str): Condition for matching records between dataframe and delta table. Reference Dataframe columns as `source` and Delta Table columns as `target`. For example `source.id = target.id`.\n        when_matched_update_list (optional list[DeltaMergeConditionValues]): Conditions(optional) and values to be used when updating rows that match the `merge_condition`. Specify `*` for Values if all columns from Dataframe should be inserted.\n        when_matched_delete_list (optional list[DeltaMergeCondition]): Conditions(optional) to be used when deleting rows that match the `merge_condition`.\n        when_not_matched_insert_list (optional list[DeltaMergeConditionValues]): Conditions(optional) and values to be used when inserting rows that do not match the `merge_condition`. Specify `*` for Values if all columns from Dataframe should be inserted.\n        when_not_matched_by_source_update_list (optional list[DeltaMergeConditionValues]): Conditions(optional) and values to be used when updating rows that do not match the `merge_condition`.\n        when_not_matched_by_source_delete_list (optional list[DeltaMergeCondition]): Conditions(optional) to be used when deleting rows that do not match the `merge_condition`.\n        try_broadcast_join (optional bool): Attempts to perform a broadcast join in the merge which can leverage data skipping using partition pruning and file pruning automatically. Can fail if dataframe being merged is large and therefore more suitable for streaming merges than batch merges\n        trigger (optional str): Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds\n        query_name (optional str): Unique name for the query in associated SparkSession\n        query_wait_interval (optional int): If set, waits for the streaming query to complete before returning. (stream) Default is None\n\n    Attributes:\n        checkpointLocation (str): Path to checkpoint files. (Streaming)\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    destination: str\n    options: dict\n    merge_condition: str\n    when_matched_update_list: List[DeltaMergeConditionValues]\n    when_matched_delete_list: List[DeltaMergeCondition]\n    when_not_matched_insert_list: List[DeltaMergeConditionValues]\n    when_not_matched_by_source_update_list: List[DeltaMergeConditionValues]\n    when_not_matched_by_source_delete_list: List[DeltaMergeCondition]\n    try_broadcast_join: bool\n    trigger: str\n    query_name: str\n    query_wait_interval: int\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n        destination: str,\n        options: dict,\n        merge_condition: str,\n        when_matched_update_list: List[DeltaMergeConditionValues] = None,\n        when_matched_delete_list: List[DeltaMergeCondition] = None,\n        when_not_matched_insert_list: List[DeltaMergeConditionValues] = None,\n        when_not_matched_by_source_update_list: List[DeltaMergeConditionValues] = None,\n        when_not_matched_by_source_delete_list: List[DeltaMergeCondition] = None,\n        try_broadcast_join: bool = False,\n        trigger=\"10 seconds\",\n        query_name: str = \"DeltaMergeDestination\",\n        query_wait_interval: int = None,\n    ) -&gt; None:\n        self.spark = spark\n        self.data = data\n        self.destination = destination\n        self.options = options\n        self.merge_condition = merge_condition\n        self.when_matched_update_list = (\n            [] if when_matched_update_list is None else when_matched_update_list\n        )\n        self.when_matched_delete_list = (\n            [] if when_matched_delete_list is None else when_matched_delete_list\n        )\n        self.when_not_matched_insert_list = (\n            [] if when_not_matched_insert_list is None else when_not_matched_insert_list\n        )\n        if (\n            isinstance(when_not_matched_by_source_update_list, list)\n            and len(when_not_matched_by_source_update_list) &gt; 0\n        ):\n            _package_version_meets_minimum(\"delta-spark\", \"2.3.0\")\n        self.when_not_matched_by_source_update_list = (\n            []\n            if when_not_matched_by_source_update_list is None\n            else when_not_matched_by_source_update_list\n        )\n        if (\n            isinstance(when_not_matched_by_source_delete_list, list)\n            and len(when_not_matched_by_source_delete_list) &gt; 0\n        ):\n            _package_version_meets_minimum(\"delta-spark\", \"2.3.0\")\n        self.when_not_matched_by_source_delete_list = (\n            []\n            if when_not_matched_by_source_delete_list is None\n            else when_not_matched_by_source_delete_list\n        )\n        self.try_broadcast_join = try_broadcast_join\n        self.trigger = trigger\n        self.query_name = query_name\n        self.query_wait_interval = query_wait_interval\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_core\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {\n            \"spark.sql.extensions\": \"io.delta.sql.DeltaSparkSessionExtension\",\n            \"spark.sql.catalog.spark_catalog\": \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n            \"spark.databricks.delta.schema.autoMerge.enabled\": \"true\",\n        }\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def _delta_merge_builder(\n        self, df: DataFrame, try_broadcast_join: bool\n    ) -&gt; DeltaMergeBuilder:\n        if \"/\" in self.destination:\n            delta_table = DeltaTable.forPath(self.spark, self.destination)\n        else:\n            delta_table = DeltaTable.forName(self.spark, self.destination)\n\n        if try_broadcast_join == True:\n            delta_merge_builder = delta_table.alias(\"target\").merge(\n                source=broadcast(df).alias(\"source\"), condition=self.merge_condition\n            )\n        else:\n            delta_merge_builder = delta_table.alias(\"target\").merge(\n                source=df.alias(\"source\"), condition=self.merge_condition\n            )\n\n        for when_matched_update in self.when_matched_update_list:\n            if when_matched_update.values == \"*\":\n                delta_merge_builder = delta_merge_builder.whenMatchedUpdateAll(\n                    condition=when_matched_update.condition,\n                )\n            else:\n                delta_merge_builder = delta_merge_builder.whenMatchedUpdate(\n                    condition=when_matched_update.condition,\n                    set=when_matched_update.values,\n                )\n\n        for when_matched_delete in self.when_matched_delete_list:\n            delta_merge_builder = delta_merge_builder.whenMatchedDelete(\n                condition=when_matched_delete.condition,\n            )\n\n        for when_not_matched_insert in self.when_not_matched_insert_list:\n            if when_not_matched_insert.values == \"*\":\n                delta_merge_builder = delta_merge_builder.whenNotMatchedInsertAll(\n                    condition=when_not_matched_insert.condition,\n                )\n            else:\n                delta_merge_builder = delta_merge_builder.whenNotMatchedInsert(\n                    condition=when_not_matched_insert.condition,\n                    values=when_not_matched_insert.values,\n                )\n\n        for (\n            when_not_matched_by_source_update\n        ) in self.when_not_matched_by_source_update_list:\n            delta_merge_builder = delta_merge_builder.whenNotMatchedBySourceUpdate(\n                condition=when_not_matched_by_source_update.condition,\n                set=when_not_matched_by_source_update.values,\n            )\n\n        for (\n            when_not_matched_by_source_delete\n        ) in self.when_not_matched_by_source_delete_list:\n            delta_merge_builder = delta_merge_builder.whenNotMatchedBySourceDelete(\n                condition=when_not_matched_by_source_delete.condition,\n            )\n\n        return delta_merge_builder\n\n    def _stream_merge_micro_batch(\n        self, micro_batch_df: DataFrame, epoch_id=None\n    ):  # NOSONAR\n        micro_batch_df.persist()\n\n        retry_delta_merge = False\n\n        if self.try_broadcast_join == True:\n            try:\n                delta_merge = self._delta_merge_builder(\n                    micro_batch_df, self.try_broadcast_join\n                )\n                delta_merge.execute()\n            except Exception as e:\n                if \"SparkOutOfMemoryError\" in str(e):\n                    retry_delta_merge = True\n                else:\n                    raise e\n\n        if self.try_broadcast_join == False or retry_delta_merge == True:\n            delta_merge = self._delta_merge_builder(micro_batch_df, False)\n            delta_merge.execute()\n\n        micro_batch_df.unpersist()\n\n    def write_batch(self):\n        \"\"\"\n        Merges batch data into a Delta Table.\n        \"\"\"\n        try:\n            delta_merge = self._delta_merge_builder(self.data, self.try_broadcast_join)\n            return delta_merge.execute()\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self):\n        \"\"\"\n        Merges streaming data to Delta using foreachBatch\n        \"\"\"\n        TRIGGER_OPTION = (\n            {\"availableNow\": True}\n            if self.trigger == \"availableNow\"\n            else {\"processingTime\": self.trigger}\n        )\n        try:\n            query = (\n                self.data.writeStream.trigger(**TRIGGER_OPTION)\n                .format(\"delta\")\n                .foreachBatch(self._stream_merge_micro_batch)\n                .queryName(self.query_name)\n                .outputMode(\"update\")\n                .options(**self.options)\n                .start()\n            )\n\n            if self.query_wait_interval:\n                while query.isActive:\n                    if query.lastProgress:\n                        logging.info(query.lastProgress)\n                    time.sleep(self.query_wait_interval)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkDeltaMergeDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta_merge.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkDeltaMergeDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Merges batch data into a Delta Table.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta_merge.py</code> <pre><code>def write_batch(self):\n    \"\"\"\n    Merges batch data into a Delta Table.\n    \"\"\"\n    try:\n        delta_merge = self._delta_merge_builder(self.data, self.try_broadcast_join)\n        return delta_merge.execute()\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkDeltaMergeDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Merges streaming data to Delta using foreachBatch</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta_merge.py</code> <pre><code>def write_stream(self):\n    \"\"\"\n    Merges streaming data to Delta using foreachBatch\n    \"\"\"\n    TRIGGER_OPTION = (\n        {\"availableNow\": True}\n        if self.trigger == \"availableNow\"\n        else {\"processingTime\": self.trigger}\n    )\n    try:\n        query = (\n            self.data.writeStream.trigger(**TRIGGER_OPTION)\n            .format(\"delta\")\n            .foreachBatch(self._stream_merge_micro_batch)\n            .queryName(self.query_name)\n            .outputMode(\"update\")\n            .options(**self.options)\n            .start()\n        )\n\n        if self.query_wait_interval:\n            while query.isActive:\n                if query.lastProgress:\n                    logging.info(query.lastProgress)\n                time.sleep(self.query_wait_interval)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkEventhubDestination","title":"<code>SparkEventhubDestination</code>","text":"<p>               Bases: <code>DestinationInterface</code></p> <p>This Spark destination class is used to write batch or streaming data to Eventhubs. Eventhub configurations need to be specified as options in a dictionary. Additionally, there are more optional configurations which can be found here. If using startingPosition or endingPosition make sure to check out Event Position section for more details and examples.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkEventhubDestination--examples","title":"Examples","text":"<p><pre><code>#Eventhub Destination for Streaming Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkEventhubDestination\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconnectionString = Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\n\neventhub_destination = SparkEventhubDestination(\n    spark=spark,\n    data=df,\n    options={\n        \"eventhubs.connectionString\": connectionString,\n        \"eventhubs.consumerGroup\": \"{YOUR-EVENTHUB-CONSUMER-GROUP}\",\n        \"checkpointLocation\": \"/{CHECKPOINT-LOCATION}/\"\n    },\n    trigger=\"10 seconds\",\n    query_name=\"EventhubDestination\",\n    query_wait_interval=None\n)\n\neventhub_destination.write_stream()\n</code></pre> <pre><code>#Eventhub Destination for Batch Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkEventhubDestination\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconnectionString = Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\n\n\neventhub_destination = SparkEventhubDestination(\n    spark=spark,\n    data=df,\n    options={\n        \"eventhubs.connectionString\": connectionString,\n        \"eventhubs.consumerGroup\": \"{YOUR-EVENTHUB-CONSUMER-GROUP}\"\n    },\n    trigger=\"10 seconds\",\n    query_name=\"EventhubDestination\",\n    query_wait_interval=None\n)\n\neventhub_destination.write_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session</p> required <code>data</code> <code>DataFrame</code> <p>Dataframe to be written to Eventhub</p> required <code>options</code> <code>dict</code> <p>A dictionary of Eventhub configurations (See Attributes table below). All Configuration options for Eventhubs can be found here.</p> required <code>trigger</code> <code>optional str</code> <p>Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds</p> <code>'10 seconds'</code> <code>query_name</code> <code>str</code> <p>Unique name for the query in associated SparkSession</p> <code>'EventhubDestination'</code> <code>query_wait_interval</code> <code>optional int</code> <p>If set, waits for the streaming query to complete before returning. (stream) Default is None</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>checkpointLocation</code> <code>str</code> <p>Path to checkpoint files. (Streaming)</p> <code>eventhubs.connectionString</code> <code>str</code> <p>Eventhubs connection string is required to connect to the Eventhubs service. (Streaming and Batch)</p> <code>eventhubs.consumerGroup</code> <code>str</code> <p>A consumer group is a view of an entire eventhub. Consumer groups enable multiple consuming applications to each have a separate view of the event stream, and to read the stream independently at their own pace and with their own offsets. (Streaming and Batch)</p> <code>eventhubs.startingPosition</code> <code>JSON str</code> <p>The starting position for your Structured Streaming job. If a specific EventPosition is not set for a partition using startingPositions, then we use the EventPosition set in startingPosition. If nothing is set in either option, we will begin consuming from the end of the partition. (Streaming and Batch)</p> <code>eventhubs.endingPosition</code> <code>JSON str</code> <p>(JSON str): The ending position of a batch query. This works the same as startingPosition. (Batch)</p> <code>maxEventsPerTrigger</code> <code>long</code> <p>Rate limit on maximum number of events processed per trigger interval. The specified total number of events will be proportionally split across partitions of different volume. (Stream)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/eventhub.py</code> <pre><code>class SparkEventhubDestination(DestinationInterface):\n    \"\"\"\n    This Spark destination class is used to write batch or streaming data to Eventhubs. Eventhub configurations need to be specified as options in a dictionary.\n    Additionally, there are more optional configurations which can be found [here.](https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/PySpark/structured-streaming-pyspark.md#event-hubs-configuration){ target=\"_blank\" }\n    If using startingPosition or endingPosition make sure to check out **Event Position** section for more details and examples.\n\n    Examples\n    --------\n    ```python\n    #Eventhub Destination for Streaming Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkEventhubDestination\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    connectionString = Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\n\n    eventhub_destination = SparkEventhubDestination(\n        spark=spark,\n        data=df,\n        options={\n            \"eventhubs.connectionString\": connectionString,\n            \"eventhubs.consumerGroup\": \"{YOUR-EVENTHUB-CONSUMER-GROUP}\",\n            \"checkpointLocation\": \"/{CHECKPOINT-LOCATION}/\"\n        },\n        trigger=\"10 seconds\",\n        query_name=\"EventhubDestination\",\n        query_wait_interval=None\n    )\n\n    eventhub_destination.write_stream()\n    ```\n    ```python\n    #Eventhub Destination for Batch Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkEventhubDestination\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    connectionString = Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\n\n\n    eventhub_destination = SparkEventhubDestination(\n        spark=spark,\n        data=df,\n        options={\n            \"eventhubs.connectionString\": connectionString,\n            \"eventhubs.consumerGroup\": \"{YOUR-EVENTHUB-CONSUMER-GROUP}\"\n        },\n        trigger=\"10 seconds\",\n        query_name=\"EventhubDestination\",\n        query_wait_interval=None\n    )\n\n    eventhub_destination.write_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session\n        data (DataFrame): Dataframe to be written to Eventhub\n        options (dict): A dictionary of Eventhub configurations (See Attributes table below). All Configuration options for Eventhubs can be found [here.](https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/PySpark/structured-streaming-pyspark.md#event-hubs-configuration){ target=\"_blank\" }\n        trigger (optional str): Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds\n        query_name (str): Unique name for the query in associated SparkSession\n        query_wait_interval (optional int): If set, waits for the streaming query to complete before returning. (stream) Default is None\n\n    Attributes:\n        checkpointLocation (str): Path to checkpoint files. (Streaming)\n        eventhubs.connectionString (str):  Eventhubs connection string is required to connect to the Eventhubs service. (Streaming and Batch)\n        eventhubs.consumerGroup (str): A consumer group is a view of an entire eventhub. Consumer groups enable multiple consuming applications to each have a separate view of the event stream, and to read the stream independently at their own pace and with their own offsets. (Streaming and Batch)\n        eventhubs.startingPosition (JSON str): The starting position for your Structured Streaming job. If a specific EventPosition is not set for a partition using startingPositions, then we use the EventPosition set in startingPosition. If nothing is set in either option, we will begin consuming from the end of the partition. (Streaming and Batch)\n        eventhubs.endingPosition: (JSON str): The ending position of a batch query. This works the same as startingPosition. (Batch)\n        maxEventsPerTrigger (long): Rate limit on maximum number of events processed per trigger interval. The specified total number of events will be proportionally split across partitions of different volume. (Stream)\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    options: dict\n    trigger: str\n    query_name: str\n    query_wait_interval: int\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n        options: dict,\n        trigger=\"10 seconds\",\n        query_name=\"EventhubDestination\",\n        query_wait_interval: int = None,\n    ) -&gt; None:\n        self.spark = spark\n        self.data = data\n        self.options = options\n        self.trigger = trigger\n        self.query_name = query_name\n        self.query_wait_interval = query_wait_interval\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        spark_libraries = Libraries()\n        spark_libraries.add_maven_library(get_default_package(\"spark_azure_eventhub\"))\n        return spark_libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def prepare_columns(self):\n        if \"body\" in self.data.columns:\n            if self.data.schema[\"body\"].dataType not in [StringType(), BinaryType()]:\n                try:\n                    self.data.withColumn(\"body\", col(\"body\").cast(StringType()))\n                except Exception as e:\n                    raise ValueError(\n                        \"'body' column must be of string or binary type\", e\n                    )\n        else:\n            self.data = self.data.withColumn(\n                \"body\",\n                to_json(\n                    struct(\n                        [\n                            col(column).alias(column)\n                            for column in self.data.columns\n                            if column not in [\"partitionId\", \"partitionKey\"]\n                        ]\n                    )\n                ),\n            )\n        for column in self.data.schema:\n            if (\n                column.name in [\"partitionId\", \"partitionKey\"]\n                and column.dataType != StringType()\n            ):\n                try:\n                    self.data = self.data.withColumn(\n                        column.name, col(column.name).cast(StringType())\n                    )\n                except Exception as e:\n                    raise ValueError(f\"Column {column.name} must be of string type\", e)\n        return self.data.select(\n            [\n                column\n                for column in self.data.columns\n                if column in [\"partitionId\", \"partitionKey\", \"body\"]\n            ]\n        )\n\n    def write_batch(self):\n        \"\"\"\n        Writes batch data to Eventhubs.\n        \"\"\"\n        eventhub_connection_string = \"eventhubs.connectionString\"\n        try:\n            if eventhub_connection_string in self.options:\n                sc = self.spark.sparkContext\n                self.options[eventhub_connection_string] = (\n                    sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                        self.options[eventhub_connection_string]\n                    )\n                )\n            df = self.prepare_columns()\n            return df.write.format(\"eventhubs\").options(**self.options).save()\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self):\n        \"\"\"\n        Writes steaming data to Eventhubs.\n        \"\"\"\n        eventhub_connection_string = \"eventhubs.connectionString\"\n        try:\n            TRIGGER_OPTION = (\n                {\"availableNow\": True}\n                if self.trigger == \"availableNow\"\n                else {\"processingTime\": self.trigger}\n            )\n            if eventhub_connection_string in self.options:\n                sc = self.spark.sparkContext\n                self.options[eventhub_connection_string] = (\n                    sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                        self.options[eventhub_connection_string]\n                    )\n                )\n            df = self.prepare_columns()\n            df = self.data.select(\n                [\n                    column\n                    for column in self.data.columns\n                    if column in [\"partitionId\", \"partitionKey\", \"body\"]\n                ]\n            )\n            query = (\n                df.writeStream.trigger(**TRIGGER_OPTION)\n                .format(\"eventhubs\")\n                .options(**self.options)\n                .queryName(self.query_name)\n                .start()\n            )\n\n            if self.query_wait_interval:\n                while query.isActive:\n                    if query.lastProgress:\n                        logging.info(query.lastProgress)\n                    time.sleep(self.query_wait_interval)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkEventhubDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/eventhub.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkEventhubDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Writes batch data to Eventhubs.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/eventhub.py</code> <pre><code>def write_batch(self):\n    \"\"\"\n    Writes batch data to Eventhubs.\n    \"\"\"\n    eventhub_connection_string = \"eventhubs.connectionString\"\n    try:\n        if eventhub_connection_string in self.options:\n            sc = self.spark.sparkContext\n            self.options[eventhub_connection_string] = (\n                sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                    self.options[eventhub_connection_string]\n                )\n            )\n        df = self.prepare_columns()\n        return df.write.format(\"eventhubs\").options(**self.options).save()\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkEventhubDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Writes steaming data to Eventhubs.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/eventhub.py</code> <pre><code>def write_stream(self):\n    \"\"\"\n    Writes steaming data to Eventhubs.\n    \"\"\"\n    eventhub_connection_string = \"eventhubs.connectionString\"\n    try:\n        TRIGGER_OPTION = (\n            {\"availableNow\": True}\n            if self.trigger == \"availableNow\"\n            else {\"processingTime\": self.trigger}\n        )\n        if eventhub_connection_string in self.options:\n            sc = self.spark.sparkContext\n            self.options[eventhub_connection_string] = (\n                sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                    self.options[eventhub_connection_string]\n                )\n            )\n        df = self.prepare_columns()\n        df = self.data.select(\n            [\n                column\n                for column in self.data.columns\n                if column in [\"partitionId\", \"partitionKey\", \"body\"]\n            ]\n        )\n        query = (\n            df.writeStream.trigger(**TRIGGER_OPTION)\n            .format(\"eventhubs\")\n            .options(**self.options)\n            .queryName(self.query_name)\n            .start()\n        )\n\n        if self.query_wait_interval:\n            while query.isActive:\n                if query.lastProgress:\n                    logging.info(query.lastProgress)\n                time.sleep(self.query_wait_interval)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKafkaDestination","title":"<code>SparkKafkaDestination</code>","text":"<p>               Bases: <code>DestinationInterface</code></p> <p>This Spark destination class is used to write batch or streaming data from Kafka. Required and optional configurations can be found in the Attributes tables below.</p> <p>Additionally, there are more optional configurations which can be found here.</p> <p>For compatibility between Spark and Kafka, the columns in the input dataframe are concatenated into one 'value' column of JSON string.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKafkaDestination--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.destinations import SparkKafkaDestination\n\nkafka_destination = SparkKafkaDestination(\n    data=df,\n    options={\n        \"kafka.bootstrap.servers\": \"host1:port1,host2:port2\"\n    },\n    trigger=\"10 seconds\",\n    query_name=\"KafkaDestination\",\n    query_wait_interval=None\n)\n\nkafka_destination.write_stream()\n\nOR\n\nkafka_destination.write_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to be written to Kafka</p> required <code>options</code> <code>dict</code> <p>A dictionary of Kafka configurations (See Attributes tables below). For more information on configuration options see here</p> required <code>trigger</code> <code>optional str</code> <p>Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds</p> <code>'10 seconds'</code> <code>query_name</code> <code>str</code> <p>Unique name for the query in associated SparkSession</p> <code>'KafkaDestination'</code> <code>query_wait_interval</code> <code>optional int</code> <p>If set, waits for the streaming query to complete before returning. (stream) Default is None</p> <code>None</code> <p>The following options must be set for the Kafka destination for both batch and streaming queries.</p> <p>Attributes:</p> Name Type Description <code>kafka.bootstrap.servers</code> <code>A comma-separated list of host\ufe30port</code> <p>The Kafka \"bootstrap.servers\" configuration. (Streaming and Batch)</p> <p>The following configurations are optional:</p> <p>Attributes:</p> Name Type Description <code>topic</code> <code>str</code> <p>Sets the topic that all rows will be written to in Kafka. This option overrides any topic column that may exist in the data. (Streaming and Batch)</p> <code>includeHeaders</code> <code>bool</code> <p>Whether to include the Kafka headers in the row. (Streaming and Batch)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kafka.py</code> <pre><code>class SparkKafkaDestination(DestinationInterface):\n    \"\"\"\n    This Spark destination class is used to write batch or streaming data from Kafka. Required and optional configurations can be found in the Attributes tables below.\n\n    Additionally, there are more optional configurations which can be found [here.](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html){ target=\"_blank\" }\n\n    For compatibility between Spark and Kafka, the columns in the input dataframe are concatenated into one 'value' column of JSON string.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.destinations import SparkKafkaDestination\n\n    kafka_destination = SparkKafkaDestination(\n        data=df,\n        options={\n            \"kafka.bootstrap.servers\": \"host1:port1,host2:port2\"\n        },\n        trigger=\"10 seconds\",\n        query_name=\"KafkaDestination\",\n        query_wait_interval=None\n    )\n\n    kafka_destination.write_stream()\n\n    OR\n\n    kafka_destination.write_batch()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe to be written to Kafka\n        options (dict): A dictionary of Kafka configurations (See Attributes tables below). For more information on configuration options see [here](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html){ target=\"_blank\" }\n        trigger (optional str): Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds\n        query_name (str): Unique name for the query in associated SparkSession\n        query_wait_interval (optional int): If set, waits for the streaming query to complete before returning. (stream) Default is None\n\n    The following options must be set for the Kafka destination for both batch and streaming queries.\n\n    Attributes:\n        kafka.bootstrap.servers (A comma-separated list of host\ufe30port): The Kafka \"bootstrap.servers\" configuration. (Streaming and Batch)\n\n    The following configurations are optional:\n\n    Attributes:\n        topic (str):Sets the topic that all rows will be written to in Kafka. This option overrides any topic column that may exist in the data. (Streaming and Batch)\n        includeHeaders (bool): Whether to include the Kafka headers in the row. (Streaming and Batch)\n\n    \"\"\"\n\n    data: DataFrame\n    options: dict\n    trigger: str\n    query_name: str\n    query_wait_interval: int\n\n    def __init__(\n        self,\n        data: DataFrame,\n        options: dict,\n        trigger=\"10 seconds\",\n        query_name=\"KafkaDestination\",\n        query_wait_interval: int = None,\n    ) -&gt; None:\n        self.data = data\n        self.options = options\n        self.trigger = trigger\n        self.query_name = query_name\n        self.query_wait_interval = query_wait_interval\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        spark_libraries = Libraries()\n        spark_libraries.add_maven_library(get_default_package(\"spark_sql_kafka\"))\n        return spark_libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def write_batch(self):\n        \"\"\"\n        Writes batch data to Kafka.\n        \"\"\"\n        try:\n            return (\n                self.data.select(to_json(struct(\"*\")).alias(\"value\"))\n                .write.format(\"kafka\")\n                .options(**self.options)\n                .save()\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self):\n        \"\"\"\n        Writes steaming data to Kafka.\n        \"\"\"\n        try:\n            TRIGGER_OPTION = (\n                {\"availableNow\": True}\n                if self.trigger == \"availableNow\"\n                else {\"processingTime\": self.trigger}\n            )\n            query = (\n                self.data.select(to_json(struct(\"*\")).alias(\"value\"))\n                .writeStream.trigger(**TRIGGER_OPTION)\n                .format(\"kafka\")\n                .options(**self.options)\n                .queryName(self.query_name)\n                .start()\n            )\n\n            if self.query_wait_interval:\n                while query.isActive:\n                    if query.lastProgress:\n                        logging.info(query.lastProgress)\n                    time.sleep(self.query_wait_interval)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKafkaDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kafka.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKafkaDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Writes batch data to Kafka.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kafka.py</code> <pre><code>def write_batch(self):\n    \"\"\"\n    Writes batch data to Kafka.\n    \"\"\"\n    try:\n        return (\n            self.data.select(to_json(struct(\"*\")).alias(\"value\"))\n            .write.format(\"kafka\")\n            .options(**self.options)\n            .save()\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKafkaDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Writes steaming data to Kafka.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kafka.py</code> <pre><code>def write_stream(self):\n    \"\"\"\n    Writes steaming data to Kafka.\n    \"\"\"\n    try:\n        TRIGGER_OPTION = (\n            {\"availableNow\": True}\n            if self.trigger == \"availableNow\"\n            else {\"processingTime\": self.trigger}\n        )\n        query = (\n            self.data.select(to_json(struct(\"*\")).alias(\"value\"))\n            .writeStream.trigger(**TRIGGER_OPTION)\n            .format(\"kafka\")\n            .options(**self.options)\n            .queryName(self.query_name)\n            .start()\n        )\n\n        if self.query_wait_interval:\n            while query.isActive:\n                if query.lastProgress:\n                    logging.info(query.lastProgress)\n                time.sleep(self.query_wait_interval)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKinesisDestination","title":"<code>SparkKinesisDestination</code>","text":"<p>               Bases: <code>DestinationInterface</code></p> <p>This Kinesis destination class is used to write batch or streaming data to Kinesis. Kinesis configurations need to be specified as options in a dictionary.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKinesisDestination--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.destinations import SparkKinesisDestination\n\nkinesis_destination = SparkKinesisDestination(\n    data=df,\n    options={\n        \"endpointUrl\": \"https://kinesis.{REGION}.amazonaws.com\",\n        \"awsAccessKey\": \"{YOUR-AWS-ACCESS-KEY}\",\n        \"awsSecretKey\": \"{YOUR-AWS-SECRET-KEY}\",\n        \"streamName\": \"{YOUR-STREAM-NAME}\"\n    },\n    mode=\"update\",\n    trigger=\"10 seconds\",\n    query_name=\"KinesisDestination\",\n    query_wait_interval=None\n)\n\nkinesis_destination.write_stream()\n\nOR\n\nkinesis_destination.write_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to be written to Delta</p> required <code>options</code> <code>dict</code> <p>A dictionary of Kinesis configurations (See Attributes table below). All Configuration options for Kinesis can be found here.</p> required <code>mode</code> <code>str</code> <p>Method of writing to Kinesis - append, complete, update</p> <code>'update'</code> <code>trigger</code> <code>optional str</code> <p>Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds</p> <code>'10 seconds'</code> <code>query_name</code> <code>str</code> <p>Unique name for the query in associated SparkSession</p> <code>'KinesisDestination'</code> <code>query_wait_interval</code> <code>optional int</code> <p>If set, waits for the streaming query to complete before returning. (stream) Default is None</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>endpointUrl</code> <code>str</code> <p>Endpoint of the kinesis stream.</p> <code>awsAccessKey</code> <code>str</code> <p>AWS access key.</p> <code>awsSecretKey</code> <code>str</code> <p>AWS secret access key corresponding to the access key.</p> <code>streamName</code> <code>List[str]</code> <p>Name of the streams in Kinesis to write to.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kinesis.py</code> <pre><code>class SparkKinesisDestination(DestinationInterface):\n    \"\"\"\n    This Kinesis destination class is used to write batch or streaming data to Kinesis. Kinesis configurations need to be specified as options in a dictionary.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.destinations import SparkKinesisDestination\n\n    kinesis_destination = SparkKinesisDestination(\n        data=df,\n        options={\n            \"endpointUrl\": \"https://kinesis.{REGION}.amazonaws.com\",\n            \"awsAccessKey\": \"{YOUR-AWS-ACCESS-KEY}\",\n            \"awsSecretKey\": \"{YOUR-AWS-SECRET-KEY}\",\n            \"streamName\": \"{YOUR-STREAM-NAME}\"\n        },\n        mode=\"update\",\n        trigger=\"10 seconds\",\n        query_name=\"KinesisDestination\",\n        query_wait_interval=None\n    )\n\n    kinesis_destination.write_stream()\n\n    OR\n\n    kinesis_destination.write_batch()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe to be written to Delta\n        options (dict): A dictionary of Kinesis configurations (See Attributes table below). All Configuration options for Kinesis can be found [here.](https://github.com/qubole/kinesis-sql#kinesis-sink-configuration){ target=\"_blank\" }\n        mode (str): Method of writing to Kinesis - append, complete, update\n        trigger (optional str): Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds\n        query_name (str): Unique name for the query in associated SparkSession\n        query_wait_interval (optional int): If set, waits for the streaming query to complete before returning. (stream) Default is None\n\n    Attributes:\n        endpointUrl (str): Endpoint of the kinesis stream.\n        awsAccessKey (str): AWS access key.\n        awsSecretKey (str): AWS secret access key corresponding to the access key.\n        streamName (List[str]): Name of the streams in Kinesis to write to.\n    \"\"\"\n\n    data: DataFrame\n    options: dict\n    mode: str\n    trigger: str\n    query_name: str\n    query_wait_interval: int\n\n    def __init__(\n        self,\n        data: DataFrame,\n        options: dict,\n        mode: str = \"update\",\n        trigger: str = \"10 seconds\",\n        query_name=\"KinesisDestination\",\n        query_wait_interval: int = None,\n    ) -&gt; None:\n        self.data = data\n        self.options = options\n        self.mode = mode\n        self.trigger = trigger\n        self.query_name = query_name\n        self.query_wait_interval = query_wait_interval\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK_DATABRICKS\n        \"\"\"\n        return SystemType.PYSPARK_DATABRICKS\n\n    @staticmethod\n    def libraries():\n        spark_libraries = Libraries()\n        return spark_libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def write_batch(self):\n        \"\"\"\n        Writes batch data to Kinesis.\n        \"\"\"\n        try:\n            return self.data.write.format(\"kinesis\").options(**self.options).save()\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self):\n        \"\"\"\n        Writes steaming data to Kinesis.\n        \"\"\"\n        try:\n            TRIGGER_OPTION = (\n                {\"availableNow\": True}\n                if self.trigger == \"availableNow\"\n                else {\"processingTime\": self.trigger}\n            )\n            query = (\n                self.data.writeStream.trigger(**TRIGGER_OPTION)\n                .format(\"kinesis\")\n                .outputMode(self.mode)\n                .options(**self.options)\n                .queryName(self.query_name)\n                .start()\n            )\n\n            if self.query_wait_interval:\n                while query.isActive:\n                    if query.lastProgress:\n                        logging.info(query.lastProgress)\n                    time.sleep(self.query_wait_interval)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKinesisDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK_DATABRICKS</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kinesis.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK_DATABRICKS\n    \"\"\"\n    return SystemType.PYSPARK_DATABRICKS\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKinesisDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Writes batch data to Kinesis.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kinesis.py</code> <pre><code>def write_batch(self):\n    \"\"\"\n    Writes batch data to Kinesis.\n    \"\"\"\n    try:\n        return self.data.write.format(\"kinesis\").options(**self.options).save()\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKinesisDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Writes steaming data to Kinesis.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kinesis.py</code> <pre><code>def write_stream(self):\n    \"\"\"\n    Writes steaming data to Kinesis.\n    \"\"\"\n    try:\n        TRIGGER_OPTION = (\n            {\"availableNow\": True}\n            if self.trigger == \"availableNow\"\n            else {\"processingTime\": self.trigger}\n        )\n        query = (\n            self.data.writeStream.trigger(**TRIGGER_OPTION)\n            .format(\"kinesis\")\n            .outputMode(self.mode)\n            .options(**self.options)\n            .queryName(self.query_name)\n            .start()\n        )\n\n        if self.query_wait_interval:\n            while query.isActive:\n                if query.lastProgress:\n                    logging.info(query.lastProgress)\n                time.sleep(self.query_wait_interval)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkRestAPIDestination","title":"<code>SparkRestAPIDestination</code>","text":"<p>               Bases: <code>DestinationInterface</code></p> <p>The Spark Rest API Destination is used to write data to a Rest API.</p> <p>The payload sent to the API is constructed by converting each row in the DataFrame to Json.</p> <p>Note</p> <p>While it is possible to use the <code>write_batch</code> method, it is easy to overwhlem a Rest API with large volumes of data. Consider reducing data volumes when writing to a Rest API in Batch mode to prevent API errors including throtting.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkRestAPIDestination--example","title":"Example","text":"<p><pre><code>#Rest API Destination for Streaming Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkRestAPIDestination\n\nrest_api_destination = SparkRestAPIDestination(\n    data=df,\n    options={\n        \"checkpointLocation\": \"{/CHECKPOINT-LOCATION/}\"\n    },\n    url=\"{REST-API-URL}\",\n    headers = {\n        'Authorization': 'Bearer {}'.format(\"{TOKEN}\")\n    },\n    batch_size=100,\n    method=\"POST\",\n    parallelism=8,\n    trigger=\"1 minute\",\n    query_name=\"DeltaRestAPIDestination\",\n    query_wait_interval=None\n)\n\nrest_api_destination.write_stream()\n</code></pre> <pre><code>#Rest API Destination for Batch Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkRestAPIDestination\n\nrest_api_destination = SparkRestAPIDestination(\n    data=df,\n    options={},\n    url=\"{REST-API-URL}\",\n    headers = {\n        'Authorization': 'Bearer {}'.format(\"{TOKEN}\")\n    },\n    batch_size=10,\n    method=\"POST\",\n    parallelism=4,\n    trigger=\"1 minute\",\n    query_name=\"DeltaRestAPIDestination\",\n    query_wait_interval=None\n)\n\nrest_api_destination.write_stream()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to be merged into a Delta Table</p> required <code>options</code> <code>dict</code> <p>A dictionary of options for streaming writes</p> required <code>url</code> <code>str</code> <p>The Rest API Url</p> required <code>headers</code> <code>dict</code> <p>A dictionary of headers to be provided to the Rest API</p> required <code>batch_size</code> <code>int</code> <p>The number of DataFrame rows to be used in each Rest API call</p> required <code>method</code> <code>str</code> <p>The method to be used when calling the Rest API. Allowed values are POST, PATCH and PUT</p> <code>'POST'</code> <code>parallelism</code> <code>int</code> <p>The number of concurrent calls to be made to the Rest API</p> <code>8</code> <code>trigger</code> <code>optional str</code> <p>Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds</p> <code>'1 minutes'</code> <code>query_name</code> <code>str</code> <p>Unique name for the query in associated SparkSession</p> <code>'DeltaRestAPIDestination'</code> <code>query_wait_interval</code> <code>optional int</code> <p>If set, waits for the streaming query to complete before returning. (stream) Default is None</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>checkpointLocation</code> <code>str</code> <p>Path to checkpoint files. (Streaming)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/rest_api.py</code> <pre><code>class SparkRestAPIDestination(DestinationInterface):\n    \"\"\"\n    The Spark Rest API Destination is used to write data to a Rest API.\n\n    The payload sent to the API is constructed by converting each row in the DataFrame to Json.\n\n    !!! Note\n        While it is possible to use the `write_batch` method, it is easy to overwhlem a Rest API with large volumes of data.\n        Consider reducing data volumes when writing to a Rest API in Batch mode to prevent API errors including throtting.\n\n    Example\n    --------\n    ```python\n    #Rest API Destination for Streaming Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkRestAPIDestination\n\n    rest_api_destination = SparkRestAPIDestination(\n        data=df,\n        options={\n            \"checkpointLocation\": \"{/CHECKPOINT-LOCATION/}\"\n        },\n        url=\"{REST-API-URL}\",\n        headers = {\n            'Authorization': 'Bearer {}'.format(\"{TOKEN}\")\n        },\n        batch_size=100,\n        method=\"POST\",\n        parallelism=8,\n        trigger=\"1 minute\",\n        query_name=\"DeltaRestAPIDestination\",\n        query_wait_interval=None\n    )\n\n    rest_api_destination.write_stream()\n    ```\n    ```python\n    #Rest API Destination for Batch Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkRestAPIDestination\n\n    rest_api_destination = SparkRestAPIDestination(\n        data=df,\n        options={},\n        url=\"{REST-API-URL}\",\n        headers = {\n            'Authorization': 'Bearer {}'.format(\"{TOKEN}\")\n        },\n        batch_size=10,\n        method=\"POST\",\n        parallelism=4,\n        trigger=\"1 minute\",\n        query_name=\"DeltaRestAPIDestination\",\n        query_wait_interval=None\n    )\n\n    rest_api_destination.write_stream()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe to be merged into a Delta Table\n        options (dict): A dictionary of options for streaming writes\n        url (str): The Rest API Url\n        headers (dict): A dictionary of headers to be provided to the Rest API\n        batch_size (int): The number of DataFrame rows to be used in each Rest API call\n        method (str): The method to be used when calling the Rest API. Allowed values are POST, PATCH and PUT\n        parallelism (int): The number of concurrent calls to be made to the Rest API\n        trigger (optional str): Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds\n        query_name (str): Unique name for the query in associated SparkSession\n        query_wait_interval (optional int): If set, waits for the streaming query to complete before returning. (stream) Default is None\n\n    Attributes:\n        checkpointLocation (str): Path to checkpoint files. (Streaming)\n    \"\"\"\n\n    data: DataFrame\n    options: dict\n    url: str\n    headers: dict\n    batch_size: int\n    method: str\n    parallelism: int\n    trigger: str\n    query_name: str\n    query_wait_interval: int\n\n    def __init__(\n        self,\n        data: DataFrame,\n        options: dict,\n        url: str,\n        headers: dict,\n        batch_size: int,\n        method: str = \"POST\",\n        parallelism: int = 8,\n        trigger=\"1 minutes\",\n        query_name: str = \"DeltaRestAPIDestination\",\n        query_wait_interval: int = None,\n    ) -&gt; None:\n        self.data = data\n        self.options = options\n        self.url = url\n        self.headers = headers\n        self.batch_size = batch_size\n        self.method = method\n        self.parallelism = parallelism\n        self.trigger = trigger\n        self.query_name = query_name\n        self.query_wait_interval = query_wait_interval\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_pypi_library(get_default_package(\"api_requests\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def _pre_batch_records_for_api_call(self, micro_batch_df: DataFrame):\n        batch_count = math.ceil(micro_batch_df.count() / self.batch_size)\n        micro_batch_df = (\n            micro_batch_df.withColumn(\"content\", to_json(struct(col(\"*\"))))\n            .withColumn(\"row_number\", row_number().over(Window().orderBy(lit(\"A\"))))\n            .withColumn(\"batch_id\", col(\"row_number\") % batch_count)\n        )\n        return micro_batch_df.groupBy(\"batch_id\").agg(\n            concat_ws(\",|\", collect_list(\"content\")).alias(\"payload\")\n        )\n\n    def _api_micro_batch(self, micro_batch_df: DataFrame, epoch_id=None):  # NOSONAR\n        url = self.url\n        method = self.method\n        headers = self.headers\n\n        @udf(\"string\")\n        def _rest_api_execute(data):\n            session = requests.Session()\n            adapter = HTTPAdapter(max_retries=3)\n            session.mount(\"http://\", adapter)  # NOSONAR\n            session.mount(\"https://\", adapter)\n\n            if method == \"POST\":\n                response = session.post(url, headers=headers, data=data, verify=False)\n            elif method == \"PATCH\":\n                response = session.patch(url, headers=headers, data=data, verify=False)\n            elif method == \"PUT\":\n                response = session.put(url, headers=headers, data=data, verify=False)\n            else:\n                raise Exception(\"Method {} is not supported\".format(method))  # NOSONAR\n\n            if not (response.status_code == 200 or response.status_code == 201):\n                raise Exception(\n                    \"Response status : {} .Response message : {}\".format(\n                        str(response.status_code), response.text\n                    )\n                )  # NOSONAR\n\n            return str(response.status_code)\n\n        micro_batch_df.persist()\n        micro_batch_df = self._pre_batch_records_for_api_call(micro_batch_df)\n\n        micro_batch_df = micro_batch_df.repartition(self.parallelism)\n\n        (\n            micro_batch_df.withColumn(\n                \"rest_api_response_code\", _rest_api_execute(micro_batch_df[\"payload\"])\n            ).collect()\n        )\n        micro_batch_df.unpersist()\n\n    def write_batch(self):\n        \"\"\"\n        Writes batch data to a Rest API\n        \"\"\"\n        try:\n            return self._api_micro_batch(self.data)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self):\n        \"\"\"\n        Writes streaming data to a Rest API\n        \"\"\"\n        try:\n            TRIGGER_OPTION = (\n                {\"availableNow\": True}\n                if self.trigger == \"availableNow\"\n                else {\"processingTime\": self.trigger}\n            )\n            query = (\n                self.data.writeStream.trigger(**TRIGGER_OPTION)\n                .foreachBatch(self._api_micro_batch)\n                .queryName(self.query_name)\n                .outputMode(\"update\")\n                .options(**self.options)\n                .start()\n            )\n\n            if self.query_wait_interval:\n                while query.isActive:\n                    if query.lastProgress:\n                        logging.info(query.lastProgress)\n                    time.sleep(self.query_wait_interval)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkRestAPIDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/rest_api.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkRestAPIDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Writes batch data to a Rest API</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/rest_api.py</code> <pre><code>def write_batch(self):\n    \"\"\"\n    Writes batch data to a Rest API\n    \"\"\"\n    try:\n        return self._api_micro_batch(self.data)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkRestAPIDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Writes streaming data to a Rest API</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/rest_api.py</code> <pre><code>def write_stream(self):\n    \"\"\"\n    Writes streaming data to a Rest API\n    \"\"\"\n    try:\n        TRIGGER_OPTION = (\n            {\"availableNow\": True}\n            if self.trigger == \"availableNow\"\n            else {\"processingTime\": self.trigger}\n        )\n        query = (\n            self.data.writeStream.trigger(**TRIGGER_OPTION)\n            .foreachBatch(self._api_micro_batch)\n            .queryName(self.query_name)\n            .outputMode(\"update\")\n            .options(**self.options)\n            .start()\n        )\n\n        if self.query_wait_interval:\n            while query.isActive:\n                if query.lastProgress:\n                    logging.info(query.lastProgress)\n                time.sleep(self.query_wait_interval)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkPCDMToDeltaDestination","title":"<code>SparkPCDMToDeltaDestination</code>","text":"<p>               Bases: <code>DestinationInterface</code></p> <p>The Process Control Data Model written to Delta.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkPCDMToDeltaDestination--example","title":"Example","text":"<p><pre><code>#PCDM Latest To Delta Destination for Streaming Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkPCDMToDeltaDestination\n\npcdm_to_delta_destination = SparkPCDMToDeltaDestination(\n    data=df,\n    options={\n        \"checkpointLocation\": \"{/CHECKPOINT-LOCATION/}\"\n    },\n    destination_float=\"{DELTA_TABLE_PATH_FLOAT}\",\n    destination_string=\"{DELTA_TABLE_PATH_STRING}\",\n    destination_integer=\"{DELTA_TABLE_PATH_INTEGER}\",\n    mode=\"append\",\n    trigger=\"10 seconds\",\n    query_name=\"PCDMToDeltaDestination\",\n    query_wait_interval=None,\n    merge=True,\n    try_broadcast_join=False,\n    remove_nanoseconds=False,\n    remove_duplicates=True\n)\n\npcdm_to_delta_destination.write_stream()\n</code></pre> <pre><code>#PCDM Latest To Delta Destination for Batch Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkPCDMToDeltaDestination\n\npcdm_to_delta_destination = SparkPCDMToDeltaDestination(\n    data=df,\n    options={\n        \"maxRecordsPerFile\", \"10000\"\n    },\n    destination_float=\"{DELTA_TABLE_PATH_FLOAT}\",\n    destination_string=\"{DELTA_TABLE_PATH_STRING}\",\n    destination_integer=\"{DELTA_TABLE_PATH_INTEGER}\",\n    mode=\"overwrite\",\n    trigger=\"10 seconds\",\n    query_name=\"PCDMToDeltaDestination\",\n    query_wait_interval=None,\n    merge=True,\n    try_broadcast_join=False,\n    remove_nanoseconds=False,\n    remove_duplicates=True\n)\n\npcdm_to_delta_destination.write_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to be merged into a Delta Table</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for batch and streaming.</p> required <code>destination_float</code> <code>str</code> <p>Either the name of the Hive Metastore or Unity Catalog Delta Table or the path to the Delta table to store float values.</p> required <code>destination_string</code> <code>Optional str</code> <p>Either the name of the Hive Metastore or Unity Catalog Delta Table or the path to the Delta table to store string values.</p> <code>None</code> <code>destination_integer</code> <code>Optional str</code> <p>Either the name of the Hive Metastore or Unity Catalog Delta Table or the path to the Delta table to store integer values</p> <code>None</code> <code>mode</code> <code>str</code> <p>Method of writing to Delta Table - append/overwrite (batch), append/complete (stream)</p> <code>None</code> <code>trigger</code> <code>optional str</code> <p>Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds</p> <code>'10 seconds'</code> <code>query_name</code> <code>str</code> <p>Unique name for the query in associated SparkSession</p> <code>'PCDMToDeltaDestination'</code> <code>query_wait_interval</code> <code>optional int</code> <p>If set, waits for the streaming query to complete before returning. (stream) Default is None</p> <code>None</code> <code>merge</code> <code>bool</code> <p>Use Delta Merge to perform inserts, updates and deletes</p> <code>True</code> <code>try_broadcast_join</code> <code>bool</code> <p>Attempts to perform a broadcast join in the merge which can leverage data skipping using partition pruning and file pruning automatically. Can fail if dataframe being merged is large and therefore more suitable for streaming merges than batch merges</p> <code>False</code> <code>remove_nanoseconds</code> <code>bool</code> <p>Removes nanoseconds from the EventTime column and replaces with zeros</p> <code>False</code> <code>remove_duplicates</code> <code>bool</code> <p>Removes duplicates before writing the data</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>checkpointLocation</code> <code>str</code> <p>Path to checkpoint files. (Streaming)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/pcdm_to_delta.py</code> <pre><code>class SparkPCDMToDeltaDestination(DestinationInterface):\n    \"\"\"\n    The Process Control Data Model written to Delta.\n\n    Example\n    --------\n    ```python\n    #PCDM Latest To Delta Destination for Streaming Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkPCDMToDeltaDestination\n\n    pcdm_to_delta_destination = SparkPCDMToDeltaDestination(\n        data=df,\n        options={\n            \"checkpointLocation\": \"{/CHECKPOINT-LOCATION/}\"\n        },\n        destination_float=\"{DELTA_TABLE_PATH_FLOAT}\",\n        destination_string=\"{DELTA_TABLE_PATH_STRING}\",\n        destination_integer=\"{DELTA_TABLE_PATH_INTEGER}\",\n        mode=\"append\",\n        trigger=\"10 seconds\",\n        query_name=\"PCDMToDeltaDestination\",\n        query_wait_interval=None,\n        merge=True,\n        try_broadcast_join=False,\n        remove_nanoseconds=False,\n        remove_duplicates=True\n    )\n\n    pcdm_to_delta_destination.write_stream()\n    ```\n    ```python\n    #PCDM Latest To Delta Destination for Batch Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkPCDMToDeltaDestination\n\n    pcdm_to_delta_destination = SparkPCDMToDeltaDestination(\n        data=df,\n        options={\n            \"maxRecordsPerFile\", \"10000\"\n        },\n        destination_float=\"{DELTA_TABLE_PATH_FLOAT}\",\n        destination_string=\"{DELTA_TABLE_PATH_STRING}\",\n        destination_integer=\"{DELTA_TABLE_PATH_INTEGER}\",\n        mode=\"overwrite\",\n        trigger=\"10 seconds\",\n        query_name=\"PCDMToDeltaDestination\",\n        query_wait_interval=None,\n        merge=True,\n        try_broadcast_join=False,\n        remove_nanoseconds=False,\n        remove_duplicates=True\n    )\n\n    pcdm_to_delta_destination.write_batch()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe to be merged into a Delta Table\n        options (dict): Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for [batch](https://docs.delta.io/latest/delta-batch.html#write-to-a-table){ target=\"_blank\" } and [streaming](https://docs.delta.io/latest/delta-streaming.html#delta-table-as-a-sink){ target=\"_blank\" }.\n        destination_float (str): Either the name of the Hive Metastore or Unity Catalog Delta Table **or** the path to the Delta table to store float values.\n        destination_string (Optional str): Either the name of the Hive Metastore or Unity Catalog Delta Table **or** the path to the Delta table to store string values.\n        destination_integer (Optional str): Either the name of the Hive Metastore or Unity Catalog Delta Table **or** the path to the Delta table to store integer values\n        mode (str): Method of writing to Delta Table - append/overwrite (batch), append/complete (stream)\n        trigger (optional str): Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds\n        query_name (str): Unique name for the query in associated SparkSession\n        query_wait_interval (optional int): If set, waits for the streaming query to complete before returning. (stream) Default is None\n        merge (bool): Use Delta Merge to perform inserts, updates and deletes\n        try_broadcast_join (bool): Attempts to perform a broadcast join in the merge which can leverage data skipping using partition pruning and file pruning automatically. Can fail if dataframe being merged is large and therefore more suitable for streaming merges than batch merges\n        remove_nanoseconds (bool): Removes nanoseconds from the EventTime column and replaces with zeros\n        remove_duplicates (bool): Removes duplicates before writing the data\n\n    Attributes:\n        checkpointLocation (str): Path to checkpoint files. (Streaming)\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    options: dict\n    destination_float: str\n    destination_string: str\n    destination_integer: str\n    mode: str\n    trigger: str\n    query_name: str\n    query_wait_interval: int\n    merge: bool\n    try_broadcast_join: bool\n    remove_nanoseconds: bool\n    remove_duplicates: bool\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n        options: dict,\n        destination_float: str,\n        destination_string: str = None,\n        destination_integer: str = None,\n        mode: str = None,\n        trigger=\"10 seconds\",\n        query_name: str = \"PCDMToDeltaDestination\",\n        query_wait_interval: int = None,\n        merge: bool = True,\n        try_broadcast_join=False,\n        remove_nanoseconds: bool = False,\n        remove_duplicates: bool = True,\n    ) -&gt; None:\n        self.spark = spark\n        self.data = data\n        self.destination_float = destination_float\n        self.destination_string = destination_string\n        self.destination_integer = destination_integer\n        self.options = options\n        self.mode = mode\n        self.trigger = trigger\n        self.query_name = query_name\n        self.query_wait_interval = query_wait_interval\n        self.merge = merge\n        self.try_broadcast_join = try_broadcast_join\n        self.remove_nanoseconds = remove_nanoseconds\n        self.remove_duplicates = remove_duplicates\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_core\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def _get_eventdate_string(self, df: DataFrame) -&gt; str:\n        dates_df = df.select(\"EventDate\").distinct()\n        dates_df = dates_df.select(\n            date_format(\"EventDate\", \"yyyy-MM-dd\").alias(\"EventDate\")\n        )\n        dates_list = list(dates_df.toPandas()[\"EventDate\"])\n        return str(dates_list).replace(\"[\", \"\").replace(\"]\", \"\")\n\n    def _write_delta_merge(self, df: DataFrame, destination: str):\n        df = df.select(\n            \"EventDate\", \"TagName\", \"EventTime\", \"Status\", \"Value\", \"ChangeType\"\n        )\n        when_matched_update_list = [\n            DeltaMergeConditionValues(\n                condition=\"(source.ChangeType IN ('insert', 'update', 'upsert')) AND ((source.Status != target.Status) OR (source.Value != target.Value))\",\n                values={\n                    \"EventDate\": \"source.EventDate\",\n                    \"TagName\": \"source.TagName\",\n                    \"EventTime\": \"source.EventTime\",\n                    \"Status\": \"source.Status\",\n                    \"Value\": \"source.Value\",\n                },\n            )\n        ]\n        when_matched_delete_list = [\n            DeltaMergeCondition(condition=\"source.ChangeType = 'delete'\")\n        ]\n        when_not_matched_insert_list = [\n            DeltaMergeConditionValues(\n                condition=\"(source.ChangeType IN ('insert', 'update', 'upsert'))\",\n                values={\n                    \"EventDate\": \"source.EventDate\",\n                    \"TagName\": \"source.TagName\",\n                    \"EventTime\": \"source.EventTime\",\n                    \"Status\": \"source.Status\",\n                    \"Value\": \"source.Value\",\n                },\n            )\n        ]\n\n        merge_condition = \"source.EventDate = target.EventDate AND source.TagName = target.TagName AND source.EventTime = target.EventTime\"\n\n        perform_merge = True\n        if self.try_broadcast_join != True:\n            eventdate_string = self._get_eventdate_string(df)\n            if eventdate_string == None or eventdate_string == \"\":\n                perform_merge = False\n            else:\n                merge_condition = (\n                    \"target.EventDate in ({}) AND \".format(eventdate_string)\n                    + merge_condition\n                )\n\n        if perform_merge == True:\n            SparkDeltaMergeDestination(\n                spark=self.spark,\n                data=df,\n                destination=destination,\n                options=self.options,\n                merge_condition=merge_condition,\n                when_matched_update_list=when_matched_update_list,\n                when_matched_delete_list=when_matched_delete_list,\n                when_not_matched_insert_list=when_not_matched_insert_list,\n                try_broadcast_join=self.try_broadcast_join,\n                trigger=self.trigger,\n                query_name=self.query_name,\n            ).write_batch()\n\n    def _write_delta_batch(self, df: DataFrame, destination: str):\n        if self.merge == True:\n            if \"EventDate\" not in df.columns:\n                df = df.withColumn(\"EventDate\", date_format(\"EventTime\", \"yyyy-MM-dd\"))\n\n            self._write_delta_merge(\n                df.filter(col(\"ChangeType\").isin(\"insert\", \"update\", \"upsert\")),\n                destination,\n            )\n            self._write_delta_merge(\n                df.filter(col(\"ChangeType\") == \"delete\"), destination\n            )\n        else:\n            df = df.select(\"TagName\", \"EventTime\", \"Status\", \"Value\")\n            SparkDeltaDestination(\n                data=df,\n                destination=destination,\n                options=self.options,\n                mode=self.mode,\n                trigger=self.trigger,\n                query_name=self.query_name,\n            ).write_batch()\n\n    def _write_data_by_type(self, df: DataFrame):\n        if self.merge == True:\n            df = df.withColumn(\n                \"ChangeType\",\n                when(df[\"ChangeType\"].isin(\"insert\", \"update\"), \"upsert\").otherwise(\n                    df[\"ChangeType\"]\n                ),\n            )\n\n        if self.remove_nanoseconds == True:\n            df = df.withColumn(\n                \"EventTime\",\n                (floor(col(\"EventTime\").cast(\"double\") * 1000) / 1000).cast(\n                    \"timestamp\"\n                ),\n            )\n\n        if self.remove_duplicates == True:\n            df = df.drop_duplicates([\"TagName\", \"EventTime\", \"ChangeType\"])\n\n        float_df = df.filter(ValueTypeConstants.FLOAT_VALUE).withColumn(\n            \"Value\", col(\"Value\").cast(\"float\")\n        )\n        self._write_delta_batch(float_df, self.destination_float)\n\n        if self.destination_string != None:\n            string_df = df.filter(ValueTypeConstants.STRING_VALUE)\n            self._write_delta_batch(string_df, self.destination_string)\n\n        if self.destination_integer != None:\n            integer_df = df.filter(ValueTypeConstants.INTEGER_VALUE).withColumn(\n                \"Value\", col(\"Value\").cast(\"integer\")\n            )\n            self._write_delta_batch(integer_df, self.destination_integer)\n\n    def _write_stream_microbatches(self, df: DataFrame, epoch_id=None):  # NOSONAR\n        df.persist()\n        self._write_data_by_type(df)\n        df.unpersist()\n\n    def write_batch(self):\n        \"\"\"\n        Writes Process Control Data Model data to Delta\n        \"\"\"\n        try:\n            if self.try_broadcast_join != True:\n                self.data.persist()\n\n            self._write_data_by_type(self.data)\n\n            if self.try_broadcast_join != True:\n                self.data.unpersist()\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self):\n        \"\"\"\n        Writes streaming Process Control Data Model data to Delta using foreachBatch\n        \"\"\"\n        try:\n            TRIGGER_OPTION = (\n                {\"availableNow\": True}\n                if self.trigger == \"availableNow\"\n                else {\"processingTime\": self.trigger}\n            )\n            if self.merge == True:\n                query = (\n                    self.data.writeStream.trigger(**TRIGGER_OPTION)\n                    .format(\"delta\")\n                    .foreachBatch(self._write_stream_microbatches)\n                    .queryName(self.query_name)\n                    .outputMode(\"update\")\n                    .options(**self.options)\n                    .start()\n                )\n            else:\n                default_checkpoint_location = None\n                float_checkpoint_location = None\n                string_checkpoint_location = None\n                integer_checkpoint_location = None\n\n                append_options = self.options.copy()\n                if \"checkpointLocation\" in self.options:\n                    default_checkpoint_location = self.options[\"checkpointLocation\"]\n                    if default_checkpoint_location[-1] != \"/\":\n                        default_checkpoint_location += \"/\"\n                    float_checkpoint_location = default_checkpoint_location + \"float\"\n                    string_checkpoint_location = default_checkpoint_location + \"string\"\n                    integer_checkpoint_location = (\n                        default_checkpoint_location + \"integer\"\n                    )\n\n                if float_checkpoint_location is not None:\n                    append_options[\"checkpointLocation\"] = float_checkpoint_location\n\n                delta_float = SparkDeltaDestination(\n                    data=self.data.select(\"TagName\", \"EventTime\", \"Status\", \"Value\")\n                    .filter(ValueTypeConstants.FLOAT_VALUE)\n                    .withColumn(\"Value\", col(\"Value\").cast(\"float\")),\n                    destination=self.destination_float,\n                    options=append_options,\n                    mode=self.mode,\n                    trigger=self.trigger,\n                    query_name=self.query_name + \"_float\",\n                )\n\n                delta_float.write_stream()\n\n                if self.destination_string != None:\n                    if string_checkpoint_location is not None:\n                        append_options[\"checkpointLocation\"] = (\n                            string_checkpoint_location\n                        )\n\n                    delta_string = SparkDeltaDestination(\n                        data=self.data.select(\n                            \"TagName\", \"EventTime\", \"Status\", \"Value\"\n                        ).filter(ValueTypeConstants.STRING_VALUE),\n                        destination=self.destination_string,\n                        options=append_options,\n                        mode=self.mode,\n                        trigger=self.trigger,\n                        query_name=self.query_name + \"_string\",\n                    )\n\n                    delta_string.write_stream()\n\n                if self.destination_integer != None:\n                    if integer_checkpoint_location is not None:\n                        append_options[\"checkpointLocation\"] = (\n                            integer_checkpoint_location\n                        )\n\n                    delta_integer = SparkDeltaDestination(\n                        data=self.data.select(\"TagName\", \"EventTime\", \"Status\", \"Value\")\n                        .filter(ValueTypeConstants.INTEGER_VALUE)\n                        .withColumn(\"Value\", col(\"Value\").cast(\"integer\")),\n                        destination=self.destination_integer,\n                        options=append_options,\n                        mode=self.mode,\n                        trigger=self.trigger,\n                        query_name=self.query_name + \"_integer\",\n                    )\n\n                    delta_integer.write_stream()\n\n                if self.query_wait_interval:\n                    while self.spark.streams.active != []:\n                        for query in self.spark.streams.active:\n                            if query.lastProgress:\n                                logging.info(\n                                    \"{}: {}\".format(query.name, query.lastProgress)\n                                )\n                        time.sleep(self.query_wait_interval)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkPCDMToDeltaDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/pcdm_to_delta.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkPCDMToDeltaDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Writes Process Control Data Model data to Delta</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/pcdm_to_delta.py</code> <pre><code>def write_batch(self):\n    \"\"\"\n    Writes Process Control Data Model data to Delta\n    \"\"\"\n    try:\n        if self.try_broadcast_join != True:\n            self.data.persist()\n\n        self._write_data_by_type(self.data)\n\n        if self.try_broadcast_join != True:\n            self.data.unpersist()\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkPCDMToDeltaDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Writes streaming Process Control Data Model data to Delta using foreachBatch</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/pcdm_to_delta.py</code> <pre><code>def write_stream(self):\n    \"\"\"\n    Writes streaming Process Control Data Model data to Delta using foreachBatch\n    \"\"\"\n    try:\n        TRIGGER_OPTION = (\n            {\"availableNow\": True}\n            if self.trigger == \"availableNow\"\n            else {\"processingTime\": self.trigger}\n        )\n        if self.merge == True:\n            query = (\n                self.data.writeStream.trigger(**TRIGGER_OPTION)\n                .format(\"delta\")\n                .foreachBatch(self._write_stream_microbatches)\n                .queryName(self.query_name)\n                .outputMode(\"update\")\n                .options(**self.options)\n                .start()\n            )\n        else:\n            default_checkpoint_location = None\n            float_checkpoint_location = None\n            string_checkpoint_location = None\n            integer_checkpoint_location = None\n\n            append_options = self.options.copy()\n            if \"checkpointLocation\" in self.options:\n                default_checkpoint_location = self.options[\"checkpointLocation\"]\n                if default_checkpoint_location[-1] != \"/\":\n                    default_checkpoint_location += \"/\"\n                float_checkpoint_location = default_checkpoint_location + \"float\"\n                string_checkpoint_location = default_checkpoint_location + \"string\"\n                integer_checkpoint_location = (\n                    default_checkpoint_location + \"integer\"\n                )\n\n            if float_checkpoint_location is not None:\n                append_options[\"checkpointLocation\"] = float_checkpoint_location\n\n            delta_float = SparkDeltaDestination(\n                data=self.data.select(\"TagName\", \"EventTime\", \"Status\", \"Value\")\n                .filter(ValueTypeConstants.FLOAT_VALUE)\n                .withColumn(\"Value\", col(\"Value\").cast(\"float\")),\n                destination=self.destination_float,\n                options=append_options,\n                mode=self.mode,\n                trigger=self.trigger,\n                query_name=self.query_name + \"_float\",\n            )\n\n            delta_float.write_stream()\n\n            if self.destination_string != None:\n                if string_checkpoint_location is not None:\n                    append_options[\"checkpointLocation\"] = (\n                        string_checkpoint_location\n                    )\n\n                delta_string = SparkDeltaDestination(\n                    data=self.data.select(\n                        \"TagName\", \"EventTime\", \"Status\", \"Value\"\n                    ).filter(ValueTypeConstants.STRING_VALUE),\n                    destination=self.destination_string,\n                    options=append_options,\n                    mode=self.mode,\n                    trigger=self.trigger,\n                    query_name=self.query_name + \"_string\",\n                )\n\n                delta_string.write_stream()\n\n            if self.destination_integer != None:\n                if integer_checkpoint_location is not None:\n                    append_options[\"checkpointLocation\"] = (\n                        integer_checkpoint_location\n                    )\n\n                delta_integer = SparkDeltaDestination(\n                    data=self.data.select(\"TagName\", \"EventTime\", \"Status\", \"Value\")\n                    .filter(ValueTypeConstants.INTEGER_VALUE)\n                    .withColumn(\"Value\", col(\"Value\").cast(\"integer\")),\n                    destination=self.destination_integer,\n                    options=append_options,\n                    mode=self.mode,\n                    trigger=self.trigger,\n                    query_name=self.query_name + \"_integer\",\n                )\n\n                delta_integer.write_stream()\n\n            if self.query_wait_interval:\n                while self.spark.streams.active != []:\n                    for query in self.spark.streams.active:\n                        if query.lastProgress:\n                            logging.info(\n                                \"{}: {}\".format(query.name, query.lastProgress)\n                            )\n                    time.sleep(self.query_wait_interval)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkPCDMLatestToDeltaDestination","title":"<code>SparkPCDMLatestToDeltaDestination</code>","text":"<p>               Bases: <code>DestinationInterface</code></p> <p>The Process Control Data Model Latest Values written to Delta.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkPCDMLatestToDeltaDestination--example","title":"Example","text":"<p><pre><code>#PCDM Latest To Delta Destination for Streaming Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkPCDMLatestToDeltaDestination\n\npcdm_latest_to_delta_destination = SparkPCDMLatestToDeltaDestination(\n    data=df,\n    options={\n        \"checkpointLocation\": \"{/CHECKPOINT-LOCATION/}\"\n    },\n    destination=\"{DELTA_TABLE_PATH}\",\n    mode=\"append\",\n    trigger=\"10 seconds\",\n    query_name=\"PCDMLatestToDeltaDestination\",\n    query_wait_interval=None\n)\n\npcdm_latest_to_delta_destination.write_stream()\n</code></pre> <pre><code>#PCDM Latest To Delta Destination for Batch Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkPCDMLatestToDeltaDestination\n\npcdm_latest_to_delta_destination = SparkPCDMLatestToDeltaDestination(\n    data=df,\n    options={\n        \"maxRecordsPerFile\", \"10000\"\n    },\n    destination=\"{DELTA_TABLE_PATH}\",\n    mode=\"overwrite\",\n    trigger=\"10 seconds\",\n    query_name=\"PCDMLatestToDeltaDestination\",\n    query_wait_interval=None\n)\n\npcdm_latest_to_delta_destination.write_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to be merged into a Delta Table</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for batch and streaming.</p> required <code>destination</code> <code>str</code> <p>Either the name of the Hive Metastore or Unity Catalog Delta Table or the path to the Delta table to store the latest values</p> required <code>mode</code> <code>str</code> <p>Method of writing to Delta Table - append/overwrite (batch), append/complete (stream)</p> <code>None</code> <code>trigger</code> <code>optional str</code> <p>Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds</p> <code>'10 seconds'</code> <code>query_name</code> <code>str</code> <p>Unique name for the query in associated SparkSession</p> <code>'PCDMLatestToDeltaDestination'</code> <code>query_wait_interval</code> <code>optional int</code> <p>If set, waits for the streaming query to complete before returning. (stream) Default is None</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>checkpointLocation</code> <code>str</code> <p>Path to checkpoint files. (Streaming)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/pcdm_latest_to_delta.py</code> <pre><code>class SparkPCDMLatestToDeltaDestination(DestinationInterface):\n    \"\"\"\n    The Process Control Data Model Latest Values written to Delta.\n\n    Example\n    --------\n    ```python\n    #PCDM Latest To Delta Destination for Streaming Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkPCDMLatestToDeltaDestination\n\n    pcdm_latest_to_delta_destination = SparkPCDMLatestToDeltaDestination(\n        data=df,\n        options={\n            \"checkpointLocation\": \"{/CHECKPOINT-LOCATION/}\"\n        },\n        destination=\"{DELTA_TABLE_PATH}\",\n        mode=\"append\",\n        trigger=\"10 seconds\",\n        query_name=\"PCDMLatestToDeltaDestination\",\n        query_wait_interval=None\n    )\n\n    pcdm_latest_to_delta_destination.write_stream()\n    ```\n    ```python\n    #PCDM Latest To Delta Destination for Batch Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkPCDMLatestToDeltaDestination\n\n    pcdm_latest_to_delta_destination = SparkPCDMLatestToDeltaDestination(\n        data=df,\n        options={\n            \"maxRecordsPerFile\", \"10000\"\n        },\n        destination=\"{DELTA_TABLE_PATH}\",\n        mode=\"overwrite\",\n        trigger=\"10 seconds\",\n        query_name=\"PCDMLatestToDeltaDestination\",\n        query_wait_interval=None\n    )\n\n    pcdm_latest_to_delta_destination.write_batch()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe to be merged into a Delta Table\n        options (dict): Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for [batch](https://docs.delta.io/latest/delta-batch.html#write-to-a-table){ target=\"_blank\" } and [streaming](https://docs.delta.io/latest/delta-streaming.html#delta-table-as-a-sink){ target=\"_blank\" }.\n        destination (str): Either the name of the Hive Metastore or Unity Catalog Delta Table **or** the path to the Delta table to store the latest values\n        mode (str): Method of writing to Delta Table - append/overwrite (batch), append/complete (stream)\n        trigger (optional str): Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds\n        query_name (str): Unique name for the query in associated SparkSession\n        query_wait_interval (optional int): If set, waits for the streaming query to complete before returning. (stream) Default is None\n\n    Attributes:\n        checkpointLocation (str): Path to checkpoint files. (Streaming)\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    options: dict\n    destination: str\n    mode: str\n    trigger: str\n    query_name: str\n    query_wait_interval: int\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n        options: dict,\n        destination: str,\n        mode: str = None,\n        trigger=\"10 seconds\",\n        query_name: str = \"PCDMLatestToDeltaDestination\",\n        query_wait_interval: int = None,\n    ) -&gt; None:\n        self.spark = spark\n        self.data = data\n        self.destination = destination\n        self.options = options\n        self.mode = mode\n        self.trigger = trigger\n        self.query_name = query_name\n        self.query_wait_interval = query_wait_interval\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_core\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def _write_latest_to_delta(self, df: DataFrame, epoch_id=None):  # NOSONAR\n        df.persist()\n\n        latest_df = (\n            df.withColumn(\n                \"Latest\",\n                max(struct(\"EventTime\", \"Status\")).over(Window.partitionBy(\"TagName\")),\n            )\n            .withColumn(\n                \"GoodLatest\",\n                when(\n                    col(\"Latest.Status\") == \"Good\",\n                    struct(col(\"EventTime\"), col(\"Value\"), col(\"ValueType\")),\n                ).otherwise(\n                    max(\n                        when(\n                            col(\"Status\") == \"Good\",\n                            struct(\"EventTime\", \"Value\", \"ValueType\"),\n                        )\n                    ).over(Window.partitionBy(\"TagName\"))\n                ),\n            )\n            .filter(col(\"EventTime\") == col(\"Latest.EventTime\"))\n            .drop(\"Latest\")\n            .dropDuplicates([\"TagName\"])\n        )\n\n        when_matched_update_list = [\n            DeltaMergeConditionValues(\n                condition=\"source.EventTime &gt; target.EventTime AND (source.GoodLatest.EventTime IS NULL OR source.GoodLatest.EventTime &lt;= target.GoodEventTime)\",\n                values={\n                    \"EventTime\": \"source.EventTime\",\n                    \"Status\": \"source.Status\",\n                    \"Value\": \"source.Value\",\n                    \"ValueType\": \"source.ValueType\",\n                },\n            ),\n            DeltaMergeConditionValues(\n                condition=\"source.EventTime &gt; target.EventTime AND (source.GoodLatest.EventTime IS NOT NULL AND (source.GoodLatest.EventTime &gt; target.GoodEventTime OR target.GoodEventTime IS NULL))\",\n                values={\n                    \"EventTime\": \"source.EventTime\",\n                    \"Status\": \"source.Status\",\n                    \"Value\": \"source.Value\",\n                    \"ValueType\": \"source.ValueType\",\n                    \"GoodEventTime\": \"source.GoodLatest.EventTime\",\n                    \"GoodValue\": \"source.GoodLatest.Value\",\n                    \"GoodValueType\": \"source.GoodLatest.ValueType\",\n                },\n            ),\n            DeltaMergeConditionValues(\n                condition=\"source.EventTime &lt;= target.EventTime AND (source.GoodLatest.EventTime IS NOT NULL AND (source.GoodLatest.EventTime &gt; target.GoodEventTime OR target.GoodEventTime IS NULL))\",\n                values={\n                    \"GoodEventTime\": \"source.GoodLatest.EventTime\",\n                    \"GoodValue\": \"source.GoodLatest.Value\",\n                    \"GoodValueType\": \"source.GoodLatest.ValueType\",\n                },\n            ),\n        ]\n\n        when_not_matched_insert_list = [\n            DeltaMergeConditionValues(\n                values={\n                    \"TagName\": \"source.TagName\",\n                    \"EventTime\": \"source.EventTime\",\n                    \"Status\": \"source.Status\",\n                    \"Value\": \"source.Value\",\n                    \"ValueType\": \"source.ValueType\",\n                    \"GoodEventTime\": \"source.GoodLatest.EventTime\",\n                    \"GoodValue\": \"source.GoodLatest.Value\",\n                    \"GoodValueType\": \"source.GoodLatest.ValueType\",\n                },\n            )\n        ]\n\n        merge_condition = \"source.TagName = target.TagName\"\n\n        SparkDeltaMergeDestination(\n            spark=self.spark,\n            data=latest_df,\n            destination=self.destination,\n            options=self.options,\n            merge_condition=merge_condition,\n            when_matched_update_list=when_matched_update_list,\n            when_not_matched_insert_list=when_not_matched_insert_list,\n            trigger=self.trigger,\n            query_name=self.query_name,\n        ).write_batch()\n\n        df.unpersist()\n\n    def write_batch(self):\n        \"\"\"\n        Writes Process Control Data Model data to Delta\n        \"\"\"\n        try:\n            self._write_latest_to_delta(self.data)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self):\n        \"\"\"\n        Writes streaming Process Control Data Model data to Delta using foreachBatch\n        \"\"\"\n        try:\n            TRIGGER_OPTION = (\n                {\"availableNow\": True}\n                if self.trigger == \"availableNow\"\n                else {\"processingTime\": self.trigger}\n            )\n\n            query = (\n                self.data.writeStream.trigger(**TRIGGER_OPTION)\n                .format(\"delta\")\n                .foreachBatch(self._write_latest_to_delta)\n                .queryName(self.query_name)\n                .outputMode(\"append\")\n                .options(**self.options)\n                .start()\n            )\n\n            if self.query_wait_interval:\n                while query.isActive:\n                    if query.lastProgress:\n                        logging.info(query.lastProgress)\n                    time.sleep(self.query_wait_interval)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkPCDMLatestToDeltaDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/pcdm_latest_to_delta.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkPCDMLatestToDeltaDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Writes Process Control Data Model data to Delta</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/pcdm_latest_to_delta.py</code> <pre><code>def write_batch(self):\n    \"\"\"\n    Writes Process Control Data Model data to Delta\n    \"\"\"\n    try:\n        self._write_latest_to_delta(self.data)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkPCDMLatestToDeltaDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Writes streaming Process Control Data Model data to Delta using foreachBatch</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/pcdm_latest_to_delta.py</code> <pre><code>def write_stream(self):\n    \"\"\"\n    Writes streaming Process Control Data Model data to Delta using foreachBatch\n    \"\"\"\n    try:\n        TRIGGER_OPTION = (\n            {\"availableNow\": True}\n            if self.trigger == \"availableNow\"\n            else {\"processingTime\": self.trigger}\n        )\n\n        query = (\n            self.data.writeStream.trigger(**TRIGGER_OPTION)\n            .format(\"delta\")\n            .foreachBatch(self._write_latest_to_delta)\n            .queryName(self.query_name)\n            .outputMode(\"append\")\n            .options(**self.options)\n            .start()\n        )\n\n        if self.query_wait_interval:\n            while query.isActive:\n                if query.lastProgress:\n                    logging.info(query.lastProgress)\n                time.sleep(self.query_wait_interval)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKafkaEventhubDestination","title":"<code>SparkKafkaEventhubDestination</code>","text":"<p>               Bases: <code>DestinationInterface</code></p> <p>This Spark Destination class is used to write batch or streaming data to an Eventhub using the Kafka protocol. This enables Eventhubs to be used as a destination in applications like Delta Live Tables or Databricks Serverless Jobs as the Spark Eventhubs JAR is not supported in these scenarios.</p> <p>Default settings will be specified if not provided in the <code>options</code> parameter:</p> <ul> <li><code>kafka.sasl.mechanism</code> will be set to <code>PLAIN</code></li> <li><code>kafka.security.protocol</code> will be set to <code>SASL_SSL</code></li> <li><code>kafka.request.timeout.ms</code> will be set to <code>60000</code></li> <li><code>kafka.session.timeout.ms</code> will be set to <code>60000</code></li> </ul>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKafkaEventhubDestination--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.destinations import SparkKafkaEventhubDestination\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconnectionString = Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\n\neventhub_destination = SparkKafkaEventhubDestination(\n    spark=spark,\n    data=df,\n    options={\n        \"kafka.bootstrap.servers\": \"host1:port1,host2:port2\"\n    },\n    connection_string=\"{YOUR-EVENTHUB-CONNECTION-STRING}\",\n    consumer_group=\"{YOUR-EVENTHUB-CONSUMER-GROUP}\",\n    trigger=\"10 seconds\",\n    query_name=\"KafkaEventhubDestination\",\n    query_wait_interval=None\n)\n\neventhub_destination.write_stream()\n\nOR\n\neventhub_destination.write_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session</p> required <code>data</code> <code>DataFrame</code> <p>Any columns not listed in the required schema here will be merged into a single column named \"value\", or ignored if \"value\" is an existing column</p> required <code>connection_string</code> <code>str</code> <p>Eventhubs connection string is required to connect to the Eventhubs service. This must include the Eventhub name as the <code>EntityPath</code> parameter. Example <code>\"Endpoint=sb://test.servicebus.windows.net/;SharedAccessKeyName=test;SharedAccessKey=test_key;EntityPath=test_eventhub\"</code></p> required <code>options</code> <code>dict</code> <p>A dictionary of Kafka configurations (See Attributes tables below)</p> required <code>consumer_group</code> <code>str</code> <p>The Eventhub consumer group to use for the connection</p> required <code>trigger</code> <code>optional str</code> <p>Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds</p> <code>'10 seconds'</code> <code>query_name</code> <code>optional str</code> <p>Unique name for the query in associated SparkSession</p> <code>'KafkaEventhubDestination'</code> <code>query_wait_interval</code> <code>optional int</code> <p>If set, waits for the streaming query to complete before returning. (stream) Default is None</p> <code>None</code> <p>The following are commonly used parameters that may be included in the options dict. kafka.bootstrap.servers is the only required config. A full list of configs can be found here</p> <p>Attributes:</p> Name Type Description <code>kafka.bootstrap.servers</code> <code>A comma-separated list of host\ufe30port</code> <p>The Kafka \"bootstrap.servers\" configuration. (Streaming and Batch)</p> <code>topic</code> <code>string</code> <p>Required if there is no existing topic column in your DataFrame. Sets the topic that all rows will be written to in Kafka. (Streaming and Batch)</p> <code>includeHeaders</code> <code>bool</code> <p>Determines whether to include the Kafka headers in the row; defaults to False. (Streaming and Batch)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kafka_eventhub.py</code> <pre><code>class SparkKafkaEventhubDestination(DestinationInterface):\n    \"\"\"\n    This Spark Destination class is used to write batch or streaming data to an Eventhub using the Kafka protocol. This enables Eventhubs to be used as a destination in applications like Delta Live Tables or Databricks Serverless Jobs as the Spark Eventhubs JAR is not supported in these scenarios.\n\n    Default settings will be specified if not provided in the `options` parameter:\n\n    - `kafka.sasl.mechanism` will be set to `PLAIN`\n    - `kafka.security.protocol` will be set to `SASL_SSL`\n    - `kafka.request.timeout.ms` will be set to `60000`\n    - `kafka.session.timeout.ms` will be set to `60000`\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.destinations import SparkKafkaEventhubDestination\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    connectionString = Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\n\n    eventhub_destination = SparkKafkaEventhubDestination(\n        spark=spark,\n        data=df,\n        options={\n            \"kafka.bootstrap.servers\": \"host1:port1,host2:port2\"\n        },\n        connection_string=\"{YOUR-EVENTHUB-CONNECTION-STRING}\",\n        consumer_group=\"{YOUR-EVENTHUB-CONSUMER-GROUP}\",\n        trigger=\"10 seconds\",\n        query_name=\"KafkaEventhubDestination\",\n        query_wait_interval=None\n    )\n\n    eventhub_destination.write_stream()\n\n    OR\n\n    eventhub_destination.write_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session\n        data (DataFrame): Any columns not listed in the required schema [here](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#writing-data-to-kafka){ target=\"_blank\" } will be merged into a single column named \"value\", or ignored if \"value\" is an existing column\n        connection_string (str): Eventhubs connection string is required to connect to the Eventhubs service. This must include the Eventhub name as the `EntityPath` parameter. Example `\"Endpoint=sb://test.servicebus.windows.net/;SharedAccessKeyName=test;SharedAccessKey=test_key;EntityPath=test_eventhub\"`\n        options (dict): A dictionary of Kafka configurations (See Attributes tables below)\n        consumer_group (str): The Eventhub consumer group to use for the connection\n        trigger (optional str): Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds\n        query_name (optional str): Unique name for the query in associated SparkSession\n        query_wait_interval (optional int): If set, waits for the streaming query to complete before returning. (stream) Default is None\n\n    The following are commonly used parameters that may be included in the options dict. kafka.bootstrap.servers is the only required config. A full list of configs can be found [here](https://kafka.apache.org/documentation/#producerconfigs){ target=\"_blank\" }\n\n    Attributes:\n        kafka.bootstrap.servers (A comma-separated list of host\ufe30port):  The Kafka \"bootstrap.servers\" configuration. (Streaming and Batch)\n        topic (string): Required if there is no existing topic column in your DataFrame. Sets the topic that all rows will be written to in Kafka. (Streaming and Batch)\n        includeHeaders (bool): Determines whether to include the Kafka headers in the row; defaults to False. (Streaming and Batch)\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    connection_string: str\n    options: dict\n    consumer_group: str\n    trigger: str\n    query_name: str\n    connection_string_properties: dict\n    query_wait_interval: int\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n        connection_string: str,\n        options: dict,\n        consumer_group: str,\n        trigger: str = \"10 seconds\",\n        query_name: str = \"KafkaEventhubDestination\",\n        query_wait_interval: int = None,\n    ) -&gt; None:\n        self.spark = spark\n        self.data = data\n        self.connection_string = connection_string\n        self.options = options\n        self.consumer_group = consumer_group\n        self.trigger = trigger\n        self.query_name = query_name\n        self.connection_string_properties = self._parse_connection_string(\n            connection_string\n        )\n        self.options = self._configure_options(options)\n        self.query_wait_interval = query_wait_interval\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        spark_libraries = Libraries()\n        spark_libraries.add_maven_library(get_default_package(\"spark_sql_kafka\"))\n        return spark_libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self) -&gt; bool:\n        return True\n\n    def post_write_validation(self) -&gt; bool:\n        return True\n\n    # Code is from Azure Eventhub Python SDK. Will import the package if possible with Conda in the  conda-forge channel in the future\n    def _parse_connection_string(self, connection_string: str):\n        conn_settings = [s.split(\"=\", 1) for s in connection_string.split(\";\")]\n        if any(len(tup) != 2 for tup in conn_settings):\n            raise ValueError(\"Connection string is either blank or malformed.\")\n        conn_settings = dict(conn_settings)\n        shared_access_signature = None\n        for key, value in conn_settings.items():\n            if key.lower() == \"sharedaccesssignature\":\n                shared_access_signature = value\n        shared_access_key = conn_settings.get(\"SharedAccessKey\")\n        shared_access_key_name = conn_settings.get(\"SharedAccessKeyName\")\n        if any([shared_access_key, shared_access_key_name]) and not all(\n            [shared_access_key, shared_access_key_name]\n        ):\n            raise ValueError(\n                \"Connection string must have both SharedAccessKeyName and SharedAccessKey.\"\n            )\n        if shared_access_signature is not None and shared_access_key is not None:\n            raise ValueError(\n                \"Only one of the SharedAccessKey or SharedAccessSignature must be present.\"\n            )\n        endpoint = conn_settings.get(\"Endpoint\")\n        if not endpoint:\n            raise ValueError(\"Connection string is either blank or malformed.\")\n        parsed = urlparse(endpoint.rstrip(\"/\"))\n        if not parsed.netloc:\n            raise ValueError(\"Invalid Endpoint on the Connection String.\")\n        namespace = parsed.netloc.strip()\n        properties = {\n            \"fully_qualified_namespace\": namespace,\n            \"endpoint\": endpoint,\n            \"eventhub_name\": conn_settings.get(\"EntityPath\"),\n            \"shared_access_signature\": shared_access_signature,\n            \"shared_access_key_name\": shared_access_key_name,\n            \"shared_access_key\": shared_access_key,\n        }\n        return properties\n\n    def _connection_string_builder(self, properties: dict) -&gt; str:\n        connection_string = \"Endpoint=\" + properties.get(\"endpoint\") + \";\"\n\n        if properties.get(\"shared_access_key\"):\n            connection_string += (\n                \"SharedAccessKey=\" + properties.get(\"shared_access_key\") + \";\"\n            )\n\n        if properties.get(\"shared_access_key_name\"):\n            connection_string += (\n                \"SharedAccessKeyName=\" + properties.get(\"shared_access_key_name\") + \";\"\n            )\n\n        if properties.get(\"shared_access_signature\"):\n            connection_string += (\n                \"SharedAccessSignature=\"\n                + properties.get(\"shared_access_signature\")\n                + \";\"\n            )\n        return connection_string\n\n    def _configure_options(self, options: dict) -&gt; dict:\n        if \"topic\" not in options:\n            options[\"topic\"] = self.connection_string_properties.get(\"eventhub_name\")\n\n        if \"kafka.bootstrap.servers\" not in options:\n            options[\"kafka.bootstrap.servers\"] = (\n                self.connection_string_properties.get(\"fully_qualified_namespace\")\n                + \":9093\"\n            )\n\n        if \"kafka.sasl.mechanism\" not in options:\n            options[\"kafka.sasl.mechanism\"] = \"PLAIN\"\n\n        if \"kafka.security.protocol\" not in options:\n            options[\"kafka.security.protocol\"] = \"SASL_SSL\"\n\n        if \"kafka.sasl.jaas.config\" not in options:\n            kafka_package = \"org.apache.kafka.common.security.plain.PlainLoginModule\"\n            if \"DATABRICKS_RUNTIME_VERSION\" in os.environ or (\n                \"_client\" in self.spark.__dict__\n                and \"databricks\" in self.spark.client.host\n            ):\n                kafka_package = \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule\"\n            connection_string = self._connection_string_builder(\n                self.connection_string_properties\n            )\n            options[\"kafka.sasl.jaas.config\"] = (\n                '{} required username=\"$ConnectionString\" password=\"{}\";'.format(\n                    kafka_package, connection_string\n                )\n            )  # NOSONAR\n\n        if \"kafka.request.timeout.ms\" not in options:\n            options[\"kafka.request.timeout.ms\"] = \"60000\"\n\n        if \"kafka.session.timeout.ms\" not in options:\n            options[\"kafka.session.timeout.ms\"] = \"60000\"\n\n        if \"kafka.group.id\" not in options:\n            options[\"kafka.group.id\"] = self.consumer_group\n\n        options[\"includeHeaders\"] = \"true\"\n\n        return options\n\n    def _transform_to_eventhub_schema(self, df: DataFrame) -&gt; DataFrame:\n        column_list = [\"key\", \"headers\", \"topic\", \"partition\"]\n        if \"value\" not in df.columns:\n            df = df.withColumn(\n                \"value\",\n                to_json(\n                    struct(\n                        [\n                            col(column).alias(column)\n                            for column in df.columns\n                            if column not in column_list\n                        ]\n                    )\n                ),\n            )\n        if \"headers\" in df.columns and (\n            df.schema[\"headers\"].dataType.elementType[\"key\"].nullable == True\n            or df.schema[\"headers\"].dataType.elementType[\"value\"].nullable == True\n        ):\n            raise ValueError(\"key and value in the headers column cannot be nullable\")\n\n        return df.select(\n            [\n                column\n                for column in df.columns\n                if column in [\"value\", \"key\", \"headers\", \"topic\", \"partition\"]\n            ]\n        )\n\n    def write_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Reads batch data from Kafka.\n        \"\"\"\n        try:\n            df = self._transform_to_eventhub_schema(self.data)\n            df.write.format(\"kafka\").options(**self.options).save()\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self) -&gt; DataFrame:\n        \"\"\"\n        Reads streaming data from Kafka.\n        \"\"\"\n        try:\n            df = self._transform_to_eventhub_schema(self.data)\n            TRIGGER_OPTION = (\n                {\"availableNow\": True}\n                if self.trigger == \"availableNow\"\n                else {\"processingTime\": self.trigger}\n            )\n            query = (\n                df.writeStream.trigger(**TRIGGER_OPTION)\n                .format(\"kafka\")\n                .options(**self.options)\n                .queryName(self.query_name)\n                .start()\n            )\n\n            if self.query_wait_interval:\n                while query.isActive:\n                    if query.lastProgress:\n                        logging.info(query.lastProgress)\n                    time.sleep(self.query_wait_interval)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKafkaEventhubDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kafka_eventhub.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKafkaEventhubDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Reads batch data from Kafka.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kafka_eventhub.py</code> <pre><code>def write_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Reads batch data from Kafka.\n    \"\"\"\n    try:\n        df = self._transform_to_eventhub_schema(self.data)\n        df.write.format(\"kafka\").options(**self.options).save()\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkKafkaEventhubDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Reads streaming data from Kafka.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kafka_eventhub.py</code> <pre><code>def write_stream(self) -&gt; DataFrame:\n    \"\"\"\n    Reads streaming data from Kafka.\n    \"\"\"\n    try:\n        df = self._transform_to_eventhub_schema(self.data)\n        TRIGGER_OPTION = (\n            {\"availableNow\": True}\n            if self.trigger == \"availableNow\"\n            else {\"processingTime\": self.trigger}\n        )\n        query = (\n            df.writeStream.trigger(**TRIGGER_OPTION)\n            .format(\"kafka\")\n            .options(**self.options)\n            .queryName(self.query_name)\n            .start()\n        )\n\n        if self.query_wait_interval:\n            while query.isActive:\n                if query.lastProgress:\n                    logging.info(query.lastProgress)\n                time.sleep(self.query_wait_interval)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.EVMContractDestination","title":"<code>EVMContractDestination</code>","text":"<p>               Bases: <code>DestinationInterface</code></p> <p>The EVM Contract Destination is used to write to a smart contract blockchain.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.EVMContractDestination--examples","title":"Examples","text":"<pre><code>from rtdip_sdk.pipelines.destinations import EVMContractDestination\n\nevm_contract_destination = EVMContractDestination(\n    url=\"https://polygon-mumbai.g.alchemy.com/v2/\u27e8API_KEY\u27e9\",\n    account=\"{ACCOUNT-ADDRESS}\",\n    private_key=\"{PRIVATE-KEY}\",\n    abi=\"{SMART-CONTRACT'S-ABI}\",\n    contract=\"{SMART-CONTRACT-ADDRESS}\",\n    function_name=\"{SMART-CONTRACT-FUNCTION}\",\n    function_params=({PARAMETER_1}, {PARAMETER_2}, {PARAMETER_3}),\n    transaction={'gas': {GAS}, 'gasPrice': {GAS-PRICE}},\n)\n\nevm_contract_destination.write_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Blockchain network URL e.g. 'https://polygon-mumbai.g.alchemy.com/v2/\u27e8API_KEY\u27e9'</p> required <code>account</code> <code>str</code> <p>Address of the sender that will be signing the transaction.</p> required <code>private_key</code> <code>str</code> <p>Private key for your blockchain account.</p> required <code>abi</code> <code>json str</code> <p>Smart contract's ABI.</p> required <code>contract</code> <code>str</code> <p>Address of the smart contract.</p> <code>None</code> <code>function_name</code> <code>str</code> <p>Smart contract method to call on.</p> <code>None</code> <code>function_params</code> <code>tuple</code> <p>Parameters of given function.</p> <code>None</code> <code>transaction</code> <code>dict</code> <p>A dictionary containing a set of instructions to interact with a smart contract deployed on the blockchain (See common parameters in Attributes table below).</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>data</code> <code>hexadecimal str</code> <p>Additional information store in the transaction.</p> <code>from</code> <code>hexadecimal str</code> <p>Address of sender for a transaction.</p> <code>gas</code> <code>int</code> <p>Amount of gas units to perform a transaction.</p> <code>gasPrice</code> <code>int Wei</code> <p>Price to pay for each unit of gas. Integers are specified in Wei, web3's to_wei function can be used to specify the amount in a different currency.</p> <code>nonce</code> <code>int</code> <p>The number of transactions sent from a given address.</p> <code>to</code> <code>hexadecimal str</code> <p>Address of recipient for a transaction.</p> <code>value</code> <code>int Wei</code> <p>Value being transferred in a transaction. Integers are specified in Wei, web3's to_wei function can be used to specify the amount in a different currency.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/blockchain/evm.py</code> <pre><code>class EVMContractDestination(DestinationInterface):\n    \"\"\"\n    The EVM Contract Destination is used to write to a smart contract blockchain.\n\n    Examples\n    --------\n    ```python\n    from rtdip_sdk.pipelines.destinations import EVMContractDestination\n\n    evm_contract_destination = EVMContractDestination(\n        url=\"https://polygon-mumbai.g.alchemy.com/v2/\u27e8API_KEY\u27e9\",\n        account=\"{ACCOUNT-ADDRESS}\",\n        private_key=\"{PRIVATE-KEY}\",\n        abi=\"{SMART-CONTRACT'S-ABI}\",\n        contract=\"{SMART-CONTRACT-ADDRESS}\",\n        function_name=\"{SMART-CONTRACT-FUNCTION}\",\n        function_params=({PARAMETER_1}, {PARAMETER_2}, {PARAMETER_3}),\n        transaction={'gas': {GAS}, 'gasPrice': {GAS-PRICE}},\n    )\n\n    evm_contract_destination.write_batch()\n    ```\n\n    Parameters:\n        url (str): Blockchain network URL e.g. 'https://polygon-mumbai.g.alchemy.com/v2/\u27e8API_KEY\u27e9'\n        account (str): Address of the sender that will be signing the transaction.\n        private_key (str): Private key for your blockchain account.\n        abi (json str): Smart contract's ABI.\n        contract (str): Address of the smart contract.\n        function_name (str): Smart contract method to call on.\n        function_params (tuple): Parameters of given function.\n        transaction (dict): A dictionary containing a set of instructions to interact with a smart contract deployed on the blockchain (See common parameters in Attributes table below).\n\n    Attributes:\n        data (hexadecimal str): Additional information store in the transaction.\n        from (hexadecimal str): Address of sender for a transaction.\n        gas (int): Amount of gas units to perform a transaction.\n        gasPrice (int Wei): Price to pay for each unit of gas. Integers are specified in Wei, web3's to_wei function can be used to specify the amount in a different currency.\n        nonce (int): The number of transactions sent from a given address.\n        to (hexadecimal str): Address of recipient for a transaction.\n        value (int Wei): Value being transferred in a transaction. Integers are specified in Wei, web3's to_wei function can be used to specify the amount in a different currency.\n    \"\"\"\n\n    url: str\n    account: str\n    private_key: str\n    abi: str\n    contract: str\n    function_name: str\n    function_params: tuple\n    transaction: dict\n\n    def __init__(\n        self,\n        url: str,\n        account: str,\n        private_key: str,\n        abi: str,\n        contract: str = None,\n        function_name: str = None,\n        function_params: tuple = None,\n        transaction: dict = None,\n    ) -&gt; None:\n        self.url = url\n        self.account = account\n        self.private_key = private_key\n        self.abi = json.loads(abi)\n        self.contract = contract\n        self.function_name = function_name\n        self.function_params = function_params\n        self.transaction = transaction\n        self.web3 = Web3(Web3.HTTPProvider(self.url))\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self) -&gt; bool:\n        return True\n\n    def post_write_validation(self) -&gt; bool:\n        return True\n\n    def _process_transaction(self):\n        if \"nonce\" not in self.transaction.keys():\n            nonce = self.web3.eth.get_transaction_count(self.account)\n            self.transaction[\"nonce\"] = nonce\n        if \"from\" not in self.transaction.keys():\n            self.transaction[\"from\"] = self.account\n\n    def write_batch(self) -&gt; str:\n        \"\"\"\n        Writes to a smart contract deployed in a blockchain and returns the transaction hash.\n\n        Example:\n        ```\n        from web3 import Web3\n\n        web3 = Web3(Web3.HTTPProvider(\"https://polygon-mumbai.g.alchemy.com/v2/&lt;API_KEY&gt;\"))\n\n        x = EVMContractDestination(\n                            url=\"https://polygon-mumbai.g.alchemy.com/v2/&lt;API_KEY&gt;\",\n                            account='&lt;ACCOUNT&gt;',\n                            private_key='&lt;PRIVATE_KEY&gt;',\n                            contract='&lt;CONTRACT&gt;',\n                            function_name='transferFrom',\n                            function_params=('&lt;FROM_ACCOUNT&gt;', '&lt;TO_ACCOUNT&gt;', 0),\n                            abi = 'ABI',\n                            transaction={\n                                'gas': 100000,\n                                'gasPrice': 1000000000 # or web3.to_wei('1', 'gwei')\n                                },\n                            )\n\n        print(x.write_batch())\n        ```\n        \"\"\"\n        contract = self.web3.eth.contract(address=self.contract, abi=self.abi)\n\n        self._process_transaction()\n        tx = contract.functions[self.function_name](\n            *self.function_params\n        ).build_transaction(self.transaction)\n\n        signed_tx = self.web3.eth.account.sign_transaction(tx, self.private_key)\n        tx_hash = self.web3.eth.send_raw_transaction(signed_tx.rawTransaction)\n        self.web3.eth.wait_for_transaction_receipt(tx_hash)\n\n        return str(self.web3.to_hex(tx_hash))\n\n    def write_stream(self):\n        \"\"\"\n        Raises:\n            NotImplementedError: Write stream is not supported.\n        \"\"\"\n        raise NotImplementedError(\"EVMContractDestination only supports batch writes.\")\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.EVMContractDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Writes to a smart contract deployed in a blockchain and returns the transaction hash.</p> <p>Example: <pre><code>from web3 import Web3\n\nweb3 = Web3(Web3.HTTPProvider(\"https://polygon-mumbai.g.alchemy.com/v2/&lt;API_KEY&gt;\"))\n\nx = EVMContractDestination(\n                    url=\"https://polygon-mumbai.g.alchemy.com/v2/&lt;API_KEY&gt;\",\n                    account='&lt;ACCOUNT&gt;',\n                    private_key='&lt;PRIVATE_KEY&gt;',\n                    contract='&lt;CONTRACT&gt;',\n                    function_name='transferFrom',\n                    function_params=('&lt;FROM_ACCOUNT&gt;', '&lt;TO_ACCOUNT&gt;', 0),\n                    abi = 'ABI',\n                    transaction={\n                        'gas': 100000,\n                        'gasPrice': 1000000000 # or web3.to_wei('1', 'gwei')\n                        },\n                    )\n\nprint(x.write_batch())\n</code></pre></p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/blockchain/evm.py</code> <pre><code>def write_batch(self) -&gt; str:\n    \"\"\"\n    Writes to a smart contract deployed in a blockchain and returns the transaction hash.\n\n    Example:\n    ```\n    from web3 import Web3\n\n    web3 = Web3(Web3.HTTPProvider(\"https://polygon-mumbai.g.alchemy.com/v2/&lt;API_KEY&gt;\"))\n\n    x = EVMContractDestination(\n                        url=\"https://polygon-mumbai.g.alchemy.com/v2/&lt;API_KEY&gt;\",\n                        account='&lt;ACCOUNT&gt;',\n                        private_key='&lt;PRIVATE_KEY&gt;',\n                        contract='&lt;CONTRACT&gt;',\n                        function_name='transferFrom',\n                        function_params=('&lt;FROM_ACCOUNT&gt;', '&lt;TO_ACCOUNT&gt;', 0),\n                        abi = 'ABI',\n                        transaction={\n                            'gas': 100000,\n                            'gasPrice': 1000000000 # or web3.to_wei('1', 'gwei')\n                            },\n                        )\n\n    print(x.write_batch())\n    ```\n    \"\"\"\n    contract = self.web3.eth.contract(address=self.contract, abi=self.abi)\n\n    self._process_transaction()\n    tx = contract.functions[self.function_name](\n        *self.function_params\n    ).build_transaction(self.transaction)\n\n    signed_tx = self.web3.eth.account.sign_transaction(tx, self.private_key)\n    tx_hash = self.web3.eth.send_raw_transaction(signed_tx.rawTransaction)\n    self.web3.eth.wait_for_transaction_receipt(tx_hash)\n\n    return str(self.web3.to_hex(tx_hash))\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.EVMContractDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Write stream is not supported.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/blockchain/evm.py</code> <pre><code>def write_stream(self):\n    \"\"\"\n    Raises:\n        NotImplementedError: Write stream is not supported.\n    \"\"\"\n    raise NotImplementedError(\"EVMContractDestination only supports batch writes.\")\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PythonDeltaDestination","title":"<code>PythonDeltaDestination</code>","text":"<p>               Bases: <code>DestinationInterface</code></p> <p>The Python Delta Destination is used to write data to a Delta table from a Polars LazyFrame.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PythonDeltaDestination--example","title":"Example","text":"AzureAWS <pre><code>from rtdip_sdk.pipelines.destinations import PythonDeltaDestination\n\npath = \"abfss://{FILE-SYSTEM}@{ACCOUNT-NAME}.dfs.core.windows.net/{PATH}/{FILE-NAME}\n\npython_delta_destination = PythonDeltaDestination(\n    data=LazyFrame\n    path=path,\n    storage_options={\n        \"azure_storage_account_name\": \"{AZURE-STORAGE-ACCOUNT-NAME}\",\n        \"azure_storage_account_key\": \"{AZURE-STORAGE-ACCOUNT-KEY}\"\n    },\n    mode=:error\",\n    overwrite_schema=False,\n    delta_write_options=None\n)\n\npython_delta_destination.read_batch()\n</code></pre> <pre><code>from rtdip_sdk.pipelines.destinations import PythonDeltaDestination\n\npath = \"https://s3.{REGION-CODE}.amazonaws.com/{BUCKET-NAME}/{KEY-NAME}\"\n\npython_delta_destination = PythonDeltaDestination(\n    data=LazyFrame\n    path=path,\n    options={\n        \"aws_access_key_id\": \"{AWS-ACCESS-KEY-ID}\",\n        \"aws_secret_access_key\": \"{AWS-SECRET-ACCESS-KEY}\"\n    },\n    mode=:error\",\n    overwrite_schema=False,\n    delta_write_options=None\n)\n\npython_delta_destination.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>LazyFrame</code> <p>Polars LazyFrame to be written to Delta</p> required <code>path</code> <code>str</code> <p>Path to Delta table to be written to; either local or remote. Locally if the Table does't exist one will be created, but to write to AWS or Azure, you must have an existing Delta Table</p> required <code>options</code> <code>Optional dict</code> <p>Used if writing to a remote location. For AWS use format {\"aws_access_key_id\": \"&lt;&gt;\", \"aws_secret_access_key\": \"&lt;&gt;\"}. For Azure use format {\"azure_storage_account_name\": \"storageaccountname\", \"azure_storage_access_key\": \"&lt;&gt;\"}</p> <code>None</code> <code>mode</code> <code>Literal['error', 'append', 'overwrite', 'ignore']</code> <p>Defaults to error if table exists, 'ignore' won't write anything if table exists</p> <code>'error'</code> <code>overwrite_schema</code> <code>bool</code> <p>If True will allow for the table schema to be overwritten</p> <code>False</code> <code>delta_write_options</code> <code>dict</code> <p>Options when writing to a Delta table. See here for all options</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/python/delta.py</code> <pre><code>class PythonDeltaDestination(DestinationInterface):\n    \"\"\"\n    The Python Delta Destination is used to write data to a Delta table from a Polars LazyFrame.\n\n     Example\n    --------\n    === \"Azure\"\n\n        ```python\n        from rtdip_sdk.pipelines.destinations import PythonDeltaDestination\n\n        path = \"abfss://{FILE-SYSTEM}@{ACCOUNT-NAME}.dfs.core.windows.net/{PATH}/{FILE-NAME}\n\n        python_delta_destination = PythonDeltaDestination(\n            data=LazyFrame\n            path=path,\n            storage_options={\n                \"azure_storage_account_name\": \"{AZURE-STORAGE-ACCOUNT-NAME}\",\n                \"azure_storage_account_key\": \"{AZURE-STORAGE-ACCOUNT-KEY}\"\n            },\n            mode=:error\",\n            overwrite_schema=False,\n            delta_write_options=None\n        )\n\n        python_delta_destination.read_batch()\n\n        ```\n    === \"AWS\"\n\n        ```python\n        from rtdip_sdk.pipelines.destinations import PythonDeltaDestination\n\n        path = \"https://s3.{REGION-CODE}.amazonaws.com/{BUCKET-NAME}/{KEY-NAME}\"\n\n        python_delta_destination = PythonDeltaDestination(\n            data=LazyFrame\n            path=path,\n            options={\n                \"aws_access_key_id\": \"{AWS-ACCESS-KEY-ID}\",\n                \"aws_secret_access_key\": \"{AWS-SECRET-ACCESS-KEY}\"\n            },\n            mode=:error\",\n            overwrite_schema=False,\n            delta_write_options=None\n        )\n\n        python_delta_destination.read_batch()\n        ```\n\n    Parameters:\n        data (LazyFrame): Polars LazyFrame to be written to Delta\n        path (str): Path to Delta table to be written to; either local or [remote](https://delta-io.github.io/delta-rs/python/usage.html#loading-a-delta-table){ target=\"_blank\" }. **Locally** if the Table does't exist one will be created, but to write to AWS or Azure, you must have an existing Delta Table\n        options (Optional dict): Used if writing to a remote location. For AWS use format {\"aws_access_key_id\": \"&lt;&gt;\", \"aws_secret_access_key\": \"&lt;&gt;\"}. For Azure use format {\"azure_storage_account_name\": \"storageaccountname\", \"azure_storage_access_key\": \"&lt;&gt;\"}\n        mode (Literal['error', 'append', 'overwrite', 'ignore']): Defaults to error if table exists, 'ignore' won't write anything if table exists\n        overwrite_schema (bool): If True will allow for the table schema to be overwritten\n        delta_write_options (dict): Options when writing to a Delta table. See [here](https://delta-io.github.io/delta-rs/python/api_reference.html#writing-deltatables){ target=\"_blank\" } for all options\n    \"\"\"\n\n    data: LazyFrame\n    path: str\n    options: dict\n    mode: Literal[\"error\", \"append\", \"overwrite\", \"ignore\"]\n    overwrite_schema: bool\n    delta_write_options: dict\n\n    def __init__(\n        self,\n        data: LazyFrame,\n        path: str,\n        options: dict = None,\n        mode: Literal[\"error\", \"append\", \"overwrite\", \"ignore\"] = \"error\",\n        overwrite_schema: bool = False,\n        delta_write_options: dict = None,\n    ) -&gt; None:\n        self.data = data\n        self.path = path\n        self.options = options\n        self.mode = mode\n        self.overwrite_schema = overwrite_schema\n        self.delta_write_options = delta_write_options\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYTHON\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def write_batch(self):\n        \"\"\"\n        Writes batch data to Delta without using Spark.\n        \"\"\"\n        if isinstance(self.data, pl.LazyFrame):\n            df = self.data.collect()\n            df.write_delta(\n                self.path,\n                mode=self.mode,\n                overwrite_schema=self.overwrite_schema,\n                storage_options=self.options,\n                delta_write_options=self.delta_write_options,\n            )\n        else:\n            raise ValueError(\n                \"Data must be a Polars LazyFrame. See https://pola-rs.github.io/polars/py-polars/html/reference/lazyframe/index.html\"\n            )\n\n    def write_stream(self):\n        \"\"\"\n        Raises:\n            NotImplementedError: Writing to a Delta table using Python is only possible for batch writes. To perform a streaming read, use the write_stream method of the SparkDeltaDestination component.\n        \"\"\"\n        raise NotImplementedError(\n            \"Writing to a Delta table using Python is only possible for batch writes. To perform a streaming read, use the write_stream method of the SparkDeltaDestination component\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PythonDeltaDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYTHON</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/python/delta.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYTHON\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PythonDeltaDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Writes batch data to Delta without using Spark.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/python/delta.py</code> <pre><code>def write_batch(self):\n    \"\"\"\n    Writes batch data to Delta without using Spark.\n    \"\"\"\n    if isinstance(self.data, pl.LazyFrame):\n        df = self.data.collect()\n        df.write_delta(\n            self.path,\n            mode=self.mode,\n            overwrite_schema=self.overwrite_schema,\n            storage_options=self.options,\n            delta_write_options=self.delta_write_options,\n        )\n    else:\n        raise ValueError(\n            \"Data must be a Polars LazyFrame. See https://pola-rs.github.io/polars/py-polars/html/reference/lazyframe/index.html\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PythonDeltaDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Writing to a Delta table using Python is only possible for batch writes. To perform a streaming read, use the write_stream method of the SparkDeltaDestination component.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/python/delta.py</code> <pre><code>def write_stream(self):\n    \"\"\"\n    Raises:\n        NotImplementedError: Writing to a Delta table using Python is only possible for batch writes. To perform a streaming read, use the write_stream method of the SparkDeltaDestination component.\n    \"\"\"\n    raise NotImplementedError(\n        \"Writing to a Delta table using Python is only possible for batch writes. To perform a streaming read, use the write_stream method of the SparkDeltaDestination component\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.DatabricksSecrets","title":"<code>DatabricksSecrets</code>","text":"<p>               Bases: <code>SecretsInterface</code></p> <p>Retrieves secrets from Databricks Secret Scopes. For more information about Databricks Secret Scopes, see here.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.DatabricksSecrets--example","title":"Example","text":"<pre><code># Reads Secrets from Databricks Secret Scopes\n\nfrom rtdip_sdk.pipelines.secrets import DatabricksSecrets\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nget_databricks_secret = DatabricksSecrets(\n    spark=spark,\n    vault=\"{NAME-OF-DATABRICKS-SECRET-SCOPE}\"\n    key=\"{KEY-NAME-OF-SECRET}\",\n)\n\nget_databricks_secret.get()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from a Delta table</p> required <code>vault</code> <code>str</code> <p>Name of the Databricks Secret Scope</p> required <code>key</code> <code>str</code> <p>Name/Key of the secret in the Databricks Secret Scope</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/databricks.py</code> <pre><code>class DatabricksSecrets(SecretsInterface):\n    \"\"\"\n    Retrieves secrets from Databricks Secret Scopes. For more information about Databricks Secret Scopes, see [here.](https://docs.databricks.com/security/secrets/secret-scopes.html)\n\n    Example\n    -------\n    ```python\n    # Reads Secrets from Databricks Secret Scopes\n\n    from rtdip_sdk.pipelines.secrets import DatabricksSecrets\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    get_databricks_secret = DatabricksSecrets(\n        spark=spark,\n        vault=\"{NAME-OF-DATABRICKS-SECRET-SCOPE}\"\n        key=\"{KEY-NAME-OF-SECRET}\",\n    )\n\n    get_databricks_secret.get()\n    ```\n\n    Parameters:\n        spark: Spark Session required to read data from a Delta table\n        vault: Name of the Databricks Secret Scope\n        key: Name/Key of the secret in the Databricks Secret Scope\n    \"\"\"\n\n    spark: SparkSession\n    vault: str\n    key: str\n\n    def __init__(self, spark: SparkSession, vault: str, key: str):\n        self.spark = spark\n        self.vault = vault\n        self.key = key\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK on Databricks\n        \"\"\"\n        return SystemType.PYSPARK_DATABRICKS\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def get(self):\n        \"\"\"\n        Retrieves the secret from the Databricks Secret Scope\n        \"\"\"\n        dbutils = get_dbutils(self.spark)\n        return dbutils.secrets.get(scope=self.vault, key=self.key)\n\n    def set(self):\n        \"\"\"\n        Sets the secret in the Secret Scope\n        Raises:\n            NotImplementedError: Will be implemented at a later point in time\n        \"\"\"\n        return NotImplementedError\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.DatabricksSecrets.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK on Databricks</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/databricks.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK on Databricks\n    \"\"\"\n    return SystemType.PYSPARK_DATABRICKS\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.DatabricksSecrets.get","title":"<code>get()</code>","text":"<p>Retrieves the secret from the Databricks Secret Scope</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/databricks.py</code> <pre><code>def get(self):\n    \"\"\"\n    Retrieves the secret from the Databricks Secret Scope\n    \"\"\"\n    dbutils = get_dbutils(self.spark)\n    return dbutils.secrets.get(scope=self.vault, key=self.key)\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.DatabricksSecrets.set","title":"<code>set()</code>","text":"<p>Sets the secret in the Secret Scope Raises:     NotImplementedError: Will be implemented at a later point in time</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/databricks.py</code> <pre><code>def set(self):\n    \"\"\"\n    Sets the secret in the Secret Scope\n    Raises:\n        NotImplementedError: Will be implemented at a later point in time\n    \"\"\"\n    return NotImplementedError\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.HashiCorpVaultSecrets","title":"<code>HashiCorpVaultSecrets</code>","text":"<p>               Bases: <code>SecretsInterface</code></p> <p>Retrieves and creates/updates secrets in a Hashicorp Vault. For more information about Hashicorp Vaults, see here.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.HashiCorpVaultSecrets--example","title":"Example","text":"<p><pre><code># Retrieves Secrets from HashiCorp Vault\n\nfrom rtdip_sdk.pipelines.secrets import HashiCorpVaultSecrets\n\nget_hashicorp_secret = HashiCorpVaultSecrets(\n    vault=\"http://127.0.0.1:8200\",\n    key=\"{KEY}\",\n    secret=None,\n    credential=\"{CREDENTIAL}\",\n    kwargs=None\n)\n\nget_hashicorp_secret.get()\n</code></pre> <pre><code># Creates or Updates Secrets in Hashicorp Vault\n\nfrom rtdip_sdk.pipelines.secrets import HashiCorpVaultSecrets\n\nset_hashicorp_secret = AzureKeyVaultSecrets(\n    vault=\"http://127.0.0.1:8200\",\n    key=\"{KEY}\",\n    secret=\"{SECRET-TO-BE-SET}\",\n    credential=\"{CREDENTIAL}\",\n    kwargs=None\n)\n\nset_hashicorp_secret.set()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>vault</code> <code>str</code> <p>Hashicorp Vault URL</p> required <code>key</code> <code>str</code> <p>Name/Key of the secret in the Hashicorp Vault</p> required <code>secret</code> <code>str</code> <p>Secret or Password to be stored in the Hashicorp Vault</p> <code>None</code> <code>credential</code> <code>str</code> <p>Token for authentication with the Hashicorp Vault</p> <code>None</code> <code>kwargs</code> <code>dict</code> <p>List of additional parameters to be passed when creating a Hashicorp Vault Client. Please see here for more details on parameters that can be provided to the client</p> <code>{}</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/hashicorp_vault.py</code> <pre><code>class HashiCorpVaultSecrets(SecretsInterface):\n    \"\"\"\n    Retrieves and creates/updates secrets in a Hashicorp Vault. For more information about Hashicorp Vaults, see [here.](https://developer.hashicorp.com/vault/docs/get-started/developer-qs)\n\n    Example\n    -------\n    ```python\n    # Retrieves Secrets from HashiCorp Vault\n\n    from rtdip_sdk.pipelines.secrets import HashiCorpVaultSecrets\n\n    get_hashicorp_secret = HashiCorpVaultSecrets(\n        vault=\"http://127.0.0.1:8200\",\n        key=\"{KEY}\",\n        secret=None,\n        credential=\"{CREDENTIAL}\",\n        kwargs=None\n    )\n\n    get_hashicorp_secret.get()\n\n    ```\n    ```python\n    # Creates or Updates Secrets in Hashicorp Vault\n\n    from rtdip_sdk.pipelines.secrets import HashiCorpVaultSecrets\n\n    set_hashicorp_secret = AzureKeyVaultSecrets(\n        vault=\"http://127.0.0.1:8200\",\n        key=\"{KEY}\",\n        secret=\"{SECRET-TO-BE-SET}\",\n        credential=\"{CREDENTIAL}\",\n        kwargs=None\n    )\n\n    set_hashicorp_secret.set()\n    ```\n\n    Parameters:\n        vault (str): Hashicorp Vault URL\n        key (str): Name/Key of the secret in the Hashicorp Vault\n        secret (str): Secret or Password to be stored in the Hashicorp Vault\n        credential (str): Token for authentication with the Hashicorp Vault\n        kwargs (dict): List of additional parameters to be passed when creating a Hashicorp Vault Client. Please see [here](https://hvac.readthedocs.io/en/stable/overview.html#initialize-the-client) for more details on parameters that can be provided to the client\n    \"\"\"\n\n    vault: str\n    key: str\n    secret: str\n    credential: str\n\n    def __init__(\n        self,\n        vault: str,\n        key: str,\n        secret: str = None,\n        credential: str = None,\n        kwargs: dict = {},\n    ):  # NOSONAR\n        self.vault = vault\n        self.key = key\n        self.secret = secret\n        self.credential = credential\n        self.kwargs = kwargs\n        self.client = self._get_hvac_client()\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYTHON\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_pypi_library(get_default_package(\"hashicorp_vault\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _get_hvac_client(self):\n        return hvac.Client(url=self.vault, token=self.credential, **self.kwargs)\n\n    def get(self):\n        \"\"\"\n        Retrieves the secret from the Hashicorp Vault\n        \"\"\"\n        response = self.client.secrets.kv.read_secret_version(path=self.key)\n        return response[\"data\"][\"data\"][\"password\"]\n\n    def set(self):\n        \"\"\"\n        Creates or updates a secret in the Hashicorp Vault\n        \"\"\"\n        self.client.secrets.kv.v2.create_or_update_secret(\n            path=self.key,\n            secret=dict(password=self.secret),\n        )\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.HashiCorpVaultSecrets.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYTHON</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/hashicorp_vault.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYTHON\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.HashiCorpVaultSecrets.get","title":"<code>get()</code>","text":"<p>Retrieves the secret from the Hashicorp Vault</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/hashicorp_vault.py</code> <pre><code>def get(self):\n    \"\"\"\n    Retrieves the secret from the Hashicorp Vault\n    \"\"\"\n    response = self.client.secrets.kv.read_secret_version(path=self.key)\n    return response[\"data\"][\"data\"][\"password\"]\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.HashiCorpVaultSecrets.set","title":"<code>set()</code>","text":"<p>Creates or updates a secret in the Hashicorp Vault</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/hashicorp_vault.py</code> <pre><code>def set(self):\n    \"\"\"\n    Creates or updates a secret in the Hashicorp Vault\n    \"\"\"\n    self.client.secrets.kv.v2.create_or_update_secret(\n        path=self.key,\n        secret=dict(password=self.secret),\n    )\n    return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.AzureKeyVaultSecrets","title":"<code>AzureKeyVaultSecrets</code>","text":"<p>               Bases: <code>SecretsInterface</code></p> <p>Retrieves and creates/updates secrets in Azure Key Vault. For more information about Azure Key Vaults, see here.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.AzureKeyVaultSecrets--example","title":"Example","text":"<p><pre><code># Retrieves Secrets from Azure Key Vault\n\nfrom rtdip_sdk.pipelines.secrets import AzureKeyVaultSecrets\n\nget_key_vault_secret = AzureKeyVaultSecrets(\n    vault=\"https://{YOUR-KEY-VAULT}.azure.net/\",\n    key=\"{KEY}\",\n    secret=None,\n    credential=\"{CREDENTIAL}\",\n    kwargs=None\n)\n\nget_key_vault_secret.get()\n</code></pre> <pre><code># Creates or Updates Secrets in Azure Key Vault\n\nfrom rtdip_sdk.pipelines.secrets import AzureKeyVaultSecrets\n\nset_key_vault_secret = AzureKeyVaultSecrets(\n    vault=\"https://{YOUR-KEY-VAULT}.azure.net/\",\n    key=\"{KEY}\",\n    secret=\"{SECRET-TO-BE-SET}\",\n    credential=\"{CREDENTIAL}\",\n    kwargs=None\n)\n\nset_key_vault_secret.set()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>vault</code> <code>str</code> <p>Azure Key Vault URL</p> required <code>key</code> <code>str</code> <p>Key for the secret</p> required <code>secret</code> <code>str</code> <p>Secret or Password to be set in the Azure Key Vault</p> <code>None</code> <code>credential</code> <code>str</code> <p>Credential for authenticating with Azure Key Vault</p> <code>None</code> <code>kwargs</code> <code>dict</code> <p>List of additional parameters to be passed when creating a Azure Key Vault Client. Please see here for more details on parameters that can be provided to the client</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/azure_key_vault.py</code> <pre><code>class AzureKeyVaultSecrets(SecretsInterface):\n    \"\"\"\n    Retrieves and creates/updates secrets in Azure Key Vault. For more information about Azure Key Vaults, see [here.](https://learn.microsoft.com/en-gb/azure/key-vault/general/overview)\n\n    Example\n    -------\n    ```python\n    # Retrieves Secrets from Azure Key Vault\n\n    from rtdip_sdk.pipelines.secrets import AzureKeyVaultSecrets\n\n    get_key_vault_secret = AzureKeyVaultSecrets(\n        vault=\"https://{YOUR-KEY-VAULT}.azure.net/\",\n        key=\"{KEY}\",\n        secret=None,\n        credential=\"{CREDENTIAL}\",\n        kwargs=None\n    )\n\n    get_key_vault_secret.get()\n\n    ```\n    ```python\n    # Creates or Updates Secrets in Azure Key Vault\n\n    from rtdip_sdk.pipelines.secrets import AzureKeyVaultSecrets\n\n    set_key_vault_secret = AzureKeyVaultSecrets(\n        vault=\"https://{YOUR-KEY-VAULT}.azure.net/\",\n        key=\"{KEY}\",\n        secret=\"{SECRET-TO-BE-SET}\",\n        credential=\"{CREDENTIAL}\",\n        kwargs=None\n    )\n\n    set_key_vault_secret.set()\n    ```\n\n    Parameters:\n        vault (str): Azure Key Vault URL\n        key (str): Key for the secret\n        secret (str): Secret or Password to be set in the Azure Key Vault\n        credential (str): Credential for authenticating with Azure Key Vault\n        kwargs (dict): List of additional parameters to be passed when creating a Azure Key Vault Client. Please see [here](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/keyvault/azure-keyvault-secrets) for more details on parameters that can be provided to the client\n    \"\"\"\n\n    vault: str\n    key: str\n    secret: str\n    credential: str\n    kwargs: dict\n\n    def __init__(\n        self,\n        vault: str,\n        key: str,\n        secret: str = None,\n        credential=None,\n        kwargs: dict = None,\n    ):\n        self.vault = vault\n        self.key = key\n        self.secret = secret\n        self.credential = credential\n        self.kwargs = {} if kwargs is None else kwargs\n        self.client = self._get_akv_client()\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYTHON\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_pypi_library(get_default_package(\"azure_key_vault_secret\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _get_akv_client(self):\n        return SecretClient(\n            vault_url=\"https://{}.vault.azure.net\".format(self.vault),\n            credential=self.credential,\n            **self.kwargs\n        )\n\n    def get(self):\n        \"\"\"\n        Retrieves the secret from the Azure Key Vault\n        \"\"\"\n        response = self.client.get_secret(name=self.key)\n        return response.value\n\n    def set(self):\n        \"\"\"\n        Creates or updates a secret in the Azure Key Vault\n        \"\"\"\n        self.client.set_secret(name=self.key, value=self.secret)\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.AzureKeyVaultSecrets.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYTHON</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/azure_key_vault.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYTHON\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.AzureKeyVaultSecrets.get","title":"<code>get()</code>","text":"<p>Retrieves the secret from the Azure Key Vault</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/azure_key_vault.py</code> <pre><code>def get(self):\n    \"\"\"\n    Retrieves the secret from the Azure Key Vault\n    \"\"\"\n    response = self.client.get_secret(name=self.key)\n    return response.value\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.AzureKeyVaultSecrets.set","title":"<code>set()</code>","text":"<p>Creates or updates a secret in the Azure Key Vault</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/azure_key_vault.py</code> <pre><code>def set(self):\n    \"\"\"\n    Creates or updates a secret in the Azure Key Vault\n    \"\"\"\n    self.client.set_secret(name=self.key, value=self.secret)\n    return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SystemType","title":"<code>SystemType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>The type of the system.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/_pipeline_utils/models.py</code> <pre><code>class SystemType(Enum):\n    \"\"\"The type of the system.\"\"\"\n\n    # Executable in a python environment\n    PYTHON = 1\n    # Executable in a pyspark environment\n    PYSPARK = 2\n    # Executable in a databricks environment\n    PYSPARK_DATABRICKS = 3\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.DeltaTableCreateUtility","title":"<code>DeltaTableCreateUtility</code>","text":"<p>               Bases: <code>UtilitiesInterface</code></p> <p>Creates a Delta Table in a Hive Metastore or in Databricks Unity Catalog.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.DeltaTableCreateUtility--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.utilities.spark.delta_table_create import DeltaTableCreateUtility, DeltaTableColumn\n\ntable_create_utility = DeltaTableCreateUtility(\n    spark=spark_session,\n    table_name=\"delta_table\",\n    columns=[\n        DeltaTableColumn(name=\"EventDate\", type=\"date\", nullable=False, metadata={\"delta.generationExpression\": \"CAST(EventTime AS DATE)\"}),\n        DeltaTableColumn(name=\"TagName\", type=\"string\", nullable=False),\n        DeltaTableColumn(name=\"EventTime\", type=\"timestamp\", nullable=False),\n        DeltaTableColumn(name=\"Status\", type=\"string\", nullable=True),\n        DeltaTableColumn(name=\"Value\", type=\"float\", nullable=True)\n    ],\n    partitioned_by=[\"EventDate\"],\n    properties={\"delta.logRetentionDuration\": \"7 days\", \"delta.enableChangeDataFeed\": \"true\"},\n    comment=\"Creation of Delta Table\"\n)\n\nresult = table_create_utility.execute()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from cloud storage</p> required <code>table_name</code> <code>str</code> <p>Name of the table, including catalog and schema if table is to be created in Unity Catalog</p> required <code>columns</code> <code>list[DeltaTableColumn]</code> <p>List of columns and their related column properties</p> required <code>partitioned_by</code> <code>list[str]</code> <p>List of column names to partition the table by</p> <code>None</code> <code>location</code> <code>str</code> <p>Path to storage location</p> <code>None</code> <code>properties</code> <code>dict</code> <p>Propoerties that can be specified for a Delta Table. Further information on the options available are here</p> <code>None</code> <code>comment</code> <code>str</code> <p>Provides a comment on the table metadata</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/delta_table_create.py</code> <pre><code>class DeltaTableCreateUtility(UtilitiesInterface):\n    \"\"\"\n    Creates a Delta Table in a Hive Metastore or in Databricks Unity Catalog.\n\n    Example\n    -------\n    ```python\n    from rtdip_sdk.pipelines.utilities.spark.delta_table_create import DeltaTableCreateUtility, DeltaTableColumn\n\n    table_create_utility = DeltaTableCreateUtility(\n        spark=spark_session,\n        table_name=\"delta_table\",\n        columns=[\n            DeltaTableColumn(name=\"EventDate\", type=\"date\", nullable=False, metadata={\"delta.generationExpression\": \"CAST(EventTime AS DATE)\"}),\n            DeltaTableColumn(name=\"TagName\", type=\"string\", nullable=False),\n            DeltaTableColumn(name=\"EventTime\", type=\"timestamp\", nullable=False),\n            DeltaTableColumn(name=\"Status\", type=\"string\", nullable=True),\n            DeltaTableColumn(name=\"Value\", type=\"float\", nullable=True)\n        ],\n        partitioned_by=[\"EventDate\"],\n        properties={\"delta.logRetentionDuration\": \"7 days\", \"delta.enableChangeDataFeed\": \"true\"},\n        comment=\"Creation of Delta Table\"\n    )\n\n    result = table_create_utility.execute()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to read data from cloud storage\n        table_name (str): Name of the table, including catalog and schema if table is to be created in Unity Catalog\n        columns (list[DeltaTableColumn]): List of columns and their related column properties\n        partitioned_by (list[str], optional): List of column names to partition the table by\n        location (str, optional): Path to storage location\n        properties (dict, optional): Propoerties that can be specified for a Delta Table. Further information on the options available are [here](https://docs.databricks.com/delta/table-properties.html#delta-table-properties)\n        comment (str, optional): Provides a comment on the table metadata\n\n\n    \"\"\"\n\n    spark: SparkSession\n    table_name: str\n    columns: List[DeltaTableColumn]\n    partitioned_by: List[str]\n    location: str\n    properties: dict\n    comment: str\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        table_name: str,\n        columns: List[StructField],\n        partitioned_by: List[str] = None,\n        location: str = None,\n        properties: dict = None,\n        comment: str = None,\n    ) -&gt; None:\n        self.spark = spark\n        self.table_name = table_name\n        self.columns = columns\n        self.partitioned_by = partitioned_by\n        self.location = location\n        self.properties = properties\n        self.comment = comment\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_core\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def execute(self) -&gt; bool:\n        try:\n            columns = [StructField.fromJson(column.dict()) for column in self.columns]\n\n            delta_table = (\n                DeltaTable.createIfNotExists(self.spark)\n                .tableName(self.table_name)\n                .addColumns(columns)\n            )\n\n            if self.partitioned_by is not None:\n                delta_table = delta_table.partitionedBy(self.partitioned_by)\n\n            if self.location is not None:\n                delta_table = delta_table.location(self.location)\n\n            if self.properties is not None:\n                for key, value in self.properties.items():\n                    delta_table = delta_table.property(key, value)\n\n            if self.comment is not None:\n                delta_table = delta_table.comment(self.comment)\n\n            delta_table.execute()\n            return True\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.DeltaTableCreateUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/delta_table_create.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.DeltaTableOptimizeUtility","title":"<code>DeltaTableOptimizeUtility</code>","text":"<p>               Bases: <code>UtilitiesInterface</code></p> <p>Optimizes a Delta Table.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.DeltaTableOptimizeUtility--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.utilities.spark.delta_table_optimize import DeltaTableOptimizeUtility\n\ntable_optimize_utility = DeltaTableOptimizeUtility(\n    spark=spark_session,\n    table_name=\"delta_table\",\n    where=\"EventDate&lt;=current_date()\",\n    zorder_by=[\"EventDate\"]\n)\n\nresult = table_optimize_utility.execute()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from cloud storage</p> required <code>table_name</code> <code>str</code> <p>Name of the table, including catalog and schema if table is to be created in Unity Catalog</p> required <code>where</code> <code>str</code> <p>Apply a partition filter to limit optimize to specific partitions. Example, \"date='2021-11-18'\" or \"EventDate&lt;=current_date()\"</p> <code>None</code> <code>zorder_by</code> <code>list[str]</code> <p>List of column names to zorder the table by. For more information, see here.</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/delta_table_optimize.py</code> <pre><code>class DeltaTableOptimizeUtility(UtilitiesInterface):\n    \"\"\"\n    [Optimizes](https://docs.delta.io/latest/optimizations-oss.html) a Delta Table.\n\n    Example\n    -------\n    ```python\n    from rtdip_sdk.pipelines.utilities.spark.delta_table_optimize import DeltaTableOptimizeUtility\n\n    table_optimize_utility = DeltaTableOptimizeUtility(\n        spark=spark_session,\n        table_name=\"delta_table\",\n        where=\"EventDate&lt;=current_date()\",\n        zorder_by=[\"EventDate\"]\n    )\n\n    result = table_optimize_utility.execute()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to read data from cloud storage\n        table_name (str): Name of the table, including catalog and schema if table is to be created in Unity Catalog\n        where (str, optional): Apply a partition filter to limit optimize to specific partitions. Example, \"date='2021-11-18'\" or \"EventDate&lt;=current_date()\"\n        zorder_by (list[str], optional): List of column names to zorder the table by. For more information, see [here.](https://docs.delta.io/latest/optimizations-oss.html#optimize-performance-with-file-management&amp;language-python)\n    \"\"\"\n\n    spark: SparkSession\n    table_name: str\n    where: Optional[str]\n    zorder_by: Optional[List[str]]\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        table_name: str,\n        where: str = None,\n        zorder_by: List[str] = None,\n    ) -&gt; None:\n        self.spark = spark\n        self.table_name = table_name\n        self.where = where\n        self.zorder_by = zorder_by\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_core\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def execute(self) -&gt; bool:\n        try:\n            delta_table = DeltaTable.forName(self.spark, self.table_name).optimize()\n\n            if self.where is not None:\n                delta_table = delta_table.where(self.where)\n\n            if self.zorder_by is not None:\n                delta_table = delta_table.executeZOrderBy(self.zorder_by)\n            else:\n                delta_table.executeCompaction()\n\n            return True\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.DeltaTableOptimizeUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/delta_table_optimize.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.DeltaTableVacuumUtility","title":"<code>DeltaTableVacuumUtility</code>","text":"<p>               Bases: <code>UtilitiesInterface</code></p> <p>Vacuums a Delta Table.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.DeltaTableVacuumUtility--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.utilities.spark.delta_table_vacuum import DeltaTableVacuumUtility\n\ntable_vacuum_utility =  DeltaTableVacuumUtility(\n    spark=spark_session,\n    table_name=\"delta_table\",\n    retention_hours=\"168\"\n)\n\nresult = table_vacuum_utility.execute()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from cloud storage</p> required <code>table_name</code> <code>str</code> <p>Name of the table, including catalog and schema if table is to be created in Unity Catalog</p> required <code>retention_hours</code> <code>int</code> <p>Sets the retention threshold in hours.</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/delta_table_vacuum.py</code> <pre><code>class DeltaTableVacuumUtility(UtilitiesInterface):\n    \"\"\"\n    [Vacuums](https://docs.delta.io/latest/delta-utility.html#-delta-vacuum) a Delta Table.\n\n    Example\n    -------\n    ```python\n    from rtdip_sdk.pipelines.utilities.spark.delta_table_vacuum import DeltaTableVacuumUtility\n\n    table_vacuum_utility =  DeltaTableVacuumUtility(\n        spark=spark_session,\n        table_name=\"delta_table\",\n        retention_hours=\"168\"\n    )\n\n    result = table_vacuum_utility.execute()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to read data from cloud storage\n        table_name (str): Name of the table, including catalog and schema if table is to be created in Unity Catalog\n        retention_hours (int, optional): Sets the retention threshold in hours.\n    \"\"\"\n\n    spark: SparkSession\n    table_name: str\n    retention_hours: Optional[int]\n\n    def __init__(\n        self, spark: SparkSession, table_name: str, retention_hours: int = None\n    ) -&gt; None:\n        self.spark = spark\n        self.table_name = table_name\n        self.retention_hours = retention_hours\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_core\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def execute(self) -&gt; bool:\n        try:\n            delta_table = DeltaTable.forName(self.spark, self.table_name)\n\n            delta_table.vacuum(retentionHours=self.retention_hours)\n\n            return True\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.DeltaTableVacuumUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/delta_table_vacuum.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkConfigurationUtility","title":"<code>SparkConfigurationUtility</code>","text":"<p>               Bases: <code>UtilitiesInterface</code></p> <p>Sets configuration key value pairs to a Spark Session</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkConfigurationUtility--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import SparkConfigurationUtility\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconfiguration_utility = SparkConfigurationUtility(\n    spark=spark,\n    config={}\n)\n\nresult = configuration_utility.execute()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from cloud storage</p> required <code>config</code> <code>dict</code> <p>Dictionary of spark configuration to be applied to the spark session</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/configuration.py</code> <pre><code>class SparkConfigurationUtility(UtilitiesInterface):\n    \"\"\"\n    Sets configuration key value pairs to a Spark Session\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.sources import SparkConfigurationUtility\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    configuration_utility = SparkConfigurationUtility(\n        spark=spark,\n        config={}\n    )\n\n    result = configuration_utility.execute()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to read data from cloud storage\n        config (dict): Dictionary of spark configuration to be applied to the spark session\n    \"\"\"\n\n    spark: SparkSession\n    config: dict\n    columns: List[StructField]\n    partitioned_by: List[str]\n    location: str\n    properties: dict\n    comment: str\n\n    def __init__(self, spark: SparkSession, config: dict) -&gt; None:\n        self.spark = spark\n        self.config = config\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def execute(self) -&gt; bool:\n        \"\"\"Executes configuration key value pairs to a Spark Session\"\"\"\n        try:\n            for configuration in self.config.items():\n                self.spark.conf.set(configuration[0], configuration[1])\n            return True\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkConfigurationUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/configuration.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkConfigurationUtility.execute","title":"<code>execute()</code>","text":"<p>Executes configuration key value pairs to a Spark Session</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/configuration.py</code> <pre><code>def execute(self) -&gt; bool:\n    \"\"\"Executes configuration key value pairs to a Spark Session\"\"\"\n    try:\n        for configuration in self.config.items():\n            self.spark.conf.set(configuration[0], configuration[1])\n        return True\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkADLSGen2SPNConnectUtility","title":"<code>SparkADLSGen2SPNConnectUtility</code>","text":"<p>               Bases: <code>UtilitiesInterface</code></p> <p>Configures Spark to Connect to an ADLS Gen 2 Storage Account using a Service Principal.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkADLSGen2SPNConnectUtility--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.utilities import SparkADLSGen2SPNConnectUtility\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nadls_gen2_connect_utility = SparkADLSGen2SPNConnectUtility(\n    spark=spark,\n    storage_account=\"YOUR-STORAGAE-ACCOUNT-NAME\",\n    tenant_id=\"YOUR-TENANT-ID\",\n    client_id=\"YOUR-CLIENT-ID\",\n    client_secret=\"YOUR-CLIENT-SECRET\"\n)\n\nresult = adls_gen2_connect_utility.execute()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from cloud storage</p> required <code>storage_account</code> <code>str</code> <p>Name of the ADLS Gen 2 Storage Account</p> required <code>tenant_id</code> <code>str</code> <p>Tenant ID of the Service Principal</p> required <code>client_id</code> <code>str</code> <p>Service Principal Client ID</p> required <code>client_secret</code> <code>str</code> <p>Service Principal Client Secret</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/adls_gen2_spn_connect.py</code> <pre><code>class SparkADLSGen2SPNConnectUtility(UtilitiesInterface):\n    \"\"\"\n    Configures Spark to Connect to an ADLS Gen 2 Storage Account using a Service Principal.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.utilities import SparkADLSGen2SPNConnectUtility\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    adls_gen2_connect_utility = SparkADLSGen2SPNConnectUtility(\n        spark=spark,\n        storage_account=\"YOUR-STORAGAE-ACCOUNT-NAME\",\n        tenant_id=\"YOUR-TENANT-ID\",\n        client_id=\"YOUR-CLIENT-ID\",\n        client_secret=\"YOUR-CLIENT-SECRET\"\n    )\n\n    result = adls_gen2_connect_utility.execute()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to read data from cloud storage\n        storage_account (str): Name of the ADLS Gen 2 Storage Account\n        tenant_id (str): Tenant ID of the Service Principal\n        client_id (str): Service Principal Client ID\n        client_secret (str): Service Principal Client Secret\n    \"\"\"\n\n    spark: SparkSession\n    storage_account: str\n    tenant_id: str\n    client_id: str\n    client_secret: str\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        storage_account: str,\n        tenant_id: str,\n        client_id: str,\n        client_secret: str,\n    ) -&gt; None:\n        self.spark = spark\n        self.storage_account = storage_account\n        self.tenant_id = tenant_id\n        self.client_id = client_id\n        self.client_secret = client_secret\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def execute(self) -&gt; bool:\n        \"\"\"Executes spark configuration to connect to an ADLS Gen 2 Storage Account using a service principal\"\"\"\n        try:\n            adls_gen2_config = SparkConfigurationUtility(\n                spark=self.spark,\n                config={\n                    \"fs.azure.account.auth.type.{}.dfs.core.windows.net\".format(\n                        self.storage_account\n                    ): \"OAuth\",\n                    \"fs.azure.account.oauth.provider.type.{}.dfs.core.windows.net\".format(\n                        self.storage_account\n                    ): \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n                    \"fs.azure.account.oauth2.client.id.{}.dfs.core.windows.net\".format(\n                        self.storage_account\n                    ): self.client_id,\n                    \"fs.azure.account.oauth2.client.secret.{}.dfs.core.windows.net\".format(\n                        self.storage_account\n                    ): self.client_secret,\n                    \"fs.azure.account.oauth2.client.endpoint.{}.dfs.core.windows.net\".format(\n                        self.storage_account\n                    ): \"https://login.microsoftonline.com/{}/oauth2/token\".format(\n                        self.tenant_id\n                    ),\n                },\n            )\n            adls_gen2_config.execute()\n            return True\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkADLSGen2SPNConnectUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/adls_gen2_spn_connect.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkADLSGen2SPNConnectUtility.execute","title":"<code>execute()</code>","text":"<p>Executes spark configuration to connect to an ADLS Gen 2 Storage Account using a service principal</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/adls_gen2_spn_connect.py</code> <pre><code>def execute(self) -&gt; bool:\n    \"\"\"Executes spark configuration to connect to an ADLS Gen 2 Storage Account using a service principal\"\"\"\n    try:\n        adls_gen2_config = SparkConfigurationUtility(\n            spark=self.spark,\n            config={\n                \"fs.azure.account.auth.type.{}.dfs.core.windows.net\".format(\n                    self.storage_account\n                ): \"OAuth\",\n                \"fs.azure.account.oauth.provider.type.{}.dfs.core.windows.net\".format(\n                    self.storage_account\n                ): \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n                \"fs.azure.account.oauth2.client.id.{}.dfs.core.windows.net\".format(\n                    self.storage_account\n                ): self.client_id,\n                \"fs.azure.account.oauth2.client.secret.{}.dfs.core.windows.net\".format(\n                    self.storage_account\n                ): self.client_secret,\n                \"fs.azure.account.oauth2.client.endpoint.{}.dfs.core.windows.net\".format(\n                    self.storage_account\n                ): \"https://login.microsoftonline.com/{}/oauth2/token\".format(\n                    self.tenant_id\n                ),\n            },\n        )\n        adls_gen2_config.execute()\n        return True\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.ADLSGen2DirectoryACLUtility","title":"<code>ADLSGen2DirectoryACLUtility</code>","text":"<p>               Bases: <code>UtilitiesInterface</code></p> <p>Assigns Azure AD Groups to ACLs on directories in an Azure Data Lake Store Gen 2 storage account.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.ADLSGen2DirectoryACLUtility--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.utilities import ADLSGen2DirectoryACLUtility\n\nadls_gen2_directory_acl_utility = ADLSGen2DirectoryACLUtility(\n    storage_account=\"YOUR-STORAGAE-ACCOUNT-NAME\",\n    container=\"YOUR-ADLS_CONTAINER_NAME\",\n    credential=\"YOUR-TOKEN-CREDENTIAL\",\n    directory=\"DIRECTORY\",\n    group_object_id=\"GROUP-OBJECT\",\n    folder_permissions=\"r-x\",\n    parent_folder_permissions=\"r-x\",\n    root_folder_permissions=\"r-x\",\n    set_as_default_acl=True,\n    create_directory_if_not_exists=True\n)\n\nresult = adls_gen2_directory_acl_utility.execute()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>storage_account</code> <code>str</code> <p>ADLS Gen 2 Storage Account Name</p> required <code>container</code> <code>str</code> <p>ADLS Gen 2 Container Name</p> required <code>credential</code> <code>TokenCredential</code> <p>Credentials to authenticate with ADLS Gen 2 Storage Account</p> required <code>directory</code> <code>str</code> <p>Directory to be assign ACLS to in an ADLS Gen 2</p> required <code>group_object_id</code> <code>str</code> <p>Azure AD Group Object ID to be assigned to Directory</p> required <code>folder_permissions</code> <code>(optional, str)</code> <p>Folder Permissions to Assign to directory</p> <code>'r-x'</code> <code>parent_folder_permissions</code> <code>(optional, str)</code> <p>Folder Permissions to Assign to parent directories. Parent Folder ACLs not set if None</p> <code>'r-x'</code> <code>root_folder_permissions</code> <code>(optional, str)</code> <p>Folder Permissions to Assign to root directory. Root Folder ACL not set if None</p> <code>'r-x'</code> <code>set_as_default_acl</code> <code>bool</code> <p>Sets the ACL as the default ACL on the folder</p> <code>True</code> <code>create_directory_if_not_exists</code> <code>bool</code> <p>Creates the directory(and Parent Directories) if it does not exist</p> <code>True</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/azure/adls_gen2_acl.py</code> <pre><code>class ADLSGen2DirectoryACLUtility(UtilitiesInterface):\n    \"\"\"\n    Assigns Azure AD Groups to ACLs on directories in an Azure Data Lake Store Gen 2 storage account.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.utilities import ADLSGen2DirectoryACLUtility\n\n    adls_gen2_directory_acl_utility = ADLSGen2DirectoryACLUtility(\n        storage_account=\"YOUR-STORAGAE-ACCOUNT-NAME\",\n        container=\"YOUR-ADLS_CONTAINER_NAME\",\n        credential=\"YOUR-TOKEN-CREDENTIAL\",\n        directory=\"DIRECTORY\",\n        group_object_id=\"GROUP-OBJECT\",\n        folder_permissions=\"r-x\",\n        parent_folder_permissions=\"r-x\",\n        root_folder_permissions=\"r-x\",\n        set_as_default_acl=True,\n        create_directory_if_not_exists=True\n    )\n\n    result = adls_gen2_directory_acl_utility.execute()\n    ```\n\n    Parameters:\n        storage_account (str): ADLS Gen 2 Storage Account Name\n        container (str): ADLS Gen 2 Container Name\n        credential (TokenCredential): Credentials to authenticate with ADLS Gen 2 Storage Account\n        directory (str): Directory to be assign ACLS to in an ADLS Gen 2\n        group_object_id (str): Azure AD Group Object ID to be assigned to Directory\n        folder_permissions (optional, str): Folder Permissions to Assign to directory\n        parent_folder_permissions (optional, str): Folder Permissions to Assign to parent directories. Parent Folder ACLs not set if None\n        root_folder_permissions (optional, str): Folder Permissions to Assign to root directory. Root Folder ACL not set if None\n        set_as_default_acl (bool, optional): Sets the ACL as the default ACL on the folder\n        create_directory_if_not_exists (bool, optional): Creates the directory(and Parent Directories) if it does not exist\n    \"\"\"\n\n    storage_account: str\n    container: str\n    credential: Union[\n        str,\n        Dict[str, str],\n        AzureNamedKeyCredential,\n        AzureSasCredential,\n        TokenCredential,\n        None,\n    ]\n    directory: str\n    group_object_id: str\n    folder_permissions: str\n    parent_folder_permissions: str\n    root_folder_permissions: str\n    set_as_default_acl: bool\n    create_directory_if_not_exists: bool\n\n    def __init__(\n        self,\n        storage_account: str,\n        container: str,\n        credential: Union[\n            str,\n            Dict[str, str],\n            AzureNamedKeyCredential,\n            AzureSasCredential,\n            TokenCredential,\n            None,\n        ],\n        directory: str,\n        group_object_id: str,\n        folder_permissions: str = \"r-x\",\n        parent_folder_permissions: Union[str, None] = \"r-x\",\n        root_folder_permissions: Union[str, None] = \"r-x\",\n        set_as_default_acl: bool = True,\n        create_directory_if_not_exists: bool = True,\n    ) -&gt; None:\n        self.storage_account = storage_account\n        self.container = container\n        self.credential = credential\n        self.directory = directory\n        self.group_object_id = group_object_id\n        self.folder_permissions = folder_permissions\n        self.parent_folder_permissions = parent_folder_permissions\n        self.root_folder_permissions = root_folder_permissions\n        self.set_as_default_acl = set_as_default_acl\n        self.create_directory_if_not_exists = create_directory_if_not_exists\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYTHON\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_pypi_library(get_default_package(\"azure_adls_gen_2\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _set_acl(\n        self,\n        file_system_client: FileSystemClient,\n        path: str,\n        group_object_id: str,\n        folder_permissions: str,\n        set_as_default_acl: bool,\n    ):\n        acl_directory_client = file_system_client.get_directory_client(path)\n\n        group_id_acl = \"group:{}:{}\".format(group_object_id, folder_permissions)\n        acl_props = acl_directory_client.get_access_control().get(\"acl\")\n        acl_props_list = acl_props.split(\",\")\n\n        for acl in acl_props_list:\n            if group_object_id in acl:\n                acl_props_list.remove(acl)\n\n        acl_props_list.append(group_id_acl)\n        if set_as_default_acl == True:\n            acl_props_list.append(\"default:{}\".format(group_id_acl))\n\n        new_acl_props = \",\".join(acl_props_list)\n        acl_directory_client.set_access_control(acl=new_acl_props)\n\n    def execute(self) -&gt; bool:\n        try:\n            # Setup file system client\n            service_client = DataLakeServiceClient(\n                account_url=\"{}://{}.dfs.core.windows.net\".format(\n                    \"https\", self.storage_account\n                ),\n                credential=self.credential,\n            )\n            file_system_client = service_client.get_file_system_client(\n                file_system=self.container\n            )\n\n            # Create directory if it doesn't already exist\n            if self.create_directory_if_not_exists:\n                directory_client = file_system_client.get_directory_client(\n                    self.directory\n                )\n                if not directory_client.exists():\n                    file_system_client.create_directory(self.directory)\n\n            group_object_id = str(self.group_object_id)\n            acl_path = \"\"\n            directory_list = self.directory.split(\"/\")\n\n            # Set Root Folder ACLs if specified\n            if self.root_folder_permissions != None:\n                self._set_acl(\n                    file_system_client,\n                    \"/\",\n                    group_object_id,\n                    self.root_folder_permissions,\n                    False,\n                )\n\n            # Set Parent Folders ACLs if specified\n            if self.parent_folder_permissions != None:\n                for directory in directory_list[:-1]:\n                    if directory == \"\":\n                        acl_path = \"/\"\n                        continue\n                    elif acl_path == \"/\":\n                        acl_path += directory\n                    else:\n                        acl_path += \"/\" + directory\n\n                    self._set_acl(\n                        file_system_client,\n                        acl_path,\n                        group_object_id,\n                        self.parent_folder_permissions,\n                        False,\n                    )\n\n            # Set Folder ACLs\n            self._set_acl(\n                file_system_client,\n                self.directory,\n                group_object_id,\n                self.folder_permissions,\n                self.set_as_default_acl,\n            )\n\n            return True\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.ADLSGen2DirectoryACLUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYTHON</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/azure/adls_gen2_acl.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYTHON\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.AzureAutoloaderResourcesUtility","title":"<code>AzureAutoloaderResourcesUtility</code>","text":"<p>               Bases: <code>UtilitiesInterface</code></p> <p>Creates the required Azure Resources for the Databricks Autoloader Notification Mode.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.AzureAutoloaderResourcesUtility--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.utilities import AzureAutoloaderResourcesUtility\n\nazure_autoloader_resources_utility = AzureAutoloaderResourcesUtility(\n    subscription_id=\"YOUR-SUBSCRIPTION-ID\",\n    resource_group_name=\"YOUR-RESOURCE-GROUP\",\n    storage_account=\"YOUR-STORAGE-ACCOUNT-NAME\",\n    container=\"YOUR-CONTAINER-NAME\",\n    directory=\"DIRECTORY\",\n    credential=\"YOUR-CLIENT-ID\",\n    event_subscription_name=\"YOUR-EVENT-SUBSCRIPTION\",\n    queue_name=\"YOUR-QUEUE-NAME\",\n    system_topic_name=None\n)\n\nresult = azure_autoloader_resources_utility.execute()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>subscription_id</code> <code>str</code> <p>Azure Subscription ID</p> required <code>resource_group_name</code> <code>str</code> <p>Resource Group Name of Subscription</p> required <code>storage_account</code> <code>str</code> <p>Storage Account Name</p> required <code>container</code> <code>str</code> <p>Container Name</p> required <code>directory</code> <code>str</code> <p>Directory to be used for filtering messages in the Event Subscription. This will be equivalent to the Databricks Autoloader Path</p> required <code>credential</code> <code>TokenCredential</code> <p>Credentials to authenticate with Storage Account</p> required <code>event_subscription_name</code> <code>str</code> <p>Name of the Event Subscription</p> required <code>queue_name</code> <code>str</code> <p>Name of the queue that will be used for the Endpoint of the Messages</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/azure/autoloader_resources.py</code> <pre><code>class AzureAutoloaderResourcesUtility(UtilitiesInterface):\n    \"\"\"\n    Creates the required Azure Resources for the Databricks Autoloader Notification Mode.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.utilities import AzureAutoloaderResourcesUtility\n\n    azure_autoloader_resources_utility = AzureAutoloaderResourcesUtility(\n        subscription_id=\"YOUR-SUBSCRIPTION-ID\",\n        resource_group_name=\"YOUR-RESOURCE-GROUP\",\n        storage_account=\"YOUR-STORAGE-ACCOUNT-NAME\",\n        container=\"YOUR-CONTAINER-NAME\",\n        directory=\"DIRECTORY\",\n        credential=\"YOUR-CLIENT-ID\",\n        event_subscription_name=\"YOUR-EVENT-SUBSCRIPTION\",\n        queue_name=\"YOUR-QUEUE-NAME\",\n        system_topic_name=None\n    )\n\n    result = azure_autoloader_resources_utility.execute()\n    ```\n\n    Parameters:\n        subscription_id (str): Azure Subscription ID\n        resource_group_name (str): Resource Group Name of Subscription\n        storage_account (str): Storage Account Name\n        container (str): Container Name\n        directory (str): Directory to be used for filtering messages in the Event Subscription. This will be equivalent to the Databricks Autoloader Path\n        credential (TokenCredential): Credentials to authenticate with Storage Account\n        event_subscription_name (str): Name of the Event Subscription\n        queue_name (str): Name of the queue that will be used for the Endpoint of the Messages\n    \"\"\"\n\n    subscription_id: str\n    resource_group_name: str\n    storage_account: str\n    container: str\n    directory: str\n    credential: TokenCredential\n    event_subscription_name: str\n    queue_name: str\n\n    def __init__(\n        self,\n        subscription_id: str,\n        resource_group_name: str,\n        storage_account: str,\n        container: str,\n        directory: str,\n        credential: TokenCredential,\n        event_subscription_name: str,\n        queue_name: str,\n    ) -&gt; None:\n        self.subscription_id = subscription_id\n        self.resource_group_name = resource_group_name\n        self.storage_account = storage_account\n        self.container = container\n        self.directory = directory\n        self.credential = credential\n        self.event_subscription_name = event_subscription_name\n        self.queue_name = queue_name\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYTHON\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_pypi_library(get_default_package(\"azure_eventgrid_mgmt\"))\n        libraries.add_pypi_library(get_default_package(\"azure_storage_mgmt\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def execute(self) -&gt; bool:\n        storage_mgmt_client = StorageManagementClient(\n            credential=self.credential, subscription_id=self.subscription_id\n        )\n\n        try:\n            queue_response = storage_mgmt_client.queue.get(\n                resource_group_name=self.resource_group_name,\n                account_name=self.storage_account,\n                queue_name=self.queue_name,\n            )\n        except ResourceNotFoundError:\n            queue_response = None\n\n        if queue_response == None:\n            storage_mgmt_client.queue.create(\n                resource_group_name=self.resource_group_name,\n                account_name=self.storage_account,\n                queue_name=self.queue_name,\n                queue=StorageQueue(),\n            )\n\n        eventgrid_client = EventGridManagementClient(\n            credential=self.credential, subscription_id=self.subscription_id\n        )\n\n        source = \"/subscriptions/{}/resourceGroups/{}/providers/Microsoft.Storage/StorageAccounts/{}\".format(\n            self.subscription_id, self.resource_group_name, self.storage_account\n        )\n\n        try:\n            event_subscription_response = eventgrid_client.event_subscriptions.get(\n                scope=source, event_subscription_name=self.event_subscription_name\n            )\n        except ResourceNotFoundError:\n            event_subscription_response = None\n\n        if event_subscription_response == None:\n            event_subscription_destination = StorageQueueEventSubscriptionDestination(\n                resource_id=source,\n                queue_name=self.queue_name,\n                queue_message_time_to_live_in_seconds=None,\n            )\n\n            event_subscription_filter = EventSubscriptionFilter(\n                subject_begins_with=\"/blobServices/default/containers/{}/blobs/{}\".format(\n                    self.container, self.directory\n                ),\n                included_event_types=[\n                    \"Microsoft.Storage.BlobCreated\",\n                    \"Microsoft.Storage.BlobRenamed\",\n                    \"Microsoft.Storage.DirectoryRenamed\",\n                ],\n                advanced_filters=[\n                    StringContainsAdvancedFilter(\n                        key=\"data.api\",\n                        values=[\n                            \"CopyBlob\",\n                            \"PutBlob\",\n                            \"PutBlockList\",\n                            \"FlushWithClose\",\n                            \"RenameFile\",\n                            \"RenameDirectory\",\n                        ],\n                    )\n                ],\n            )\n\n            retry_policy = RetryPolicy()\n\n            event_subscription_info = EventSubscription(\n                destination=event_subscription_destination,\n                filter=event_subscription_filter,\n                event_delivery_schema=EventDeliverySchema.EVENT_GRID_SCHEMA,\n                retry_policy=retry_policy,\n            )\n\n            eventgrid_client.event_subscriptions.begin_create_or_update(\n                scope=source,\n                event_subscription_name=self.event_subscription_name,\n                event_subscription_info=event_subscription_info,\n            ).result()\n\n            return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.AzureAutoloaderResourcesUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYTHON</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/azure/autoloader_resources.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYTHON\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PipelineComponentsGetUtility","title":"<code>PipelineComponentsGetUtility</code>","text":"<p>               Bases: <code>UtilitiesInterface</code></p> <p>Gets the list of imported RTDIP components. Returns the libraries and settings of the components to be used in the pipeline.</p> <p>Call this component after all imports of the RTDIP components to ensure that the components can be determined.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>optional str</code> <p>Provide the module to use for imports of rtdip-sdk components. If not populated, it will use the calling module to check for imports</p> <code>None</code> <code>spark_config</code> <code>optional dict</code> <p>Additional spark configuration to be applied to the spark session</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/pipeline_components.py</code> <pre><code>class PipelineComponentsGetUtility(UtilitiesInterface):\n    \"\"\"\n    Gets the list of imported RTDIP components. Returns the libraries and settings of the components to be used in the pipeline.\n\n    Call this component after all imports of the RTDIP components to ensure that the components can be determined.\n\n    Parameters:\n        module (optional str): Provide the module to use for imports of rtdip-sdk components. If not populated, it will use the calling module to check for imports\n        spark_config (optional dict): Additional spark configuration to be applied to the spark session\n    \"\"\"\n\n    def __init__(self, module: str = None, spark_config: dict = None) -&gt; None:\n        if module == None:\n            frm = inspect.stack()[1]\n            mod = inspect.getmodule(frm[0])\n            self.module = mod.__name__\n        else:\n            self.module = module\n        self.spark_config = {} if spark_config is None else spark_config\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYTHON\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def execute(self) -&gt; Tuple[Libraries, dict]:\n        from ..sources.interfaces import SourceInterface\n        from ..destinations.interfaces import DestinationInterface\n        from ..deploy.interfaces import DeployInterface\n        from ..secrets.interfaces import SecretsInterface\n        from ..transformers.interfaces import TransformerInterface\n\n        try:\n            classes_imported = inspect.getmembers(\n                sys.modules[self.module], inspect.isclass\n            )\n            component_list = []\n            for cls in classes_imported:\n                class_check = getattr(sys.modules[self.module], cls[0])\n                if (\n                    (\n                        issubclass(class_check, SourceInterface)\n                        and class_check != SourceInterface\n                    )\n                    or (\n                        issubclass(class_check, DestinationInterface)\n                        and class_check != DestinationInterface\n                    )\n                    or (\n                        issubclass(class_check, DeployInterface)\n                        and class_check != DeployInterface\n                    )\n                    or (\n                        issubclass(class_check, SecretsInterface)\n                        and class_check != SecretsInterface\n                    )\n                    or (\n                        issubclass(class_check, TransformerInterface)\n                        and class_check != TransformerInterface\n                    )\n                    or (\n                        issubclass(class_check, UtilitiesInterface)\n                        and class_check != UtilitiesInterface\n                    )\n                ):\n                    component_list.append(cls[1])\n\n            task_libraries = Libraries()\n            task_libraries.get_libraries_from_components(component_list)\n            spark_configuration = self.spark_config\n            for component in component_list:\n                spark_configuration = {**spark_configuration, **component.settings()}\n            return (task_libraries, spark_configuration)\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PipelineComponentsGetUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYTHON</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/pipeline_components.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYTHON\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkSessionUtility","title":"<code>SparkSessionUtility</code>","text":"<p>               Bases: <code>UtilitiesInterface</code></p> <p>Creates or Gets a Spark Session and uses settings and libraries of the imported RTDIP components to populate the spark configuration and jars in the spark session.</p> <p>Call this component after all imports of the RTDIP components to ensure that the spark session is configured correctly.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkSessionUtility--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\nspark_session_utility = SparkSessionUtility(\n    config={},\n    module=None,\n    remote=None\n)\n\nresult = spark_session_utility.execute()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>optional dict</code> <p>Dictionary of spark configuration to be applied to the spark session</p> <code>None</code> <code>module</code> <code>optional str</code> <p>Provide the module to use for imports of rtdip-sdk components. If not populated, it will use the calling module to check for imports</p> <code>None</code> <code>remote</code> <code>optional str</code> <p>Specify the remote parameters if intending to use Spark Connect</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/session.py</code> <pre><code>class SparkSessionUtility(UtilitiesInterface):\n    \"\"\"\n    Creates or Gets a Spark Session and uses settings and libraries of the imported RTDIP components to populate the spark configuration and jars in the spark session.\n\n    Call this component after all imports of the RTDIP components to ensure that the spark session is configured correctly.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    spark_session_utility = SparkSessionUtility(\n        config={},\n        module=None,\n        remote=None\n    )\n\n    result = spark_session_utility.execute()\n    ```\n\n    Parameters:\n        config (optional dict): Dictionary of spark configuration to be applied to the spark session\n        module (optional str): Provide the module to use for imports of rtdip-sdk components. If not populated, it will use the calling module to check for imports\n        remote (optional str): Specify the remote parameters if intending to use Spark Connect\n    \"\"\"\n\n    spark: SparkSession\n    config: dict\n    module: str\n\n    def __init__(\n        self, config: dict = None, module: str = None, remote: str = None\n    ) -&gt; None:\n        self.config = config\n        if module == None:\n            frm = inspect.stack()[1]\n            mod = inspect.getmodule(frm[0])\n            self.module = mod.__name__\n        else:\n            self.module = module\n        self.remote = remote\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def execute(self) -&gt; SparkSession:\n        \"\"\"To execute\"\"\"\n        try:\n            (task_libraries, spark_configuration) = PipelineComponentsGetUtility(\n                self.module, self.config\n            ).execute()\n            self.spark = SparkClient(\n                spark_configuration=spark_configuration,\n                spark_libraries=task_libraries,\n                spark_remote=self.remote,\n            ).spark_session\n            return self.spark\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkSessionUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/session.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.SparkSessionUtility.execute","title":"<code>execute()</code>","text":"<p>To execute</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/session.py</code> <pre><code>def execute(self) -&gt; SparkSession:\n    \"\"\"To execute\"\"\"\n    try:\n        (task_libraries, spark_configuration) = PipelineComponentsGetUtility(\n            self.module, self.config\n        ).execute()\n        self.spark = SparkClient(\n            spark_configuration=spark_configuration,\n            spark_libraries=task_libraries,\n            spark_remote=self.remote,\n        ).spark_session\n        return self.spark\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PipelineJobFromJsonConverter","title":"<code>PipelineJobFromJsonConverter</code>","text":"<p>               Bases: <code>ConverterInterface</code></p> <p>Converts a json string into a Pipeline Job.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PipelineJobFromJsonConverter--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.secrets import PipelineJobFromJsonConverter\n\nconvert_json_string_to_pipline_job = PipelineJobFromJsonConverter(\n    pipeline_json = \"{JSON-STRING}\"\n)\n\nconvert_json_string_to_pipline_job.convert()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline_json</code> <code>str</code> <p>Json representing PipelineJob information, including tasks and related steps</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/converters/pipeline_job_json.py</code> <pre><code>class PipelineJobFromJsonConverter(ConverterInterface):\n    \"\"\"\n    Converts a json string into a Pipeline Job.\n\n    Example\n    -------\n    ```python\n    from rtdip_sdk.pipelines.secrets import PipelineJobFromJsonConverter\n\n    convert_json_string_to_pipline_job = PipelineJobFromJsonConverter(\n        pipeline_json = \"{JSON-STRING}\"\n    )\n\n    convert_json_string_to_pipline_job.convert()\n    ```\n\n    Parameters:\n        pipeline_json (str): Json representing PipelineJob information, including tasks and related steps\n    \"\"\"\n\n    pipeline_json: str\n\n    def __init__(self, pipeline_json: str):\n        self.pipeline_json = pipeline_json\n\n    def _try_convert_to_pipeline_secret(self, value):\n        try:\n            if \"pipeline_secret\" in value:\n                value[\"pipeline_secret\"][\"type\"] = getattr(\n                    sys.modules[__name__], value[\"pipeline_secret\"][\"type\"]\n                )\n            return PipelineSecret.parse_obj(value[\"pipeline_secret\"])\n        except:  # NOSONAR\n            return value\n\n    def convert(self) -&gt; PipelineJob:\n        \"\"\"\n        Converts a json string to a Pipeline Job\n        \"\"\"\n        pipeline_job_dict = json.loads(self.pipeline_json)\n\n        # convert string component to class\n        for task in pipeline_job_dict[\"task_list\"]:\n            for step in task[\"step_list\"]:\n                step[\"component\"] = getattr(sys.modules[__name__], step[\"component\"])\n                for param_key, param_value in step[\"component_parameters\"].items():\n                    step[\"component_parameters\"][param_key] = (\n                        self._try_convert_to_pipeline_secret(param_value)\n                    )\n                    if not isinstance(\n                        step[\"component_parameters\"][param_key], PipelineSecret\n                    ) and isinstance(param_value, dict):\n                        for key, value in param_value.items():\n                            step[\"component_parameters\"][param_key][key] = (\n                                self._try_convert_to_pipeline_secret(value)\n                            )\n\n        return PipelineJob(**pipeline_job_dict)\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PipelineJobFromJsonConverter.convert","title":"<code>convert()</code>","text":"<p>Converts a json string to a Pipeline Job</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/converters/pipeline_job_json.py</code> <pre><code>def convert(self) -&gt; PipelineJob:\n    \"\"\"\n    Converts a json string to a Pipeline Job\n    \"\"\"\n    pipeline_job_dict = json.loads(self.pipeline_json)\n\n    # convert string component to class\n    for task in pipeline_job_dict[\"task_list\"]:\n        for step in task[\"step_list\"]:\n            step[\"component\"] = getattr(sys.modules[__name__], step[\"component\"])\n            for param_key, param_value in step[\"component_parameters\"].items():\n                step[\"component_parameters\"][param_key] = (\n                    self._try_convert_to_pipeline_secret(param_value)\n                )\n                if not isinstance(\n                    step[\"component_parameters\"][param_key], PipelineSecret\n                ) and isinstance(param_value, dict):\n                    for key, value in param_value.items():\n                        step[\"component_parameters\"][param_key][key] = (\n                            self._try_convert_to_pipeline_secret(value)\n                        )\n\n    return PipelineJob(**pipeline_job_dict)\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PipelineJobToJsonConverter","title":"<code>PipelineJobToJsonConverter</code>","text":"<p>               Bases: <code>ConverterInterface</code></p> <p>Converts a Pipeline Job into a json string.</p>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PipelineJobToJsonConverter--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.secrets import PipelineJobToJsonConverter\n\nconvert_pipeline_job_to_json_string = PipelineJobFromJsonConverter(\n    pipeline_json = PipelineJob\n)\n\nconvert_pipeline_job_to_json_string.convert()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline_job</code> <code>PipelineJob</code> <p>A Pipeline Job consisting of tasks and steps</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/converters/pipeline_job_json.py</code> <pre><code>class PipelineJobToJsonConverter(ConverterInterface):\n    \"\"\"\n    Converts a Pipeline Job into a json string.\n\n    Example\n    -------\n    ```python\n    from rtdip_sdk.pipelines.secrets import PipelineJobToJsonConverter\n\n    convert_pipeline_job_to_json_string = PipelineJobFromJsonConverter(\n        pipeline_json = PipelineJob\n    )\n\n    convert_pipeline_job_to_json_string.convert()\n    ```\n\n    Parameters:\n        pipeline_job (PipelineJob): A Pipeline Job consisting of tasks and steps\n    \"\"\"\n\n    pipeline_job: PipelineJob\n\n    def __init__(self, pipeline_job: PipelineJob):\n        self.pipeline_job = pipeline_job\n\n    def convert(self):\n        \"\"\"\n        Converts a Pipeline Job to a json string\n        \"\"\"\n        # required because pydantic does not use encoders in subclasses\n        for task in self.pipeline_job.task_list:\n            step_dict_list = []\n            for step in task.step_list:\n                step_dict_list.append(\n                    json.loads(step.json(models_as_dict=False, exclude_none=True))\n                )\n            task.step_list = step_dict_list\n\n        pipeline_job_json = self.pipeline_job.json(exclude_none=True)\n        return pipeline_job_json\n</code></pre>"},{"location":"sdk/code-reference/pipelines/converters/pipeline_job_json/#src.sdk.python.rtdip_sdk.pipelines.converters.pipeline_job_json.PipelineJobToJsonConverter.convert","title":"<code>convert()</code>","text":"<p>Converts a Pipeline Job to a json string</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/converters/pipeline_job_json.py</code> <pre><code>def convert(self):\n    \"\"\"\n    Converts a Pipeline Job to a json string\n    \"\"\"\n    # required because pydantic does not use encoders in subclasses\n    for task in self.pipeline_job.task_list:\n        step_dict_list = []\n        for step in task.step_list:\n            step_dict_list.append(\n                json.loads(step.json(models_as_dict=False, exclude_none=True))\n            )\n        task.step_list = step_dict_list\n\n    pipeline_job_json = self.pipeline_job.json(exclude_none=True)\n    return pipeline_job_json\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/dimensionality_reduction/","title":"Dimensionality Reduction","text":""},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/dimensionality_reduction/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.dimensionality_reduction.DimensionalityReduction","title":"<code>DimensionalityReduction</code>","text":"<p>               Bases: <code>DataManipulationBaseInterface</code></p> <p>Detects and combines columns based on correlation or exact duplicates.</p>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/dimensionality_reduction/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.dimensionality_reduction.DimensionalityReduction--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.data_quality.data_manipulation.spark.dimensionality_reduction import DimensionalityReduction\n\nfrom pyspark.sql import SparkSession\n\ncolumn_correlation_monitor = DimensionalityReduction(\n    df,\n    columns=['column1', 'column2'],\n    threshold=0.95,\n    combination_method='mean'\n)\n\nresult = column_correlation_monitor.filter_data()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>PySpark DataFrame to be analyzed and transformed.</p> required <code>columns</code> <code>list</code> <p>List of column names to check for correlation. Only two columns are supported.</p> required <code>threshold</code> <code>float</code> <p>Correlation threshold for column combination [0-1]. If the absolute value of the correlation is equal or bigger, than the columns are combined. Defaults to 0.9.</p> <code>0.9</code> <code>combination_method</code> <code>str</code> <p>Method to combine correlated columns. Supported methods: - 'mean': Average the values of both columns and write the result to the first column   (New value = (column1 + column2) / 2) - 'sum': Sum the values of both columns and write the result to the first column   (New value = column1 + column2) - 'first': Keep the first column, drop the second column - 'second': Keep the second column, drop the first column - 'delete': Remove both columns entirely from the DataFrame Defaults to 'mean'.</p> <code>'mean'</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/dimensionality_reduction.py</code> <pre><code>class DimensionalityReduction(DataManipulationBaseInterface):\n    \"\"\"\n    Detects and combines columns based on correlation or exact duplicates.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.data_quality.data_manipulation.spark.dimensionality_reduction import DimensionalityReduction\n\n    from pyspark.sql import SparkSession\n\n    column_correlation_monitor = DimensionalityReduction(\n        df,\n        columns=['column1', 'column2'],\n        threshold=0.95,\n        combination_method='mean'\n    )\n\n    result = column_correlation_monitor.filter_data()\n    ```\n\n    Parameters:\n        df (DataFrame): PySpark DataFrame to be analyzed and transformed.\n        columns (list): List of column names to check for correlation. Only two columns are supported.\n        threshold (float, optional): Correlation threshold for column combination [0-1]. If the absolute value of the correlation is equal or bigger, than the columns are combined. Defaults to 0.9.\n        combination_method (str, optional): Method to combine correlated columns.\n            Supported methods:\n            - 'mean': Average the values of both columns and write the result to the first column\n              (New value = (column1 + column2) / 2)\n            - 'sum': Sum the values of both columns and write the result to the first column\n              (New value = column1 + column2)\n            - 'first': Keep the first column, drop the second column\n            - 'second': Keep the second column, drop the first column\n            - 'delete': Remove both columns entirely from the DataFrame\n            Defaults to 'mean'.\n    \"\"\"\n\n    df: PySparkDataFrame\n    columns_to_check: list\n    threshold: float\n    combination_method: str\n\n    def __init__(\n        self,\n        df: PySparkDataFrame,\n        columns: list,\n        threshold: float = 0.9,\n        combination_method: str = \"mean\",\n    ) -&gt; None:\n        # Validate inputs\n        if not columns or not isinstance(columns, list):\n            raise ValueError(\"columns must be a non-empty list of column names.\")\n        if len(columns) != 2:\n            raise ValueError(\n                \"columns must contain exactly two columns for correlation.\"\n            )\n\n        if not 0 &lt;= threshold &lt;= 1:\n            raise ValueError(\"Threshold must be between 0 and 1.\")\n\n        valid_methods = [\"mean\", \"sum\", \"first\", \"second\", \"delete\"]\n        if combination_method not in valid_methods:\n            raise ValueError(f\"combination_method must be one of {valid_methods}\")\n\n        self.df = df\n        self.columns_to_check = columns\n        self.threshold = threshold\n        self.combination_method = combination_method\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _calculate_correlation(self) -&gt; float:\n        \"\"\"\n        Calculate correlation between specified columns.\n\n        Returns:\n            float: Correlation matrix between columns\n        \"\"\"\n        assembler = VectorAssembler(\n            inputCols=self.columns_to_check, outputCol=\"features\"\n        )\n        vector_df = assembler.transform(self.df)\n\n        correlation_matrix = Correlation.corr(\n            vector_df, \"features\", method=\"pearson\"\n        ).collect()[0][0]\n\n        # Correlation between first and second column\n        return correlation_matrix.toArray()[0][1]\n\n    def filter_data(self) -&gt; PySparkDataFrame:\n        \"\"\"\n        Process DataFrame by detecting and combining correlated columns.\n\n        Returns:\n            PySparkDataFrame: Transformed PySpark DataFrame\n        \"\"\"\n        correlation = self._calculate_correlation()\n\n        # If correlation is below threshold, return original DataFrame\n        if correlation &lt; self.threshold:\n            return self.df\n\n        col1, col2 = self.columns_to_check\n        if self.combination_method == \"mean\":\n            return self.df.withColumn(col1, (col(col1) + col(col2)) / 2).drop(col2)\n        elif self.combination_method == \"sum\":\n            return self.df.withColumn(col1, col(col1) + col(col2)).drop(col2)\n        elif self.combination_method == \"first\":\n            return self.df.drop(col2)\n        elif self.combination_method == \"second\":\n            return self.df.drop(col2)\n        elif self.combination_method == \"delete\":\n            return self.df.drop(col1).drop(col2)\n        else:\n            return self.df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/dimensionality_reduction/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.dimensionality_reduction.DimensionalityReduction.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/dimensionality_reduction.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/dimensionality_reduction/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.dimensionality_reduction.DimensionalityReduction.filter_data","title":"<code>filter_data()</code>","text":"<p>Process DataFrame by detecting and combining correlated columns.</p> <p>Returns:</p> Name Type Description <code>PySparkDataFrame</code> <code>DataFrame</code> <p>Transformed PySpark DataFrame</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/dimensionality_reduction.py</code> <pre><code>def filter_data(self) -&gt; PySparkDataFrame:\n    \"\"\"\n    Process DataFrame by detecting and combining correlated columns.\n\n    Returns:\n        PySparkDataFrame: Transformed PySpark DataFrame\n    \"\"\"\n    correlation = self._calculate_correlation()\n\n    # If correlation is below threshold, return original DataFrame\n    if correlation &lt; self.threshold:\n        return self.df\n\n    col1, col2 = self.columns_to_check\n    if self.combination_method == \"mean\":\n        return self.df.withColumn(col1, (col(col1) + col(col2)) / 2).drop(col2)\n    elif self.combination_method == \"sum\":\n        return self.df.withColumn(col1, col(col1) + col(col2)).drop(col2)\n    elif self.combination_method == \"first\":\n        return self.df.drop(col2)\n    elif self.combination_method == \"second\":\n        return self.df.drop(col2)\n    elif self.combination_method == \"delete\":\n        return self.df.drop(col1).drop(col2)\n    else:\n        return self.df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/duplicate_detection/","title":"Duplicate Detetection","text":""},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/duplicate_detection/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.duplicate_detection.DuplicateDetection","title":"<code>DuplicateDetection</code>","text":"<p>               Bases: <code>DataManipulationBaseInterface</code>, <code>InputValidator</code></p> <p>Cleanses a PySpark DataFrame from duplicates.</p>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/duplicate_detection/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.duplicate_detection.DuplicateDetection--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.data_quality.data_manipulation.spark.duplicate_detection import DuplicateDetection\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.dataframe import DataFrame\n\nduplicate_detection_monitor = DuplicateDetection(df, primary_key_columns=[\"TagName\", \"EventTime\"])\n\nresult = duplicate_detection_monitor.filter_data()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>PySpark DataFrame to be cleansed.</p> required <code>primary_key_columns</code> <code>list</code> <p>List of column names that serve as primary key for duplicate detection.</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/duplicate_detection.py</code> <pre><code>class DuplicateDetection(DataManipulationBaseInterface, InputValidator):\n    \"\"\"\n    Cleanses a PySpark DataFrame from duplicates.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.data_quality.data_manipulation.spark.duplicate_detection import DuplicateDetection\n\n    from pyspark.sql import SparkSession\n    from pyspark.sql.dataframe import DataFrame\n\n    duplicate_detection_monitor = DuplicateDetection(df, primary_key_columns=[\"TagName\", \"EventTime\"])\n\n    result = duplicate_detection_monitor.filter_data()\n    ```\n\n    Parameters:\n        df (DataFrame): PySpark DataFrame to be cleansed.\n        primary_key_columns (list): List of column names that serve as primary key for duplicate detection.\n    \"\"\"\n\n    df: PySparkDataFrame\n    primary_key_columns: list\n\n    def __init__(self, df: PySparkDataFrame, primary_key_columns: list) -&gt; None:\n        if not primary_key_columns or not isinstance(primary_key_columns, list):\n            raise ValueError(\n                \"primary_key_columns must be a non-empty list of column names.\"\n            )\n        self.df = df\n        self.primary_key_columns = primary_key_columns\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def filter_data(self) -&gt; PySparkDataFrame:\n        \"\"\"\n        Returns:\n            PySparkDataFrame: A cleansed PySpark DataFrame from all duplicates based on primary key columns.\n        \"\"\"\n        cleansed_df = self.df.dropDuplicates(self.primary_key_columns)\n        return cleansed_df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/duplicate_detection/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.duplicate_detection.DuplicateDetection.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/duplicate_detection.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/duplicate_detection/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.duplicate_detection.DuplicateDetection.filter_data","title":"<code>filter_data()</code>","text":"<p>Returns:</p> Name Type Description <code>PySparkDataFrame</code> <code>DataFrame</code> <p>A cleansed PySpark DataFrame from all duplicates based on primary key columns.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/duplicate_detection.py</code> <pre><code>def filter_data(self) -&gt; PySparkDataFrame:\n    \"\"\"\n    Returns:\n        PySparkDataFrame: A cleansed PySpark DataFrame from all duplicates based on primary key columns.\n    \"\"\"\n    cleansed_df = self.df.dropDuplicates(self.primary_key_columns)\n    return cleansed_df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/flatline_filter/","title":"Flatline Filter","text":""},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/flatline_filter/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.flatline_filter.FlatlineFilter","title":"<code>FlatlineFilter</code>","text":"<p>               Bases: <code>DataManipulationBaseInterface</code></p> <p>Removes and logs rows with flatlining detected in specified columns of a PySpark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to process.</p> required <code>watch_columns</code> <code>list</code> <p>List of column names to monitor for flatlining (null or zero values).</p> required <code>tolerance_timespan</code> <code>int</code> <p>Maximum allowed consecutive flatlining period. Rows exceeding this period are removed.</p> required Example <pre><code>from pyspark.sql import SparkSession\nfrom rtdip_sdk.pipelines.data_quality.data_manipulation.spark.flatline_filter import FlatlineFilter\n\n\nspark = SparkSession.builder.master(\"local[1]\").appName(\"FlatlineFilterExample\").getOrCreate()\n\n# Example DataFrame\ndata = [\n    (1, \"2024-01-02 03:49:45.000\", 0.0),\n    (1, \"2024-01-02 03:50:45.000\", 0.0),\n    (1, \"2024-01-02 03:51:45.000\", 0.0),\n    (2, \"2024-01-02 03:49:45.000\", 5.0),\n]\ncolumns = [\"TagName\", \"EventTime\", \"Value\"]\ndf = spark.createDataFrame(data, columns)\n\nfilter_flatlining_rows = FlatlineFilter(\n    df=df,\n    watch_columns=[\"Value\"],\n    tolerance_timespan=2,\n)\n\nresult_df = filter_flatlining_rows.filter_data()\nresult_df.show()\n</code></pre> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/flatline_filter.py</code> <pre><code>class FlatlineFilter(DataManipulationBaseInterface):\n    \"\"\"\n    Removes and logs rows with flatlining detected in specified columns of a PySpark DataFrame.\n\n    Args:\n        df (pyspark.sql.DataFrame): The input DataFrame to process.\n        watch_columns (list): List of column names to monitor for flatlining (null or zero values).\n        tolerance_timespan (int): Maximum allowed consecutive flatlining period. Rows exceeding this period are removed.\n\n    Example:\n        ```python\n        from pyspark.sql import SparkSession\n        from rtdip_sdk.pipelines.data_quality.data_manipulation.spark.flatline_filter import FlatlineFilter\n\n\n        spark = SparkSession.builder.master(\"local[1]\").appName(\"FlatlineFilterExample\").getOrCreate()\n\n        # Example DataFrame\n        data = [\n            (1, \"2024-01-02 03:49:45.000\", 0.0),\n            (1, \"2024-01-02 03:50:45.000\", 0.0),\n            (1, \"2024-01-02 03:51:45.000\", 0.0),\n            (2, \"2024-01-02 03:49:45.000\", 5.0),\n        ]\n        columns = [\"TagName\", \"EventTime\", \"Value\"]\n        df = spark.createDataFrame(data, columns)\n\n        filter_flatlining_rows = FlatlineFilter(\n            df=df,\n            watch_columns=[\"Value\"],\n            tolerance_timespan=2,\n        )\n\n        result_df = filter_flatlining_rows.filter_data()\n        result_df.show()\n        ```\n    \"\"\"\n\n    def __init__(\n        self, df: PySparkDataFrame, watch_columns: list, tolerance_timespan: int\n    ) -&gt; None:\n        self.df = df\n        self.flatline_detection = FlatlineDetection(\n            df=df, watch_columns=watch_columns, tolerance_timespan=tolerance_timespan\n        )\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def filter_data(self) -&gt; PySparkDataFrame:\n        \"\"\"\n        Removes rows with flatlining detected.\n\n        Returns:\n            pyspark.sql.DataFrame: A DataFrame without rows with flatlining detected.\n        \"\"\"\n        flatlined_rows = self.flatline_detection.check_for_flatlining()\n        flatlined_rows = flatlined_rows.select(*self.df.columns)\n        return self.df.subtract(flatlined_rows)\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/flatline_filter/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.flatline_filter.FlatlineFilter.filter_data","title":"<code>filter_data()</code>","text":"<p>Removes rows with flatlining detected.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: A DataFrame without rows with flatlining detected.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/flatline_filter.py</code> <pre><code>def filter_data(self) -&gt; PySparkDataFrame:\n    \"\"\"\n    Removes rows with flatlining detected.\n\n    Returns:\n        pyspark.sql.DataFrame: A DataFrame without rows with flatlining detected.\n    \"\"\"\n    flatlined_rows = self.flatline_detection.check_for_flatlining()\n    flatlined_rows = flatlined_rows.select(*self.df.columns)\n    return self.df.subtract(flatlined_rows)\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/gaussian_smoothing/","title":"Gaussian Smoothing","text":""},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/gaussian_smoothing/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.gaussian_smoothing.GaussianSmoothing","title":"<code>GaussianSmoothing</code>","text":"<p>               Bases: <code>DataManipulationBaseInterface</code></p> <p>Applies Gaussian smoothing to a PySpark DataFrame. This method smooths the values in a specified column using a Gaussian filter, which helps reduce noise and fluctuations in time-series or spatial data.</p> <p>The smoothing can be performed in two modes: - Temporal mode: Applies smoothing along the time axis within each unique ID. - Spatial mode: Applies smoothing across different IDs for the same timestamp.</p>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/gaussian_smoothing/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.gaussian_smoothing.GaussianSmoothing--example","title":"Example","text":"<pre><code>from pyspark.sql import SparkSession\nfrom rtdip_sdk.pipelines.data_quality.data_manipulation.spark.gaussian_smoothing import GaussianSmoothing\n\n\nspark = SparkSession.builder.getOrCreate()\ndf = ...  # Load your PySpark DataFrame\n\nsmoothed_df = GaussianSmoothing(\n    df=df,\n    sigma=2.0,\n    mode=\"temporal\",\n    id_col=\"sensor_id\",\n    timestamp_col=\"timestamp\",\n    value_col=\"measurement\"\n).filter_data()\n\nsmoothed_df.show()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>sigma</code> <code>float</code> <p>The standard deviation for the Gaussian kernel, controlling the amount of smoothing.</p> required <code>mode</code> <code>str</code> <p>The smoothing mode, either <code>\"temporal\"</code> (default) or <code>\"spatial\"</code>.</p> <code>'temporal'</code> <code>id_col</code> <code>str</code> <p>The name of the column representing unique entity IDs (default: <code>\"id\"</code>).</p> <code>'id'</code> <code>timestamp_col</code> <code>str</code> <p>The name of the column representing timestamps (default: <code>\"timestamp\"</code>).</p> <code>'timestamp'</code> <code>value_col</code> <code>str</code> <p>The name of the column containing the values to be smoothed (default: <code>\"value\"</code>).</p> <code>'value'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>df</code> is not a PySpark DataFrame.</p> <code>ValueError</code> <p>If <code>sigma</code> is not a positive number.</p> <code>ValueError</code> <p>If <code>mode</code> is not <code>\"temporal\"</code> or <code>\"spatial\"</code>.</p> <code>ValueError</code> <p>If <code>id_col</code>, <code>timestamp_col</code>, or <code>value_col</code> are not found in the DataFrame.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/gaussian_smoothing.py</code> <pre><code>class GaussianSmoothing(DataManipulationBaseInterface):\n    \"\"\"\n    Applies Gaussian smoothing to a PySpark DataFrame. This method smooths the values in a specified column\n    using a Gaussian filter, which helps reduce noise and fluctuations in time-series or spatial data.\n\n    The smoothing can be performed in two modes:\n    - **Temporal mode**: Applies smoothing along the time axis within each unique ID.\n    - **Spatial mode**: Applies smoothing across different IDs for the same timestamp.\n\n    Example\n    --------\n    ```python\n    from pyspark.sql import SparkSession\n    from rtdip_sdk.pipelines.data_quality.data_manipulation.spark.gaussian_smoothing import GaussianSmoothing\n\n\n    spark = SparkSession.builder.getOrCreate()\n    df = ...  # Load your PySpark DataFrame\n\n    smoothed_df = GaussianSmoothing(\n        df=df,\n        sigma=2.0,\n        mode=\"temporal\",\n        id_col=\"sensor_id\",\n        timestamp_col=\"timestamp\",\n        value_col=\"measurement\"\n    ).filter_data()\n\n    smoothed_df.show()\n    ```\n\n    Parameters:\n        df (PySparkDataFrame): The input PySpark DataFrame.\n        sigma (float): The standard deviation for the Gaussian kernel, controlling the amount of smoothing.\n        mode (str, optional): The smoothing mode, either `\"temporal\"` (default) or `\"spatial\"`.\n        id_col (str, optional): The name of the column representing unique entity IDs (default: `\"id\"`).\n        timestamp_col (str, optional): The name of the column representing timestamps (default: `\"timestamp\"`).\n        value_col (str, optional): The name of the column containing the values to be smoothed (default: `\"value\"`).\n\n    Raises:\n        TypeError: If `df` is not a PySpark DataFrame.\n        ValueError: If `sigma` is not a positive number.\n        ValueError: If `mode` is not `\"temporal\"` or `\"spatial\"`.\n        ValueError: If `id_col`, `timestamp_col`, or `value_col` are not found in the DataFrame.\n    \"\"\"\n\n    def __init__(\n        self,\n        df: PySparkDataFrame,\n        sigma: float,\n        mode: str = \"temporal\",\n        id_col: str = \"id\",\n        timestamp_col: str = \"timestamp\",\n        value_col: str = \"value\",\n    ) -&gt; None:\n        if not isinstance(df, PySparkDataFrame):\n            raise TypeError(\"df must be a PySpark DataFrame\")\n        if not isinstance(sigma, (int, float)) or sigma &lt;= 0:\n            raise ValueError(\"sigma must be a positive number\")\n        if mode not in [\"temporal\", \"spatial\"]:\n            raise ValueError(\"mode must be either 'temporal' or 'spatial'\")\n\n        if id_col not in df.columns:\n            raise ValueError(f\"Column {id_col} not found in DataFrame\")\n        if timestamp_col not in df.columns:\n            raise ValueError(f\"Column {timestamp_col} not found in DataFrame\")\n        if value_col not in df.columns:\n            raise ValueError(f\"Column {value_col} not found in DataFrame\")\n\n        self.df = df\n        self.sigma = sigma\n        self.mode = mode\n        self.id_col = id_col\n        self.timestamp_col = timestamp_col\n        self.value_col = value_col\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    @staticmethod\n    def create_gaussian_smoother(sigma_value):\n        def apply_gaussian(values):\n            if not values:\n                return None\n            values_array = np.array([float(v) for v in values])\n            smoothed = gaussian_filter1d(values_array, sigma=sigma_value)\n            return float(smoothed[-1])\n\n        return apply_gaussian\n\n    def filter_data(self) -&gt; PySparkDataFrame:\n\n        smooth_udf = F.udf(self.create_gaussian_smoother(self.sigma), FloatType())\n\n        if self.mode == \"temporal\":\n            window = (\n                Window.partitionBy(self.id_col)\n                .orderBy(self.timestamp_col)\n                .rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n            )\n        else:  # spatial mode\n            window = (\n                Window.partitionBy(self.timestamp_col)\n                .orderBy(self.id_col)\n                .rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n            )\n\n        collect_list_expr = F.collect_list(F.col(self.value_col)).over(window)\n\n        return self.df.withColumn(self.value_col, smooth_udf(collect_list_expr))\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/interval_filtering/","title":"Interval Filtering","text":""},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/interval_filtering/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.interval_filtering.IntervalFiltering","title":"<code>IntervalFiltering</code>","text":"<p>               Bases: <code>DataManipulationBaseInterface</code>, <code>InputValidator</code></p> <p>Cleanses a DataFrame by removing rows outside a specified interval window. Supported time stamp columns are DateType and StringType.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>A SparkSession object.</p> required <code>df</code> <code>DataFrame</code> <p>PySpark DataFrame to be converted</p> required <code>interval</code> <code>int</code> <p>The interval length for cleansing.</p> required <code>interval_unit</code> <code>str</code> <p>'hours', 'minutes', 'seconds' or 'milliseconds' to specify the unit of the interval.</p> required <code>time_stamp_column_name</code> <code>str</code> <p>The name of the column containing the time stamps. Default is 'EventTime'.</p> <code>None</code> <code>tolerance</code> <code>int</code> <p>The tolerance for the interval. Default is None.</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/interval_filtering.py</code> <pre><code>class IntervalFiltering(DataManipulationBaseInterface, InputValidator):\n    \"\"\"\n    Cleanses a DataFrame by removing rows outside a specified interval window. Supported time stamp columns are DateType and StringType.\n\n    Parameters:\n        spark (SparkSession): A SparkSession object.\n        df (DataFrame): PySpark DataFrame to be converted\n        interval (int): The interval length for cleansing.\n        interval_unit (str): 'hours', 'minutes', 'seconds' or 'milliseconds' to specify the unit of the interval.\n        time_stamp_column_name (str): The name of the column containing the time stamps. Default is 'EventTime'.\n        tolerance (int): The tolerance for the interval. Default is None.\n    \"\"\"\n\n    \"\"\" Default time stamp column name if not set in the constructor \"\"\"\n    DEFAULT_TIME_STAMP_COLUMN_NAME: str = \"EventTime\"\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        df: DataFrame,\n        interval: int,\n        interval_unit: str,\n        time_stamp_column_name: str = None,\n        tolerance: int = None,\n    ) -&gt; None:\n        self.spark = spark\n        self.df = df\n        self.interval = interval\n        self.interval_unit = interval_unit\n        self.tolerance = tolerance\n        if time_stamp_column_name is None:\n            self.time_stamp_column_name = self.DEFAULT_TIME_STAMP_COLUMN_NAME\n        else:\n            self.time_stamp_column_name = time_stamp_column_name\n\n    def filter_data(self) -&gt; DataFrame:\n        \"\"\"\n        Filters the DataFrame based on the interval\n        \"\"\"\n\n        if self.time_stamp_column_name not in self.df.columns:\n            raise ValueError(\n                f\"Column {self.time_stamp_column_name} not found in the DataFrame.\"\n            )\n        is_string_time_stamp = isinstance(\n            self.df.schema[self.time_stamp_column_name].dataType, StringType\n        )\n\n        original_schema = self.df.schema\n        self.df = self.convert_column_to_timestamp().orderBy(\n            self.time_stamp_column_name\n        )\n\n        tolerance_in_ms = None\n        if self.tolerance is not None:\n            tolerance_in_ms = self.get_time_delta(self.tolerance).total_seconds() * 1000\n\n        time_delta_in_ms = self.get_time_delta(self.interval).total_seconds() * 1000\n\n        rows = self.df.collect()\n        last_time_stamp = rows[0][self.time_stamp_column_name]\n        first_row = rows[0].asDict()\n\n        first_row[self.time_stamp_column_name] = (\n            self.format_date_time_to_string(first_row[self.time_stamp_column_name])\n            if is_string_time_stamp\n            else first_row[self.time_stamp_column_name]\n        )\n\n        cleansed_df = [first_row]\n\n        for i in range(1, len(rows)):\n            current_row = rows[i]\n            current_time_stamp = current_row[self.time_stamp_column_name]\n\n            if self.check_outside_of_interval(\n                current_time_stamp, last_time_stamp, time_delta_in_ms, tolerance_in_ms\n            ):\n                current_row_dict = current_row.asDict()\n                current_row_dict[self.time_stamp_column_name] = (\n                    self.format_date_time_to_string(\n                        current_row_dict[self.time_stamp_column_name]\n                    )\n                    if is_string_time_stamp\n                    else current_row_dict[self.time_stamp_column_name]\n                )\n\n                cleansed_df.append(current_row_dict)\n                last_time_stamp = current_time_stamp\n\n        result_df = self.spark.createDataFrame(cleansed_df, schema=original_schema)\n\n        return result_df\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def convert_column_to_timestamp(self) -&gt; DataFrame:\n        try:\n            return self.df.withColumn(\n                self.time_stamp_column_name, F.to_timestamp(self.time_stamp_column_name)\n            )\n        except Exception as e:\n            raise ValueError(\n                f\"Error converting column {self.time_stamp_column_name} to timestamp: {e}\"\n                f\"{self.df.schema[self.time_stamp_column_name].dataType} might be unsupported!\"\n            )\n\n    def get_time_delta(self, value: int) -&gt; timedelta:\n        if self.interval_unit == \"minutes\":\n            return timedelta(minutes=value)\n        elif self.interval_unit == \"days\":\n            return timedelta(days=value)\n        elif self.interval_unit == \"hours\":\n            return timedelta(hours=value)\n        elif self.interval_unit == \"seconds\":\n            return timedelta(seconds=value)\n        elif self.interval_unit == \"milliseconds\":\n            return timedelta(milliseconds=value)\n        else:\n            raise ValueError(\n                \"interval_unit must be either 'days', 'hours', 'minutes', 'seconds' or 'milliseconds'\"\n            )\n\n    def check_outside_of_interval(\n        self,\n        current_time_stamp: pd.Timestamp,\n        last_time_stamp: pd.Timestamp,\n        time_delta_in_ms: float,\n        tolerance_in_ms: float,\n    ) -&gt; bool:\n        time_difference = (current_time_stamp - last_time_stamp).total_seconds() * 1000\n        if not tolerance_in_ms is None:\n            time_difference += tolerance_in_ms\n        return time_difference &gt;= time_delta_in_ms\n\n    def format_date_time_to_string(self, time_stamp: pd.Timestamp) -&gt; str:\n        try:\n            return time_stamp.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n        except Exception as e:\n            raise ValueError(f\"Error converting timestamp to string: {e}\")\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/interval_filtering/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.interval_filtering.IntervalFiltering.filter_data","title":"<code>filter_data()</code>","text":"<p>Filters the DataFrame based on the interval</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/interval_filtering.py</code> <pre><code>def filter_data(self) -&gt; DataFrame:\n    \"\"\"\n    Filters the DataFrame based on the interval\n    \"\"\"\n\n    if self.time_stamp_column_name not in self.df.columns:\n        raise ValueError(\n            f\"Column {self.time_stamp_column_name} not found in the DataFrame.\"\n        )\n    is_string_time_stamp = isinstance(\n        self.df.schema[self.time_stamp_column_name].dataType, StringType\n    )\n\n    original_schema = self.df.schema\n    self.df = self.convert_column_to_timestamp().orderBy(\n        self.time_stamp_column_name\n    )\n\n    tolerance_in_ms = None\n    if self.tolerance is not None:\n        tolerance_in_ms = self.get_time_delta(self.tolerance).total_seconds() * 1000\n\n    time_delta_in_ms = self.get_time_delta(self.interval).total_seconds() * 1000\n\n    rows = self.df.collect()\n    last_time_stamp = rows[0][self.time_stamp_column_name]\n    first_row = rows[0].asDict()\n\n    first_row[self.time_stamp_column_name] = (\n        self.format_date_time_to_string(first_row[self.time_stamp_column_name])\n        if is_string_time_stamp\n        else first_row[self.time_stamp_column_name]\n    )\n\n    cleansed_df = [first_row]\n\n    for i in range(1, len(rows)):\n        current_row = rows[i]\n        current_time_stamp = current_row[self.time_stamp_column_name]\n\n        if self.check_outside_of_interval(\n            current_time_stamp, last_time_stamp, time_delta_in_ms, tolerance_in_ms\n        ):\n            current_row_dict = current_row.asDict()\n            current_row_dict[self.time_stamp_column_name] = (\n                self.format_date_time_to_string(\n                    current_row_dict[self.time_stamp_column_name]\n                )\n                if is_string_time_stamp\n                else current_row_dict[self.time_stamp_column_name]\n            )\n\n            cleansed_df.append(current_row_dict)\n            last_time_stamp = current_time_stamp\n\n    result_df = self.spark.createDataFrame(cleansed_df, schema=original_schema)\n\n    return result_df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/interval_filtering/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.interval_filtering.IntervalFiltering.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/interval_filtering.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/k_sigma_anomaly_detection/","title":"K-Sigma Anomaly Detection","text":""},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/k_sigma_anomaly_detection/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.k_sigma_anomaly_detection.KSigmaAnomalyDetection","title":"<code>KSigmaAnomalyDetection</code>","text":"<p>               Bases: <code>DataManipulationBaseInterface</code>, <code>InputValidator</code></p> <p>Anomaly detection with the k-sigma method. This method either computes the mean and standard deviation, or the median and the median absolute deviation (MAD) of the data. The k-sigma method then filters out all data points that are k times the standard deviation away from the mean, or k times the MAD away from the median. Assuming a normal distribution, this method keeps around 99.7% of the data points when k=3 and use_median=False.</p>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/k_sigma_anomaly_detection/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.k_sigma_anomaly_detection.KSigmaAnomalyDetection--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.data_quality.data_manipulation.spark.k_sigma_anomaly_detection import KSigmaAnomalyDetection\n\n\nspark = ... # SparkSession\ndf = ... # Get a PySpark DataFrame\n\nfiltered_df = KSigmaAnomalyDetection(\n    spark, df, [\"&lt;column to filter&gt;\"]\n).filter_data()\n\nfiltered_df.show()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>A SparkSession object.</p> required <code>df</code> <code>DataFrame</code> <p>Dataframe containing the raw data.</p> required <code>column_names</code> <code>list[str]</code> <p>The names of the columns to be filtered (currently only one column is supported).</p> required <code>k_value</code> <code>float</code> <p>The number of deviations to build the threshold.</p> <code>3.0</code> <code>use_median</code> <code>book</code> <p>If True the median and the median absolute deviation (MAD) are used, instead of the mean and standard deviation.</p> <code>False</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/k_sigma_anomaly_detection.py</code> <pre><code>class KSigmaAnomalyDetection(DataManipulationBaseInterface, InputValidator):\n    \"\"\"\n    Anomaly detection with the k-sigma method. This method either computes the mean and standard deviation, or the median and the median absolute deviation (MAD) of the data.\n    The k-sigma method then filters out all data points that are k times the standard deviation away from the mean, or k times the MAD away from the median.\n    Assuming a normal distribution, this method keeps around 99.7% of the data points when k=3 and use_median=False.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.data_quality.data_manipulation.spark.k_sigma_anomaly_detection import KSigmaAnomalyDetection\n\n\n    spark = ... # SparkSession\n    df = ... # Get a PySpark DataFrame\n\n    filtered_df = KSigmaAnomalyDetection(\n        spark, df, [\"&lt;column to filter&gt;\"]\n    ).filter_data()\n\n    filtered_df.show()\n    ```\n\n    Parameters:\n        spark (SparkSession): A SparkSession object.\n        df (DataFrame): Dataframe containing the raw data.\n        column_names (list[str]): The names of the columns to be filtered (currently only one column is supported).\n        k_value (float): The number of deviations to build the threshold.\n        use_median (book): If True the median and the median absolute deviation (MAD) are used, instead of the mean and standard deviation.\n    \"\"\"\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        df: DataFrame,\n        column_names: list[str],\n        k_value: float = 3.0,\n        use_median: bool = False,\n    ) -&gt; None:\n        if len(column_names) == 0:\n            raise Exception(\"You must provide at least one column name\")\n        if len(column_names) &gt; 1:\n            raise NotImplementedError(\"Multiple columns are not supported yet\")\n\n        self.column_names = column_names\n        self.use_median = use_median\n        self.spark = spark\n        self.df = df\n        self.k_value = k_value\n\n        self.validate(\n            StructType(\n                [StructField(column, DoubleType(), True) for column in column_names]\n            )\n        )\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def filter_data(self) -&gt; DataFrame:\n        \"\"\"\n        Filter anomalies based on the k-sigma rule\n        \"\"\"\n\n        column_name = self.column_names[0]\n        mean_value, deviation = 0, 0\n\n        if self.use_median:\n            mean_value = self.df.approxQuantile(column_name, [0.5], 0.0)[0]\n            if mean_value is None:\n                raise Exception(\"Failed to calculate the mean value\")\n\n            df_with_deviation = self.df.withColumn(\n                \"absolute_deviation\", abs(col(column_name) - mean_value)\n            )\n            deviation = df_with_deviation.approxQuantile(\n                \"absolute_deviation\", [0.5], 0.0\n            )[0]\n            if deviation is None:\n                raise Exception(\"Failed to calculate the deviation value\")\n        else:\n            stats = self.df.select(\n                mean(column_name), stddev(self.column_names[0])\n            ).first()\n            if stats is None:\n                raise Exception(\n                    \"Failed to calculate the mean value and the standard deviation value\"\n                )\n\n            mean_value = stats[0]\n            deviation = stats[1]\n\n        shift = self.k_value * deviation\n        lower_bound = mean_value - shift\n        upper_bound = mean_value + shift\n\n        return self.df.filter(\n            (self.df[column_name] &gt;= lower_bound)\n            &amp; (self.df[column_name] &lt;= upper_bound)\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/k_sigma_anomaly_detection/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.k_sigma_anomaly_detection.KSigmaAnomalyDetection.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/k_sigma_anomaly_detection.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/k_sigma_anomaly_detection/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.k_sigma_anomaly_detection.KSigmaAnomalyDetection.filter_data","title":"<code>filter_data()</code>","text":"<p>Filter anomalies based on the k-sigma rule</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/k_sigma_anomaly_detection.py</code> <pre><code>def filter_data(self) -&gt; DataFrame:\n    \"\"\"\n    Filter anomalies based on the k-sigma rule\n    \"\"\"\n\n    column_name = self.column_names[0]\n    mean_value, deviation = 0, 0\n\n    if self.use_median:\n        mean_value = self.df.approxQuantile(column_name, [0.5], 0.0)[0]\n        if mean_value is None:\n            raise Exception(\"Failed to calculate the mean value\")\n\n        df_with_deviation = self.df.withColumn(\n            \"absolute_deviation\", abs(col(column_name) - mean_value)\n        )\n        deviation = df_with_deviation.approxQuantile(\n            \"absolute_deviation\", [0.5], 0.0\n        )[0]\n        if deviation is None:\n            raise Exception(\"Failed to calculate the deviation value\")\n    else:\n        stats = self.df.select(\n            mean(column_name), stddev(self.column_names[0])\n        ).first()\n        if stats is None:\n            raise Exception(\n                \"Failed to calculate the mean value and the standard deviation value\"\n            )\n\n        mean_value = stats[0]\n        deviation = stats[1]\n\n    shift = self.k_value * deviation\n    lower_bound = mean_value - shift\n    upper_bound = mean_value + shift\n\n    return self.df.filter(\n        (self.df[column_name] &gt;= lower_bound)\n        &amp; (self.df[column_name] &lt;= upper_bound)\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/missing_value_imputation/","title":"Missing Value Imputation","text":""},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/missing_value_imputation/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.missing_value_imputation.MissingValueImputation","title":"<code>MissingValueImputation</code>","text":"<p>               Bases: <code>DataManipulationBaseInterface</code>, <code>InputValidator</code></p> <p>Imputes missing values in a univariate time series creating a continuous curve of data points. For that, the time intervals of each individual source is calculated, to then insert empty records at the missing timestamps with NaN values. Through spline interpolation the missing NaN values are calculated resulting in a consistent data set and thus enhance your data quality.</p>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/missing_value_imputation/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.missing_value_imputation.MissingValueImputation--example","title":"Example","text":"<pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.dataframe import DataFrame\nfrom pyspark.sql.types import StructType, StructField, StringType\nfrom rtdip_sdk.pipelines.data_quality.data_manipulation.spark.missing_value_imputation import (\n    MissingValueImputation,\n)\n\nspark = spark_session()\n\nschema = StructType([\n    StructField(\"TagName\", StringType(), True),\n    StructField(\"EventTime\", StringType(), True),\n    StructField(\"Status\", StringType(), True),\n    StructField(\"Value\", StringType(), True)\n])\n\ndata = [\n    (\"A2PS64V0J.:ZUX09R\", \"2024-01-01 03:29:21.000\", \"Good\", \"1.0\"),\n    (\"A2PS64V0J.:ZUX09R\", \"2024-01-01 07:32:55.000\", \"Good\", \"2.0\"),\n    (\"A2PS64V0J.:ZUX09R\", \"2024-01-01 11:36:29.000\", \"Good\", \"3.0\"),\n    (\"A2PS64V0J.:ZUX09R\", \"2024-01-01 15:39:03.000\", \"Good\", \"4.0\"),\n    (\"A2PS64V0J.:ZUX09R\", \"2024-01-01 19:42:37.000\", \"Good\", \"5.0\"),\n    #(\"A2PS64V0J.:ZUX09R\", \"2024-01-01 23:46:11.000\", \"Good\", \"6.0\"), # Test values\n    #(\"A2PS64V0J.:ZUX09R\", \"2024-01-02 03:49:45.000\", \"Good\", \"7.0\"),\n    (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 07:53:11.000\", \"Good\", \"8.0\"),\n]\ndf = spark.createDataFrame(data, schema=schema)\n\nmissing_value_imputation = MissingValueImputation(spark, df)\nresult = missing_value_imputation.filter_data()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe containing the raw data.</p> required <code>tolerance_percentage</code> <code>int</code> <p>Percentage value that indicates how much the time series data points may vary in each interval</p> <code>5</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/missing_value_imputation.py</code> <pre><code>class MissingValueImputation(DataManipulationBaseInterface, InputValidator):\n    \"\"\"\n    Imputes missing values in a univariate time series creating a continuous curve of data points. For that, the\n    time intervals of each individual source is calculated, to then insert empty records at the missing timestamps with\n    NaN values. Through spline interpolation the missing NaN values are calculated resulting in a consistent data set\n    and thus enhance your data quality.\n\n    Example\n    --------\n    ```python\n    from pyspark.sql import SparkSession\n    from pyspark.sql.dataframe import DataFrame\n    from pyspark.sql.types import StructType, StructField, StringType\n    from rtdip_sdk.pipelines.data_quality.data_manipulation.spark.missing_value_imputation import (\n        MissingValueImputation,\n    )\n\n    spark = spark_session()\n\n    schema = StructType([\n        StructField(\"TagName\", StringType(), True),\n        StructField(\"EventTime\", StringType(), True),\n        StructField(\"Status\", StringType(), True),\n        StructField(\"Value\", StringType(), True)\n    ])\n\n    data = [\n        (\"A2PS64V0J.:ZUX09R\", \"2024-01-01 03:29:21.000\", \"Good\", \"1.0\"),\n        (\"A2PS64V0J.:ZUX09R\", \"2024-01-01 07:32:55.000\", \"Good\", \"2.0\"),\n        (\"A2PS64V0J.:ZUX09R\", \"2024-01-01 11:36:29.000\", \"Good\", \"3.0\"),\n        (\"A2PS64V0J.:ZUX09R\", \"2024-01-01 15:39:03.000\", \"Good\", \"4.0\"),\n        (\"A2PS64V0J.:ZUX09R\", \"2024-01-01 19:42:37.000\", \"Good\", \"5.0\"),\n        #(\"A2PS64V0J.:ZUX09R\", \"2024-01-01 23:46:11.000\", \"Good\", \"6.0\"), # Test values\n        #(\"A2PS64V0J.:ZUX09R\", \"2024-01-02 03:49:45.000\", \"Good\", \"7.0\"),\n        (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 07:53:11.000\", \"Good\", \"8.0\"),\n    ]\n    df = spark.createDataFrame(data, schema=schema)\n\n    missing_value_imputation = MissingValueImputation(spark, df)\n    result = missing_value_imputation.filter_data()\n    ```\n\n    Parameters:\n        df (DataFrame): Dataframe containing the raw data.\n        tolerance_percentage (int): Percentage value that indicates how much the time series data points may vary\n            in each interval\n    \"\"\"\n\n    df: PySparkDataFrame\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        df: PySparkDataFrame,\n        tolerance_percentage: int = 5,\n    ) -&gt; None:\n        self.spark = spark\n        self.df = df\n        self.tolerance_percentage = tolerance_percentage\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    @staticmethod\n    def _impute_missing_values_sp(df) -&gt; PySparkDataFrame:\n        \"\"\"\n        Imputes missing values by Spline Interpolation\n        \"\"\"\n        data = np.array(\n            df.select(\"Value\").rdd.flatMap(lambda x: x).collect(), dtype=float\n        )\n        mask = np.isnan(data)\n\n        x_data = np.arange(len(data))\n        y_data = data[~mask]\n\n        spline = UnivariateSpline(x_data[~mask], y_data, s=0)\n\n        data_imputed = data.copy()\n        data_imputed[mask] = spline(x_data[mask])\n        data_imputed_list = data_imputed.tolist()\n\n        imputed_rdd = df.rdd.zipWithIndex().map(\n            lambda row: Row(\n                TagName=row[0][0],\n                EventTime=row[0][1],\n                Status=row[0][2],\n                Value=float(data_imputed_list[row[1]]),\n            )\n        )\n        imputed_df = imputed_rdd.toDF(df.schema)\n\n        return imputed_df\n\n    @staticmethod\n    def _flag_missing_values(df, tolerance_percentage) -&gt; PySparkDataFrame:\n        \"\"\"\n        Determines intervals of each respective source time series and inserts empty records at missing timestamps\n        with NaN values\n        \"\"\"\n        window_spec = Window.partitionBy(\"TagName\").orderBy(\"EventTime\")\n\n        df = df.withColumn(\"prev_event_time\", F.lag(\"EventTime\").over(window_spec))\n        df = df.withColumn(\n            \"time_diff_seconds\",\n            (F.unix_timestamp(\"EventTime\") - F.unix_timestamp(\"prev_event_time\")),\n        )\n\n        df_diff = df.filter(F.col(\"time_diff_seconds\").isNotNull())\n        interval_counts = df_diff.groupBy(\"time_diff_seconds\").count()\n        most_frequent_interval = interval_counts.orderBy(F.desc(\"count\")).first()\n        expected_interval = (\n            most_frequent_interval[\"time_diff_seconds\"]\n            if most_frequent_interval\n            else None\n        )\n\n        tolerance = (\n            (expected_interval * tolerance_percentage) / 100 if expected_interval else 0\n        )\n\n        existing_timestamps = (\n            df.select(\"TagName\", \"EventTime\")\n            .rdd.map(lambda row: (row[\"TagName\"], row[\"EventTime\"]))\n            .groupByKey()\n            .collectAsMap()\n        )\n\n        def generate_missing_timestamps(prev_event_time, event_time, tag_name):\n            # Check for first row\n            if (\n                prev_event_time is None\n                or event_time is None\n                or expected_interval is None\n            ):\n                return []\n\n            # Check against existing timestamps to avoid duplicates\n            tag_timestamps = set(existing_timestamps.get(tag_name, []))\n            missing_timestamps = []\n            current_time = prev_event_time\n\n            while current_time &lt; event_time:\n                next_expected_time = current_time + timedelta(seconds=expected_interval)\n                time_diff = abs((next_expected_time - event_time).total_seconds())\n                if time_diff &lt;= tolerance:\n                    break\n                if next_expected_time not in tag_timestamps:\n                    missing_timestamps.append(next_expected_time)\n                current_time = next_expected_time\n\n            return missing_timestamps\n\n        generate_missing_timestamps_udf = udf(\n            generate_missing_timestamps, ArrayType(TimestampType())\n        )\n\n        df_with_missing = df.withColumn(\n            \"missing_timestamps\",\n            generate_missing_timestamps_udf(\"prev_event_time\", \"EventTime\", \"TagName\"),\n        )\n\n        df_missing_entries = df_with_missing.select(\n            \"TagName\",\n            F.explode(\"missing_timestamps\").alias(\"EventTime\"),\n            F.lit(\"Good\").alias(\"Status\"),\n            F.lit(float(\"nan\")).cast(FloatType()).alias(\"Value\"),\n        )\n\n        df_combined = (\n            df.select(\"TagName\", \"EventTime\", \"Status\", \"Value\")\n            .union(df_missing_entries)\n            .orderBy(\"EventTime\")\n        )\n\n        return df_combined\n\n    @staticmethod\n    def _is_column_type(df, column_name, data_type):\n        \"\"\"\n        Helper method for data type checking\n        \"\"\"\n        type_ = df.schema[column_name]\n\n        return isinstance(type_.dataType, data_type)\n\n    def filter_data(self) -&gt; PySparkDataFrame:\n        \"\"\"\n        Imputate missing values based on [Spline Interpolation, ]\n        \"\"\"\n        if not all(\n            col_ in self.df.columns\n            for col_ in [\"TagName\", \"EventTime\", \"Value\", \"Status\"]\n        ):\n            raise ValueError(\"Columns not as expected\")\n\n        if not self._is_column_type(self.df, \"EventTime\", TimestampType):\n            if self._is_column_type(self.df, \"EventTime\", StringType):\n                # Attempt to parse the first format, then fallback to the second\n                self.df = self.df.withColumn(\n                    \"EventTime\",\n                    F.coalesce(\n                        F.to_timestamp(\"EventTime\", \"yyyy-MM-dd HH:mm:ss.SSS\"),\n                        F.to_timestamp(\"EventTime\", \"dd.MM.yyyy HH:mm:ss\"),\n                    ),\n                )\n        if not self._is_column_type(self.df, \"Value\", FloatType):\n            self.df = self.df.withColumn(\"Value\", self.df[\"Value\"].cast(FloatType()))\n\n        dfs_by_source = self._split_by_source()\n\n        imputed_dfs: List[PySparkDataFrame] = []\n\n        for source, df in dfs_by_source.items():\n            # Determine, insert and flag all the missing entries\n            flagged_df = self._flag_missing_values(df, self.tolerance_percentage)\n\n            # Impute the missing values of flagged entries\n            try:\n                imputed_df_sp = self._impute_missing_values_sp(flagged_df)\n            except Exception as e:\n                if flagged_df.count() != 1:  # Account for single entries\n                    raise Exception(\n                        \"Something went wrong while imputing missing values\"\n                    )\n\n            imputed_dfs.append(imputed_df_sp)\n\n        result_df = imputed_dfs[0]\n        for df in imputed_dfs[1:]:\n            result_df = result_df.unionByName(df)\n\n        return result_df\n\n    def _split_by_source(self) -&gt; dict:\n        \"\"\"\n        Helper method to separate individual time series based on their source\n        \"\"\"\n        tag_names = self.df.select(\"TagName\").distinct().collect()\n        tag_names = [row[\"TagName\"] for row in tag_names]\n        source_dict = {\n            tag: self.df.filter(col(\"TagName\") == tag).orderBy(\"EventTime\")\n            for tag in tag_names\n        }\n\n        return source_dict\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/missing_value_imputation/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.missing_value_imputation.MissingValueImputation.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/missing_value_imputation.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/missing_value_imputation/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.missing_value_imputation.MissingValueImputation.filter_data","title":"<code>filter_data()</code>","text":"<p>Imputate missing values based on [Spline Interpolation, ]</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/missing_value_imputation.py</code> <pre><code>def filter_data(self) -&gt; PySparkDataFrame:\n    \"\"\"\n    Imputate missing values based on [Spline Interpolation, ]\n    \"\"\"\n    if not all(\n        col_ in self.df.columns\n        for col_ in [\"TagName\", \"EventTime\", \"Value\", \"Status\"]\n    ):\n        raise ValueError(\"Columns not as expected\")\n\n    if not self._is_column_type(self.df, \"EventTime\", TimestampType):\n        if self._is_column_type(self.df, \"EventTime\", StringType):\n            # Attempt to parse the first format, then fallback to the second\n            self.df = self.df.withColumn(\n                \"EventTime\",\n                F.coalesce(\n                    F.to_timestamp(\"EventTime\", \"yyyy-MM-dd HH:mm:ss.SSS\"),\n                    F.to_timestamp(\"EventTime\", \"dd.MM.yyyy HH:mm:ss\"),\n                ),\n            )\n    if not self._is_column_type(self.df, \"Value\", FloatType):\n        self.df = self.df.withColumn(\"Value\", self.df[\"Value\"].cast(FloatType()))\n\n    dfs_by_source = self._split_by_source()\n\n    imputed_dfs: List[PySparkDataFrame] = []\n\n    for source, df in dfs_by_source.items():\n        # Determine, insert and flag all the missing entries\n        flagged_df = self._flag_missing_values(df, self.tolerance_percentage)\n\n        # Impute the missing values of flagged entries\n        try:\n            imputed_df_sp = self._impute_missing_values_sp(flagged_df)\n        except Exception as e:\n            if flagged_df.count() != 1:  # Account for single entries\n                raise Exception(\n                    \"Something went wrong while imputing missing values\"\n                )\n\n        imputed_dfs.append(imputed_df_sp)\n\n    result_df = imputed_dfs[0]\n    for df in imputed_dfs[1:]:\n        result_df = result_df.unionByName(df)\n\n    return result_df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/out_of_range_value_filter/","title":"Out of Range Value Filter","text":""},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/out_of_range_value_filter/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.out_of_range_value_filter.OutOfRangeValueFilter","title":"<code>OutOfRangeValueFilter</code>","text":"<p>               Bases: <code>DataManipulationBaseInterface</code></p> <p>Filters data in a DataFrame by checking the 'Value' column against expected ranges for specified TagNames. Logs events when 'Value' exceeds the defined ranges for any TagName and deletes the rows.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to monitor.</p> required <code>tag_ranges</code> <code>dict</code> <p>A dictionary where keys are TagNames and values are dictionaries specifying 'min' and/or 'max', and optionally 'inclusive_bounds' values. Example:     {         'A2PS64V0J.:ZUX09R': {'min': 0, 'max': 100, 'inclusive_bounds': True},         'B3TS64V0K.:ZUX09R': {'min': 10, 'max': 200, 'inclusive_bounds': False},     }</p> required Example <pre><code>from pyspark.sql import SparkSession\nfrom rtdip_sdk.pipelines.data_quality.data_manipulation.spark.out_of_range_value_filter import OutOfRangeValueFilter\n\n\nspark = SparkSession.builder.master(\"local[1]\").appName(\"DeleteOutOfRangeValuesExample\").getOrCreate()\n\ndata = [\n    (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 03:49:45.000\", \"Good\", 25.0),\n    (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 07:53:11.000\", \"Good\", -5.0),\n    (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 11:56:42.000\", \"Good\", 50.0),\n    (\"B3TS64V0K.:ZUX09R\", \"2024-01-02 16:00:12.000\", \"Good\", 80.0),\n    (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 20:03:46.000\", \"Good\", 100.0),\n]\n\ncolumns = [\"TagName\", \"EventTime\", \"Status\", \"Value\"]\n\ndf = spark.createDataFrame(data, columns)\n\ntag_ranges = {\n    \"A2PS64V0J.:ZUX09R\": {\"min\": 0, \"max\": 50, \"inclusive_bounds\": True},\n    \"B3TS64V0K.:ZUX09R\": {\"min\": 50, \"max\": 100, \"inclusive_bounds\": False},\n}\n\nout_of_range_value_filter = OutOfRangeValueFilter(\n    df=df,\n    tag_ranges=tag_ranges,\n)\n\nresult_df = out_of_range_value_filter.filter_data()\n</code></pre> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/out_of_range_value_filter.py</code> <pre><code>class OutOfRangeValueFilter(DataManipulationBaseInterface):\n    \"\"\"\n    Filters data in a DataFrame by checking the 'Value' column against expected ranges for specified TagNames.\n    Logs events when 'Value' exceeds the defined ranges for any TagName and deletes the rows.\n\n    Args:\n        df (pyspark.sql.DataFrame): The DataFrame to monitor.\n        tag_ranges (dict): A dictionary where keys are TagNames and values are dictionaries specifying 'min' and/or\n            'max', and optionally 'inclusive_bounds' values.\n            Example:\n                {\n                    'A2PS64V0J.:ZUX09R': {'min': 0, 'max': 100, 'inclusive_bounds': True},\n                    'B3TS64V0K.:ZUX09R': {'min': 10, 'max': 200, 'inclusive_bounds': False},\n                }\n\n    Example:\n        ```python\n        from pyspark.sql import SparkSession\n        from rtdip_sdk.pipelines.data_quality.data_manipulation.spark.out_of_range_value_filter import OutOfRangeValueFilter\n\n\n        spark = SparkSession.builder.master(\"local[1]\").appName(\"DeleteOutOfRangeValuesExample\").getOrCreate()\n\n        data = [\n            (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 03:49:45.000\", \"Good\", 25.0),\n            (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 07:53:11.000\", \"Good\", -5.0),\n            (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 11:56:42.000\", \"Good\", 50.0),\n            (\"B3TS64V0K.:ZUX09R\", \"2024-01-02 16:00:12.000\", \"Good\", 80.0),\n            (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 20:03:46.000\", \"Good\", 100.0),\n        ]\n\n        columns = [\"TagName\", \"EventTime\", \"Status\", \"Value\"]\n\n        df = spark.createDataFrame(data, columns)\n\n        tag_ranges = {\n            \"A2PS64V0J.:ZUX09R\": {\"min\": 0, \"max\": 50, \"inclusive_bounds\": True},\n            \"B3TS64V0K.:ZUX09R\": {\"min\": 50, \"max\": 100, \"inclusive_bounds\": False},\n        }\n\n        out_of_range_value_filter = OutOfRangeValueFilter(\n            df=df,\n            tag_ranges=tag_ranges,\n        )\n\n        result_df = out_of_range_value_filter.filter_data()\n        ```\n    \"\"\"\n\n    df: PySparkDataFrame\n\n    def __init__(\n        self,\n        df: PySparkDataFrame,\n        tag_ranges: dict,\n    ) -&gt; None:\n        self.df = df\n        self.check_value_ranges = CheckValueRanges(df=df, tag_ranges=tag_ranges)\n\n        # Configure logging\n        self.logger = logging.getLogger(self.__class__.__name__)\n        if not self.logger.handlers:\n            handler = logging.StreamHandler()\n            formatter = logging.Formatter(\n                \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n            )\n            handler.setFormatter(formatter)\n            self.logger.addHandler(handler)\n            self.logger.setLevel(logging.INFO)\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def filter_data(self) -&gt; PySparkDataFrame:\n        \"\"\"\n        Executes the value range checking logic for the specified TagNames. Identifies, logs and deletes any rows\n        where 'Value' exceeds the defined ranges for each TagName.\n\n        Returns:\n            pyspark.sql.DataFrame:\n                Returns a PySpark DataFrame without the rows that were out of range.\n        \"\"\"\n        out_of_range_df = self.check_value_ranges.check_for_out_of_range()\n\n        if out_of_range_df.count() &gt; 0:\n            self.check_value_ranges.log_out_of_range_values(out_of_range_df)\n        else:\n            self.logger.info(f\"No out of range values found in 'Value' column.\")\n        return self.df.subtract(out_of_range_df)\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/out_of_range_value_filter/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.out_of_range_value_filter.OutOfRangeValueFilter.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/out_of_range_value_filter.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/out_of_range_value_filter/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.out_of_range_value_filter.OutOfRangeValueFilter.filter_data","title":"<code>filter_data()</code>","text":"<p>Executes the value range checking logic for the specified TagNames. Identifies, logs and deletes any rows where 'Value' exceeds the defined ranges for each TagName.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: Returns a PySpark DataFrame without the rows that were out of range.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/out_of_range_value_filter.py</code> <pre><code>def filter_data(self) -&gt; PySparkDataFrame:\n    \"\"\"\n    Executes the value range checking logic for the specified TagNames. Identifies, logs and deletes any rows\n    where 'Value' exceeds the defined ranges for each TagName.\n\n    Returns:\n        pyspark.sql.DataFrame:\n            Returns a PySpark DataFrame without the rows that were out of range.\n    \"\"\"\n    out_of_range_df = self.check_value_ranges.check_for_out_of_range()\n\n    if out_of_range_df.count() &gt; 0:\n        self.check_value_ranges.log_out_of_range_values(out_of_range_df)\n    else:\n        self.logger.info(f\"No out of range values found in 'Value' column.\")\n    return self.df.subtract(out_of_range_df)\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/normalization/denormalization/","title":"Denormalization","text":""},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/normalization/denormalization/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.normalization.denormalization.Denormalization","title":"<code>Denormalization</code>","text":"<p>               Bases: <code>DataManipulationBaseInterface</code>, <code>InputValidator</code></p> <p>Applies the appropriate denormalization method to revert values to their original scale.</p> <p>Example</p> <p>```python from rtdip_sdk.pipelines.data_quality.data_manipulation.spark.normalization.denormalization import Denormalization  from pyspark.sql import SparkSession  from pyspark.sql.dataframe import DataFrame</p> <p>denormalization = Denormalization(normalized_df, normalization)  denormalized_df = denormalization.filter_data()  ```</p> <p>Parameters:      df (DataFrame): PySpark DataFrame to be reverted to its original scale.      normalization_to_revert (NormalizationBaseClass): An instance of the specific normalization subclass (NormalizationZScore, NormalizationMinMax, NormalizationMean) that was originally used to normalize the data.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/normalization/denormalization.py</code> <pre><code>class Denormalization(DataManipulationBaseInterface, InputValidator):\n    \"\"\"\n     Applies the appropriate denormalization method to revert values to their original scale.\n\n     Example\n     --------\n     ```python\n    from rtdip_sdk.pipelines.data_quality.data_manipulation.spark.normalization.denormalization import Denormalization\n     from pyspark.sql import SparkSession\n     from pyspark.sql.dataframe import DataFrame\n\n     denormalization = Denormalization(normalized_df, normalization)\n     denormalized_df = denormalization.filter_data()\n     ```\n\n     Parameters:\n         df (DataFrame): PySpark DataFrame to be reverted to its original scale.\n         normalization_to_revert (NormalizationBaseClass): An instance of the specific normalization subclass (NormalizationZScore, NormalizationMinMax, NormalizationMean) that was originally used to normalize the data.\n    \"\"\"\n\n    df: PySparkDataFrame\n    normalization_to_revert: NormalizationBaseClass\n\n    def __init__(\n        self, df: PySparkDataFrame, normalization_to_revert: NormalizationBaseClass\n    ) -&gt; None:\n        self.df = df\n        self.normalization_to_revert = normalization_to_revert\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def filter_data(self) -&gt; PySparkDataFrame:\n        return self.normalization_to_revert.denormalize(self.df)\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/normalization/denormalization/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.normalization.denormalization.Denormalization.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/normalization/denormalization.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/normalization/normalization/","title":"Normalization","text":""},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/normalization/normalization/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.normalization.normalization.NormalizationBaseClass","title":"<code>NormalizationBaseClass</code>","text":"<p>               Bases: <code>DataManipulationBaseInterface</code>, <code>InputValidator</code></p> <p>A base class for applying normalization techniques to multiple columns in a PySpark DataFrame. This class serves as a framework to support various normalization methods (e.g., Z-Score, Min-Max, and Mean), with specific implementations in separate subclasses for each normalization type.</p> <p>Subclasses should implement specific normalization and denormalization methods by inheriting from this base class.</p>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/normalization/normalization/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.normalization.normalization.NormalizationBaseClass--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.data_quality.data_manipulation.spark.normalization.normalization import NormalizationZScore\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.dataframe import DataFrame\n\nnormalization = NormalizationZScore(df, column_names=[\"value_column_1\", \"value_column_2\"], in_place=False)\nnormalized_df = normalization.filter_data()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>PySpark DataFrame to be normalized.</p> required <code>column_names</code> <code>List[str]</code> <p>List of columns in the DataFrame to be normalized.</p> required <code>in_place</code> <code>bool</code> <p>If true, then result of normalization is stored in the same column.</p> <code>False</code> <p>NORMALIZATION_NAME_POSTFIX : str     Suffix added to the column name if a new column is created for normalized values.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/normalization/normalization.py</code> <pre><code>class NormalizationBaseClass(DataManipulationBaseInterface, InputValidator):\n    \"\"\"\n    A base class for applying normalization techniques to multiple columns in a PySpark DataFrame.\n    This class serves as a framework to support various normalization methods (e.g., Z-Score, Min-Max, and Mean),\n    with specific implementations in separate subclasses for each normalization type.\n\n    Subclasses should implement specific normalization and denormalization methods by inheriting from this base class.\n\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.data_quality.data_manipulation.spark.normalization.normalization import NormalizationZScore\n\n    from pyspark.sql import SparkSession\n    from pyspark.sql.dataframe import DataFrame\n\n    normalization = NormalizationZScore(df, column_names=[\"value_column_1\", \"value_column_2\"], in_place=False)\n    normalized_df = normalization.filter_data()\n    ```\n\n    Parameters:\n        df (DataFrame): PySpark DataFrame to be normalized.\n        column_names (List[str]): List of columns in the DataFrame to be normalized.\n        in_place (bool): If true, then result of normalization is stored in the same column.\n\n    Attributes:\n    NORMALIZATION_NAME_POSTFIX : str\n        Suffix added to the column name if a new column is created for normalized values.\n\n    \"\"\"\n\n    df: PySparkDataFrame\n    column_names: List[str]\n    in_place: bool\n\n    reversal_value: List[float]\n\n    # Appended to column name if new column is added\n    NORMALIZATION_NAME_POSTFIX: str = \"normalization\"\n\n    def __init__(\n        self, df: PySparkDataFrame, column_names: List[str], in_place: bool = False\n    ) -&gt; None:\n        self.df = df\n        self.column_names = column_names\n        self.in_place = in_place\n\n        EXPECTED_SCHEMA = StructType(\n            [StructField(column_name, DoubleType()) for column_name in column_names]\n        )\n        self.validate(EXPECTED_SCHEMA)\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def filter_data(self):\n        return self.normalize()\n\n    def normalize(self) -&gt; PySparkDataFrame:\n        \"\"\"\n        Applies the specified normalization to each column in column_names.\n\n        Returns:\n            DataFrame: A PySpark DataFrame with the normalized values.\n        \"\"\"\n        normalized_df = self.df\n        for column in self.column_names:\n            normalized_df = self._normalize_column(normalized_df, column)\n        return normalized_df\n\n    def denormalize(self, input_df) -&gt; PySparkDataFrame:\n        \"\"\"\n        Denormalizes the input DataFrame. Intended to be used by the denormalization component.\n\n        Parameters:\n            input_df (DataFrame): Dataframe containing the current data.\n        \"\"\"\n        denormalized_df = input_df\n        if not self.in_place:\n            for column in self.column_names:\n                denormalized_df = denormalized_df.drop(\n                    self._get_norm_column_name(column)\n                )\n        else:\n            for column in self.column_names:\n                denormalized_df = self._denormalize_column(denormalized_df, column)\n        return denormalized_df\n\n    @property\n    @abstractmethod\n    def NORMALIZED_COLUMN_NAME(self): ...\n\n    @abstractmethod\n    def _normalize_column(self, df: PySparkDataFrame, column: str) -&gt; PySparkDataFrame:\n        pass\n\n    @abstractmethod\n    def _denormalize_column(\n        self, df: PySparkDataFrame, column: str\n    ) -&gt; PySparkDataFrame:\n        pass\n\n    def _get_norm_column_name(self, column_name: str) -&gt; str:\n        if not self.in_place:\n            return f\"{column_name}_{self.NORMALIZED_COLUMN_NAME}_{self.NORMALIZATION_NAME_POSTFIX}\"\n        else:\n            return column_name\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/normalization/normalization/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.normalization.normalization.NormalizationBaseClass.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/normalization/normalization.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/normalization/normalization/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.normalization.normalization.NormalizationBaseClass.normalize","title":"<code>normalize()</code>","text":"<p>Applies the specified normalization to each column in column_names.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A PySpark DataFrame with the normalized values.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/normalization/normalization.py</code> <pre><code>def normalize(self) -&gt; PySparkDataFrame:\n    \"\"\"\n    Applies the specified normalization to each column in column_names.\n\n    Returns:\n        DataFrame: A PySpark DataFrame with the normalized values.\n    \"\"\"\n    normalized_df = self.df\n    for column in self.column_names:\n        normalized_df = self._normalize_column(normalized_df, column)\n    return normalized_df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/normalization/normalization/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.normalization.normalization.NormalizationBaseClass.denormalize","title":"<code>denormalize(input_df)</code>","text":"<p>Denormalizes the input DataFrame. Intended to be used by the denormalization component.</p> <p>Parameters:</p> Name Type Description Default <code>input_df</code> <code>DataFrame</code> <p>Dataframe containing the current data.</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/normalization/normalization.py</code> <pre><code>def denormalize(self, input_df) -&gt; PySparkDataFrame:\n    \"\"\"\n    Denormalizes the input DataFrame. Intended to be used by the denormalization component.\n\n    Parameters:\n        input_df (DataFrame): Dataframe containing the current data.\n    \"\"\"\n    denormalized_df = input_df\n    if not self.in_place:\n        for column in self.column_names:\n            denormalized_df = denormalized_df.drop(\n                self._get_norm_column_name(column)\n            )\n    else:\n        for column in self.column_names:\n            denormalized_df = self._denormalize_column(denormalized_df, column)\n    return denormalized_df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/normalization/normalization_mean/","title":"Normalization Mean","text":""},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/normalization/normalization_mean/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.normalization.normalization_mean.NormalizationMean","title":"<code>NormalizationMean</code>","text":"<p>               Bases: <code>NormalizationBaseClass</code></p> <p>Implements mean normalization for specified columns in a PySpark DataFrame.</p>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/normalization/normalization_mean/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.normalization.normalization_mean.NormalizationMean--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.data_quality.data_manipulation.spark.normalization.normalization_mean import NormalizationMean\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.dataframe import DataFrame\n\nnormalization = NormalizationMean(df, column_names=[\"value_column_1\", \"value_column_2\"], in_place=False)\nnormalized_df = normalization.filter_data()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>PySpark DataFrame to be normalized.</p> required <code>column_names</code> <code>List[str]</code> <p>List of columns in the DataFrame to be normalized.</p> required <code>in_place</code> <code>bool</code> <p>If true, then result of normalization is stored in the same column.</p> <code>False</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/normalization/normalization_mean.py</code> <pre><code>class NormalizationMean(NormalizationBaseClass):\n    \"\"\"\n    Implements mean normalization for specified columns in a PySpark DataFrame.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.data_quality.data_manipulation.spark.normalization.normalization_mean import NormalizationMean\n    from pyspark.sql import SparkSession\n    from pyspark.sql.dataframe import DataFrame\n\n    normalization = NormalizationMean(df, column_names=[\"value_column_1\", \"value_column_2\"], in_place=False)\n    normalized_df = normalization.filter_data()\n    ```\n\n    Parameters:\n        df (DataFrame): PySpark DataFrame to be normalized.\n        column_names (List[str]): List of columns in the DataFrame to be normalized.\n        in_place (bool): If true, then result of normalization is stored in the same column.\n    \"\"\"\n\n    NORMALIZED_COLUMN_NAME = \"mean\"\n\n    def _normalize_column(self, df: PySparkDataFrame, column: str) -&gt; PySparkDataFrame:\n        \"\"\"\n        Private method to apply Mean normalization to the specified column.\n        Mean normalization: (value - mean) / (max - min)\n        \"\"\"\n        mean_val = df.select(F.mean(F.col(column))).collect()[0][0]\n        min_val = df.select(F.min(F.col(column))).collect()[0][0]\n        max_val = df.select(F.max(F.col(column))).collect()[0][0]\n\n        divisor = max_val - min_val\n        if math.isclose(divisor, 0.0, abs_tol=10e-8) or not math.isfinite(divisor):\n            raise ZeroDivisionError(\"Division by Zero in Mean\")\n\n        store_column = self._get_norm_column_name(column)\n        self.reversal_value = [mean_val, min_val, max_val]\n\n        return df.withColumn(\n            store_column,\n            (F.col(column) - F.lit(mean_val)) / (F.lit(max_val) - F.lit(min_val)),\n        )\n\n    def _denormalize_column(\n        self, df: PySparkDataFrame, column: str\n    ) -&gt; PySparkDataFrame:\n        \"\"\"\n        Private method to revert Mean normalization to the specified column.\n        Mean denormalization: normalized_value * (max - min) + mean = value\n        \"\"\"\n        mean_val = self.reversal_value[0]\n        min_val = self.reversal_value[1]\n        max_val = self.reversal_value[2]\n\n        store_column = self._get_norm_column_name(column)\n\n        return df.withColumn(\n            store_column,\n            F.col(column) * (F.lit(max_val) - F.lit(min_val)) + F.lit(mean_val),\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/normalization/normalization_minmax/","title":"Normalization MinMax","text":""},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/normalization/normalization_minmax/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.normalization.normalization_minmax.NormalizationMinMax","title":"<code>NormalizationMinMax</code>","text":"<p>               Bases: <code>NormalizationBaseClass</code></p> <p>Implements Min-Max normalization for specified columns in a PySpark DataFrame.</p>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/normalization/normalization_minmax/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.normalization.normalization_minmax.NormalizationMinMax--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.data_quality.data_manipulation.spark.normalization.normalization_minmax import NormalizationMinMax\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.dataframe import DataFrame\n\nnormalization = NormalizationMinMax(df, column_names=[\"value_column_1\", \"value_column_2\"], in_place=False)\nnormalized_df = normalization.filter_data()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>PySpark DataFrame to be normalized.</p> required <code>column_names</code> <code>List[str]</code> <p>List of columns in the DataFrame to be normalized.</p> required <code>in_place</code> <code>bool</code> <p>If true, then result of normalization is stored in the same column.</p> <code>False</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/normalization/normalization_minmax.py</code> <pre><code>class NormalizationMinMax(NormalizationBaseClass):\n    \"\"\"\n    Implements Min-Max normalization for specified columns in a PySpark DataFrame.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.data_quality.data_manipulation.spark.normalization.normalization_minmax import NormalizationMinMax\n    from pyspark.sql import SparkSession\n    from pyspark.sql.dataframe import DataFrame\n\n    normalization = NormalizationMinMax(df, column_names=[\"value_column_1\", \"value_column_2\"], in_place=False)\n    normalized_df = normalization.filter_data()\n    ```\n\n    Parameters:\n        df (DataFrame): PySpark DataFrame to be normalized.\n        column_names (List[str]): List of columns in the DataFrame to be normalized.\n        in_place (bool): If true, then result of normalization is stored in the same column.\n    \"\"\"\n\n    NORMALIZED_COLUMN_NAME = \"minmax\"\n\n    def _normalize_column(self, df: PySparkDataFrame, column: str) -&gt; PySparkDataFrame:\n        \"\"\"\n        Private method to revert Min-Max normalization to the specified column.\n        Min-Max denormalization: normalized_value * (max - min) + min = value\n        \"\"\"\n        min_val = df.select(F.min(F.col(column))).collect()[0][0]\n        max_val = df.select(F.max(F.col(column))).collect()[0][0]\n\n        divisor = max_val - min_val\n        if math.isclose(divisor, 0.0, abs_tol=10e-8) or not math.isfinite(divisor):\n            raise ZeroDivisionError(\"Division by Zero in MinMax\")\n\n        store_column = self._get_norm_column_name(column)\n        self.reversal_value = [min_val, max_val]\n\n        return df.withColumn(\n            store_column,\n            (F.col(column) - F.lit(min_val)) / (F.lit(max_val) - F.lit(min_val)),\n        )\n\n    def _denormalize_column(\n        self, df: PySparkDataFrame, column: str\n    ) -&gt; PySparkDataFrame:\n        \"\"\"\n        Private method to revert Z-Score normalization to the specified column.\n        Z-Score denormalization: normalized_value * std_dev + mean = value\n        \"\"\"\n        min_val = self.reversal_value[0]\n        max_val = self.reversal_value[1]\n\n        store_column = self._get_norm_column_name(column)\n\n        return df.withColumn(\n            store_column,\n            (F.col(column) * (F.lit(max_val) - F.lit(min_val))) + F.lit(min_val),\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/normalization/normalization_zscore/","title":"Normalization ZScore","text":""},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/normalization/normalization_zscore/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.normalization.normalization_zscore.NormalizationZScore","title":"<code>NormalizationZScore</code>","text":"<p>               Bases: <code>NormalizationBaseClass</code></p> <p>Implements Z-Score normalization for specified columns in a PySpark DataFrame.</p>"},{"location":"sdk/code-reference/pipelines/data_quality/data_manipulation/spark/normalization/normalization_zscore/#src.sdk.python.rtdip_sdk.pipelines.data_quality.data_manipulation.spark.normalization.normalization_zscore.NormalizationZScore--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.data_quality.data_manipulation.spark.normalization.normalization_zscore import NormalizationZScore\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.dataframe import DataFrame\n\nnormalization = NormalizationZScore(df, column_names=[\"value_column_1\", \"value_column_2\"], in_place=False)\nnormalized_df = normalization.filter_data()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>PySpark DataFrame to be normalized.</p> required <code>column_names</code> <code>List[str]</code> <p>List of columns in the DataFrame to be normalized.</p> required <code>in_place</code> <code>bool</code> <p>If true, then result of normalization is stored in the same column.</p> <code>False</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/data_manipulation/spark/normalization/normalization_zscore.py</code> <pre><code>class NormalizationZScore(NormalizationBaseClass):\n    \"\"\"\n    Implements Z-Score normalization for specified columns in a PySpark DataFrame.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.data_quality.data_manipulation.spark.normalization.normalization_zscore import NormalizationZScore\n    from pyspark.sql import SparkSession\n    from pyspark.sql.dataframe import DataFrame\n\n    normalization = NormalizationZScore(df, column_names=[\"value_column_1\", \"value_column_2\"], in_place=False)\n    normalized_df = normalization.filter_data()\n    ```\n\n    Parameters:\n        df (DataFrame): PySpark DataFrame to be normalized.\n        column_names (List[str]): List of columns in the DataFrame to be normalized.\n        in_place (bool): If true, then result of normalization is stored in the same column.\n    \"\"\"\n\n    NORMALIZED_COLUMN_NAME = \"zscore\"\n\n    def _normalize_column(self, df: PySparkDataFrame, column: str) -&gt; PySparkDataFrame:\n        \"\"\"\n        Private method to apply Z-Score normalization to the specified column.\n        Z-Score normalization: (value - mean) / std_dev\n        \"\"\"\n        mean_val = df.select(F.mean(F.col(column))).collect()[0][0]\n        std_dev_val = df.select(F.stddev(F.col(column))).collect()[0][0]\n\n        if math.isclose(std_dev_val, 0.0, abs_tol=10e-8) or not math.isfinite(\n            std_dev_val\n        ):\n            raise ZeroDivisionError(\"Division by Zero in ZScore\")\n\n        store_column = self._get_norm_column_name(column)\n        self.reversal_value = [mean_val, std_dev_val]\n\n        return df.withColumn(\n            store_column, (F.col(column) - F.lit(mean_val)) / F.lit(std_dev_val)\n        )\n\n    def _denormalize_column(\n        self, df: PySparkDataFrame, column: str\n    ) -&gt; PySparkDataFrame:\n        \"\"\"\n        Private method to revert Z-Score normalization to the specified column.\n        Z-Score denormalization: normalized_value * std_dev + mean = value\n        \"\"\"\n        mean_val = self.reversal_value[0]\n        std_dev_val = self.reversal_value[1]\n\n        store_column = self._get_norm_column_name(column)\n\n        return df.withColumn(\n            store_column, F.col(column) * F.lit(std_dev_val) + F.lit(mean_val)\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/check_value_ranges/","title":"Check Value Ranges","text":""},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/check_value_ranges/#src.sdk.python.rtdip_sdk.pipelines.data_quality.monitoring.spark.check_value_ranges.CheckValueRanges","title":"<code>CheckValueRanges</code>","text":"<p>               Bases: <code>MonitoringBaseInterface</code>, <code>InputValidator</code></p> <p>Monitors data in a DataFrame by checking the 'Value' column against expected ranges for specified TagNames. Logs events when 'Value' exceeds the defined ranges for any TagName.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to monitor.</p> required <code>tag_ranges</code> <code>dict</code> <p>A dictionary where keys are TagNames and values are dictionaries specifying 'min' and/or 'max', and optionally 'inclusive_bounds' values. Example:     {         'A2PS64V0J.:ZUX09R': {'min': 0, 'max': 100, 'inclusive_bounds': True},         'B3TS64V0K.:ZUX09R': {'min': 10, 'max': 200, 'inclusive_bounds': False},     }</p> required Example <pre><code>from pyspark.sql import SparkSession\nfrom rtdip_sdk.pipelines.data_quality.monitoring.spark.check_value_ranges import CheckValueRanges\n\n\nspark = SparkSession.builder.master(\"local[1]\").appName(\"CheckValueRangesExample\").getOrCreate()\n\ndata = [\n    (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 03:49:45.000\", \"Good\", 25.0),\n    (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 07:53:11.000\", \"Good\", -5.0),\n    (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 11:56:42.000\", \"Good\", 50.0),\n    (\"B3TS64V0K.:ZUX09R\", \"2024-01-02 16:00:12.000\", \"Good\", 80.0),\n    (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 20:03:46.000\", \"Good\", 100.0),\n]\n\ncolumns = [\"TagName\", \"EventTime\", \"Status\", \"Value\"]\n\ndf = spark.createDataFrame(data, columns)\n\ntag_ranges = {\n    \"A2PS64V0J.:ZUX09R\": {\"min\": 0, \"max\": 50, \"inclusive_bounds\": True},\n    \"B3TS64V0K.:ZUX09R\": {\"min\": 50, \"max\": 100, \"inclusive_bounds\": False},\n}\n\ncheck_value_ranges = CheckValueRanges(\n    df=df,\n    tag_ranges=tag_ranges,\n)\n\nresult_df = check_value_ranges.check()\n</code></pre> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/monitoring/spark/check_value_ranges.py</code> <pre><code>class CheckValueRanges(MonitoringBaseInterface, InputValidator):\n    \"\"\"\n    Monitors data in a DataFrame by checking the 'Value' column against expected ranges for specified TagNames.\n    Logs events when 'Value' exceeds the defined ranges for any TagName.\n\n    Args:\n        df (pyspark.sql.DataFrame): The DataFrame to monitor.\n        tag_ranges (dict): A dictionary where keys are TagNames and values are dictionaries specifying 'min' and/or\n            'max', and optionally 'inclusive_bounds' values.\n            Example:\n                {\n                    'A2PS64V0J.:ZUX09R': {'min': 0, 'max': 100, 'inclusive_bounds': True},\n                    'B3TS64V0K.:ZUX09R': {'min': 10, 'max': 200, 'inclusive_bounds': False},\n                }\n\n    Example:\n        ```python\n        from pyspark.sql import SparkSession\n        from rtdip_sdk.pipelines.data_quality.monitoring.spark.check_value_ranges import CheckValueRanges\n\n\n        spark = SparkSession.builder.master(\"local[1]\").appName(\"CheckValueRangesExample\").getOrCreate()\n\n        data = [\n            (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 03:49:45.000\", \"Good\", 25.0),\n            (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 07:53:11.000\", \"Good\", -5.0),\n            (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 11:56:42.000\", \"Good\", 50.0),\n            (\"B3TS64V0K.:ZUX09R\", \"2024-01-02 16:00:12.000\", \"Good\", 80.0),\n            (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 20:03:46.000\", \"Good\", 100.0),\n        ]\n\n        columns = [\"TagName\", \"EventTime\", \"Status\", \"Value\"]\n\n        df = spark.createDataFrame(data, columns)\n\n        tag_ranges = {\n            \"A2PS64V0J.:ZUX09R\": {\"min\": 0, \"max\": 50, \"inclusive_bounds\": True},\n            \"B3TS64V0K.:ZUX09R\": {\"min\": 50, \"max\": 100, \"inclusive_bounds\": False},\n        }\n\n        check_value_ranges = CheckValueRanges(\n            df=df,\n            tag_ranges=tag_ranges,\n        )\n\n        result_df = check_value_ranges.check()\n        ```\n    \"\"\"\n\n    df: PySparkDataFrame\n    tag_ranges: dict\n    EXPECTED_SCHEMA = StructType(\n        [\n            StructField(\"TagName\", StringType(), True),\n            StructField(\"EventTime\", TimestampType(), True),\n            StructField(\"Status\", StringType(), True),\n            StructField(\"Value\", FloatType(), True),\n        ]\n    )\n\n    def __init__(\n        self,\n        df: PySparkDataFrame,\n        tag_ranges: dict,\n    ) -&gt; None:\n        self.df = df\n        self.validate(self.EXPECTED_SCHEMA)\n        self.tag_ranges = tag_ranges\n\n        # Configure logging\n        self.logger = logging.getLogger(self.__class__.__name__)\n        if not self.logger.handlers:\n            handler = logging.StreamHandler()\n            formatter = logging.Formatter(\n                \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n            )\n            handler.setFormatter(formatter)\n            self.logger.addHandler(handler)\n            self.logger.setLevel(logging.INFO)\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def check(self) -&gt; PySparkDataFrame:\n        \"\"\"\n        Executes the value range checking logic for the specified TagNames. Identifies and logs any rows\n        where 'Value' exceeds the defined ranges for each TagName.\n\n        Returns:\n            pyspark.sql.DataFrame:\n                Returns the original PySpark DataFrame without changes.\n        \"\"\"\n        out_of_range_df = self.check_for_out_of_range()\n\n        if out_of_range_df.count() &gt; 0:\n            self.log_out_of_range_values(out_of_range_df)\n        else:\n            self.logger.info(f\"No out of range values found in 'Value' column.\")\n\n        return self.df\n\n    def check_for_out_of_range(self) -&gt; PySparkDataFrame:\n        \"\"\"\n        Identifies rows where 'Value' exceeds defined ranges.\n\n        Returns:\n        pyspark.sql.DataFrame: A DataFrame containing rows with out-of-range values.\n        \"\"\"\n\n        self._validate_inputs()\n\n        out_of_range_df = self.df.filter(\"1=0\")\n\n        for tag_name, range_dict in self.tag_ranges.items():\n            df = self.df.filter(col(\"TagName\") == tag_name)\n\n            if df.count() == 0:\n                self.logger.warning(f\"No data found for TagName '{tag_name}'.\")\n                continue\n\n            min_value = range_dict.get(\"min\", None)\n            max_value = range_dict.get(\"max\", None)\n            inclusive_bounds = range_dict.get(\"inclusive_bounds\", True)\n\n            conditions = []\n\n            # Build minimum value condition\n            self.add_min_value_condition(min_value, inclusive_bounds, conditions)\n\n            # Build maximum value condition\n            self.add_max_value_condition(max_value, inclusive_bounds, conditions)\n\n            if conditions:\n                condition = reduce(or_, conditions)\n                tag_out_of_range_df = df.filter(condition)\n                out_of_range_df = out_of_range_df.union(tag_out_of_range_df)\n\n        return out_of_range_df\n\n    def add_min_value_condition(self, min_value, inclusive_bounds, conditions):\n        if min_value is not None:\n            if inclusive_bounds:\n                min_condition = col(\"Value\") &lt; min_value\n            else:\n                min_condition = col(\"Value\") &lt;= min_value\n            conditions.append(min_condition)\n\n    def add_max_value_condition(self, max_value, inclusive_bounds, conditions):\n        if max_value is not None:\n            if inclusive_bounds:\n                max_condition = col(\"Value\") &gt; max_value\n            else:\n                max_condition = col(\"Value\") &gt;= max_value\n            conditions.append(max_condition)\n\n    def log_out_of_range_values(self, out_of_range_df: PySparkDataFrame):\n        \"\"\"\n        Logs out-of-range values for all TagNames.\n        \"\"\"\n        for tag_name in (\n            out_of_range_df.select(\"TagName\")\n            .distinct()\n            .rdd.map(lambda row: row[0])\n            .collect()\n        ):\n            tag_out_of_range_df = out_of_range_df.filter(col(\"TagName\") == tag_name)\n            count = tag_out_of_range_df.count()\n            self.logger.info(\n                f\"Found {count} rows in 'Value' column for TagName '{tag_name}' out of range.\"\n            )\n            for row in tag_out_of_range_df.collect():\n                self.logger.info(f\"Out of range row for TagName '{tag_name}': {row}\")\n\n    def _validate_inputs(self):\n        if not isinstance(self.tag_ranges, dict):\n            raise TypeError(\"tag_ranges must be a dictionary.\")\n\n        available_tags = (\n            self.df.select(\"TagName\").distinct().rdd.map(lambda row: row[0]).collect()\n        )\n\n        for tag_name, range_dict in self.tag_ranges.items():\n            self.validate_tag_name(available_tags, tag_name, range_dict)\n\n            inclusive_bounds = range_dict.get(\"inclusive_bounds\", True)\n            if not isinstance(inclusive_bounds, bool):\n                raise ValueError(\n                    f\"Inclusive_bounds for TagName '{tag_name}' must be a boolean.\"\n                )\n\n            min_value = range_dict.get(\"min\", None)\n            max_value = range_dict.get(\"max\", None)\n            if min_value is not None and not isinstance(min_value, (int, float)):\n                raise ValueError(\n                    f\"Minimum value for TagName '{tag_name}' must be a number.\"\n                )\n            if max_value is not None and not isinstance(max_value, (int, float)):\n                raise ValueError(\n                    f\"Maximum value for TagName '{tag_name}' must be a number.\"\n                )\n\n    def validate_tag_name(self, available_tags, tag_name, range_dict):\n        if not isinstance(tag_name, str):\n            raise ValueError(f\"TagName '{tag_name}' must be a string.\")\n\n        if tag_name not in available_tags:\n            raise ValueError(f\"TagName '{tag_name}' not found in DataFrame.\")\n\n        if \"min\" not in range_dict and \"max\" not in range_dict:\n            raise ValueError(\n                f\"TagName '{tag_name}' must have at least 'min' or 'max' specified.\"\n            )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/check_value_ranges/#src.sdk.python.rtdip_sdk.pipelines.data_quality.monitoring.spark.check_value_ranges.CheckValueRanges.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/monitoring/spark/check_value_ranges.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/check_value_ranges/#src.sdk.python.rtdip_sdk.pipelines.data_quality.monitoring.spark.check_value_ranges.CheckValueRanges.check","title":"<code>check()</code>","text":"<p>Executes the value range checking logic for the specified TagNames. Identifies and logs any rows where 'Value' exceeds the defined ranges for each TagName.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: Returns the original PySpark DataFrame without changes.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/monitoring/spark/check_value_ranges.py</code> <pre><code>def check(self) -&gt; PySparkDataFrame:\n    \"\"\"\n    Executes the value range checking logic for the specified TagNames. Identifies and logs any rows\n    where 'Value' exceeds the defined ranges for each TagName.\n\n    Returns:\n        pyspark.sql.DataFrame:\n            Returns the original PySpark DataFrame without changes.\n    \"\"\"\n    out_of_range_df = self.check_for_out_of_range()\n\n    if out_of_range_df.count() &gt; 0:\n        self.log_out_of_range_values(out_of_range_df)\n    else:\n        self.logger.info(f\"No out of range values found in 'Value' column.\")\n\n    return self.df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/check_value_ranges/#src.sdk.python.rtdip_sdk.pipelines.data_quality.monitoring.spark.check_value_ranges.CheckValueRanges.check_for_out_of_range","title":"<code>check_for_out_of_range()</code>","text":"<p>Identifies rows where 'Value' exceeds defined ranges.</p> <p>Returns: pyspark.sql.DataFrame: A DataFrame containing rows with out-of-range values.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/monitoring/spark/check_value_ranges.py</code> <pre><code>def check_for_out_of_range(self) -&gt; PySparkDataFrame:\n    \"\"\"\n    Identifies rows where 'Value' exceeds defined ranges.\n\n    Returns:\n    pyspark.sql.DataFrame: A DataFrame containing rows with out-of-range values.\n    \"\"\"\n\n    self._validate_inputs()\n\n    out_of_range_df = self.df.filter(\"1=0\")\n\n    for tag_name, range_dict in self.tag_ranges.items():\n        df = self.df.filter(col(\"TagName\") == tag_name)\n\n        if df.count() == 0:\n            self.logger.warning(f\"No data found for TagName '{tag_name}'.\")\n            continue\n\n        min_value = range_dict.get(\"min\", None)\n        max_value = range_dict.get(\"max\", None)\n        inclusive_bounds = range_dict.get(\"inclusive_bounds\", True)\n\n        conditions = []\n\n        # Build minimum value condition\n        self.add_min_value_condition(min_value, inclusive_bounds, conditions)\n\n        # Build maximum value condition\n        self.add_max_value_condition(max_value, inclusive_bounds, conditions)\n\n        if conditions:\n            condition = reduce(or_, conditions)\n            tag_out_of_range_df = df.filter(condition)\n            out_of_range_df = out_of_range_df.union(tag_out_of_range_df)\n\n    return out_of_range_df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/check_value_ranges/#src.sdk.python.rtdip_sdk.pipelines.data_quality.monitoring.spark.check_value_ranges.CheckValueRanges.log_out_of_range_values","title":"<code>log_out_of_range_values(out_of_range_df)</code>","text":"<p>Logs out-of-range values for all TagNames.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/monitoring/spark/check_value_ranges.py</code> <pre><code>def log_out_of_range_values(self, out_of_range_df: PySparkDataFrame):\n    \"\"\"\n    Logs out-of-range values for all TagNames.\n    \"\"\"\n    for tag_name in (\n        out_of_range_df.select(\"TagName\")\n        .distinct()\n        .rdd.map(lambda row: row[0])\n        .collect()\n    ):\n        tag_out_of_range_df = out_of_range_df.filter(col(\"TagName\") == tag_name)\n        count = tag_out_of_range_df.count()\n        self.logger.info(\n            f\"Found {count} rows in 'Value' column for TagName '{tag_name}' out of range.\"\n        )\n        for row in tag_out_of_range_df.collect():\n            self.logger.info(f\"Out of range row for TagName '{tag_name}': {row}\")\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/flatline_detection/","title":"Flatline Detection","text":""},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/flatline_detection/#src.sdk.python.rtdip_sdk.pipelines.data_quality.monitoring.spark.flatline_detection.FlatlineDetection","title":"<code>FlatlineDetection</code>","text":"<p>               Bases: <code>MonitoringBaseInterface</code>, <code>InputValidator</code></p> <p>Detects flatlining in specified columns of a PySpark DataFrame and logs warnings.</p> <p>Flatlining occurs when a column contains consecutive null or zero values exceeding a specified tolerance period. This class identifies such occurrences and logs the rows where flatlining is detected.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to monitor for flatlining.</p> required <code>watch_columns</code> <code>list</code> <p>List of column names to monitor for flatlining (null or zero values).</p> required <code>tolerance_timespan</code> <code>int</code> <p>Maximum allowed consecutive flatlining period. If exceeded, a warning is logged.</p> required Example <pre><code>from rtdip_sdk.pipelines.data_quality.monitoring.spark.flatline_detection import FlatlineDetection\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.master(\"local[1]\").appName(\"FlatlineDetectionExample\").getOrCreate()\n\n# Example DataFrame\ndata = [\n    (1, 1),\n    (2, 0),\n    (3, 0),\n    (4, 0),\n    (5, 5),\n]\ncolumns = [\"ID\", \"Value\"]\ndf = spark.createDataFrame(data, columns)\n\n# Initialize FlatlineDetection\nflatline_detection = FlatlineDetection(\n    df,\n    watch_columns=[\"Value\"],\n    tolerance_timespan=2\n)\n\n# Detect flatlining\nflatline_detection.check()\n</code></pre> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/monitoring/spark/flatline_detection.py</code> <pre><code>class FlatlineDetection(MonitoringBaseInterface, InputValidator):\n    \"\"\"\n    Detects flatlining in specified columns of a PySpark DataFrame and logs warnings.\n\n    Flatlining occurs when a column contains consecutive null or zero values exceeding a specified tolerance period.\n    This class identifies such occurrences and logs the rows where flatlining is detected.\n\n    Args:\n        df (pyspark.sql.DataFrame): The input DataFrame to monitor for flatlining.\n        watch_columns (list): List of column names to monitor for flatlining (null or zero values).\n        tolerance_timespan (int): Maximum allowed consecutive flatlining period. If exceeded, a warning is logged.\n\n    Example:\n        ```python\n        from rtdip_sdk.pipelines.data_quality.monitoring.spark.flatline_detection import FlatlineDetection\n\n        from pyspark.sql import SparkSession\n\n        spark = SparkSession.builder.master(\"local[1]\").appName(\"FlatlineDetectionExample\").getOrCreate()\n\n        # Example DataFrame\n        data = [\n            (1, 1),\n            (2, 0),\n            (3, 0),\n            (4, 0),\n            (5, 5),\n        ]\n        columns = [\"ID\", \"Value\"]\n        df = spark.createDataFrame(data, columns)\n\n        # Initialize FlatlineDetection\n        flatline_detection = FlatlineDetection(\n            df,\n            watch_columns=[\"Value\"],\n            tolerance_timespan=2\n        )\n\n        # Detect flatlining\n        flatline_detection.check()\n        ```\n    \"\"\"\n\n    df: PySparkDataFrame\n    watch_columns: list\n    tolerance_timespan: int\n    EXPECTED_SCHEMA = StructType(\n        [\n            StructField(\"TagName\", StringType(), True),\n            StructField(\"EventTime\", TimestampType(), True),\n            StructField(\"Status\", StringType(), True),\n            StructField(\"Value\", FloatType(), True),\n        ]\n    )\n\n    def __init__(\n        self, df: PySparkDataFrame, watch_columns: list, tolerance_timespan: int\n    ) -&gt; None:\n        if not watch_columns or not isinstance(watch_columns, list):\n            raise ValueError(\"watch_columns must be a non-empty list of column names.\")\n        if not isinstance(tolerance_timespan, int) or tolerance_timespan &lt;= 0:\n            raise ValueError(\"tolerance_timespan must be a positive integer.\")\n\n        self.df = df\n        self.validate(self.EXPECTED_SCHEMA)\n        self.watch_columns = watch_columns\n        self.tolerance_timespan = tolerance_timespan\n\n        self.logger = logging.getLogger(self.__class__.__name__)\n        if not self.logger.handlers:\n            handler = logging.StreamHandler()\n            formatter = logging.Formatter(\n                \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n            )\n            handler.setFormatter(formatter)\n            self.logger.addHandler(handler)\n            self.logger.setLevel(logging.INFO)\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def check(self) -&gt; PySparkDataFrame:\n        \"\"\"\n        Detects flatlining and logs relevant rows.\n\n        Returns:\n            pyspark.sql.DataFrame: The original DataFrame with additional flatline detection metadata.\n        \"\"\"\n        flatlined_rows = self.check_for_flatlining()\n        print(\"Flatlined Rows:\")\n        flatlined_rows.show(truncate=False)\n        self.log_flatlining_rows(flatlined_rows)\n        return self.df\n\n    def check_for_flatlining(self) -&gt; PySparkDataFrame:\n        \"\"\"\n        Identifies rows with flatlining based on the specified columns and tolerance.\n\n        Returns:\n            pyspark.sql.DataFrame: A DataFrame containing rows with flatlining detected.\n        \"\"\"\n        partition_column = \"TagName\"\n        sort_column = \"EventTime\"\n        window_spec = Window.partitionBy(partition_column).orderBy(sort_column)\n\n        # Start with an empty DataFrame, ensure it has the required schema\n        flatlined_rows = (\n            self.df.withColumn(\"Value_flatline_flag\", lit(None).cast(\"int\"))\n            .withColumn(\"Value_group\", lit(None).cast(\"bigint\"))\n            .filter(\"1=0\")\n        )\n\n        for column in self.watch_columns:\n            flagged_column = f\"{column}_flatline_flag\"\n            group_column = f\"{column}_group\"\n\n            # Add flag and group columns\n            df_with_flags = self.df.withColumn(\n                flagged_column,\n                when(\n                    (col(column).isNull()) | (abs(col(column) - 0.0) &lt;= 1e-09),\n                    1,\n                ).otherwise(0),\n            ).withColumn(\n                group_column,\n                sum(\n                    when(\n                        col(flagged_column)\n                        != lag(col(flagged_column), 1, 0).over(window_spec),\n                        1,\n                    ).otherwise(0)\n                ).over(window_spec),\n            )\n\n            # Identify flatlining groups\n            group_counts = (\n                df_with_flags.filter(col(flagged_column) == 1)\n                .groupBy(group_column)\n                .count()\n            )\n            large_groups = group_counts.filter(col(\"count\") &gt; self.tolerance_timespan)\n            large_group_ids = [row[group_column] for row in large_groups.collect()]\n\n            if large_group_ids:\n                relevant_rows = df_with_flags.filter(\n                    col(group_column).isin(large_group_ids)\n                )\n\n                # Ensure both DataFrames have the same columns\n                for col_name in flatlined_rows.columns:\n                    if col_name not in relevant_rows.columns:\n                        relevant_rows = relevant_rows.withColumn(col_name, lit(None))\n\n                flatlined_rows = flatlined_rows.union(relevant_rows)\n\n        return flatlined_rows\n\n    def log_flatlining_rows(self, flatlined_rows: PySparkDataFrame):\n        \"\"\"\n        Logs flatlining rows for all monitored columns.\n\n        Args:\n            flatlined_rows (pyspark.sql.DataFrame): The DataFrame containing rows with flatlining detected.\n        \"\"\"\n        if flatlined_rows.count() == 0:\n            self.logger.info(\"No flatlining detected.\")\n            return\n\n        for column in self.watch_columns:\n            flagged_column = f\"{column}_flatline_flag\"\n\n            if flagged_column not in flatlined_rows.columns:\n                self.logger.warning(\n                    f\"Expected column '{flagged_column}' not found in DataFrame.\"\n                )\n                continue\n\n            relevant_rows = flatlined_rows.filter(col(flagged_column) == 1).collect()\n\n            if relevant_rows:\n                for row in relevant_rows:\n                    self.logger.warning(\n                        f\"Flatlining detected in column '{column}' at row: {row}.\"\n                    )\n            else:\n                self.logger.info(f\"No flatlining detected in column '{column}'.\")\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/flatline_detection/#src.sdk.python.rtdip_sdk.pipelines.data_quality.monitoring.spark.flatline_detection.FlatlineDetection.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/monitoring/spark/flatline_detection.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/flatline_detection/#src.sdk.python.rtdip_sdk.pipelines.data_quality.monitoring.spark.flatline_detection.FlatlineDetection.check","title":"<code>check()</code>","text":"<p>Detects flatlining and logs relevant rows.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: The original DataFrame with additional flatline detection metadata.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/monitoring/spark/flatline_detection.py</code> <pre><code>def check(self) -&gt; PySparkDataFrame:\n    \"\"\"\n    Detects flatlining and logs relevant rows.\n\n    Returns:\n        pyspark.sql.DataFrame: The original DataFrame with additional flatline detection metadata.\n    \"\"\"\n    flatlined_rows = self.check_for_flatlining()\n    print(\"Flatlined Rows:\")\n    flatlined_rows.show(truncate=False)\n    self.log_flatlining_rows(flatlined_rows)\n    return self.df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/flatline_detection/#src.sdk.python.rtdip_sdk.pipelines.data_quality.monitoring.spark.flatline_detection.FlatlineDetection.check_for_flatlining","title":"<code>check_for_flatlining()</code>","text":"<p>Identifies rows with flatlining based on the specified columns and tolerance.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: A DataFrame containing rows with flatlining detected.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/monitoring/spark/flatline_detection.py</code> <pre><code>def check_for_flatlining(self) -&gt; PySparkDataFrame:\n    \"\"\"\n    Identifies rows with flatlining based on the specified columns and tolerance.\n\n    Returns:\n        pyspark.sql.DataFrame: A DataFrame containing rows with flatlining detected.\n    \"\"\"\n    partition_column = \"TagName\"\n    sort_column = \"EventTime\"\n    window_spec = Window.partitionBy(partition_column).orderBy(sort_column)\n\n    # Start with an empty DataFrame, ensure it has the required schema\n    flatlined_rows = (\n        self.df.withColumn(\"Value_flatline_flag\", lit(None).cast(\"int\"))\n        .withColumn(\"Value_group\", lit(None).cast(\"bigint\"))\n        .filter(\"1=0\")\n    )\n\n    for column in self.watch_columns:\n        flagged_column = f\"{column}_flatline_flag\"\n        group_column = f\"{column}_group\"\n\n        # Add flag and group columns\n        df_with_flags = self.df.withColumn(\n            flagged_column,\n            when(\n                (col(column).isNull()) | (abs(col(column) - 0.0) &lt;= 1e-09),\n                1,\n            ).otherwise(0),\n        ).withColumn(\n            group_column,\n            sum(\n                when(\n                    col(flagged_column)\n                    != lag(col(flagged_column), 1, 0).over(window_spec),\n                    1,\n                ).otherwise(0)\n            ).over(window_spec),\n        )\n\n        # Identify flatlining groups\n        group_counts = (\n            df_with_flags.filter(col(flagged_column) == 1)\n            .groupBy(group_column)\n            .count()\n        )\n        large_groups = group_counts.filter(col(\"count\") &gt; self.tolerance_timespan)\n        large_group_ids = [row[group_column] for row in large_groups.collect()]\n\n        if large_group_ids:\n            relevant_rows = df_with_flags.filter(\n                col(group_column).isin(large_group_ids)\n            )\n\n            # Ensure both DataFrames have the same columns\n            for col_name in flatlined_rows.columns:\n                if col_name not in relevant_rows.columns:\n                    relevant_rows = relevant_rows.withColumn(col_name, lit(None))\n\n            flatlined_rows = flatlined_rows.union(relevant_rows)\n\n    return flatlined_rows\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/flatline_detection/#src.sdk.python.rtdip_sdk.pipelines.data_quality.monitoring.spark.flatline_detection.FlatlineDetection.log_flatlining_rows","title":"<code>log_flatlining_rows(flatlined_rows)</code>","text":"<p>Logs flatlining rows for all monitored columns.</p> <p>Parameters:</p> Name Type Description Default <code>flatlined_rows</code> <code>DataFrame</code> <p>The DataFrame containing rows with flatlining detected.</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/monitoring/spark/flatline_detection.py</code> <pre><code>def log_flatlining_rows(self, flatlined_rows: PySparkDataFrame):\n    \"\"\"\n    Logs flatlining rows for all monitored columns.\n\n    Args:\n        flatlined_rows (pyspark.sql.DataFrame): The DataFrame containing rows with flatlining detected.\n    \"\"\"\n    if flatlined_rows.count() == 0:\n        self.logger.info(\"No flatlining detected.\")\n        return\n\n    for column in self.watch_columns:\n        flagged_column = f\"{column}_flatline_flag\"\n\n        if flagged_column not in flatlined_rows.columns:\n            self.logger.warning(\n                f\"Expected column '{flagged_column}' not found in DataFrame.\"\n            )\n            continue\n\n        relevant_rows = flatlined_rows.filter(col(flagged_column) == 1).collect()\n\n        if relevant_rows:\n            for row in relevant_rows:\n                self.logger.warning(\n                    f\"Flatlining detected in column '{column}' at row: {row}.\"\n                )\n        else:\n            self.logger.info(f\"No flatlining detected in column '{column}'.\")\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/great_expectations/","title":"Examine Data Quality with Great Expectations","text":"<p>Great Expectations is a Python-based open-source library for validating, documenting, and profiling your data. It helps you to maintain data quality and improve communication about data between teams.</p>"},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/great_expectations/#src.sdk.python.rtdip_sdk.pipelines.data_quality.monitoring.spark.great_expectations_data_quality.GreatExpectationsDataQuality","title":"<code>GreatExpectationsDataQuality</code>","text":"<p>               Bases: <code>MonitoringBaseInterface</code>, <code>InputValidator</code></p> <p>Data Quality Monitoring using Great Expectations allowing you to create and check your data quality expectations.</p>"},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/great_expectations/#src.sdk.python.rtdip_sdk.pipelines.data_quality.monitoring.spark.great_expectations_data_quality.GreatExpectationsDataQuality--example","title":"Example","text":"<pre><code>from src.sdk.python.rtdip_sdk.monitoring.data_manipulation.great_expectations.python.great_expectations_data_quality import  GreatExpectationsDataQuality\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\nimport json\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\ndf = spark_dataframe\ncontext_root_dir = \"/dbfs/great_expectations/\",\nexpectation_suite_name = \"great_expectations_suite_name\"\ndf_datasource_name = \"my_spark_in_memory_datasource\",\ndf_asset_name = \"df_asset_name\",\n\n\nexpectation_type = \"expect_column_values_to_not_be_null\"\nexception_dict = {\n    \"column\": \"column_name\",\n    \"mostly\": 0.75,\n}\nmeta_dict = {\n    \"notes\": {\n        \"format\": \"markdown\",\n        \"content\": \"Comment about this expectation.\",\n    }\n}\n\n#Configure the Great Expectations Data Quality\n\nGX = GreatExpectationsDataQuality(spark, context_root_dir, df, expectation_suite_name, df_datasource_name, df_asset_name)\n\nvalidator, suite = GX.create_expectations()\n\nexpectation_configuration = GX.build_expectations(\n    exception_type, exception_dict, meta_dict\n)\n\nGX.add_expectations(suite, expectation_configuration)\n\nGX.save_expectations(validator)\n\nGX.display_expectations(suite)\n\n#Run the Data Quality Check by Validating your data against set expectations in the suite\n\ncheckpoint_name = \"checkpoint_name\"\nrun_name_template = \"run_name_template\"\naction_list = [\n    {\n        \"name\": \"store_validation_result\",\n        \"action\": {\"class_name\": \"StoreValidationResultAction\"},\n    },\n    {\"name\": \"update_data_docs\", \"action\": {\"class_name\": \"UpdateDataDocsAction\"}},\n]\n\ncheckpoint_result = GX.check(checkpoint_name, run_name_template, action_list)\n\nprint(checkpoint_result)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe containing the raw data.</p> required <code>context_root_dir</code> <code>str</code> <p>The root directory of the Great Expectations project.</p> required <code>expectation_suite_name</code> <code>str</code> <p>The name of the expectation suite to be created.</p> required <code>df_datasource_name</code> <code>str</code> <p>The name of the datasource.</p> <code>'my_spark_in_memory_datasource'</code> <code>df_asset_name</code> <code>str</code> <p>The name of the asset.</p> <code>'df_asset_name'</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/monitoring/spark/great_expectations_data_quality.py</code> <pre><code>class GreatExpectationsDataQuality(MonitoringBaseInterface, InputValidator):\n    \"\"\"\n    Data Quality Monitoring using Great Expectations allowing you to create and check your data quality expectations.\n\n    Example\n    --------\n    ```python\n    from src.sdk.python.rtdip_sdk.monitoring.data_manipulation.great_expectations.python.great_expectations_data_quality import  GreatExpectationsDataQuality\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n    import json\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    df = spark_dataframe\n    context_root_dir = \"/dbfs/great_expectations/\",\n    expectation_suite_name = \"great_expectations_suite_name\"\n    df_datasource_name = \"my_spark_in_memory_datasource\",\n    df_asset_name = \"df_asset_name\",\n\n\n    expectation_type = \"expect_column_values_to_not_be_null\"\n    exception_dict = {\n        \"column\": \"column_name\",\n        \"mostly\": 0.75,\n    }\n    meta_dict = {\n        \"notes\": {\n            \"format\": \"markdown\",\n            \"content\": \"Comment about this expectation.\",\n        }\n    }\n\n    #Configure the Great Expectations Data Quality\n\n    GX = GreatExpectationsDataQuality(spark, context_root_dir, df, expectation_suite_name, df_datasource_name, df_asset_name)\n\n    validator, suite = GX.create_expectations()\n\n    expectation_configuration = GX.build_expectations(\n        exception_type, exception_dict, meta_dict\n    )\n\n    GX.add_expectations(suite, expectation_configuration)\n\n    GX.save_expectations(validator)\n\n    GX.display_expectations(suite)\n\n    #Run the Data Quality Check by Validating your data against set expectations in the suite\n\n    checkpoint_name = \"checkpoint_name\"\n    run_name_template = \"run_name_template\"\n    action_list = [\n        {\n            \"name\": \"store_validation_result\",\n            \"action\": {\"class_name\": \"StoreValidationResultAction\"},\n        },\n        {\"name\": \"update_data_docs\", \"action\": {\"class_name\": \"UpdateDataDocsAction\"}},\n    ]\n\n    checkpoint_result = GX.check(checkpoint_name, run_name_template, action_list)\n\n    print(checkpoint_result)\n\n    ```\n\n    Parameters:\n        df (DataFrame): Dataframe containing the raw data.\n        context_root_dir (str): The root directory of the Great Expectations project.\n        expectation_suite_name (str): The name of the expectation suite to be created.\n        df_datasource_name (str): The name of the datasource.\n        df_asset_name (str): The name of the asset.\n    \"\"\"\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        context_root_dir: str,\n        df: DataFrame,\n        expectation_suite_name: str,\n        df_datasource_name: str = \"my_spark_in_memory_datasource\",\n        df_asset_name: str = \"df_asset_name\",\n    ) -&gt; None:\n        self.spark = spark\n        self.context_root_dir = context_root_dir\n        self.df = df\n        self.expectation_suite_name = expectation_suite_name\n        self.df_datasource_name = df_datasource_name\n        self.df_asset_name = df_asset_name\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    # Create a new context\n    def _create_context(self):\n        \"\"\"\n        Create a new context\n        Returns: context\n        \"\"\"\n        context = gx.get_context(context_root_dir=self.context_root_dir)\n        return context\n\n    # Create a batch request from a dataframe\n    def _create_batch_request(self):\n        \"\"\"\n        Create a batch request from a dataframe\n        Returns: batch_request\n        \"\"\"\n        context = self._create_context()\n\n        dataframe_datasource = context.sources.add_or_update_spark(\n            name=self.df_datasource_name,\n        )\n        dataframe_asset = dataframe_datasource.add_dataframe_asset(\n            name=self.df_asset_name,\n            dataframe=self.df,\n        )\n\n        batch_request = (dataframe_asset).build_batch_request()\n        return batch_request\n\n    # Create Expectations\n\n    def create_expectations(self):\n        context = self._create_context()\n        batch_request = self._create_batch_request()\n\n        suite = context.add_or_update_expectation_suite(\n            expectation_suite_name=self.expectation_suite_name\n        )\n        validator = context.get_validator(\n            batch_request=batch_request,\n            expectation_suite_name=self.expectation_suite_name,\n        )\n        return validator, suite\n\n    def build_expectations(\n        self, exception_type: str, exception_dict: dict, meta_dict: dict\n    ):\n        expectation_configuration = ExpectationConfiguration(\n            expectation_type=exception_type, kwargs=exception_dict, meta=meta_dict\n        )\n        return expectation_configuration\n\n    def add_expectations(self, suite, expectation_configuration):\n        suite.add_expectation_configuration(\n            expectation_configuration=expectation_configuration\n        )\n\n    def remove_expectations(\n        self, suite, expectation_configuration, remove_multiple_matches=True\n    ):\n        suite.remove_expectation(\n            expectation_configuration=expectation_configuration,\n            match_type=\"domain\",\n            remove_multiple_matches=remove_multiple_matches,\n        )\n\n    def display_expectations(self, suite):\n        expectation = suite.show_expectations_by_expectation_type()\n        return expectation\n\n    def save_expectations(self, validator):\n        validator.save_expectation_suite(discard_failed_expectations=False)\n        return validator\n\n    # Validate your data\n\n    def check(\n        self,\n        checkpoint_name: str,\n        run_name_template: str,\n        action_list: list,\n    ):\n        \"\"\"\n        Validate your data against set expectations in the suite\n        Args:\n            checkpoint_name (str): The name of the checkpoint.\n            run_name_template (str): The name of the run.\n            action_list (list): The list of actions to be performed.\n         Returns: checkpoint_result(dict)\n        \"\"\"\n        context = self._create_context()\n        batch_request = self._create_batch_request()\n\n        checkpoint = Checkpoint(\n            name=checkpoint_name,\n            run_name_template=run_name_template,\n            data_context=context,\n            batch_request=batch_request,\n            expectation_suite_name=self.expectation_suite_name,\n            action_list=action_list,\n        )\n        context.add_or_update_checkpoint(checkpoint=checkpoint)\n        checkpoint_result = checkpoint.run()\n        return checkpoint_result\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/great_expectations/#src.sdk.python.rtdip_sdk.pipelines.data_quality.monitoring.spark.great_expectations_data_quality.GreatExpectationsDataQuality.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/monitoring/spark/great_expectations_data_quality.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/great_expectations/#src.sdk.python.rtdip_sdk.pipelines.data_quality.monitoring.spark.great_expectations_data_quality.GreatExpectationsDataQuality.check","title":"<code>check(checkpoint_name, run_name_template, action_list)</code>","text":"<p>Validate your data against set expectations in the suite Args:     checkpoint_name (str): The name of the checkpoint.     run_name_template (str): The name of the run.     action_list (list): The list of actions to be performed.  Returns: checkpoint_result(dict)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/monitoring/spark/great_expectations_data_quality.py</code> <pre><code>def check(\n    self,\n    checkpoint_name: str,\n    run_name_template: str,\n    action_list: list,\n):\n    \"\"\"\n    Validate your data against set expectations in the suite\n    Args:\n        checkpoint_name (str): The name of the checkpoint.\n        run_name_template (str): The name of the run.\n        action_list (list): The list of actions to be performed.\n     Returns: checkpoint_result(dict)\n    \"\"\"\n    context = self._create_context()\n    batch_request = self._create_batch_request()\n\n    checkpoint = Checkpoint(\n        name=checkpoint_name,\n        run_name_template=run_name_template,\n        data_context=context,\n        batch_request=batch_request,\n        expectation_suite_name=self.expectation_suite_name,\n        action_list=action_list,\n    )\n    context.add_or_update_checkpoint(checkpoint=checkpoint)\n    checkpoint_result = checkpoint.run()\n    return checkpoint_result\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/identify_missing_data_interval/","title":"Interval Based","text":""},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/identify_missing_data_interval/#src.sdk.python.rtdip_sdk.pipelines.data_quality.monitoring.spark.identify_missing_data_interval.IdentifyMissingDataInterval","title":"<code>IdentifyMissingDataInterval</code>","text":"<p>               Bases: <code>MonitoringBaseInterface</code>, <code>InputValidator</code></p> <p>Detects missing data intervals in a DataFrame by identifying time differences between consecutive measurements that exceed a specified tolerance or a multiple of the Median Absolute Deviation (MAD). Logs the start and end times of missing intervals along with their durations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Dataframe</code> <p>DataFrame containing at least the 'EventTime' column.</p> required <code>interval</code> <code>str</code> <p>Expected interval between data points (e.g., '10ms', '500ms'). If not specified, the median of time differences is used.</p> <code>None</code> <code>tolerance</code> <code>str</code> <p>Tolerance time beyond which an interval is considered missing (e.g., '10ms'). If not specified, it defaults to 'mad_multiplier' times the Median Absolute Deviation (MAD) of time differences.</p> <code>None</code> <code>mad_multiplier</code> <code>float</code> <p>Multiplier for MAD to calculate tolerance. Default is 3.</p> <code>3</code> <code>min_tolerance</code> <code>str</code> <p>Minimum tolerance for pattern-based detection (e.g., '100ms'). Default is '10ms'.</p> <code>'10ms'</code> <p>Returns:</p> Name Type Description <code>df</code> <code>Dataframe</code> <p>Returns the original PySparkDataFrame without changes.</p>"},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/identify_missing_data_interval/#src.sdk.python.rtdip_sdk.pipelines.data_quality.monitoring.spark.identify_missing_data_interval.IdentifyMissingDataInterval--example","title":"Example","text":"<p>```python from rtdip_sdk.pipelines.data_quality.monitoring.spark.identify_missing_data_interval import IdentifyMissingDataInterval</p> <p>from pyspark.sql import SparkSession</p> <pre><code>missing_data_monitor = IdentifyMissingDataInterval(\n    df=df,\n    interval='100ms',\n    tolerance='10ms',\n)\n\ndf_result = missing_data_monitor.check()\n```\n</code></pre> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/monitoring/spark/identify_missing_data_interval.py</code> <pre><code>class IdentifyMissingDataInterval(MonitoringBaseInterface, InputValidator):\n    \"\"\"\n    Detects missing data intervals in a DataFrame by identifying time differences between consecutive\n    measurements that exceed a specified tolerance or a multiple of the Median Absolute Deviation (MAD).\n    Logs the start and end times of missing intervals along with their durations.\n\n\n    Args:\n        df (pyspark.sql.Dataframe): DataFrame containing at least the 'EventTime' column.\n        interval (str, optional): Expected interval between data points (e.g., '10ms', '500ms'). If not specified, the median of time differences is used.\n        tolerance (str, optional): Tolerance time beyond which an interval is considered missing (e.g., '10ms'). If not specified, it defaults to 'mad_multiplier' times the Median Absolute Deviation (MAD) of time differences.\n        mad_multiplier (float, optional): Multiplier for MAD to calculate tolerance. Default is 3.\n        min_tolerance (str, optional): Minimum tolerance for pattern-based detection (e.g., '100ms'). Default is '10ms'.\n\n    Returns:\n        df (pyspark.sql.Dataframe): Returns the original PySparkDataFrame without changes.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.data_quality.monitoring.spark.identify_missing_data_interval import IdentifyMissingDataInterval\n\n    from pyspark.sql import SparkSession\n\n        missing_data_monitor = IdentifyMissingDataInterval(\n            df=df,\n            interval='100ms',\n            tolerance='10ms',\n        )\n\n        df_result = missing_data_monitor.check()\n        ```\n\n    \"\"\"\n\n    df: PySparkDataFrame\n    EXPECTED_SCHEMA = StructType(\n        [\n            StructField(\"TagName\", StringType(), True),\n            StructField(\"EventTime\", TimestampType(), True),\n            StructField(\"Status\", StringType(), True),\n            StructField(\"Value\", FloatType(), True),\n        ]\n    )\n\n    def __init__(\n        self,\n        df: PySparkDataFrame,\n        interval: str = None,\n        tolerance: str = None,\n        mad_multiplier: float = 3,\n        min_tolerance: str = \"10ms\",\n    ) -&gt; None:\n\n        self.df = df\n        self.interval = interval\n        self.tolerance = tolerance\n        self.mad_multiplier = mad_multiplier\n        self.min_tolerance = min_tolerance\n        self.validate(self.EXPECTED_SCHEMA)\n\n        # Use global pipeline logger\n        self.logger_manager = LoggerManager()\n        self.logger = self.logger_manager.create_logger(\"IdentifyMissingDataInterval\")\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def check(self) -&gt; PySparkDataFrame:\n        \"\"\"\n        Executes the identify missing data logic.\n\n        Returns:\n            pyspark.sql.DataFrame:\n                Returns the original PySpark DataFrame without changes.\n        \"\"\"\n        if \"EventTime\" not in self.df.columns:\n            self.logger.error(\"The DataFrame must contain an 'EventTime' column.\")\n            raise ValueError(\"The DataFrame must contain an 'EventTime' column.\")\n\n        df = self.df.withColumn(\"EventTime\", F.to_timestamp(\"EventTime\"))\n        df_sorted = df.orderBy(\"EventTime\")\n        # Calculate time difference in milliseconds between consecutive rows\n        df_with_diff = df_sorted.withColumn(\n            \"TimeDeltaMs\",\n            (\n                F.col(\"EventTime\").cast(\"double\")\n                - F.lag(\"EventTime\").over(Window.orderBy(\"EventTime\")).cast(\"double\")\n            )\n            * 1000,\n        ).withColumn(\n            \"StartMissing\", F.lag(\"EventTime\").over(Window.orderBy(\"EventTime\"))\n        )\n        # Parse interval to milliseconds if given\n        if self.interval is not None:\n            try:\n                interval_ms = parse_time_string_to_ms(self.interval)\n                self.logger.info(f\"Using provided expected interval: {interval_ms} ms\")\n            except ValueError as e:\n                self.logger.error(e)\n                raise\n        else:\n            # Calculate interval based on median of time differences\n            median_expr = F.expr(\"percentile_approx(TimeDeltaMs, 0.5)\")\n            median_row = df_with_diff.select(median_expr.alias(\"median\")).collect()[0]\n            interval_ms = median_row[\"median\"]\n            self.logger.info(\n                f\"Using median of time differences as expected interval: {interval_ms} ms\"\n            )\n        # Parse tolernace to milliseconds if given\n        if self.tolerance is not None:\n            try:\n                tolerance_ms = parse_time_string_to_ms(self.tolerance)\n                self.logger.info(f\"Using provided tolerance: {tolerance_ms} ms\")\n            except ValueError as e:\n                self.logger.error(e)\n                raise\n        else:\n            # Calculate tolerance based on MAD\n            mad_expr = F.expr(\n                f\"percentile_approx(abs(TimeDeltaMs - {interval_ms}), 0.5)\"\n            )\n            mad_row = df_with_diff.select(mad_expr.alias(\"mad\")).collect()[0]\n            mad = mad_row[\"mad\"]\n            calculated_tolerance_ms = self.mad_multiplier * mad\n            min_tolerance_ms = parse_time_string_to_ms(self.min_tolerance)\n            tolerance_ms = max(calculated_tolerance_ms, min_tolerance_ms)\n            self.logger.info(f\"Calculated tolerance: {tolerance_ms} ms (MAD-based)\")\n        # Calculate the maximum acceptable interval with tolerance\n        max_interval_with_tolerance_ms = interval_ms + tolerance_ms\n        self.logger.info(\n            f\"Maximum acceptable interval with tolerance: {max_interval_with_tolerance_ms} ms\"\n        )\n\n        # Identify missing intervals\n        missing_intervals_df = df_with_diff.filter(\n            (F.col(\"TimeDeltaMs\") &gt; max_interval_with_tolerance_ms)\n            &amp; (F.col(\"StartMissing\").isNotNull())\n        ).select(\n            \"TagName\",\n            \"StartMissing\",\n            F.col(\"EventTime\").alias(\"EndMissing\"),\n            \"TimeDeltaMs\",\n        )\n        # Convert time delta to readable format\n        missing_intervals_df = missing_intervals_df.withColumn(\n            \"DurationMissing\",\n            F.concat(\n                F.floor(F.col(\"TimeDeltaMs\") / 3600000).cast(\"string\"),\n                F.lit(\"h \"),\n                F.floor((F.col(\"TimeDeltaMs\") % 3600000) / 60000).cast(\"string\"),\n                F.lit(\"m \"),\n                F.floor(((F.col(\"TimeDeltaMs\") % 3600000) % 60000) / 1000).cast(\n                    \"string\"\n                ),\n                F.lit(\"s\"),\n            ),\n        ).select(\"TagName\", \"StartMissing\", \"EndMissing\", \"DurationMissing\")\n        missing_intervals = missing_intervals_df.collect()\n        if missing_intervals:\n            self.logger.info(\"Detected Missing Intervals:\")\n            for row in missing_intervals:\n                self.logger.info(\n                    f\"Tag: {row['TagName']} Missing Interval from {row['StartMissing']} to {row['EndMissing']} \"\n                    f\"Duration: {row['DurationMissing']}\"\n                )\n        else:\n            self.logger.info(\"No missing intervals detected.\")\n        return self.df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/identify_missing_data_interval/#src.sdk.python.rtdip_sdk.pipelines.data_quality.monitoring.spark.identify_missing_data_interval.IdentifyMissingDataInterval.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/monitoring/spark/identify_missing_data_interval.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/identify_missing_data_interval/#src.sdk.python.rtdip_sdk.pipelines.data_quality.monitoring.spark.identify_missing_data_interval.IdentifyMissingDataInterval.check","title":"<code>check()</code>","text":"<p>Executes the identify missing data logic.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: Returns the original PySpark DataFrame without changes.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/monitoring/spark/identify_missing_data_interval.py</code> <pre><code>def check(self) -&gt; PySparkDataFrame:\n    \"\"\"\n    Executes the identify missing data logic.\n\n    Returns:\n        pyspark.sql.DataFrame:\n            Returns the original PySpark DataFrame without changes.\n    \"\"\"\n    if \"EventTime\" not in self.df.columns:\n        self.logger.error(\"The DataFrame must contain an 'EventTime' column.\")\n        raise ValueError(\"The DataFrame must contain an 'EventTime' column.\")\n\n    df = self.df.withColumn(\"EventTime\", F.to_timestamp(\"EventTime\"))\n    df_sorted = df.orderBy(\"EventTime\")\n    # Calculate time difference in milliseconds between consecutive rows\n    df_with_diff = df_sorted.withColumn(\n        \"TimeDeltaMs\",\n        (\n            F.col(\"EventTime\").cast(\"double\")\n            - F.lag(\"EventTime\").over(Window.orderBy(\"EventTime\")).cast(\"double\")\n        )\n        * 1000,\n    ).withColumn(\n        \"StartMissing\", F.lag(\"EventTime\").over(Window.orderBy(\"EventTime\"))\n    )\n    # Parse interval to milliseconds if given\n    if self.interval is not None:\n        try:\n            interval_ms = parse_time_string_to_ms(self.interval)\n            self.logger.info(f\"Using provided expected interval: {interval_ms} ms\")\n        except ValueError as e:\n            self.logger.error(e)\n            raise\n    else:\n        # Calculate interval based on median of time differences\n        median_expr = F.expr(\"percentile_approx(TimeDeltaMs, 0.5)\")\n        median_row = df_with_diff.select(median_expr.alias(\"median\")).collect()[0]\n        interval_ms = median_row[\"median\"]\n        self.logger.info(\n            f\"Using median of time differences as expected interval: {interval_ms} ms\"\n        )\n    # Parse tolernace to milliseconds if given\n    if self.tolerance is not None:\n        try:\n            tolerance_ms = parse_time_string_to_ms(self.tolerance)\n            self.logger.info(f\"Using provided tolerance: {tolerance_ms} ms\")\n        except ValueError as e:\n            self.logger.error(e)\n            raise\n    else:\n        # Calculate tolerance based on MAD\n        mad_expr = F.expr(\n            f\"percentile_approx(abs(TimeDeltaMs - {interval_ms}), 0.5)\"\n        )\n        mad_row = df_with_diff.select(mad_expr.alias(\"mad\")).collect()[0]\n        mad = mad_row[\"mad\"]\n        calculated_tolerance_ms = self.mad_multiplier * mad\n        min_tolerance_ms = parse_time_string_to_ms(self.min_tolerance)\n        tolerance_ms = max(calculated_tolerance_ms, min_tolerance_ms)\n        self.logger.info(f\"Calculated tolerance: {tolerance_ms} ms (MAD-based)\")\n    # Calculate the maximum acceptable interval with tolerance\n    max_interval_with_tolerance_ms = interval_ms + tolerance_ms\n    self.logger.info(\n        f\"Maximum acceptable interval with tolerance: {max_interval_with_tolerance_ms} ms\"\n    )\n\n    # Identify missing intervals\n    missing_intervals_df = df_with_diff.filter(\n        (F.col(\"TimeDeltaMs\") &gt; max_interval_with_tolerance_ms)\n        &amp; (F.col(\"StartMissing\").isNotNull())\n    ).select(\n        \"TagName\",\n        \"StartMissing\",\n        F.col(\"EventTime\").alias(\"EndMissing\"),\n        \"TimeDeltaMs\",\n    )\n    # Convert time delta to readable format\n    missing_intervals_df = missing_intervals_df.withColumn(\n        \"DurationMissing\",\n        F.concat(\n            F.floor(F.col(\"TimeDeltaMs\") / 3600000).cast(\"string\"),\n            F.lit(\"h \"),\n            F.floor((F.col(\"TimeDeltaMs\") % 3600000) / 60000).cast(\"string\"),\n            F.lit(\"m \"),\n            F.floor(((F.col(\"TimeDeltaMs\") % 3600000) % 60000) / 1000).cast(\n                \"string\"\n            ),\n            F.lit(\"s\"),\n        ),\n    ).select(\"TagName\", \"StartMissing\", \"EndMissing\", \"DurationMissing\")\n    missing_intervals = missing_intervals_df.collect()\n    if missing_intervals:\n        self.logger.info(\"Detected Missing Intervals:\")\n        for row in missing_intervals:\n            self.logger.info(\n                f\"Tag: {row['TagName']} Missing Interval from {row['StartMissing']} to {row['EndMissing']} \"\n                f\"Duration: {row['DurationMissing']}\"\n            )\n    else:\n        self.logger.info(\"No missing intervals detected.\")\n    return self.df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/identify_missing_data_pattern/","title":"Pattern Based","text":""},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/identify_missing_data_pattern/#src.sdk.python.rtdip_sdk.pipelines.data_quality.monitoring.spark.identify_missing_data_pattern.IdentifyMissingDataPattern","title":"<code>IdentifyMissingDataPattern</code>","text":"<p>               Bases: <code>MonitoringBaseInterface</code>, <code>InputValidator</code></p> <p>Identifies missing data in a DataFrame based on specified time patterns. Logs the expected missing times.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Dataframe</code> <p>DataFrame containing at least the 'EventTime' column.</p> required <code>patterns</code> <code>list of dict</code> <p>List of dictionaries specifying the time patterns. - For 'minutely' frequency: Specify 'second' and optionally 'millisecond'.   Example: [{'second': 0}, {'second': 13}, {'second': 49}] - For 'hourly' frequency: Specify 'minute', 'second', and optionally 'millisecond'.   Example: [{'minute': 0, 'second': 0}, {'minute': 30, 'second': 30}]</p> required <code>frequency</code> <code>str</code> <p>Frequency of the patterns. Must be either 'minutely' or 'hourly'. - 'minutely': Patterns are checked every minute at specified seconds. - 'hourly': Patterns are checked every hour at specified minutes and seconds.</p> <code>'minutely'</code> <code>tolerance</code> <code>str</code> <p>Maximum allowed deviation from the pattern (e.g., '1s', '500ms'). Default is '10ms'.</p> <code>'10ms'</code> Example <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.master(\"local[1]\").appName(\"IdentifyMissingDataPatternExample\").getOrCreate()\n\npatterns = [\n    {\"second\": 0},\n    {\"second\": 20},\n]\n\nfrequency = \"minutely\"\ntolerance = \"1s\"\n\nidentify_missing_data = IdentifyMissingDataPattern(\n    df=df,\n    patterns=patterns,\n    frequency=frequency,\n    tolerance=tolerance,\n)\n\nidentify_missing_data.check()\n</code></pre> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/monitoring/spark/identify_missing_data_pattern.py</code> <pre><code>class IdentifyMissingDataPattern(MonitoringBaseInterface, InputValidator):\n    \"\"\"\n    Identifies missing data in a DataFrame based on specified time patterns.\n    Logs the expected missing times.\n\n    Args:\n        df (pyspark.sql.Dataframe): DataFrame containing at least the 'EventTime' column.\n        patterns (list of dict): List of dictionaries specifying the time patterns.\n            - For 'minutely' frequency: Specify 'second' and optionally 'millisecond'.\n              Example: [{'second': 0}, {'second': 13}, {'second': 49}]\n            - For 'hourly' frequency: Specify 'minute', 'second', and optionally 'millisecond'.\n              Example: [{'minute': 0, 'second': 0}, {'minute': 30, 'second': 30}]\n        frequency (str): Frequency of the patterns. Must be either 'minutely' or 'hourly'.\n            - 'minutely': Patterns are checked every minute at specified seconds.\n            - 'hourly': Patterns are checked every hour at specified minutes and seconds.\n        tolerance (str, optional): Maximum allowed deviation from the pattern (e.g., '1s', '500ms').\n            Default is '10ms'.\n\n    Example:\n        ```python\n        from pyspark.sql import SparkSession\n\n        spark = SparkSession.builder.master(\"local[1]\").appName(\"IdentifyMissingDataPatternExample\").getOrCreate()\n\n        patterns = [\n            {\"second\": 0},\n            {\"second\": 20},\n        ]\n\n        frequency = \"minutely\"\n        tolerance = \"1s\"\n\n        identify_missing_data = IdentifyMissingDataPattern(\n            df=df,\n            patterns=patterns,\n            frequency=frequency,\n            tolerance=tolerance,\n        )\n\n        identify_missing_data.check()\n        ```\n\n    \"\"\"\n\n    df: PySparkDataFrame\n    EXPECTED_SCHEMA = StructType(\n        [\n            StructField(\"TagName\", StringType(), True),\n            StructField(\"EventTime\", TimestampType(), True),\n            StructField(\"Status\", StringType(), True),\n            StructField(\"Value\", FloatType(), True),\n        ]\n    )\n\n    def __init__(\n        self,\n        df: PySparkDataFrame,\n        patterns: list,\n        frequency: str = \"minutely\",\n        tolerance: str = \"10ms\",\n    ) -&gt; None:\n\n        self.df = df\n        self.patterns = patterns\n        self.frequency = frequency.lower()\n        self.tolerance = tolerance\n        self.validate(self.EXPECTED_SCHEMA)\n\n        # Configure logging\n        self.logger = LoggerManager().create_logger(self.__class__.__name__)\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def check(self) -&gt; PySparkDataFrame:\n        \"\"\"\n        Executes the missing pattern detection logic. Identifies and logs any missing patterns\n        based on the provided patterns and frequency within the specified tolerance.\n\n        Returns:\n            pyspark.sql.DataFrame:\n                Returns the original PySpark DataFrame without changes.\n        \"\"\"\n        self._validate_inputs()\n        df = self.df.withColumn(\"EventTime\", F.to_timestamp(\"EventTime\"))\n        df_sorted = df.orderBy(\"EventTime\")\n        # Determine if the DataFrame is empty\n        count = df_sorted.count()\n        if count == 0:\n            self.logger.info(\"Generated 0 expected times based on patterns.\")\n            self.logger.info(\"DataFrame is empty. No missing patterns to detect.\")\n            return self.df\n        # Determine the time range of the data\n        min_time, max_time = df_sorted.agg(\n            F.min(\"EventTime\"), F.max(\"EventTime\")\n        ).first()\n        if not min_time or not max_time:\n            self.logger.info(\"Generated 0 expected times based on patterns.\")\n            self.logger.info(\"DataFrame is empty. No missing patterns to detect.\")\n            return self.df\n        # Generate all expected times based on patterns and frequency\n        expected_times_df = self._generate_expected_times(min_time, max_time)\n        # Identify missing patterns by left joining expected times with actual EventTimes within tolerance\n        missing_patterns_df = self._find_missing_patterns(expected_times_df, df_sorted)\n        self._log_missing_patterns(missing_patterns_df)\n        return self.df\n\n    def _validate_inputs(self):\n        if self.frequency not in [\"minutely\", \"hourly\"]:\n            error_msg = \"Frequency must be either 'minutely' or 'hourly'.\"\n            self.logger.error(error_msg)\n            raise ValueError(error_msg)\n        for pattern in self.patterns:\n            if self.frequency == \"minutely\":\n                self.validate_minutely_pattern(pattern)\n            elif self.frequency == \"hourly\":\n                self.validate_hourly_patterns(pattern)\n        try:\n            self.tolerance_ms = parse_time_string_to_ms(self.tolerance)\n            self.tolerance_seconds = self.tolerance_ms / 1000\n            self.logger.info(\n                f\"Using tolerance: {self.tolerance_ms} ms ({self.tolerance_seconds} seconds)\"\n            )\n        except ValueError as e:\n            error_msg = f\"Invalid tolerance format: {self.tolerance}\"\n            self.logger.error(error_msg)\n            raise ValueError(error_msg) from e\n\n    def validate_hourly_patterns(self, pattern):\n        if \"minute\" not in pattern or \"second\" not in pattern:\n            raise ValueError(\n                \"Each pattern must have 'minute' and 'second' keys for 'hourly' frequency.\"\n            )\n        if pattern.get(\"minute\", 0) &gt;= 60:\n            raise ValueError(\"For 'hourly' frequency, 'minute' must be less than 60.\")\n        if \"hour\" in pattern:\n            raise ValueError(\n                \"For 'hourly' frequency, pattern should not contain 'hour'.\"\n            )\n\n    def validate_minutely_pattern(self, pattern):\n        if \"second\" not in pattern:\n            raise ValueError(\n                \"Each pattern must have a 'second' key for 'minutely' frequency.\"\n            )\n        if pattern.get(\"second\", 0) &gt;= 60:\n            raise ValueError(\"For 'minutely' frequency, 'second' must be less than 60.\")\n        if \"minute\" in pattern or \"hour\" in pattern:\n            raise ValueError(\n                \"For 'minutely' frequency, pattern should not contain 'minute' or 'hour'.\"\n            )\n\n    def _generate_expected_times(self, min_time, max_time) -&gt; PySparkDataFrame:\n        floor_min_time = self._get_floor_min_time(min_time)\n        ceil_max_time = self._get_ceil_max_time(max_time)\n        base_times_df = self._create_base_times_df(floor_min_time, ceil_max_time)\n        expected_times_df = self._apply_patterns(\n            base_times_df, floor_min_time, max_time\n        )\n        return expected_times_df\n\n    def _get_floor_min_time(self, min_time):\n        if self.frequency == \"minutely\":\n            return min_time.replace(second=0, microsecond=0)\n        elif self.frequency == \"hourly\":\n            return min_time.replace(minute=0, second=0, microsecond=0)\n\n    def _get_ceil_max_time(self, max_time):\n        if self.frequency == \"minutely\":\n            return (max_time + pd.Timedelta(minutes=1)).replace(second=0, microsecond=0)\n        elif self.frequency == \"hourly\":\n            return (max_time + pd.Timedelta(hours=1)).replace(\n                minute=0, second=0, microsecond=0\n            )\n\n    def _create_base_times_df(self, floor_min_time, ceil_max_time):\n        step = F.expr(f\"INTERVAL 1 {self.frequency.upper()[:-2]}\")\n        return self.df.sparkSession.createDataFrame(\n            [(floor_min_time, ceil_max_time)], [\"start\", \"end\"]\n        ).select(\n            F.explode(\n                F.sequence(\n                    F.col(\"start\").cast(\"timestamp\"),\n                    F.col(\"end\").cast(\"timestamp\"),\n                    step,\n                )\n            ).alias(\"BaseTime\")\n        )\n\n    def _apply_patterns(self, base_times_df, floor_min_time, max_time):\n        expected_times = []\n        for pattern in self.patterns:\n            expected_time = self._calculate_expected_time(base_times_df, pattern)\n            expected_times.append(expected_time)\n        expected_times_df = (\n            base_times_df.withColumn(\n                \"ExpectedTime\", F.explode(F.array(*expected_times))\n            )\n            .select(\"ExpectedTime\")\n            .distinct()\n            .filter(\n                (F.col(\"ExpectedTime\") &gt;= F.lit(floor_min_time))\n                &amp; (F.col(\"ExpectedTime\") &lt;= F.lit(max_time))\n            )\n        )\n        return expected_times_df\n\n    def _calculate_expected_time(self, base_times_df, pattern):\n        if self.frequency == \"minutely\":\n            seconds = pattern.get(\"second\", 0)\n            milliseconds = pattern.get(\"millisecond\", 0)\n            return (\n                F.col(\"BaseTime\")\n                + F.expr(f\"INTERVAL {seconds} SECOND\")\n                + F.expr(f\"INTERVAL {milliseconds} MILLISECOND\")\n            )\n        elif self.frequency == \"hourly\":\n            minutes = pattern.get(\"minute\", 0)\n            seconds = pattern.get(\"second\", 0)\n            milliseconds = pattern.get(\"millisecond\", 0)\n            return (\n                F.col(\"BaseTime\")\n                + F.expr(f\"INTERVAL {minutes} MINUTE\")\n                + F.expr(f\"INTERVAL {seconds} SECOND\")\n                + F.expr(f\"INTERVAL {milliseconds} MILLISECOND\")\n            )\n\n    def _find_missing_patterns(\n        self, expected_times_df: PySparkDataFrame, actual_df: PySparkDataFrame\n    ) -&gt; PySparkDataFrame:\n        \"\"\"\n        Finds missing patterns by comparing expected times with actual EventTimes within tolerance.\n\n        Args:\n            expected_times_df (PySparkDataFrame): DataFrame with expected 'ExpectedTime'.\n            actual_df (PySparkDataFrame): Actual DataFrame with 'EventTime'.\n\n        Returns:\n            PySparkDataFrame: DataFrame with missing 'ExpectedTime'.\n        \"\"\"\n        # Format tolerance for SQL INTERVAL\n        tolerance_str = self._format_timedelta_for_sql(self.tolerance_ms)\n        # Perform left join with tolerance window\n        actual_event_time = \"at.EventTime\"\n        missing_patterns_df = (\n            expected_times_df.alias(\"et\")\n            .join(\n                actual_df.alias(\"at\"),\n                (\n                    F.col(actual_event_time)\n                    &gt;= F.expr(f\"et.ExpectedTime - INTERVAL {tolerance_str}\")\n                )\n                &amp; (\n                    F.col(actual_event_time)\n                    &lt;= F.expr(f\"et.ExpectedTime + INTERVAL {tolerance_str}\")\n                ),\n                how=\"left\",\n            )\n            .filter(F.col(actual_event_time).isNull())\n            .select(F.col(\"et.ExpectedTime\"))\n        )\n        self.logger.info(f\"Identified {missing_patterns_df.count()} missing patterns.\")\n        return missing_patterns_df\n\n    def _log_missing_patterns(self, missing_patterns_df: PySparkDataFrame):\n        \"\"\"\n        Logs the missing patterns.\n\n        Args:\n            missing_patterns_df (PySparkDataFrame): DataFrame with missing 'ExpectedTime'.\n        \"\"\"\n        missing_patterns = missing_patterns_df.collect()\n        if missing_patterns:\n            self.logger.info(\"Detected Missing Patterns:\")\n            # Sort missing patterns by ExpectedTime\n            sorted_missing_patterns = sorted(\n                missing_patterns, key=lambda row: row[\"ExpectedTime\"]\n            )\n            for row in sorted_missing_patterns:\n                # Format ExpectedTime to include milliseconds correctly\n                formatted_time = row[\"ExpectedTime\"].strftime(\"%Y-%m-%d %H:%M:%S.%f\")[\n                    :-3\n                ]\n                self.logger.info(f\"Missing Pattern at {formatted_time}\")\n        else:\n            self.logger.info(\"No missing patterns detected.\")\n\n    @staticmethod\n    def _format_timedelta_for_sql(tolerance_ms: float) -&gt; str:\n        \"\"\"\n        Formats a tolerance in milliseconds to a string suitable for SQL INTERVAL.\n\n        Args:\n            tolerance_ms (float): Tolerance in milliseconds.\n\n        Returns:\n            str: Formatted string (e.g., '1 SECOND', '500 MILLISECONDS').\n        \"\"\"\n        if tolerance_ms &gt;= 3600000:\n            hours = int(tolerance_ms // 3600000)\n            return f\"{hours} HOURS\"\n        elif tolerance_ms &gt;= 60000:\n            minutes = int(tolerance_ms // 60000)\n            return f\"{minutes} MINUTES\"\n        elif tolerance_ms &gt;= 1000:\n            seconds = int(tolerance_ms // 1000)\n            return f\"{seconds} SECONDS\"\n        else:\n            milliseconds = int(tolerance_ms)\n            return f\"{milliseconds} MILLISECONDS\"\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/identify_missing_data_pattern/#src.sdk.python.rtdip_sdk.pipelines.data_quality.monitoring.spark.identify_missing_data_pattern.IdentifyMissingDataPattern.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/monitoring/spark/identify_missing_data_pattern.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/identify_missing_data_pattern/#src.sdk.python.rtdip_sdk.pipelines.data_quality.monitoring.spark.identify_missing_data_pattern.IdentifyMissingDataPattern.check","title":"<code>check()</code>","text":"<p>Executes the missing pattern detection logic. Identifies and logs any missing patterns based on the provided patterns and frequency within the specified tolerance.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: Returns the original PySpark DataFrame without changes.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/monitoring/spark/identify_missing_data_pattern.py</code> <pre><code>def check(self) -&gt; PySparkDataFrame:\n    \"\"\"\n    Executes the missing pattern detection logic. Identifies and logs any missing patterns\n    based on the provided patterns and frequency within the specified tolerance.\n\n    Returns:\n        pyspark.sql.DataFrame:\n            Returns the original PySpark DataFrame without changes.\n    \"\"\"\n    self._validate_inputs()\n    df = self.df.withColumn(\"EventTime\", F.to_timestamp(\"EventTime\"))\n    df_sorted = df.orderBy(\"EventTime\")\n    # Determine if the DataFrame is empty\n    count = df_sorted.count()\n    if count == 0:\n        self.logger.info(\"Generated 0 expected times based on patterns.\")\n        self.logger.info(\"DataFrame is empty. No missing patterns to detect.\")\n        return self.df\n    # Determine the time range of the data\n    min_time, max_time = df_sorted.agg(\n        F.min(\"EventTime\"), F.max(\"EventTime\")\n    ).first()\n    if not min_time or not max_time:\n        self.logger.info(\"Generated 0 expected times based on patterns.\")\n        self.logger.info(\"DataFrame is empty. No missing patterns to detect.\")\n        return self.df\n    # Generate all expected times based on patterns and frequency\n    expected_times_df = self._generate_expected_times(min_time, max_time)\n    # Identify missing patterns by left joining expected times with actual EventTimes within tolerance\n    missing_patterns_df = self._find_missing_patterns(expected_times_df, df_sorted)\n    self._log_missing_patterns(missing_patterns_df)\n    return self.df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/moving_average/","title":"Moving Average","text":""},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/moving_average/#src.sdk.python.rtdip_sdk.pipelines.data_quality.monitoring.spark.moving_average.MovingAverage","title":"<code>MovingAverage</code>","text":"<p>               Bases: <code>MonitoringBaseInterface</code>, <code>InputValidator</code></p> <p>Computes and logs the moving average over a specified window size for a given PySpark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to process.</p> required <code>window_size</code> <code>int</code> <p>The size of the moving window.</p> required Example <pre><code>from pyspark.sql import SparkSession\nfrom rtdip_sdk.pipelines.data_quality.monitoring.spark.data_quality.moving_average import MovingAverage\n\nspark = SparkSession.builder.master(\"local[1]\").appName(\"MovingAverageExample\").getOrCreate()\n\ndata = [\n    (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 03:49:45.000\", \"Good\", 1.0),\n    (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 07:53:11.000\", \"Good\", 2.0),\n    (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 11:56:42.000\", \"Good\", 3.0),\n    (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 16:00:12.000\", \"Good\", 4.0),\n    (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 20:03:46.000\", \"Good\", 5.0),\n]\n\ncolumns = [\"TagName\", \"EventTime\", \"Status\", \"Value\"]\n\ndf = spark.createDataFrame(data, columns)\n\nmoving_avg = MovingAverage(\n    df=df,\n    window_size=3,\n)\n\nmoving_avg.check()\n</code></pre> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/monitoring/spark/moving_average.py</code> <pre><code>class MovingAverage(MonitoringBaseInterface, InputValidator):\n    \"\"\"\n    Computes and logs the moving average over a specified window size for a given PySpark DataFrame.\n\n    Args:\n        df (pyspark.sql.DataFrame): The DataFrame to process.\n        window_size (int): The size of the moving window.\n\n    Example:\n        ```python\n        from pyspark.sql import SparkSession\n        from rtdip_sdk.pipelines.data_quality.monitoring.spark.data_quality.moving_average import MovingAverage\n\n        spark = SparkSession.builder.master(\"local[1]\").appName(\"MovingAverageExample\").getOrCreate()\n\n        data = [\n            (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 03:49:45.000\", \"Good\", 1.0),\n            (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 07:53:11.000\", \"Good\", 2.0),\n            (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 11:56:42.000\", \"Good\", 3.0),\n            (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 16:00:12.000\", \"Good\", 4.0),\n            (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 20:03:46.000\", \"Good\", 5.0),\n        ]\n\n        columns = [\"TagName\", \"EventTime\", \"Status\", \"Value\"]\n\n        df = spark.createDataFrame(data, columns)\n\n        moving_avg = MovingAverage(\n            df=df,\n            window_size=3,\n        )\n\n        moving_avg.check()\n        ```\n    \"\"\"\n\n    df: PySparkDataFrame\n    window_size: int\n    EXPECTED_SCHEMA = StructType(\n        [\n            StructField(\"TagName\", StringType(), True),\n            StructField(\"EventTime\", TimestampType(), True),\n            StructField(\"Status\", StringType(), True),\n            StructField(\"Value\", FloatType(), True),\n        ]\n    )\n\n    def __init__(\n        self,\n        df: PySparkDataFrame,\n        window_size: int,\n    ) -&gt; None:\n        if not isinstance(window_size, int) or window_size &lt;= 0:\n            raise ValueError(\"window_size must be a positive integer.\")\n\n        self.df = df\n        self.validate(self.EXPECTED_SCHEMA)\n        self.window_size = window_size\n\n        self.logger = logging.getLogger(self.__class__.__name__)\n        if not self.logger.handlers:\n            handler = logging.StreamHandler()\n            formatter = logging.Formatter(\n                \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n            )\n            handler.setFormatter(formatter)\n            self.logger.addHandler(handler)\n            self.logger.setLevel(logging.INFO)\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def check(self) -&gt; None:\n        \"\"\"\n        Computes and logs the moving average using a specified window size.\n        \"\"\"\n\n        self._validate_inputs()\n\n        window_spec = (\n            Window.partitionBy(\"TagName\")\n            .orderBy(\"EventTime\")\n            .rowsBetween(-(self.window_size - 1), 0)\n        )\n\n        self.logger.info(\"Computing moving averages:\")\n\n        for row in (\n            self.df.withColumn(\"MovingAverage\", avg(col(\"Value\")).over(window_spec))\n            .select(\"TagName\", \"EventTime\", \"Value\", \"MovingAverage\")\n            .collect()\n        ):\n            self.logger.info(\n                f\"Tag: {row.TagName}, Time: {row.EventTime}, Value: {row.Value}, Moving Avg: {row.MovingAverage}\"\n            )\n\n    def _validate_inputs(self):\n        if not isinstance(self.window_size, int) or self.window_size &lt;= 0:\n            raise ValueError(\"window_size must be a positive integer.\")\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/moving_average/#src.sdk.python.rtdip_sdk.pipelines.data_quality.monitoring.spark.moving_average.MovingAverage.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/monitoring/spark/moving_average.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/data_quality/monitoring/spark/moving_average/#src.sdk.python.rtdip_sdk.pipelines.data_quality.monitoring.spark.moving_average.MovingAverage.check","title":"<code>check()</code>","text":"<p>Computes and logs the moving average using a specified window size.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/data_quality/monitoring/spark/moving_average.py</code> <pre><code>def check(self) -&gt; None:\n    \"\"\"\n    Computes and logs the moving average using a specified window size.\n    \"\"\"\n\n    self._validate_inputs()\n\n    window_spec = (\n        Window.partitionBy(\"TagName\")\n        .orderBy(\"EventTime\")\n        .rowsBetween(-(self.window_size - 1), 0)\n    )\n\n    self.logger.info(\"Computing moving averages:\")\n\n    for row in (\n        self.df.withColumn(\"MovingAverage\", avg(col(\"Value\")).over(window_spec))\n        .select(\"TagName\", \"EventTime\", \"Value\", \"MovingAverage\")\n        .collect()\n    ):\n        self.logger.info(\n            f\"Tag: {row.TagName}, Time: {row.EventTime}, Value: {row.Value}, Moving Avg: {row.MovingAverage}\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/","title":"Databricks","text":""},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.CreateJob","title":"<code>CreateJob</code>  <code>dataclass</code>","text":"Source code in <code>src/sdk/python/rtdip_sdk/pipelines/deploy/databricks.py</code> <pre><code>@dataclass\nclass CreateJob:\n    access_control_list: Optional[List[JobAccessControlRequest]] = None\n    \"\"\"List of permissions to set on the job.\"\"\"\n\n    budget_policy_id: Optional[str] = None\n    \"\"\"The id of the user specified budget policy to use for this job. If not specified, a default\n    budget policy may be applied when creating or modifying the job. See\n    `effective_budget_policy_id` for the budget policy used by this workload.\"\"\"\n\n    continuous: Optional[Continuous] = None\n    \"\"\"An optional continuous property for this job. The continuous property will ensure that there is\n    always one run executing. Only one of `schedule` and `continuous` can be used.\"\"\"\n\n    deployment: Optional[JobDeployment] = None\n    \"\"\"Deployment information for jobs managed by external sources.\"\"\"\n\n    description: Optional[str] = None\n    \"\"\"An optional description for the job. The maximum length is 27700 characters in UTF-8 encoding.\"\"\"\n\n    edit_mode: Optional[JobEditMode] = None\n    \"\"\"Edit mode of the job.\n\n    * `UI_LOCKED`: The job is in a locked UI state and cannot be modified. * `EDITABLE`: The job is\n    in an editable state and can be modified.\"\"\"\n\n    email_notifications: Optional[JobEmailNotifications] = None\n    \"\"\"An optional set of email addresses that is notified when runs of this job begin or complete as\n    well as when this job is deleted.\"\"\"\n\n    environments: Optional[List[JobEnvironment]] = None\n    \"\"\"A list of task execution environment specifications that can be referenced by serverless tasks\n    of this job. An environment is required to be present for serverless tasks. For serverless\n    notebook tasks, the environment is accessible in the notebook environment panel. For other\n    serverless tasks, the task environment is required to be specified using environment_key in the\n    task settings.\"\"\"\n\n    format: Optional[Format] = None\n    \"\"\"Used to tell what is the format of the job. This field is ignored in Create/Update/Reset calls.\n    When using the Jobs API 2.1 this value is always set to `\"MULTI_TASK\"`.\"\"\"\n\n    git_source: Optional[GitSource] = None\n    \"\"\"An optional specification for a remote Git repository containing the source code used by tasks.\n    Version-controlled source code is supported by notebook, dbt, Python script, and SQL File tasks.\n\n    If `git_source` is set, these tasks retrieve the file from the remote repository by default.\n    However, this behavior can be overridden by setting `source` to `WORKSPACE` on the task.\n\n    Note: dbt and SQL File tasks support only version-controlled sources. If dbt or SQL File tasks\n    are used, `git_source` must be defined on the job.\"\"\"\n\n    health: Optional[JobsHealthRules] = None\n\n    job_clusters: Optional[List[JobCluster]] = None\n    \"\"\"A list of job cluster specifications that can be shared and reused by tasks of this job.\n    Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in\n    task settings.\"\"\"\n\n    max_concurrent_runs: Optional[int] = None\n    \"\"\"An optional maximum allowed number of concurrent runs of the job. Set this value if you want to\n    be able to execute multiple runs of the same job concurrently. This is useful for example if you\n    trigger your job on a frequent schedule and want to allow consecutive runs to overlap with each\n    other, or if you want to trigger multiple runs which differ by their input parameters. This\n    setting affects only new runs. For example, suppose the job\u2019s concurrency is 4 and there are 4\n    concurrent active runs. Then setting the concurrency to 3 won\u2019t kill any of the active runs.\n    However, from then on, new runs are skipped unless there are fewer than 3 active runs. This\n    value cannot exceed 1000. Setting this value to `0` causes all new runs to be skipped.\"\"\"\n\n    name: Optional[str] = None\n    \"\"\"An optional name for the job. The maximum length is 4096 bytes in UTF-8 encoding.\"\"\"\n\n    notification_settings: Optional[JobNotificationSettings] = None\n    \"\"\"Optional notification settings that are used when sending notifications to each of the\n    `email_notifications` and `webhook_notifications` for this job.\"\"\"\n\n    parameters: Optional[List[JobParameterDefinition]] = None\n    \"\"\"Job-level parameter definitions\"\"\"\n\n    performance_target: Optional[PerformanceTarget] = None\n    \"\"\"The performance mode on a serverless job. This field determines the level of compute performance\n    or cost-efficiency for the run.\n\n    * `STANDARD`: Enables cost-efficient execution of serverless workloads. *\n    `PERFORMANCE_OPTIMIZED`: Prioritizes fast startup and execution times through rapid scaling and\n    optimized cluster performance.\"\"\"\n\n    queue: Optional[QueueSettings] = None\n    \"\"\"The queue settings of the job.\"\"\"\n\n    run_as: Optional[JobRunAs] = None\n\n    schedule: Optional[CronSchedule] = None\n    \"\"\"An optional periodic schedule for this job. The default behavior is that the job only runs when\n    triggered by clicking \u201cRun Now\u201d in the Jobs UI or sending an API request to `runNow`.\"\"\"\n\n    tags: Optional[Dict[str, str]] = None\n    \"\"\"A map of tags associated with the job. These are forwarded to the cluster as cluster tags for\n    jobs clusters, and are subject to the same limitations as cluster tags. A maximum of 25 tags can\n    be added to the job.\"\"\"\n\n    tasks: Optional[List[Task]] = None\n    \"\"\"A list of task specifications to be executed by this job. It supports up to 1000 elements in\n    write endpoints (:method:jobs/create, :method:jobs/reset, :method:jobs/update,\n    :method:jobs/submit). Read endpoints return only 100 tasks. If more than 100 tasks are\n    available, you can paginate through them using :method:jobs/get. Use the `next_page_token` field\n    at the object root to determine if more results are available.\"\"\"\n\n    timeout_seconds: Optional[int] = None\n    \"\"\"An optional timeout applied to each run of this job. A value of `0` means no timeout.\"\"\"\n\n    trigger: Optional[TriggerSettings] = None\n    \"\"\"A configuration to trigger a run when certain conditions are met. The default behavior is that\n    the job runs only when triggered by clicking \u201cRun Now\u201d in the Jobs UI or sending an API\n    request to `runNow`.\"\"\"\n\n    webhook_notifications: Optional[WebhookNotifications] = None\n    \"\"\"A collection of system notification IDs to notify when runs of this job begin or complete.\"\"\"\n\n    def as_dict(self) -&gt; dict:  # pragma: no cover\n        \"\"\"Serializes the CreateJob into a dictionary suitable for use as a JSON request body.\"\"\"\n        body = {}\n        if self.access_control_list:\n            body[\"access_control_list\"] = [\n                v.as_dict() for v in self.access_control_list\n            ]\n        if self.budget_policy_id is not None:\n            body[\"budget_policy_id\"] = self.budget_policy_id\n        if self.continuous:\n            body[\"continuous\"] = self.continuous.as_dict()\n        if self.deployment:\n            body[\"deployment\"] = self.deployment.as_dict()\n        if self.description is not None:\n            body[\"description\"] = self.description\n        if self.edit_mode is not None:\n            body[\"edit_mode\"] = self.edit_mode.value\n        if self.email_notifications:\n            body[\"email_notifications\"] = self.email_notifications.as_dict()\n        if self.environments:\n            body[\"environments\"] = [v.as_dict() for v in self.environments]\n        if self.format is not None:\n            body[\"format\"] = self.format.value\n        if self.git_source:\n            body[\"git_source\"] = self.git_source.as_dict()\n        if self.health:\n            body[\"health\"] = self.health.as_dict()\n        if self.job_clusters:\n            body[\"job_clusters\"] = [v.as_dict() for v in self.job_clusters]\n        if self.max_concurrent_runs is not None:\n            body[\"max_concurrent_runs\"] = self.max_concurrent_runs\n        if self.name is not None:\n            body[\"name\"] = self.name\n        if self.notification_settings:\n            body[\"notification_settings\"] = self.notification_settings.as_dict()\n        if self.parameters:\n            body[\"parameters\"] = [v.as_dict() for v in self.parameters]\n        if self.performance_target is not None:\n            body[\"performance_target\"] = self.performance_target.value\n        if self.queue:\n            body[\"queue\"] = self.queue.as_dict()\n        if self.run_as:\n            body[\"run_as\"] = self.run_as.as_dict()\n        if self.schedule:\n            body[\"schedule\"] = self.schedule.as_dict()\n        if self.tags:\n            body[\"tags\"] = self.tags\n        if self.tasks:\n            body[\"tasks\"] = [v.as_dict() for v in self.tasks]\n        if self.timeout_seconds is not None:\n            body[\"timeout_seconds\"] = self.timeout_seconds\n        if self.trigger:\n            body[\"trigger\"] = self.trigger.as_dict()\n        if self.webhook_notifications:\n            body[\"webhook_notifications\"] = self.webhook_notifications.as_dict()\n        return body\n\n    def as_shallow_dict(self) -&gt; dict:  # pragma: no cover\n        \"\"\"Serializes the CreateJob into a shallow dictionary of its immediate attributes.\"\"\"\n        body = {}\n        if self.access_control_list:\n            body[\"access_control_list\"] = self.access_control_list\n        if self.budget_policy_id is not None:\n            body[\"budget_policy_id\"] = self.budget_policy_id\n        if self.continuous:\n            body[\"continuous\"] = self.continuous\n        if self.deployment:\n            body[\"deployment\"] = self.deployment\n        if self.description is not None:\n            body[\"description\"] = self.description\n        if self.edit_mode is not None:\n            body[\"edit_mode\"] = self.edit_mode\n        if self.email_notifications:\n            body[\"email_notifications\"] = self.email_notifications\n        if self.environments:\n            body[\"environments\"] = self.environments\n        if self.format is not None:\n            body[\"format\"] = self.format\n        if self.git_source:\n            body[\"git_source\"] = self.git_source\n        if self.health:\n            body[\"health\"] = self.health\n        if self.job_clusters:\n            body[\"job_clusters\"] = self.job_clusters\n        if self.max_concurrent_runs is not None:\n            body[\"max_concurrent_runs\"] = self.max_concurrent_runs\n        if self.name is not None:\n            body[\"name\"] = self.name\n        if self.notification_settings:\n            body[\"notification_settings\"] = self.notification_settings\n        if self.parameters:\n            body[\"parameters\"] = self.parameters\n        if self.performance_target is not None:\n            body[\"performance_target\"] = self.performance_target\n        if self.queue:\n            body[\"queue\"] = self.queue\n        if self.run_as:\n            body[\"run_as\"] = self.run_as\n        if self.schedule:\n            body[\"schedule\"] = self.schedule\n        if self.tags:\n            body[\"tags\"] = self.tags\n        if self.tasks:\n            body[\"tasks\"] = self.tasks\n        if self.timeout_seconds is not None:\n            body[\"timeout_seconds\"] = self.timeout_seconds\n        if self.trigger:\n            body[\"trigger\"] = self.trigger\n        if self.webhook_notifications:\n            body[\"webhook_notifications\"] = self.webhook_notifications\n        return body\n</code></pre>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.CreateJob.access_control_list","title":"<code>access_control_list: Optional[List[JobAccessControlRequest]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List of permissions to set on the job.</p>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.CreateJob.budget_policy_id","title":"<code>budget_policy_id: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The id of the user specified budget policy to use for this job. If not specified, a default budget policy may be applied when creating or modifying the job. See <code>effective_budget_policy_id</code> for the budget policy used by this workload.</p>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.CreateJob.continuous","title":"<code>continuous: Optional[Continuous] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An optional continuous property for this job. The continuous property will ensure that there is always one run executing. Only one of <code>schedule</code> and <code>continuous</code> can be used.</p>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.CreateJob.deployment","title":"<code>deployment: Optional[JobDeployment] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Deployment information for jobs managed by external sources.</p>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.CreateJob.description","title":"<code>description: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An optional description for the job. The maximum length is 27700 characters in UTF-8 encoding.</p>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.CreateJob.edit_mode","title":"<code>edit_mode: Optional[JobEditMode] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Edit mode of the job.</p> <ul> <li><code>UI_LOCKED</code>: The job is in a locked UI state and cannot be modified. * <code>EDITABLE</code>: The job is in an editable state and can be modified.</li> </ul>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.CreateJob.email_notifications","title":"<code>email_notifications: Optional[JobEmailNotifications] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An optional set of email addresses that is notified when runs of this job begin or complete as well as when this job is deleted.</p>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.CreateJob.environments","title":"<code>environments: Optional[List[JobEnvironment]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A list of task execution environment specifications that can be referenced by serverless tasks of this job. An environment is required to be present for serverless tasks. For serverless notebook tasks, the environment is accessible in the notebook environment panel. For other serverless tasks, the task environment is required to be specified using environment_key in the task settings.</p>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.CreateJob.format","title":"<code>format: Optional[Format] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Used to tell what is the format of the job. This field is ignored in Create/Update/Reset calls. When using the Jobs API 2.1 this value is always set to <code>\"MULTI_TASK\"</code>.</p>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.CreateJob.git_source","title":"<code>git_source: Optional[GitSource] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An optional specification for a remote Git repository containing the source code used by tasks. Version-controlled source code is supported by notebook, dbt, Python script, and SQL File tasks.</p> <p>If <code>git_source</code> is set, these tasks retrieve the file from the remote repository by default. However, this behavior can be overridden by setting <code>source</code> to <code>WORKSPACE</code> on the task.</p> <p>Note: dbt and SQL File tasks support only version-controlled sources. If dbt or SQL File tasks are used, <code>git_source</code> must be defined on the job.</p>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.CreateJob.job_clusters","title":"<code>job_clusters: Optional[List[JobCluster]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A list of job cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings.</p>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.CreateJob.max_concurrent_runs","title":"<code>max_concurrent_runs: Optional[int] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An optional maximum allowed number of concurrent runs of the job. Set this value if you want to be able to execute multiple runs of the same job concurrently. This is useful for example if you trigger your job on a frequent schedule and want to allow consecutive runs to overlap with each other, or if you want to trigger multiple runs which differ by their input parameters. This setting affects only new runs. For example, suppose the job\u2019s concurrency is 4 and there are 4 concurrent active runs. Then setting the concurrency to 3 won\u2019t kill any of the active runs. However, from then on, new runs are skipped unless there are fewer than 3 active runs. This value cannot exceed 1000. Setting this value to <code>0</code> causes all new runs to be skipped.</p>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.CreateJob.name","title":"<code>name: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An optional name for the job. The maximum length is 4096 bytes in UTF-8 encoding.</p>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.CreateJob.notification_settings","title":"<code>notification_settings: Optional[JobNotificationSettings] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optional notification settings that are used when sending notifications to each of the <code>email_notifications</code> and <code>webhook_notifications</code> for this job.</p>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.CreateJob.parameters","title":"<code>parameters: Optional[List[JobParameterDefinition]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Job-level parameter definitions</p>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.CreateJob.performance_target","title":"<code>performance_target: Optional[PerformanceTarget] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The performance mode on a serverless job. This field determines the level of compute performance or cost-efficiency for the run.</p> <ul> <li><code>STANDARD</code>: Enables cost-efficient execution of serverless workloads. * <code>PERFORMANCE_OPTIMIZED</code>: Prioritizes fast startup and execution times through rapid scaling and optimized cluster performance.</li> </ul>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.CreateJob.queue","title":"<code>queue: Optional[QueueSettings] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The queue settings of the job.</p>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.CreateJob.schedule","title":"<code>schedule: Optional[CronSchedule] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An optional periodic schedule for this job. The default behavior is that the job only runs when triggered by clicking \u201cRun Now\u201d in the Jobs UI or sending an API request to <code>runNow</code>.</p>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.CreateJob.tags","title":"<code>tags: Optional[Dict[str, str]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A map of tags associated with the job. These are forwarded to the cluster as cluster tags for jobs clusters, and are subject to the same limitations as cluster tags. A maximum of 25 tags can be added to the job.</p>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.CreateJob.tasks","title":"<code>tasks: Optional[List[Task]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A list of task specifications to be executed by this job. It supports up to 1000 elements in write endpoints (:method:jobs/create, :method:jobs/reset, :method:jobs/update, :method:jobs/submit). Read endpoints return only 100 tasks. If more than 100 tasks are available, you can paginate through them using :method:jobs/get. Use the <code>next_page_token</code> field at the object root to determine if more results are available.</p>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.CreateJob.timeout_seconds","title":"<code>timeout_seconds: Optional[int] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An optional timeout applied to each run of this job. A value of <code>0</code> means no timeout.</p>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.CreateJob.trigger","title":"<code>trigger: Optional[TriggerSettings] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A configuration to trigger a run when certain conditions are met. The default behavior is that the job runs only when triggered by clicking \u201cRun Now\u201d in the Jobs UI or sending an API request to <code>runNow</code>.</p>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.CreateJob.webhook_notifications","title":"<code>webhook_notifications: Optional[WebhookNotifications] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A collection of system notification IDs to notify when runs of this job begin or complete.</p>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.CreateJob.as_dict","title":"<code>as_dict()</code>","text":"<p>Serializes the CreateJob into a dictionary suitable for use as a JSON request body.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/deploy/databricks.py</code> <pre><code>def as_dict(self) -&gt; dict:  # pragma: no cover\n    \"\"\"Serializes the CreateJob into a dictionary suitable for use as a JSON request body.\"\"\"\n    body = {}\n    if self.access_control_list:\n        body[\"access_control_list\"] = [\n            v.as_dict() for v in self.access_control_list\n        ]\n    if self.budget_policy_id is not None:\n        body[\"budget_policy_id\"] = self.budget_policy_id\n    if self.continuous:\n        body[\"continuous\"] = self.continuous.as_dict()\n    if self.deployment:\n        body[\"deployment\"] = self.deployment.as_dict()\n    if self.description is not None:\n        body[\"description\"] = self.description\n    if self.edit_mode is not None:\n        body[\"edit_mode\"] = self.edit_mode.value\n    if self.email_notifications:\n        body[\"email_notifications\"] = self.email_notifications.as_dict()\n    if self.environments:\n        body[\"environments\"] = [v.as_dict() for v in self.environments]\n    if self.format is not None:\n        body[\"format\"] = self.format.value\n    if self.git_source:\n        body[\"git_source\"] = self.git_source.as_dict()\n    if self.health:\n        body[\"health\"] = self.health.as_dict()\n    if self.job_clusters:\n        body[\"job_clusters\"] = [v.as_dict() for v in self.job_clusters]\n    if self.max_concurrent_runs is not None:\n        body[\"max_concurrent_runs\"] = self.max_concurrent_runs\n    if self.name is not None:\n        body[\"name\"] = self.name\n    if self.notification_settings:\n        body[\"notification_settings\"] = self.notification_settings.as_dict()\n    if self.parameters:\n        body[\"parameters\"] = [v.as_dict() for v in self.parameters]\n    if self.performance_target is not None:\n        body[\"performance_target\"] = self.performance_target.value\n    if self.queue:\n        body[\"queue\"] = self.queue.as_dict()\n    if self.run_as:\n        body[\"run_as\"] = self.run_as.as_dict()\n    if self.schedule:\n        body[\"schedule\"] = self.schedule.as_dict()\n    if self.tags:\n        body[\"tags\"] = self.tags\n    if self.tasks:\n        body[\"tasks\"] = [v.as_dict() for v in self.tasks]\n    if self.timeout_seconds is not None:\n        body[\"timeout_seconds\"] = self.timeout_seconds\n    if self.trigger:\n        body[\"trigger\"] = self.trigger.as_dict()\n    if self.webhook_notifications:\n        body[\"webhook_notifications\"] = self.webhook_notifications.as_dict()\n    return body\n</code></pre>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.CreateJob.as_shallow_dict","title":"<code>as_shallow_dict()</code>","text":"<p>Serializes the CreateJob into a shallow dictionary of its immediate attributes.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/deploy/databricks.py</code> <pre><code>def as_shallow_dict(self) -&gt; dict:  # pragma: no cover\n    \"\"\"Serializes the CreateJob into a shallow dictionary of its immediate attributes.\"\"\"\n    body = {}\n    if self.access_control_list:\n        body[\"access_control_list\"] = self.access_control_list\n    if self.budget_policy_id is not None:\n        body[\"budget_policy_id\"] = self.budget_policy_id\n    if self.continuous:\n        body[\"continuous\"] = self.continuous\n    if self.deployment:\n        body[\"deployment\"] = self.deployment\n    if self.description is not None:\n        body[\"description\"] = self.description\n    if self.edit_mode is not None:\n        body[\"edit_mode\"] = self.edit_mode\n    if self.email_notifications:\n        body[\"email_notifications\"] = self.email_notifications\n    if self.environments:\n        body[\"environments\"] = self.environments\n    if self.format is not None:\n        body[\"format\"] = self.format\n    if self.git_source:\n        body[\"git_source\"] = self.git_source\n    if self.health:\n        body[\"health\"] = self.health\n    if self.job_clusters:\n        body[\"job_clusters\"] = self.job_clusters\n    if self.max_concurrent_runs is not None:\n        body[\"max_concurrent_runs\"] = self.max_concurrent_runs\n    if self.name is not None:\n        body[\"name\"] = self.name\n    if self.notification_settings:\n        body[\"notification_settings\"] = self.notification_settings\n    if self.parameters:\n        body[\"parameters\"] = self.parameters\n    if self.performance_target is not None:\n        body[\"performance_target\"] = self.performance_target\n    if self.queue:\n        body[\"queue\"] = self.queue\n    if self.run_as:\n        body[\"run_as\"] = self.run_as\n    if self.schedule:\n        body[\"schedule\"] = self.schedule\n    if self.tags:\n        body[\"tags\"] = self.tags\n    if self.tasks:\n        body[\"tasks\"] = self.tasks\n    if self.timeout_seconds is not None:\n        body[\"timeout_seconds\"] = self.timeout_seconds\n    if self.trigger:\n        body[\"trigger\"] = self.trigger\n    if self.webhook_notifications:\n        body[\"webhook_notifications\"] = self.webhook_notifications\n    return body\n</code></pre>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.DatabricksSDKDeploy","title":"<code>DatabricksSDKDeploy</code>","text":"<p>               Bases: <code>DeployInterface</code></p> <p>Deploys an RTDIP Pipeline to Databricks Workflows leveraging the Databricks SDK.</p> <p>Deploying an RTDIP Pipeline to Databricks requires only a few additional pieces of information to ensure the RTDIP Pipeline Job can be run in Databricks. This information includes:</p> <ul> <li>Cluster: This can be defined a the Job or Task level and includes the size of the cluster to be used for the job</li> <li>Task: The cluster to be used to execute the task, as well as any task scheduling information, if required.</li> </ul> <p>All options available in the Databricks Jobs REST API v2.1 can be configured in the Databricks classes that have been defined in <code>rtdip_sdk.pipelines.deploy.models.databricks</code>, enabling full control of the configuration of the Databricks Workflow :</p> <ul> <li><code>CreateJob</code></li> <li><code>Task</code></li> </ul> <p>RTDIP Pipeline Components provide Databricks with all the required Python packages and JARs to execute each component and these will be setup on the Workflow automatically during the Databricks Workflow creation.</p> Example <p>This example assumes that a PipelineJob has already been defined by a variable called <code>pipeline_job</code></p> <pre><code>from rtdip_sdk.pipelines.deploy import DatabricksSDKDeploy, CreateJob, JobCluster, ClusterSpec, Task, NotebookTask, ComputeSpecKind, AutoScale, RuntimeEngine, DataSecurityMode\n\ncluster_list = []\ncluster_list.append(JobCluster(\n    job_cluster_key=\"test_cluster\",\n    new_cluster=ClusterSpec(\n        node_type_id=\"Standard_E4ds_v5\",\n        autoscale=AutoScale(min_workers=1, max_workers=3),\n        spark_version=\"13.2.x-scala2.12\",\n        data_security_mode=DataSecurityMode.SINGLE_USER,\n        runtime_engine=RuntimeEngine.PHOTON\n    )\n))\n\ntask_list = []\ntask_list.append(Task(\n    task_key=\"test_task\",\n    job_cluster_key=\"test_cluster\",\n    notebook_task=NotebookTask(\n        notebook_path=\"/path/to/pipeline/rtdip_pipeline.py\"\n    )\n))\njob = CreateJob(\n    name=\"test_job_rtdip\",\n    job_clusters=cluster_list,\n    tasks=task_list\n)\n\ndatabricks_job = DatabricksSDKDeploy(databricks_job=job, host=\"https://test.databricks.net\", token=\"test_token\")\n\n# Execute the deploy method to create a Workflow in the specified Databricks Environment\ndeploy_result = databricks_job.deploy()\n\n# If the job should be executed immediately, execute the `launch` method\nlaunch_result = databricks_job.launch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>databricks_job</code> <code>DatabricksJob</code> <p>Contains Databricks specific information required for deploying the RTDIP Pipeline Job to Databricks, such as cluster and workflow scheduling information. This can be any field in the Databricks Jobs REST API v2.1</p> required <code>host</code> <code>str</code> <p>Databricks URL</p> required <code>token</code> <code>str</code> <p>Token for authenticating with Databricks such as a Databricks PAT Token or Azure AD Token</p> required <code>workspace_directory</code> <code>str</code> <p>Determines the folder location in the Databricks Workspace. Defaults to /rtdip</p> <code>'/rtdip'</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/deploy/databricks.py</code> <pre><code>class DatabricksSDKDeploy(DeployInterface):\n    \"\"\"\n    Deploys an RTDIP Pipeline to Databricks Workflows leveraging the Databricks [SDK.](https://docs.databricks.com/dev-tools/sdk-python.html)\n\n    Deploying an RTDIP Pipeline to Databricks requires only a few additional pieces of information to ensure the RTDIP Pipeline Job can be run in Databricks. This information includes:\n\n    - **Cluster**: This can be defined a the Job or Task level and includes the size of the cluster to be used for the job\n    - **Task**: The cluster to be used to execute the task, as well as any task scheduling information, if required.\n\n    All options available in the [Databricks Jobs REST API v2.1](https://docs.databricks.com/dev-tools/api/latest/jobs.html) can be configured in the Databricks classes that have been defined in `rtdip_sdk.pipelines.deploy.models.databricks`, enabling full control of the configuration of the Databricks Workflow :\n\n    - `CreateJob`\n    - `Task`\n\n    RTDIP Pipeline Components provide Databricks with all the required Python packages and JARs to execute each component and these will be setup on the Workflow automatically during the Databricks Workflow creation.\n\n    Example:\n        This example assumes that a PipelineJob has already been defined by a variable called `pipeline_job`\n\n        ```python\n        from rtdip_sdk.pipelines.deploy import DatabricksSDKDeploy, CreateJob, JobCluster, ClusterSpec, Task, NotebookTask, ComputeSpecKind, AutoScale, RuntimeEngine, DataSecurityMode\n\n        cluster_list = []\n        cluster_list.append(JobCluster(\n            job_cluster_key=\"test_cluster\",\n            new_cluster=ClusterSpec(\n                node_type_id=\"Standard_E4ds_v5\",\n                autoscale=AutoScale(min_workers=1, max_workers=3),\n                spark_version=\"13.2.x-scala2.12\",\n                data_security_mode=DataSecurityMode.SINGLE_USER,\n                runtime_engine=RuntimeEngine.PHOTON\n            )\n        ))\n\n        task_list = []\n        task_list.append(Task(\n            task_key=\"test_task\",\n            job_cluster_key=\"test_cluster\",\n            notebook_task=NotebookTask(\n                notebook_path=\"/path/to/pipeline/rtdip_pipeline.py\"\n            )\n        ))\n        job = CreateJob(\n            name=\"test_job_rtdip\",\n            job_clusters=cluster_list,\n            tasks=task_list\n        )\n\n        databricks_job = DatabricksSDKDeploy(databricks_job=job, host=\"https://test.databricks.net\", token=\"test_token\")\n\n        # Execute the deploy method to create a Workflow in the specified Databricks Environment\n        deploy_result = databricks_job.deploy()\n\n        # If the job should be executed immediately, execute the `launch` method\n        launch_result = databricks_job.launch()\n        ```\n\n    Parameters:\n        databricks_job (DatabricksJob): Contains Databricks specific information required for deploying the RTDIP Pipeline Job to Databricks, such as cluster and workflow scheduling information. This can be any field in the [Databricks Jobs REST API v2.1](https://docs.databricks.com/dev-tools/api/latest/jobs.html)\n        host (str): Databricks URL\n        token (str): Token for authenticating with Databricks such as a Databricks PAT Token or Azure AD Token\n        workspace_directory (str, optional): Determines the folder location in the Databricks Workspace. Defaults to /rtdip\n    \"\"\"\n\n    def __init__(\n        self,\n        databricks_job: CreateJob,\n        host: str,\n        token: str,\n        workspace_directory: str = \"/rtdip\",\n    ) -&gt; None:\n        if databricks_job.name is None or databricks_job.name == \"\":\n            raise ValueError(\"databricks_job.name cannot be empty\")\n        self.databricks_job = databricks_job\n        self.host = host\n        self.token = token\n        self.workspace_directory = workspace_directory\n\n    def _convert_file_to_binary(self, path) -&gt; BytesIO:  # pragma: no cover\n        with open(path, \"rb\") as f:\n            return BytesIO(f.read())\n\n    def _load_module(self, module_name, path):  # pragma: no cover\n        spec = spec_from_file_location(module_name, path)\n        module = module_from_spec(spec)\n        spec.loader.exec_module(module)\n        sys.modules[module.__name__] = module\n        return module\n\n    def deploy(self) -&gt; Union[bool, ValueError]:\n        \"\"\"\n        Deploys an RTDIP Pipeline Job to Databricks Workflows. The deployment is managed by the Job Name and therefore will overwrite any existing workflow in Databricks with the same name.\n        \"\"\"\n        # Add libraries to Databricks Job\n        workspace_client = WorkspaceClient(\n            config=Config(\n                product=\"RTDIP\",\n                host=self.host,\n                token=self.token,\n                auth_type=\"pat\",\n            )\n        )\n        for task in self.databricks_job.tasks:  # pragma: no cover\n            if task.notebook_task is None and task.spark_python_task is None:\n                return ValueError(\n                    \"A Notebook or Spark Python Task must be populated for each task in the Databricks Job\"\n                )  # NOSONAR\n            if task.notebook_task is not None:\n                module = self._load_module(\n                    task.task_key + \"file_upload\", task.notebook_task.notebook_path\n                )\n                (task_libraries, spark_configuration) = PipelineComponentsGetUtility(\n                    module.__name__\n                ).execute()\n                workspace_client.workspace.mkdirs(path=self.workspace_directory)\n                path = \"{}/{}\".format(\n                    self.workspace_directory,\n                    Path(task.notebook_task.notebook_path).name,\n                )\n                workspace_client.workspace.upload(\n                    path=path,\n                    overwrite=True,\n                    content=self._convert_file_to_binary(\n                        task.notebook_task.notebook_path\n                    ),\n                )\n                task.notebook_task.notebook_path = path\n            else:\n                module = self._load_module(\n                    task.task_key + \"file_upload\", task.spark_python_task.python_file\n                )\n                (task_libraries, spark_configuration) = PipelineComponentsGetUtility(\n                    module\n                ).execute()\n                workspace_client.workspace.mkdirs(path=self.workspace_directory)\n                path = \"{}/{}\".format(\n                    self.workspace_directory,\n                    Path(task.spark_python_task.python_file).name,\n                )\n                workspace_client.workspace.upload(\n                    path=path,\n                    overwrite=True,\n                    content=self._convert_file_to_binary(\n                        task.spark_python_task.python_file\n                    ),\n                )\n                task.spark_python_task.python_file = path\n\n            task.libraries = []\n            for pypi_library in task_libraries.pypi_libraries:\n                task.libraries.append(\n                    Library(\n                        pypi=PythonPyPiLibrary(\n                            package=pypi_library.to_string(), repo=pypi_library.repo\n                        )\n                    )\n                )\n            for maven_library in task_libraries.maven_libraries:\n                if not maven_library.group_id in [\"io.delta\", \"org.apache.spark\"]:\n                    task.libraries.append(\n                        Library(\n                            maven=MavenLibrary(\n                                coordinates=maven_library.to_string(),\n                                repo=maven_library.repo,\n                            )\n                        )\n                    )\n            for wheel_library in task_libraries.pythonwheel_libraries:\n                task.libraries.append(Library(whl=wheel_library))\n\n            try:\n                rtdip_version = version(\"rtdip-sdk\")\n                task.libraries.append(\n                    Library(\n                        pypi=PythonPyPiLibrary(\n                            package=\"rtdip-sdk[pipelines]=={}\".format(rtdip_version)\n                        )\n                    )\n                )\n            except PackageNotFoundError as e:\n                task.libraries.append(\n                    Library(pypi=PythonPyPiLibrary(package=\"rtdip-sdk[pipelines]\"))\n                )\n\n            # Add Spark Configuration to Databricks Job\n            if (\n                task.new_cluster is None\n                and task.job_cluster_key is None\n                and task.compute_key is None\n            ):\n                return ValueError(\n                    \"A Cluster or Compute must be specified for each task in the Databricks Job\"\n                )\n            if task.new_cluster is not None:\n                if spark_configuration is not None:\n                    if task.new_cluster.spark_conf is None:\n                        task.new_cluster.spark_conf = {}\n                    task.new_cluster.spark_conf.update(spark_configuration)\n            elif task.job_cluster_key is not None:\n                for job_cluster in self.databricks_job.job_clusters:\n                    if job_cluster.job_cluster_key == task.job_cluster_key:\n                        if spark_configuration is not None:\n                            if job_cluster.new_cluster.spark_conf is None:\n                                job_cluster.new_cluster.spark_conf = {}\n                            job_cluster.new_cluster.spark_conf.update(\n                                spark_configuration\n                            )\n                        break\n            elif task.compute_key is not None:\n                for compute in self.databricks_job.compute:\n                    if compute.compute_key == task.compute_key:\n                        # TODO : Add spark config for compute. Does not seem to be currently available in the Databricks SDK # NOSONAR\n                        # compute.spark_conf.update(spark_configuration)\n                        break\n\n        # Create Databricks Job\n        job_found = False\n        for existing_job in workspace_client.jobs.list(name=self.databricks_job.name):\n            new_settings = JobSettings()\n            for key, value in self.databricks_job.__dict__.items():\n                if key in new_settings.__dict__:\n                    setattr(new_settings, key, value)\n            workspace_client.jobs.reset(\n                job_id=existing_job.job_id, new_settings=new_settings\n            )\n            job_found = True\n            break\n\n        if job_found == False:\n            workspace_client.jobs.create(**self.databricks_job.__dict__)\n\n        return True\n\n    def launch(self):\n        \"\"\"\n        Launches an RTDIP Pipeline Job in Databricks Workflows. This will perform the equivalent of a `Run Now` in Databricks Workflows\n        \"\"\"\n        workspace_client = WorkspaceClient(\n            config=Config(\n                product=\"RTDIP\",\n                host=self.host,\n                token=self.token,\n                auth_type=\"pat\",\n            )\n        )\n        job_found = False\n        for existing_job in workspace_client.jobs.list(name=self.databricks_job.name):\n            workspace_client.jobs.run_now(job_id=existing_job.job_id)\n            job_found = True\n            break\n\n        if job_found == False:\n            raise ValueError(\"Job not found in Databricks Workflows\")\n\n        return True\n\n    def stop(self):\n        \"\"\"\n        Cancels an RTDIP Pipeline Job in Databricks Workflows. This will perform the equivalent of a `Cancel All Runs` in Databricks Workflows\n        \"\"\"\n        workspace_client = WorkspaceClient(\n            config=Config(\n                product=\"RTDIP\",\n                host=self.host,\n                token=self.token,\n                auth_type=\"pat\",\n            )\n        )\n        job_found = False\n        for existing_job in workspace_client.jobs.list(name=self.databricks_job.name):\n            workspace_client.jobs.cancel_all_runs(job_id=existing_job.job_id)\n            job_found = True\n            break\n\n        if job_found == False:\n            raise ValueError(\"Job not found in Databricks Workflows\")\n\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.DatabricksSDKDeploy.deploy","title":"<code>deploy()</code>","text":"<p>Deploys an RTDIP Pipeline Job to Databricks Workflows. The deployment is managed by the Job Name and therefore will overwrite any existing workflow in Databricks with the same name.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/deploy/databricks.py</code> <pre><code>def deploy(self) -&gt; Union[bool, ValueError]:\n    \"\"\"\n    Deploys an RTDIP Pipeline Job to Databricks Workflows. The deployment is managed by the Job Name and therefore will overwrite any existing workflow in Databricks with the same name.\n    \"\"\"\n    # Add libraries to Databricks Job\n    workspace_client = WorkspaceClient(\n        config=Config(\n            product=\"RTDIP\",\n            host=self.host,\n            token=self.token,\n            auth_type=\"pat\",\n        )\n    )\n    for task in self.databricks_job.tasks:  # pragma: no cover\n        if task.notebook_task is None and task.spark_python_task is None:\n            return ValueError(\n                \"A Notebook or Spark Python Task must be populated for each task in the Databricks Job\"\n            )  # NOSONAR\n        if task.notebook_task is not None:\n            module = self._load_module(\n                task.task_key + \"file_upload\", task.notebook_task.notebook_path\n            )\n            (task_libraries, spark_configuration) = PipelineComponentsGetUtility(\n                module.__name__\n            ).execute()\n            workspace_client.workspace.mkdirs(path=self.workspace_directory)\n            path = \"{}/{}\".format(\n                self.workspace_directory,\n                Path(task.notebook_task.notebook_path).name,\n            )\n            workspace_client.workspace.upload(\n                path=path,\n                overwrite=True,\n                content=self._convert_file_to_binary(\n                    task.notebook_task.notebook_path\n                ),\n            )\n            task.notebook_task.notebook_path = path\n        else:\n            module = self._load_module(\n                task.task_key + \"file_upload\", task.spark_python_task.python_file\n            )\n            (task_libraries, spark_configuration) = PipelineComponentsGetUtility(\n                module\n            ).execute()\n            workspace_client.workspace.mkdirs(path=self.workspace_directory)\n            path = \"{}/{}\".format(\n                self.workspace_directory,\n                Path(task.spark_python_task.python_file).name,\n            )\n            workspace_client.workspace.upload(\n                path=path,\n                overwrite=True,\n                content=self._convert_file_to_binary(\n                    task.spark_python_task.python_file\n                ),\n            )\n            task.spark_python_task.python_file = path\n\n        task.libraries = []\n        for pypi_library in task_libraries.pypi_libraries:\n            task.libraries.append(\n                Library(\n                    pypi=PythonPyPiLibrary(\n                        package=pypi_library.to_string(), repo=pypi_library.repo\n                    )\n                )\n            )\n        for maven_library in task_libraries.maven_libraries:\n            if not maven_library.group_id in [\"io.delta\", \"org.apache.spark\"]:\n                task.libraries.append(\n                    Library(\n                        maven=MavenLibrary(\n                            coordinates=maven_library.to_string(),\n                            repo=maven_library.repo,\n                        )\n                    )\n                )\n        for wheel_library in task_libraries.pythonwheel_libraries:\n            task.libraries.append(Library(whl=wheel_library))\n\n        try:\n            rtdip_version = version(\"rtdip-sdk\")\n            task.libraries.append(\n                Library(\n                    pypi=PythonPyPiLibrary(\n                        package=\"rtdip-sdk[pipelines]=={}\".format(rtdip_version)\n                    )\n                )\n            )\n        except PackageNotFoundError as e:\n            task.libraries.append(\n                Library(pypi=PythonPyPiLibrary(package=\"rtdip-sdk[pipelines]\"))\n            )\n\n        # Add Spark Configuration to Databricks Job\n        if (\n            task.new_cluster is None\n            and task.job_cluster_key is None\n            and task.compute_key is None\n        ):\n            return ValueError(\n                \"A Cluster or Compute must be specified for each task in the Databricks Job\"\n            )\n        if task.new_cluster is not None:\n            if spark_configuration is not None:\n                if task.new_cluster.spark_conf is None:\n                    task.new_cluster.spark_conf = {}\n                task.new_cluster.spark_conf.update(spark_configuration)\n        elif task.job_cluster_key is not None:\n            for job_cluster in self.databricks_job.job_clusters:\n                if job_cluster.job_cluster_key == task.job_cluster_key:\n                    if spark_configuration is not None:\n                        if job_cluster.new_cluster.spark_conf is None:\n                            job_cluster.new_cluster.spark_conf = {}\n                        job_cluster.new_cluster.spark_conf.update(\n                            spark_configuration\n                        )\n                    break\n        elif task.compute_key is not None:\n            for compute in self.databricks_job.compute:\n                if compute.compute_key == task.compute_key:\n                    # TODO : Add spark config for compute. Does not seem to be currently available in the Databricks SDK # NOSONAR\n                    # compute.spark_conf.update(spark_configuration)\n                    break\n\n    # Create Databricks Job\n    job_found = False\n    for existing_job in workspace_client.jobs.list(name=self.databricks_job.name):\n        new_settings = JobSettings()\n        for key, value in self.databricks_job.__dict__.items():\n            if key in new_settings.__dict__:\n                setattr(new_settings, key, value)\n        workspace_client.jobs.reset(\n            job_id=existing_job.job_id, new_settings=new_settings\n        )\n        job_found = True\n        break\n\n    if job_found == False:\n        workspace_client.jobs.create(**self.databricks_job.__dict__)\n\n    return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.DatabricksSDKDeploy.launch","title":"<code>launch()</code>","text":"<p>Launches an RTDIP Pipeline Job in Databricks Workflows. This will perform the equivalent of a <code>Run Now</code> in Databricks Workflows</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/deploy/databricks.py</code> <pre><code>def launch(self):\n    \"\"\"\n    Launches an RTDIP Pipeline Job in Databricks Workflows. This will perform the equivalent of a `Run Now` in Databricks Workflows\n    \"\"\"\n    workspace_client = WorkspaceClient(\n        config=Config(\n            product=\"RTDIP\",\n            host=self.host,\n            token=self.token,\n            auth_type=\"pat\",\n        )\n    )\n    job_found = False\n    for existing_job in workspace_client.jobs.list(name=self.databricks_job.name):\n        workspace_client.jobs.run_now(job_id=existing_job.job_id)\n        job_found = True\n        break\n\n    if job_found == False:\n        raise ValueError(\"Job not found in Databricks Workflows\")\n\n    return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/deploy/databricks/#src.sdk.python.rtdip_sdk.pipelines.deploy.databricks.DatabricksSDKDeploy.stop","title":"<code>stop()</code>","text":"<p>Cancels an RTDIP Pipeline Job in Databricks Workflows. This will perform the equivalent of a <code>Cancel All Runs</code> in Databricks Workflows</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/deploy/databricks.py</code> <pre><code>def stop(self):\n    \"\"\"\n    Cancels an RTDIP Pipeline Job in Databricks Workflows. This will perform the equivalent of a `Cancel All Runs` in Databricks Workflows\n    \"\"\"\n    workspace_client = WorkspaceClient(\n        config=Config(\n            product=\"RTDIP\",\n            host=self.host,\n            token=self.token,\n            auth_type=\"pat\",\n        )\n    )\n    job_found = False\n    for existing_job in workspace_client.jobs.list(name=self.databricks_job.name):\n        workspace_client.jobs.cancel_all_runs(job_id=existing_job.job_id)\n        job_found = True\n        break\n\n    if job_found == False:\n        raise ValueError(\"Job not found in Databricks Workflows\")\n\n    return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/blockchain/evm/","title":"Write to EVM Blockchain","text":""},{"location":"sdk/code-reference/pipelines/destinations/blockchain/evm/#src.sdk.python.rtdip_sdk.pipelines.destinations.blockchain.evm.EVMContractDestination","title":"<code>EVMContractDestination</code>","text":"<p>               Bases: <code>DestinationInterface</code></p> <p>The EVM Contract Destination is used to write to a smart contract blockchain.</p>"},{"location":"sdk/code-reference/pipelines/destinations/blockchain/evm/#src.sdk.python.rtdip_sdk.pipelines.destinations.blockchain.evm.EVMContractDestination--examples","title":"Examples","text":"<pre><code>from rtdip_sdk.pipelines.destinations import EVMContractDestination\n\nevm_contract_destination = EVMContractDestination(\n    url=\"https://polygon-mumbai.g.alchemy.com/v2/\u27e8API_KEY\u27e9\",\n    account=\"{ACCOUNT-ADDRESS}\",\n    private_key=\"{PRIVATE-KEY}\",\n    abi=\"{SMART-CONTRACT'S-ABI}\",\n    contract=\"{SMART-CONTRACT-ADDRESS}\",\n    function_name=\"{SMART-CONTRACT-FUNCTION}\",\n    function_params=({PARAMETER_1}, {PARAMETER_2}, {PARAMETER_3}),\n    transaction={'gas': {GAS}, 'gasPrice': {GAS-PRICE}},\n)\n\nevm_contract_destination.write_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Blockchain network URL e.g. 'https://polygon-mumbai.g.alchemy.com/v2/\u27e8API_KEY\u27e9'</p> required <code>account</code> <code>str</code> <p>Address of the sender that will be signing the transaction.</p> required <code>private_key</code> <code>str</code> <p>Private key for your blockchain account.</p> required <code>abi</code> <code>json str</code> <p>Smart contract's ABI.</p> required <code>contract</code> <code>str</code> <p>Address of the smart contract.</p> <code>None</code> <code>function_name</code> <code>str</code> <p>Smart contract method to call on.</p> <code>None</code> <code>function_params</code> <code>tuple</code> <p>Parameters of given function.</p> <code>None</code> <code>transaction</code> <code>dict</code> <p>A dictionary containing a set of instructions to interact with a smart contract deployed on the blockchain (See common parameters in Attributes table below).</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>data</code> <code>hexadecimal str</code> <p>Additional information store in the transaction.</p> <code>from</code> <code>hexadecimal str</code> <p>Address of sender for a transaction.</p> <code>gas</code> <code>int</code> <p>Amount of gas units to perform a transaction.</p> <code>gasPrice</code> <code>int Wei</code> <p>Price to pay for each unit of gas. Integers are specified in Wei, web3's to_wei function can be used to specify the amount in a different currency.</p> <code>nonce</code> <code>int</code> <p>The number of transactions sent from a given address.</p> <code>to</code> <code>hexadecimal str</code> <p>Address of recipient for a transaction.</p> <code>value</code> <code>int Wei</code> <p>Value being transferred in a transaction. Integers are specified in Wei, web3's to_wei function can be used to specify the amount in a different currency.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/blockchain/evm.py</code> <pre><code>class EVMContractDestination(DestinationInterface):\n    \"\"\"\n    The EVM Contract Destination is used to write to a smart contract blockchain.\n\n    Examples\n    --------\n    ```python\n    from rtdip_sdk.pipelines.destinations import EVMContractDestination\n\n    evm_contract_destination = EVMContractDestination(\n        url=\"https://polygon-mumbai.g.alchemy.com/v2/\u27e8API_KEY\u27e9\",\n        account=\"{ACCOUNT-ADDRESS}\",\n        private_key=\"{PRIVATE-KEY}\",\n        abi=\"{SMART-CONTRACT'S-ABI}\",\n        contract=\"{SMART-CONTRACT-ADDRESS}\",\n        function_name=\"{SMART-CONTRACT-FUNCTION}\",\n        function_params=({PARAMETER_1}, {PARAMETER_2}, {PARAMETER_3}),\n        transaction={'gas': {GAS}, 'gasPrice': {GAS-PRICE}},\n    )\n\n    evm_contract_destination.write_batch()\n    ```\n\n    Parameters:\n        url (str): Blockchain network URL e.g. 'https://polygon-mumbai.g.alchemy.com/v2/\u27e8API_KEY\u27e9'\n        account (str): Address of the sender that will be signing the transaction.\n        private_key (str): Private key for your blockchain account.\n        abi (json str): Smart contract's ABI.\n        contract (str): Address of the smart contract.\n        function_name (str): Smart contract method to call on.\n        function_params (tuple): Parameters of given function.\n        transaction (dict): A dictionary containing a set of instructions to interact with a smart contract deployed on the blockchain (See common parameters in Attributes table below).\n\n    Attributes:\n        data (hexadecimal str): Additional information store in the transaction.\n        from (hexadecimal str): Address of sender for a transaction.\n        gas (int): Amount of gas units to perform a transaction.\n        gasPrice (int Wei): Price to pay for each unit of gas. Integers are specified in Wei, web3's to_wei function can be used to specify the amount in a different currency.\n        nonce (int): The number of transactions sent from a given address.\n        to (hexadecimal str): Address of recipient for a transaction.\n        value (int Wei): Value being transferred in a transaction. Integers are specified in Wei, web3's to_wei function can be used to specify the amount in a different currency.\n    \"\"\"\n\n    url: str\n    account: str\n    private_key: str\n    abi: str\n    contract: str\n    function_name: str\n    function_params: tuple\n    transaction: dict\n\n    def __init__(\n        self,\n        url: str,\n        account: str,\n        private_key: str,\n        abi: str,\n        contract: str = None,\n        function_name: str = None,\n        function_params: tuple = None,\n        transaction: dict = None,\n    ) -&gt; None:\n        self.url = url\n        self.account = account\n        self.private_key = private_key\n        self.abi = json.loads(abi)\n        self.contract = contract\n        self.function_name = function_name\n        self.function_params = function_params\n        self.transaction = transaction\n        self.web3 = Web3(Web3.HTTPProvider(self.url))\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self) -&gt; bool:\n        return True\n\n    def post_write_validation(self) -&gt; bool:\n        return True\n\n    def _process_transaction(self):\n        if \"nonce\" not in self.transaction.keys():\n            nonce = self.web3.eth.get_transaction_count(self.account)\n            self.transaction[\"nonce\"] = nonce\n        if \"from\" not in self.transaction.keys():\n            self.transaction[\"from\"] = self.account\n\n    def write_batch(self) -&gt; str:\n        \"\"\"\n        Writes to a smart contract deployed in a blockchain and returns the transaction hash.\n\n        Example:\n        ```\n        from web3 import Web3\n\n        web3 = Web3(Web3.HTTPProvider(\"https://polygon-mumbai.g.alchemy.com/v2/&lt;API_KEY&gt;\"))\n\n        x = EVMContractDestination(\n                            url=\"https://polygon-mumbai.g.alchemy.com/v2/&lt;API_KEY&gt;\",\n                            account='&lt;ACCOUNT&gt;',\n                            private_key='&lt;PRIVATE_KEY&gt;',\n                            contract='&lt;CONTRACT&gt;',\n                            function_name='transferFrom',\n                            function_params=('&lt;FROM_ACCOUNT&gt;', '&lt;TO_ACCOUNT&gt;', 0),\n                            abi = 'ABI',\n                            transaction={\n                                'gas': 100000,\n                                'gasPrice': 1000000000 # or web3.to_wei('1', 'gwei')\n                                },\n                            )\n\n        print(x.write_batch())\n        ```\n        \"\"\"\n        contract = self.web3.eth.contract(address=self.contract, abi=self.abi)\n\n        self._process_transaction()\n        tx = contract.functions[self.function_name](\n            *self.function_params\n        ).build_transaction(self.transaction)\n\n        signed_tx = self.web3.eth.account.sign_transaction(tx, self.private_key)\n        tx_hash = self.web3.eth.send_raw_transaction(signed_tx.rawTransaction)\n        self.web3.eth.wait_for_transaction_receipt(tx_hash)\n\n        return str(self.web3.to_hex(tx_hash))\n\n    def write_stream(self):\n        \"\"\"\n        Raises:\n            NotImplementedError: Write stream is not supported.\n        \"\"\"\n        raise NotImplementedError(\"EVMContractDestination only supports batch writes.\")\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/blockchain/evm/#src.sdk.python.rtdip_sdk.pipelines.destinations.blockchain.evm.EVMContractDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Writes to a smart contract deployed in a blockchain and returns the transaction hash.</p> <p>Example: <pre><code>from web3 import Web3\n\nweb3 = Web3(Web3.HTTPProvider(\"https://polygon-mumbai.g.alchemy.com/v2/&lt;API_KEY&gt;\"))\n\nx = EVMContractDestination(\n                    url=\"https://polygon-mumbai.g.alchemy.com/v2/&lt;API_KEY&gt;\",\n                    account='&lt;ACCOUNT&gt;',\n                    private_key='&lt;PRIVATE_KEY&gt;',\n                    contract='&lt;CONTRACT&gt;',\n                    function_name='transferFrom',\n                    function_params=('&lt;FROM_ACCOUNT&gt;', '&lt;TO_ACCOUNT&gt;', 0),\n                    abi = 'ABI',\n                    transaction={\n                        'gas': 100000,\n                        'gasPrice': 1000000000 # or web3.to_wei('1', 'gwei')\n                        },\n                    )\n\nprint(x.write_batch())\n</code></pre></p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/blockchain/evm.py</code> <pre><code>def write_batch(self) -&gt; str:\n    \"\"\"\n    Writes to a smart contract deployed in a blockchain and returns the transaction hash.\n\n    Example:\n    ```\n    from web3 import Web3\n\n    web3 = Web3(Web3.HTTPProvider(\"https://polygon-mumbai.g.alchemy.com/v2/&lt;API_KEY&gt;\"))\n\n    x = EVMContractDestination(\n                        url=\"https://polygon-mumbai.g.alchemy.com/v2/&lt;API_KEY&gt;\",\n                        account='&lt;ACCOUNT&gt;',\n                        private_key='&lt;PRIVATE_KEY&gt;',\n                        contract='&lt;CONTRACT&gt;',\n                        function_name='transferFrom',\n                        function_params=('&lt;FROM_ACCOUNT&gt;', '&lt;TO_ACCOUNT&gt;', 0),\n                        abi = 'ABI',\n                        transaction={\n                            'gas': 100000,\n                            'gasPrice': 1000000000 # or web3.to_wei('1', 'gwei')\n                            },\n                        )\n\n    print(x.write_batch())\n    ```\n    \"\"\"\n    contract = self.web3.eth.contract(address=self.contract, abi=self.abi)\n\n    self._process_transaction()\n    tx = contract.functions[self.function_name](\n        *self.function_params\n    ).build_transaction(self.transaction)\n\n    signed_tx = self.web3.eth.account.sign_transaction(tx, self.private_key)\n    tx_hash = self.web3.eth.send_raw_transaction(signed_tx.rawTransaction)\n    self.web3.eth.wait_for_transaction_receipt(tx_hash)\n\n    return str(self.web3.to_hex(tx_hash))\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/blockchain/evm/#src.sdk.python.rtdip_sdk.pipelines.destinations.blockchain.evm.EVMContractDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Write stream is not supported.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/blockchain/evm.py</code> <pre><code>def write_stream(self):\n    \"\"\"\n    Raises:\n        NotImplementedError: Write stream is not supported.\n    \"\"\"\n    raise NotImplementedError(\"EVMContractDestination only supports batch writes.\")\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/python/delta/","title":"Write to Delta","text":""},{"location":"sdk/code-reference/pipelines/destinations/python/delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.python.delta.PythonDeltaDestination","title":"<code>PythonDeltaDestination</code>","text":"<p>               Bases: <code>DestinationInterface</code></p> <p>The Python Delta Destination is used to write data to a Delta table from a Polars LazyFrame.</p>"},{"location":"sdk/code-reference/pipelines/destinations/python/delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.python.delta.PythonDeltaDestination--example","title":"Example","text":"AzureAWS <pre><code>from rtdip_sdk.pipelines.destinations import PythonDeltaDestination\n\npath = \"abfss://{FILE-SYSTEM}@{ACCOUNT-NAME}.dfs.core.windows.net/{PATH}/{FILE-NAME}\n\npython_delta_destination = PythonDeltaDestination(\n    data=LazyFrame\n    path=path,\n    storage_options={\n        \"azure_storage_account_name\": \"{AZURE-STORAGE-ACCOUNT-NAME}\",\n        \"azure_storage_account_key\": \"{AZURE-STORAGE-ACCOUNT-KEY}\"\n    },\n    mode=:error\",\n    overwrite_schema=False,\n    delta_write_options=None\n)\n\npython_delta_destination.read_batch()\n</code></pre> <pre><code>from rtdip_sdk.pipelines.destinations import PythonDeltaDestination\n\npath = \"https://s3.{REGION-CODE}.amazonaws.com/{BUCKET-NAME}/{KEY-NAME}\"\n\npython_delta_destination = PythonDeltaDestination(\n    data=LazyFrame\n    path=path,\n    options={\n        \"aws_access_key_id\": \"{AWS-ACCESS-KEY-ID}\",\n        \"aws_secret_access_key\": \"{AWS-SECRET-ACCESS-KEY}\"\n    },\n    mode=:error\",\n    overwrite_schema=False,\n    delta_write_options=None\n)\n\npython_delta_destination.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>LazyFrame</code> <p>Polars LazyFrame to be written to Delta</p> required <code>path</code> <code>str</code> <p>Path to Delta table to be written to; either local or remote. Locally if the Table does't exist one will be created, but to write to AWS or Azure, you must have an existing Delta Table</p> required <code>options</code> <code>Optional dict</code> <p>Used if writing to a remote location. For AWS use format {\"aws_access_key_id\": \"&lt;&gt;\", \"aws_secret_access_key\": \"&lt;&gt;\"}. For Azure use format {\"azure_storage_account_name\": \"storageaccountname\", \"azure_storage_access_key\": \"&lt;&gt;\"}</p> <code>None</code> <code>mode</code> <code>Literal['error', 'append', 'overwrite', 'ignore']</code> <p>Defaults to error if table exists, 'ignore' won't write anything if table exists</p> <code>'error'</code> <code>overwrite_schema</code> <code>bool</code> <p>If True will allow for the table schema to be overwritten</p> <code>False</code> <code>delta_write_options</code> <code>dict</code> <p>Options when writing to a Delta table. See here for all options</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/python/delta.py</code> <pre><code>class PythonDeltaDestination(DestinationInterface):\n    \"\"\"\n    The Python Delta Destination is used to write data to a Delta table from a Polars LazyFrame.\n\n     Example\n    --------\n    === \"Azure\"\n\n        ```python\n        from rtdip_sdk.pipelines.destinations import PythonDeltaDestination\n\n        path = \"abfss://{FILE-SYSTEM}@{ACCOUNT-NAME}.dfs.core.windows.net/{PATH}/{FILE-NAME}\n\n        python_delta_destination = PythonDeltaDestination(\n            data=LazyFrame\n            path=path,\n            storage_options={\n                \"azure_storage_account_name\": \"{AZURE-STORAGE-ACCOUNT-NAME}\",\n                \"azure_storage_account_key\": \"{AZURE-STORAGE-ACCOUNT-KEY}\"\n            },\n            mode=:error\",\n            overwrite_schema=False,\n            delta_write_options=None\n        )\n\n        python_delta_destination.read_batch()\n\n        ```\n    === \"AWS\"\n\n        ```python\n        from rtdip_sdk.pipelines.destinations import PythonDeltaDestination\n\n        path = \"https://s3.{REGION-CODE}.amazonaws.com/{BUCKET-NAME}/{KEY-NAME}\"\n\n        python_delta_destination = PythonDeltaDestination(\n            data=LazyFrame\n            path=path,\n            options={\n                \"aws_access_key_id\": \"{AWS-ACCESS-KEY-ID}\",\n                \"aws_secret_access_key\": \"{AWS-SECRET-ACCESS-KEY}\"\n            },\n            mode=:error\",\n            overwrite_schema=False,\n            delta_write_options=None\n        )\n\n        python_delta_destination.read_batch()\n        ```\n\n    Parameters:\n        data (LazyFrame): Polars LazyFrame to be written to Delta\n        path (str): Path to Delta table to be written to; either local or [remote](https://delta-io.github.io/delta-rs/python/usage.html#loading-a-delta-table){ target=\"_blank\" }. **Locally** if the Table does't exist one will be created, but to write to AWS or Azure, you must have an existing Delta Table\n        options (Optional dict): Used if writing to a remote location. For AWS use format {\"aws_access_key_id\": \"&lt;&gt;\", \"aws_secret_access_key\": \"&lt;&gt;\"}. For Azure use format {\"azure_storage_account_name\": \"storageaccountname\", \"azure_storage_access_key\": \"&lt;&gt;\"}\n        mode (Literal['error', 'append', 'overwrite', 'ignore']): Defaults to error if table exists, 'ignore' won't write anything if table exists\n        overwrite_schema (bool): If True will allow for the table schema to be overwritten\n        delta_write_options (dict): Options when writing to a Delta table. See [here](https://delta-io.github.io/delta-rs/python/api_reference.html#writing-deltatables){ target=\"_blank\" } for all options\n    \"\"\"\n\n    data: LazyFrame\n    path: str\n    options: dict\n    mode: Literal[\"error\", \"append\", \"overwrite\", \"ignore\"]\n    overwrite_schema: bool\n    delta_write_options: dict\n\n    def __init__(\n        self,\n        data: LazyFrame,\n        path: str,\n        options: dict = None,\n        mode: Literal[\"error\", \"append\", \"overwrite\", \"ignore\"] = \"error\",\n        overwrite_schema: bool = False,\n        delta_write_options: dict = None,\n    ) -&gt; None:\n        self.data = data\n        self.path = path\n        self.options = options\n        self.mode = mode\n        self.overwrite_schema = overwrite_schema\n        self.delta_write_options = delta_write_options\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYTHON\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def write_batch(self):\n        \"\"\"\n        Writes batch data to Delta without using Spark.\n        \"\"\"\n        if isinstance(self.data, pl.LazyFrame):\n            df = self.data.collect()\n            df.write_delta(\n                self.path,\n                mode=self.mode,\n                overwrite_schema=self.overwrite_schema,\n                storage_options=self.options,\n                delta_write_options=self.delta_write_options,\n            )\n        else:\n            raise ValueError(\n                \"Data must be a Polars LazyFrame. See https://pola-rs.github.io/polars/py-polars/html/reference/lazyframe/index.html\"\n            )\n\n    def write_stream(self):\n        \"\"\"\n        Raises:\n            NotImplementedError: Writing to a Delta table using Python is only possible for batch writes. To perform a streaming read, use the write_stream method of the SparkDeltaDestination component.\n        \"\"\"\n        raise NotImplementedError(\n            \"Writing to a Delta table using Python is only possible for batch writes. To perform a streaming read, use the write_stream method of the SparkDeltaDestination component\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/python/delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.python.delta.PythonDeltaDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYTHON</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/python/delta.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYTHON\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/python/delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.python.delta.PythonDeltaDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Writes batch data to Delta without using Spark.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/python/delta.py</code> <pre><code>def write_batch(self):\n    \"\"\"\n    Writes batch data to Delta without using Spark.\n    \"\"\"\n    if isinstance(self.data, pl.LazyFrame):\n        df = self.data.collect()\n        df.write_delta(\n            self.path,\n            mode=self.mode,\n            overwrite_schema=self.overwrite_schema,\n            storage_options=self.options,\n            delta_write_options=self.delta_write_options,\n        )\n    else:\n        raise ValueError(\n            \"Data must be a Polars LazyFrame. See https://pola-rs.github.io/polars/py-polars/html/reference/lazyframe/index.html\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/python/delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.python.delta.PythonDeltaDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Writing to a Delta table using Python is only possible for batch writes. To perform a streaming read, use the write_stream method of the SparkDeltaDestination component.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/python/delta.py</code> <pre><code>def write_stream(self):\n    \"\"\"\n    Raises:\n        NotImplementedError: Writing to a Delta table using Python is only possible for batch writes. To perform a streaming read, use the write_stream method of the SparkDeltaDestination component.\n    \"\"\"\n    raise NotImplementedError(\n        \"Writing to a Delta table using Python is only possible for batch writes. To perform a streaming read, use the write_stream method of the SparkDeltaDestination component\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/delta/","title":"Write to Delta","text":""},{"location":"sdk/code-reference/pipelines/destinations/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.delta.SparkDeltaDestination","title":"<code>SparkDeltaDestination</code>","text":"<p>               Bases: <code>DestinationInterface</code></p> <p>The Spark Delta Destination is used to write data to a Delta table.</p>"},{"location":"sdk/code-reference/pipelines/destinations/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.delta.SparkDeltaDestination--examples","title":"Examples","text":"<p><pre><code>#Delta Destination for Streaming Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkDeltaDestination\n\ndelta_destination = SparkDeltaDestination(\n    data=df,\n    options={\n        \"checkpointLocation\": \"/{CHECKPOINT-LOCATION}/\"\n    },\n    destination=\"DELTA-TABLE-PATH\",\n    mode=\"append\",\n    trigger=\"10 seconds\",\n    query_name=\"DeltaDestination\",\n    query_wait_interval=None\n)\n\ndelta_destination.write_stream()\n</code></pre> <pre><code>#Delta Destination for Batch Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkDeltaDestination\n\ndelta_destination = SparkDeltaDestination(\n    data=df,\n    options={\n        \"overwriteSchema\": True\n    },\n    destination=\"DELTA-TABLE-PATH\",\n    mode=\"append\",\n    trigger=\"10 seconds\",\n    query_name=\"DeltaDestination\",\n    query_wait_interval=None\n)\n\ndelta_destination.write_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to be written to Delta</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for a Delta Table write operation (See Attributes table below). Further information on the options is available for batch and streaming.</p> required <code>destination</code> <code>str</code> <p>Either the name of the Hive Metastore or Unity Catalog Delta Table or the path to the Delta table</p> required <code>mode</code> <code>optional str</code> <p>Method of writing to Delta Table - append/overwrite (batch), append/update/complete (stream). Default is append</p> <code>'append'</code> <code>trigger</code> <code>optional str</code> <p>Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds</p> <code>'10 seconds'</code> <code>query_name</code> <code>optional str</code> <p>Unique name for the query in associated SparkSession. (stream) Default is DeltaDestination</p> <code>'DeltaDestination'</code> <code>query_wait_interval</code> <code>optional int</code> <p>If set, waits for the streaming query to complete before returning. (stream) Default is None</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>checkpointLocation</code> <code>str</code> <p>Path to checkpoint files. (Streaming)</p> <code>txnAppId</code> <code>str</code> <p>A unique string that you can pass on each DataFrame write. (Batch &amp; Streaming)</p> <code>txnVersion</code> <code>str</code> <p>A monotonically increasing number that acts as transaction version. (Batch &amp; Streaming)</p> <code>maxRecordsPerFile</code> <code>int str</code> <p>Specify the maximum number of records to write to a single file for a Delta Lake table. (Batch)</p> <code>replaceWhere</code> <code>str</code> <p>Condition(s) for overwriting. (Batch)</p> <code>partitionOverwriteMode</code> <code>str</code> <p>When set to dynamic, overwrites all existing data in each logical partition for which the write will commit new data. Default is static. (Batch)</p> <code>overwriteSchema</code> <code>bool str</code> <p>If True, overwrites the schema as well as the table data. (Batch)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta.py</code> <pre><code>class SparkDeltaDestination(DestinationInterface):\n    \"\"\"\n    The Spark Delta Destination is used to write data to a Delta table.\n\n    Examples\n    --------\n    ```python\n    #Delta Destination for Streaming Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkDeltaDestination\n\n    delta_destination = SparkDeltaDestination(\n        data=df,\n        options={\n            \"checkpointLocation\": \"/{CHECKPOINT-LOCATION}/\"\n        },\n        destination=\"DELTA-TABLE-PATH\",\n        mode=\"append\",\n        trigger=\"10 seconds\",\n        query_name=\"DeltaDestination\",\n        query_wait_interval=None\n    )\n\n    delta_destination.write_stream()\n    ```\n    ```python\n    #Delta Destination for Batch Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkDeltaDestination\n\n    delta_destination = SparkDeltaDestination(\n        data=df,\n        options={\n            \"overwriteSchema\": True\n        },\n        destination=\"DELTA-TABLE-PATH\",\n        mode=\"append\",\n        trigger=\"10 seconds\",\n        query_name=\"DeltaDestination\",\n        query_wait_interval=None\n    )\n\n    delta_destination.write_batch()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe to be written to Delta\n        options (dict): Options that can be specified for a Delta Table write operation (See Attributes table below). Further information on the options is available for [batch](https://docs.delta.io/latest/delta-batch.html#write-to-a-table){ target=\"_blank\" } and [streaming](https://docs.delta.io/latest/delta-streaming.html#delta-table-as-a-sink){ target=\"_blank\" }.\n        destination (str): Either the name of the Hive Metastore or Unity Catalog Delta Table **or** the path to the Delta table\n        mode (optional str): Method of writing to Delta Table - append/overwrite (batch), append/update/complete (stream). Default is append\n        trigger (optional str): Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds\n        query_name (optional str): Unique name for the query in associated SparkSession. (stream) Default is DeltaDestination\n        query_wait_interval (optional int): If set, waits for the streaming query to complete before returning. (stream) Default is None\n\n    Attributes:\n        checkpointLocation (str): Path to checkpoint files. (Streaming)\n        txnAppId (str): A unique string that you can pass on each DataFrame write. (Batch &amp; Streaming)\n        txnVersion (str): A monotonically increasing number that acts as transaction version. (Batch &amp; Streaming)\n        maxRecordsPerFile (int str): Specify the maximum number of records to write to a single file for a Delta Lake table. (Batch)\n        replaceWhere (str): Condition(s) for overwriting. (Batch)\n        partitionOverwriteMode (str): When set to dynamic, overwrites all existing data in each logical partition for which the write will commit new data. Default is static. (Batch)\n        overwriteSchema (bool str): If True, overwrites the schema as well as the table data. (Batch)\n    \"\"\"\n\n    data: DataFrame\n    options: dict\n    destination: str\n    mode: str\n    trigger: str\n    query_name: str\n    query_wait_interval: int\n\n    def __init__(\n        self,\n        data: DataFrame,\n        options: dict,\n        destination: str,\n        mode: str = \"append\",\n        trigger: str = \"10 seconds\",\n        query_name: str = \"DeltaDestination\",\n        query_wait_interval: int = None,\n    ) -&gt; None:\n        self.data = data\n        self.options = options\n        self.destination = destination\n        self.mode = mode\n        self.trigger = trigger\n        self.query_name = query_name\n        self.query_wait_interval = query_wait_interval\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_core\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {\n            \"spark.sql.extensions\": \"io.delta.sql.DeltaSparkSessionExtension\",\n            \"spark.sql.catalog.spark_catalog\": \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n        }\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def write_batch(self):\n        \"\"\"\n        Writes batch data to Delta. Most of the options provided by the Apache Spark DataFrame write API are supported for performing batch writes on tables.\n        \"\"\"\n        try:\n            if \"/\" in self.destination:\n                return (\n                    self.data.write.format(\"delta\")\n                    .mode(self.mode)\n                    .options(**self.options)\n                    .save(self.destination)\n                )\n            else:\n                return (\n                    self.data.write.format(\"delta\")\n                    .mode(self.mode)\n                    .options(**self.options)\n                    .saveAsTable(self.destination)\n                )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self):\n        \"\"\"\n        Writes streaming data to Delta. Exactly-once processing is guaranteed\n        \"\"\"\n        TRIGGER_OPTION = (\n            {\"availableNow\": True}\n            if self.trigger == \"availableNow\"\n            else {\"processingTime\": self.trigger}\n        )\n        try:\n            if \"/\" in self.destination:\n                query = (\n                    self.data.writeStream.trigger(**TRIGGER_OPTION)\n                    .format(\"delta\")\n                    .queryName(self.query_name)\n                    .outputMode(self.mode)\n                    .options(**self.options)\n                    .start(self.destination)\n                )\n            else:\n                query = (\n                    self.data.writeStream.trigger(**TRIGGER_OPTION)\n                    .format(\"delta\")\n                    .queryName(self.query_name)\n                    .outputMode(self.mode)\n                    .options(**self.options)\n                    .toTable(self.destination)\n                )\n\n            if self.query_wait_interval:\n                while query.isActive:\n                    if query.lastProgress:\n                        logging.info(query.lastProgress)\n                    time.sleep(self.query_wait_interval)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.delta.SparkDeltaDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.delta.SparkDeltaDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Writes batch data to Delta. Most of the options provided by the Apache Spark DataFrame write API are supported for performing batch writes on tables.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta.py</code> <pre><code>def write_batch(self):\n    \"\"\"\n    Writes batch data to Delta. Most of the options provided by the Apache Spark DataFrame write API are supported for performing batch writes on tables.\n    \"\"\"\n    try:\n        if \"/\" in self.destination:\n            return (\n                self.data.write.format(\"delta\")\n                .mode(self.mode)\n                .options(**self.options)\n                .save(self.destination)\n            )\n        else:\n            return (\n                self.data.write.format(\"delta\")\n                .mode(self.mode)\n                .options(**self.options)\n                .saveAsTable(self.destination)\n            )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.delta.SparkDeltaDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Writes streaming data to Delta. Exactly-once processing is guaranteed</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta.py</code> <pre><code>def write_stream(self):\n    \"\"\"\n    Writes streaming data to Delta. Exactly-once processing is guaranteed\n    \"\"\"\n    TRIGGER_OPTION = (\n        {\"availableNow\": True}\n        if self.trigger == \"availableNow\"\n        else {\"processingTime\": self.trigger}\n    )\n    try:\n        if \"/\" in self.destination:\n            query = (\n                self.data.writeStream.trigger(**TRIGGER_OPTION)\n                .format(\"delta\")\n                .queryName(self.query_name)\n                .outputMode(self.mode)\n                .options(**self.options)\n                .start(self.destination)\n            )\n        else:\n            query = (\n                self.data.writeStream.trigger(**TRIGGER_OPTION)\n                .format(\"delta\")\n                .queryName(self.query_name)\n                .outputMode(self.mode)\n                .options(**self.options)\n                .toTable(self.destination)\n            )\n\n        if self.query_wait_interval:\n            while query.isActive:\n                if query.lastProgress:\n                    logging.info(query.lastProgress)\n                time.sleep(self.query_wait_interval)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/delta_merge/","title":"Write to Delta using Merge","text":""},{"location":"sdk/code-reference/pipelines/destinations/spark/delta_merge/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.delta_merge.SparkDeltaMergeDestination","title":"<code>SparkDeltaMergeDestination</code>","text":"<p>               Bases: <code>DestinationInterface</code></p> <p>The Spark Delta Merge Destination is used to merge data into a Delta table. Refer to this documentation for more information about Delta Merge.</p>"},{"location":"sdk/code-reference/pipelines/destinations/spark/delta_merge/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.delta_merge.SparkDeltaMergeDestination--examples","title":"Examples","text":"<p><pre><code>#Delta Merge Destination for Streaming Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkDeltaMergeDestination\n\ndelta_merge_destination = SparkDeltaMergeDestination(\n    data=df,\n    destination=\"DELTA-TABLE-PATH\",\n    options={\n        \"checkpointLocation\": \"/{CHECKPOINT-LOCATION}/\"\n    },\n    merge_condition=\"`source.id = target.id`\"\n    when_matched_update_list=None\n    when_matched_delete_list=None\n    when_not_matched_insert_list=None\n    when_not_matched_by_source_update_list=None\n    when_not_matched_by_source_delete_list=None\n    try_broadcast_join=False\n    trigger=\"10 seconds\",\n    query_name=\"DeltaDestination\"\n    query_wait_interval=None\n)\n\ndelta_merge_destination.write_stream()\n</code></pre> <pre><code>#Delta Merge Destination for Batch Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkDeltaMergeDestination\n\ndelta_merge_destination = SparkDeltaMergeDestination(\n    data=df,\n    destination=\"DELTA-TABLE-PATH\",\n    options={},\n    merge_condition=\"`source.id = target.id`\",\n    when_matched_update_list=None,\n    when_matched_delete_list=None,\n    when_not_matched_insert_list=None,\n    when_not_matched_by_source_update_list=None,\n    when_not_matched_by_source_delete_list=None,\n    try_broadcast_join=False,\n    trigger=\"10 seconds\",\n    query_name=\"DeltaDestination\"\n    query_wait_interval=None\n)\n\ndelta_merge_destination.write_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to be merged into a Delta Table</p> required <code>destination</code> <code>str</code> <p>Either the name of the Hive Metastore or Unity Catalog Delta Table or the path to the Delta table</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for batch and streaming.</p> required <code>merge_condition</code> <code>str</code> <p>Condition for matching records between dataframe and delta table. Reference Dataframe columns as <code>source</code> and Delta Table columns as <code>target</code>. For example <code>source.id = target.id</code>.</p> required <code>when_matched_update_list</code> <code>optional list[DeltaMergeConditionValues]</code> <p>Conditions(optional) and values to be used when updating rows that match the <code>merge_condition</code>. Specify <code>*</code> for Values if all columns from Dataframe should be inserted.</p> <code>None</code> <code>when_matched_delete_list</code> <code>optional list[DeltaMergeCondition]</code> <p>Conditions(optional) to be used when deleting rows that match the <code>merge_condition</code>.</p> <code>None</code> <code>when_not_matched_insert_list</code> <code>optional list[DeltaMergeConditionValues]</code> <p>Conditions(optional) and values to be used when inserting rows that do not match the <code>merge_condition</code>. Specify <code>*</code> for Values if all columns from Dataframe should be inserted.</p> <code>None</code> <code>when_not_matched_by_source_update_list</code> <code>optional list[DeltaMergeConditionValues]</code> <p>Conditions(optional) and values to be used when updating rows that do not match the <code>merge_condition</code>.</p> <code>None</code> <code>when_not_matched_by_source_delete_list</code> <code>optional list[DeltaMergeCondition]</code> <p>Conditions(optional) to be used when deleting rows that do not match the <code>merge_condition</code>.</p> <code>None</code> <code>try_broadcast_join</code> <code>optional bool</code> <p>Attempts to perform a broadcast join in the merge which can leverage data skipping using partition pruning and file pruning automatically. Can fail if dataframe being merged is large and therefore more suitable for streaming merges than batch merges</p> <code>False</code> <code>trigger</code> <code>optional str</code> <p>Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds</p> <code>'10 seconds'</code> <code>query_name</code> <code>optional str</code> <p>Unique name for the query in associated SparkSession</p> <code>'DeltaMergeDestination'</code> <code>query_wait_interval</code> <code>optional int</code> <p>If set, waits for the streaming query to complete before returning. (stream) Default is None</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>checkpointLocation</code> <code>str</code> <p>Path to checkpoint files. (Streaming)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta_merge.py</code> <pre><code>class SparkDeltaMergeDestination(DestinationInterface):\n    \"\"\"\n    The Spark Delta Merge Destination is used to merge data into a Delta table. Refer to this [documentation](https://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge&amp;language-python) for more information about Delta Merge.\n\n    Examples\n    --------\n    ```python\n    #Delta Merge Destination for Streaming Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkDeltaMergeDestination\n\n    delta_merge_destination = SparkDeltaMergeDestination(\n        data=df,\n        destination=\"DELTA-TABLE-PATH\",\n        options={\n            \"checkpointLocation\": \"/{CHECKPOINT-LOCATION}/\"\n        },\n        merge_condition=\"`source.id = target.id`\"\n        when_matched_update_list=None\n        when_matched_delete_list=None\n        when_not_matched_insert_list=None\n        when_not_matched_by_source_update_list=None\n        when_not_matched_by_source_delete_list=None\n        try_broadcast_join=False\n        trigger=\"10 seconds\",\n        query_name=\"DeltaDestination\"\n        query_wait_interval=None\n    )\n\n    delta_merge_destination.write_stream()\n    ```\n    ```python\n    #Delta Merge Destination for Batch Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkDeltaMergeDestination\n\n    delta_merge_destination = SparkDeltaMergeDestination(\n        data=df,\n        destination=\"DELTA-TABLE-PATH\",\n        options={},\n        merge_condition=\"`source.id = target.id`\",\n        when_matched_update_list=None,\n        when_matched_delete_list=None,\n        when_not_matched_insert_list=None,\n        when_not_matched_by_source_update_list=None,\n        when_not_matched_by_source_delete_list=None,\n        try_broadcast_join=False,\n        trigger=\"10 seconds\",\n        query_name=\"DeltaDestination\"\n        query_wait_interval=None\n    )\n\n    delta_merge_destination.write_batch()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe to be merged into a Delta Table\n        destination (str): Either the name of the Hive Metastore or Unity Catalog Delta Table **or** the path to the Delta table\n        options (dict): Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for [batch](https://docs.delta.io/latest/delta-batch.html#write-to-a-table){ target=\"_blank\" } and [streaming](https://docs.delta.io/latest/delta-streaming.html#delta-table-as-a-sink){ target=\"_blank\" }.\n        merge_condition (str): Condition for matching records between dataframe and delta table. Reference Dataframe columns as `source` and Delta Table columns as `target`. For example `source.id = target.id`.\n        when_matched_update_list (optional list[DeltaMergeConditionValues]): Conditions(optional) and values to be used when updating rows that match the `merge_condition`. Specify `*` for Values if all columns from Dataframe should be inserted.\n        when_matched_delete_list (optional list[DeltaMergeCondition]): Conditions(optional) to be used when deleting rows that match the `merge_condition`.\n        when_not_matched_insert_list (optional list[DeltaMergeConditionValues]): Conditions(optional) and values to be used when inserting rows that do not match the `merge_condition`. Specify `*` for Values if all columns from Dataframe should be inserted.\n        when_not_matched_by_source_update_list (optional list[DeltaMergeConditionValues]): Conditions(optional) and values to be used when updating rows that do not match the `merge_condition`.\n        when_not_matched_by_source_delete_list (optional list[DeltaMergeCondition]): Conditions(optional) to be used when deleting rows that do not match the `merge_condition`.\n        try_broadcast_join (optional bool): Attempts to perform a broadcast join in the merge which can leverage data skipping using partition pruning and file pruning automatically. Can fail if dataframe being merged is large and therefore more suitable for streaming merges than batch merges\n        trigger (optional str): Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds\n        query_name (optional str): Unique name for the query in associated SparkSession\n        query_wait_interval (optional int): If set, waits for the streaming query to complete before returning. (stream) Default is None\n\n    Attributes:\n        checkpointLocation (str): Path to checkpoint files. (Streaming)\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    destination: str\n    options: dict\n    merge_condition: str\n    when_matched_update_list: List[DeltaMergeConditionValues]\n    when_matched_delete_list: List[DeltaMergeCondition]\n    when_not_matched_insert_list: List[DeltaMergeConditionValues]\n    when_not_matched_by_source_update_list: List[DeltaMergeConditionValues]\n    when_not_matched_by_source_delete_list: List[DeltaMergeCondition]\n    try_broadcast_join: bool\n    trigger: str\n    query_name: str\n    query_wait_interval: int\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n        destination: str,\n        options: dict,\n        merge_condition: str,\n        when_matched_update_list: List[DeltaMergeConditionValues] = None,\n        when_matched_delete_list: List[DeltaMergeCondition] = None,\n        when_not_matched_insert_list: List[DeltaMergeConditionValues] = None,\n        when_not_matched_by_source_update_list: List[DeltaMergeConditionValues] = None,\n        when_not_matched_by_source_delete_list: List[DeltaMergeCondition] = None,\n        try_broadcast_join: bool = False,\n        trigger=\"10 seconds\",\n        query_name: str = \"DeltaMergeDestination\",\n        query_wait_interval: int = None,\n    ) -&gt; None:\n        self.spark = spark\n        self.data = data\n        self.destination = destination\n        self.options = options\n        self.merge_condition = merge_condition\n        self.when_matched_update_list = (\n            [] if when_matched_update_list is None else when_matched_update_list\n        )\n        self.when_matched_delete_list = (\n            [] if when_matched_delete_list is None else when_matched_delete_list\n        )\n        self.when_not_matched_insert_list = (\n            [] if when_not_matched_insert_list is None else when_not_matched_insert_list\n        )\n        if (\n            isinstance(when_not_matched_by_source_update_list, list)\n            and len(when_not_matched_by_source_update_list) &gt; 0\n        ):\n            _package_version_meets_minimum(\"delta-spark\", \"2.3.0\")\n        self.when_not_matched_by_source_update_list = (\n            []\n            if when_not_matched_by_source_update_list is None\n            else when_not_matched_by_source_update_list\n        )\n        if (\n            isinstance(when_not_matched_by_source_delete_list, list)\n            and len(when_not_matched_by_source_delete_list) &gt; 0\n        ):\n            _package_version_meets_minimum(\"delta-spark\", \"2.3.0\")\n        self.when_not_matched_by_source_delete_list = (\n            []\n            if when_not_matched_by_source_delete_list is None\n            else when_not_matched_by_source_delete_list\n        )\n        self.try_broadcast_join = try_broadcast_join\n        self.trigger = trigger\n        self.query_name = query_name\n        self.query_wait_interval = query_wait_interval\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_core\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {\n            \"spark.sql.extensions\": \"io.delta.sql.DeltaSparkSessionExtension\",\n            \"spark.sql.catalog.spark_catalog\": \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n            \"spark.databricks.delta.schema.autoMerge.enabled\": \"true\",\n        }\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def _delta_merge_builder(\n        self, df: DataFrame, try_broadcast_join: bool\n    ) -&gt; DeltaMergeBuilder:\n        if \"/\" in self.destination:\n            delta_table = DeltaTable.forPath(self.spark, self.destination)\n        else:\n            delta_table = DeltaTable.forName(self.spark, self.destination)\n\n        if try_broadcast_join == True:\n            delta_merge_builder = delta_table.alias(\"target\").merge(\n                source=broadcast(df).alias(\"source\"), condition=self.merge_condition\n            )\n        else:\n            delta_merge_builder = delta_table.alias(\"target\").merge(\n                source=df.alias(\"source\"), condition=self.merge_condition\n            )\n\n        for when_matched_update in self.when_matched_update_list:\n            if when_matched_update.values == \"*\":\n                delta_merge_builder = delta_merge_builder.whenMatchedUpdateAll(\n                    condition=when_matched_update.condition,\n                )\n            else:\n                delta_merge_builder = delta_merge_builder.whenMatchedUpdate(\n                    condition=when_matched_update.condition,\n                    set=when_matched_update.values,\n                )\n\n        for when_matched_delete in self.when_matched_delete_list:\n            delta_merge_builder = delta_merge_builder.whenMatchedDelete(\n                condition=when_matched_delete.condition,\n            )\n\n        for when_not_matched_insert in self.when_not_matched_insert_list:\n            if when_not_matched_insert.values == \"*\":\n                delta_merge_builder = delta_merge_builder.whenNotMatchedInsertAll(\n                    condition=when_not_matched_insert.condition,\n                )\n            else:\n                delta_merge_builder = delta_merge_builder.whenNotMatchedInsert(\n                    condition=when_not_matched_insert.condition,\n                    values=when_not_matched_insert.values,\n                )\n\n        for (\n            when_not_matched_by_source_update\n        ) in self.when_not_matched_by_source_update_list:\n            delta_merge_builder = delta_merge_builder.whenNotMatchedBySourceUpdate(\n                condition=when_not_matched_by_source_update.condition,\n                set=when_not_matched_by_source_update.values,\n            )\n\n        for (\n            when_not_matched_by_source_delete\n        ) in self.when_not_matched_by_source_delete_list:\n            delta_merge_builder = delta_merge_builder.whenNotMatchedBySourceDelete(\n                condition=when_not_matched_by_source_delete.condition,\n            )\n\n        return delta_merge_builder\n\n    def _stream_merge_micro_batch(\n        self, micro_batch_df: DataFrame, epoch_id=None\n    ):  # NOSONAR\n        micro_batch_df.persist()\n\n        retry_delta_merge = False\n\n        if self.try_broadcast_join == True:\n            try:\n                delta_merge = self._delta_merge_builder(\n                    micro_batch_df, self.try_broadcast_join\n                )\n                delta_merge.execute()\n            except Exception as e:\n                if \"SparkOutOfMemoryError\" in str(e):\n                    retry_delta_merge = True\n                else:\n                    raise e\n\n        if self.try_broadcast_join == False or retry_delta_merge == True:\n            delta_merge = self._delta_merge_builder(micro_batch_df, False)\n            delta_merge.execute()\n\n        micro_batch_df.unpersist()\n\n    def write_batch(self):\n        \"\"\"\n        Merges batch data into a Delta Table.\n        \"\"\"\n        try:\n            delta_merge = self._delta_merge_builder(self.data, self.try_broadcast_join)\n            return delta_merge.execute()\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self):\n        \"\"\"\n        Merges streaming data to Delta using foreachBatch\n        \"\"\"\n        TRIGGER_OPTION = (\n            {\"availableNow\": True}\n            if self.trigger == \"availableNow\"\n            else {\"processingTime\": self.trigger}\n        )\n        try:\n            query = (\n                self.data.writeStream.trigger(**TRIGGER_OPTION)\n                .format(\"delta\")\n                .foreachBatch(self._stream_merge_micro_batch)\n                .queryName(self.query_name)\n                .outputMode(\"update\")\n                .options(**self.options)\n                .start()\n            )\n\n            if self.query_wait_interval:\n                while query.isActive:\n                    if query.lastProgress:\n                        logging.info(query.lastProgress)\n                    time.sleep(self.query_wait_interval)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/delta_merge/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.delta_merge.SparkDeltaMergeDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta_merge.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/delta_merge/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.delta_merge.SparkDeltaMergeDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Merges batch data into a Delta Table.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta_merge.py</code> <pre><code>def write_batch(self):\n    \"\"\"\n    Merges batch data into a Delta Table.\n    \"\"\"\n    try:\n        delta_merge = self._delta_merge_builder(self.data, self.try_broadcast_join)\n        return delta_merge.execute()\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/delta_merge/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.delta_merge.SparkDeltaMergeDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Merges streaming data to Delta using foreachBatch</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/delta_merge.py</code> <pre><code>def write_stream(self):\n    \"\"\"\n    Merges streaming data to Delta using foreachBatch\n    \"\"\"\n    TRIGGER_OPTION = (\n        {\"availableNow\": True}\n        if self.trigger == \"availableNow\"\n        else {\"processingTime\": self.trigger}\n    )\n    try:\n        query = (\n            self.data.writeStream.trigger(**TRIGGER_OPTION)\n            .format(\"delta\")\n            .foreachBatch(self._stream_merge_micro_batch)\n            .queryName(self.query_name)\n            .outputMode(\"update\")\n            .options(**self.options)\n            .start()\n        )\n\n        if self.query_wait_interval:\n            while query.isActive:\n                if query.lastProgress:\n                    logging.info(query.lastProgress)\n                time.sleep(self.query_wait_interval)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/eventhub/","title":"Write to Eventhub","text":""},{"location":"sdk/code-reference/pipelines/destinations/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.eventhub.SparkEventhubDestination","title":"<code>SparkEventhubDestination</code>","text":"<p>               Bases: <code>DestinationInterface</code></p> <p>This Spark destination class is used to write batch or streaming data to Eventhubs. Eventhub configurations need to be specified as options in a dictionary. Additionally, there are more optional configurations which can be found here. If using startingPosition or endingPosition make sure to check out Event Position section for more details and examples.</p>"},{"location":"sdk/code-reference/pipelines/destinations/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.eventhub.SparkEventhubDestination--examples","title":"Examples","text":"<p><pre><code>#Eventhub Destination for Streaming Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkEventhubDestination\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconnectionString = Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\n\neventhub_destination = SparkEventhubDestination(\n    spark=spark,\n    data=df,\n    options={\n        \"eventhubs.connectionString\": connectionString,\n        \"eventhubs.consumerGroup\": \"{YOUR-EVENTHUB-CONSUMER-GROUP}\",\n        \"checkpointLocation\": \"/{CHECKPOINT-LOCATION}/\"\n    },\n    trigger=\"10 seconds\",\n    query_name=\"EventhubDestination\",\n    query_wait_interval=None\n)\n\neventhub_destination.write_stream()\n</code></pre> <pre><code>#Eventhub Destination for Batch Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkEventhubDestination\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconnectionString = Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\n\n\neventhub_destination = SparkEventhubDestination(\n    spark=spark,\n    data=df,\n    options={\n        \"eventhubs.connectionString\": connectionString,\n        \"eventhubs.consumerGroup\": \"{YOUR-EVENTHUB-CONSUMER-GROUP}\"\n    },\n    trigger=\"10 seconds\",\n    query_name=\"EventhubDestination\",\n    query_wait_interval=None\n)\n\neventhub_destination.write_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session</p> required <code>data</code> <code>DataFrame</code> <p>Dataframe to be written to Eventhub</p> required <code>options</code> <code>dict</code> <p>A dictionary of Eventhub configurations (See Attributes table below). All Configuration options for Eventhubs can be found here.</p> required <code>trigger</code> <code>optional str</code> <p>Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds</p> <code>'10 seconds'</code> <code>query_name</code> <code>str</code> <p>Unique name for the query in associated SparkSession</p> <code>'EventhubDestination'</code> <code>query_wait_interval</code> <code>optional int</code> <p>If set, waits for the streaming query to complete before returning. (stream) Default is None</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>checkpointLocation</code> <code>str</code> <p>Path to checkpoint files. (Streaming)</p> <code>eventhubs.connectionString</code> <code>str</code> <p>Eventhubs connection string is required to connect to the Eventhubs service. (Streaming and Batch)</p> <code>eventhubs.consumerGroup</code> <code>str</code> <p>A consumer group is a view of an entire eventhub. Consumer groups enable multiple consuming applications to each have a separate view of the event stream, and to read the stream independently at their own pace and with their own offsets. (Streaming and Batch)</p> <code>eventhubs.startingPosition</code> <code>JSON str</code> <p>The starting position for your Structured Streaming job. If a specific EventPosition is not set for a partition using startingPositions, then we use the EventPosition set in startingPosition. If nothing is set in either option, we will begin consuming from the end of the partition. (Streaming and Batch)</p> <code>eventhubs.endingPosition</code> <code>JSON str</code> <p>(JSON str): The ending position of a batch query. This works the same as startingPosition. (Batch)</p> <code>maxEventsPerTrigger</code> <code>long</code> <p>Rate limit on maximum number of events processed per trigger interval. The specified total number of events will be proportionally split across partitions of different volume. (Stream)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/eventhub.py</code> <pre><code>class SparkEventhubDestination(DestinationInterface):\n    \"\"\"\n    This Spark destination class is used to write batch or streaming data to Eventhubs. Eventhub configurations need to be specified as options in a dictionary.\n    Additionally, there are more optional configurations which can be found [here.](https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/PySpark/structured-streaming-pyspark.md#event-hubs-configuration){ target=\"_blank\" }\n    If using startingPosition or endingPosition make sure to check out **Event Position** section for more details and examples.\n\n    Examples\n    --------\n    ```python\n    #Eventhub Destination for Streaming Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkEventhubDestination\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    connectionString = Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\n\n    eventhub_destination = SparkEventhubDestination(\n        spark=spark,\n        data=df,\n        options={\n            \"eventhubs.connectionString\": connectionString,\n            \"eventhubs.consumerGroup\": \"{YOUR-EVENTHUB-CONSUMER-GROUP}\",\n            \"checkpointLocation\": \"/{CHECKPOINT-LOCATION}/\"\n        },\n        trigger=\"10 seconds\",\n        query_name=\"EventhubDestination\",\n        query_wait_interval=None\n    )\n\n    eventhub_destination.write_stream()\n    ```\n    ```python\n    #Eventhub Destination for Batch Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkEventhubDestination\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    connectionString = Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\n\n\n    eventhub_destination = SparkEventhubDestination(\n        spark=spark,\n        data=df,\n        options={\n            \"eventhubs.connectionString\": connectionString,\n            \"eventhubs.consumerGroup\": \"{YOUR-EVENTHUB-CONSUMER-GROUP}\"\n        },\n        trigger=\"10 seconds\",\n        query_name=\"EventhubDestination\",\n        query_wait_interval=None\n    )\n\n    eventhub_destination.write_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session\n        data (DataFrame): Dataframe to be written to Eventhub\n        options (dict): A dictionary of Eventhub configurations (See Attributes table below). All Configuration options for Eventhubs can be found [here.](https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/PySpark/structured-streaming-pyspark.md#event-hubs-configuration){ target=\"_blank\" }\n        trigger (optional str): Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds\n        query_name (str): Unique name for the query in associated SparkSession\n        query_wait_interval (optional int): If set, waits for the streaming query to complete before returning. (stream) Default is None\n\n    Attributes:\n        checkpointLocation (str): Path to checkpoint files. (Streaming)\n        eventhubs.connectionString (str):  Eventhubs connection string is required to connect to the Eventhubs service. (Streaming and Batch)\n        eventhubs.consumerGroup (str): A consumer group is a view of an entire eventhub. Consumer groups enable multiple consuming applications to each have a separate view of the event stream, and to read the stream independently at their own pace and with their own offsets. (Streaming and Batch)\n        eventhubs.startingPosition (JSON str): The starting position for your Structured Streaming job. If a specific EventPosition is not set for a partition using startingPositions, then we use the EventPosition set in startingPosition. If nothing is set in either option, we will begin consuming from the end of the partition. (Streaming and Batch)\n        eventhubs.endingPosition: (JSON str): The ending position of a batch query. This works the same as startingPosition. (Batch)\n        maxEventsPerTrigger (long): Rate limit on maximum number of events processed per trigger interval. The specified total number of events will be proportionally split across partitions of different volume. (Stream)\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    options: dict\n    trigger: str\n    query_name: str\n    query_wait_interval: int\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n        options: dict,\n        trigger=\"10 seconds\",\n        query_name=\"EventhubDestination\",\n        query_wait_interval: int = None,\n    ) -&gt; None:\n        self.spark = spark\n        self.data = data\n        self.options = options\n        self.trigger = trigger\n        self.query_name = query_name\n        self.query_wait_interval = query_wait_interval\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        spark_libraries = Libraries()\n        spark_libraries.add_maven_library(get_default_package(\"spark_azure_eventhub\"))\n        return spark_libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def prepare_columns(self):\n        if \"body\" in self.data.columns:\n            if self.data.schema[\"body\"].dataType not in [StringType(), BinaryType()]:\n                try:\n                    self.data.withColumn(\"body\", col(\"body\").cast(StringType()))\n                except Exception as e:\n                    raise ValueError(\n                        \"'body' column must be of string or binary type\", e\n                    )\n        else:\n            self.data = self.data.withColumn(\n                \"body\",\n                to_json(\n                    struct(\n                        [\n                            col(column).alias(column)\n                            for column in self.data.columns\n                            if column not in [\"partitionId\", \"partitionKey\"]\n                        ]\n                    )\n                ),\n            )\n        for column in self.data.schema:\n            if (\n                column.name in [\"partitionId\", \"partitionKey\"]\n                and column.dataType != StringType()\n            ):\n                try:\n                    self.data = self.data.withColumn(\n                        column.name, col(column.name).cast(StringType())\n                    )\n                except Exception as e:\n                    raise ValueError(f\"Column {column.name} must be of string type\", e)\n        return self.data.select(\n            [\n                column\n                for column in self.data.columns\n                if column in [\"partitionId\", \"partitionKey\", \"body\"]\n            ]\n        )\n\n    def write_batch(self):\n        \"\"\"\n        Writes batch data to Eventhubs.\n        \"\"\"\n        eventhub_connection_string = \"eventhubs.connectionString\"\n        try:\n            if eventhub_connection_string in self.options:\n                sc = self.spark.sparkContext\n                self.options[eventhub_connection_string] = (\n                    sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                        self.options[eventhub_connection_string]\n                    )\n                )\n            df = self.prepare_columns()\n            return df.write.format(\"eventhubs\").options(**self.options).save()\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self):\n        \"\"\"\n        Writes steaming data to Eventhubs.\n        \"\"\"\n        eventhub_connection_string = \"eventhubs.connectionString\"\n        try:\n            TRIGGER_OPTION = (\n                {\"availableNow\": True}\n                if self.trigger == \"availableNow\"\n                else {\"processingTime\": self.trigger}\n            )\n            if eventhub_connection_string in self.options:\n                sc = self.spark.sparkContext\n                self.options[eventhub_connection_string] = (\n                    sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                        self.options[eventhub_connection_string]\n                    )\n                )\n            df = self.prepare_columns()\n            df = self.data.select(\n                [\n                    column\n                    for column in self.data.columns\n                    if column in [\"partitionId\", \"partitionKey\", \"body\"]\n                ]\n            )\n            query = (\n                df.writeStream.trigger(**TRIGGER_OPTION)\n                .format(\"eventhubs\")\n                .options(**self.options)\n                .queryName(self.query_name)\n                .start()\n            )\n\n            if self.query_wait_interval:\n                while query.isActive:\n                    if query.lastProgress:\n                        logging.info(query.lastProgress)\n                    time.sleep(self.query_wait_interval)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.eventhub.SparkEventhubDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/eventhub.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.eventhub.SparkEventhubDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Writes batch data to Eventhubs.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/eventhub.py</code> <pre><code>def write_batch(self):\n    \"\"\"\n    Writes batch data to Eventhubs.\n    \"\"\"\n    eventhub_connection_string = \"eventhubs.connectionString\"\n    try:\n        if eventhub_connection_string in self.options:\n            sc = self.spark.sparkContext\n            self.options[eventhub_connection_string] = (\n                sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                    self.options[eventhub_connection_string]\n                )\n            )\n        df = self.prepare_columns()\n        return df.write.format(\"eventhubs\").options(**self.options).save()\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.eventhub.SparkEventhubDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Writes steaming data to Eventhubs.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/eventhub.py</code> <pre><code>def write_stream(self):\n    \"\"\"\n    Writes steaming data to Eventhubs.\n    \"\"\"\n    eventhub_connection_string = \"eventhubs.connectionString\"\n    try:\n        TRIGGER_OPTION = (\n            {\"availableNow\": True}\n            if self.trigger == \"availableNow\"\n            else {\"processingTime\": self.trigger}\n        )\n        if eventhub_connection_string in self.options:\n            sc = self.spark.sparkContext\n            self.options[eventhub_connection_string] = (\n                sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                    self.options[eventhub_connection_string]\n                )\n            )\n        df = self.prepare_columns()\n        df = self.data.select(\n            [\n                column\n                for column in self.data.columns\n                if column in [\"partitionId\", \"partitionKey\", \"body\"]\n            ]\n        )\n        query = (\n            df.writeStream.trigger(**TRIGGER_OPTION)\n            .format(\"eventhubs\")\n            .options(**self.options)\n            .queryName(self.query_name)\n            .start()\n        )\n\n        if self.query_wait_interval:\n            while query.isActive:\n                if query.lastProgress:\n                    logging.info(query.lastProgress)\n                time.sleep(self.query_wait_interval)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kafka/","title":"Write to Kafka","text":""},{"location":"sdk/code-reference/pipelines/destinations/spark/kafka/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kafka.SparkKafkaDestination","title":"<code>SparkKafkaDestination</code>","text":"<p>               Bases: <code>DestinationInterface</code></p> <p>This Spark destination class is used to write batch or streaming data from Kafka. Required and optional configurations can be found in the Attributes tables below.</p> <p>Additionally, there are more optional configurations which can be found here.</p> <p>For compatibility between Spark and Kafka, the columns in the input dataframe are concatenated into one 'value' column of JSON string.</p>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kafka/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kafka.SparkKafkaDestination--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.destinations import SparkKafkaDestination\n\nkafka_destination = SparkKafkaDestination(\n    data=df,\n    options={\n        \"kafka.bootstrap.servers\": \"host1:port1,host2:port2\"\n    },\n    trigger=\"10 seconds\",\n    query_name=\"KafkaDestination\",\n    query_wait_interval=None\n)\n\nkafka_destination.write_stream()\n\nOR\n\nkafka_destination.write_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to be written to Kafka</p> required <code>options</code> <code>dict</code> <p>A dictionary of Kafka configurations (See Attributes tables below). For more information on configuration options see here</p> required <code>trigger</code> <code>optional str</code> <p>Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds</p> <code>'10 seconds'</code> <code>query_name</code> <code>str</code> <p>Unique name for the query in associated SparkSession</p> <code>'KafkaDestination'</code> <code>query_wait_interval</code> <code>optional int</code> <p>If set, waits for the streaming query to complete before returning. (stream) Default is None</p> <code>None</code> <p>The following options must be set for the Kafka destination for both batch and streaming queries.</p> <p>Attributes:</p> Name Type Description <code>kafka.bootstrap.servers</code> <code>A comma-separated list of host\ufe30port</code> <p>The Kafka \"bootstrap.servers\" configuration. (Streaming and Batch)</p> <p>The following configurations are optional:</p> <p>Attributes:</p> Name Type Description <code>topic</code> <code>str</code> <p>Sets the topic that all rows will be written to in Kafka. This option overrides any topic column that may exist in the data. (Streaming and Batch)</p> <code>includeHeaders</code> <code>bool</code> <p>Whether to include the Kafka headers in the row. (Streaming and Batch)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kafka.py</code> <pre><code>class SparkKafkaDestination(DestinationInterface):\n    \"\"\"\n    This Spark destination class is used to write batch or streaming data from Kafka. Required and optional configurations can be found in the Attributes tables below.\n\n    Additionally, there are more optional configurations which can be found [here.](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html){ target=\"_blank\" }\n\n    For compatibility between Spark and Kafka, the columns in the input dataframe are concatenated into one 'value' column of JSON string.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.destinations import SparkKafkaDestination\n\n    kafka_destination = SparkKafkaDestination(\n        data=df,\n        options={\n            \"kafka.bootstrap.servers\": \"host1:port1,host2:port2\"\n        },\n        trigger=\"10 seconds\",\n        query_name=\"KafkaDestination\",\n        query_wait_interval=None\n    )\n\n    kafka_destination.write_stream()\n\n    OR\n\n    kafka_destination.write_batch()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe to be written to Kafka\n        options (dict): A dictionary of Kafka configurations (See Attributes tables below). For more information on configuration options see [here](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html){ target=\"_blank\" }\n        trigger (optional str): Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds\n        query_name (str): Unique name for the query in associated SparkSession\n        query_wait_interval (optional int): If set, waits for the streaming query to complete before returning. (stream) Default is None\n\n    The following options must be set for the Kafka destination for both batch and streaming queries.\n\n    Attributes:\n        kafka.bootstrap.servers (A comma-separated list of host\ufe30port): The Kafka \"bootstrap.servers\" configuration. (Streaming and Batch)\n\n    The following configurations are optional:\n\n    Attributes:\n        topic (str):Sets the topic that all rows will be written to in Kafka. This option overrides any topic column that may exist in the data. (Streaming and Batch)\n        includeHeaders (bool): Whether to include the Kafka headers in the row. (Streaming and Batch)\n\n    \"\"\"\n\n    data: DataFrame\n    options: dict\n    trigger: str\n    query_name: str\n    query_wait_interval: int\n\n    def __init__(\n        self,\n        data: DataFrame,\n        options: dict,\n        trigger=\"10 seconds\",\n        query_name=\"KafkaDestination\",\n        query_wait_interval: int = None,\n    ) -&gt; None:\n        self.data = data\n        self.options = options\n        self.trigger = trigger\n        self.query_name = query_name\n        self.query_wait_interval = query_wait_interval\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        spark_libraries = Libraries()\n        spark_libraries.add_maven_library(get_default_package(\"spark_sql_kafka\"))\n        return spark_libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def write_batch(self):\n        \"\"\"\n        Writes batch data to Kafka.\n        \"\"\"\n        try:\n            return (\n                self.data.select(to_json(struct(\"*\")).alias(\"value\"))\n                .write.format(\"kafka\")\n                .options(**self.options)\n                .save()\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self):\n        \"\"\"\n        Writes steaming data to Kafka.\n        \"\"\"\n        try:\n            TRIGGER_OPTION = (\n                {\"availableNow\": True}\n                if self.trigger == \"availableNow\"\n                else {\"processingTime\": self.trigger}\n            )\n            query = (\n                self.data.select(to_json(struct(\"*\")).alias(\"value\"))\n                .writeStream.trigger(**TRIGGER_OPTION)\n                .format(\"kafka\")\n                .options(**self.options)\n                .queryName(self.query_name)\n                .start()\n            )\n\n            if self.query_wait_interval:\n                while query.isActive:\n                    if query.lastProgress:\n                        logging.info(query.lastProgress)\n                    time.sleep(self.query_wait_interval)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kafka/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kafka.SparkKafkaDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kafka.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kafka/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kafka.SparkKafkaDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Writes batch data to Kafka.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kafka.py</code> <pre><code>def write_batch(self):\n    \"\"\"\n    Writes batch data to Kafka.\n    \"\"\"\n    try:\n        return (\n            self.data.select(to_json(struct(\"*\")).alias(\"value\"))\n            .write.format(\"kafka\")\n            .options(**self.options)\n            .save()\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kafka/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kafka.SparkKafkaDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Writes steaming data to Kafka.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kafka.py</code> <pre><code>def write_stream(self):\n    \"\"\"\n    Writes steaming data to Kafka.\n    \"\"\"\n    try:\n        TRIGGER_OPTION = (\n            {\"availableNow\": True}\n            if self.trigger == \"availableNow\"\n            else {\"processingTime\": self.trigger}\n        )\n        query = (\n            self.data.select(to_json(struct(\"*\")).alias(\"value\"))\n            .writeStream.trigger(**TRIGGER_OPTION)\n            .format(\"kafka\")\n            .options(**self.options)\n            .queryName(self.query_name)\n            .start()\n        )\n\n        if self.query_wait_interval:\n            while query.isActive:\n                if query.lastProgress:\n                    logging.info(query.lastProgress)\n                time.sleep(self.query_wait_interval)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kafka_eventhub/","title":"Write to Eventhub using Kafka","text":""},{"location":"sdk/code-reference/pipelines/destinations/spark/kafka_eventhub/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kafka_eventhub.SparkKafkaEventhubDestination","title":"<code>SparkKafkaEventhubDestination</code>","text":"<p>               Bases: <code>DestinationInterface</code></p> <p>This Spark Destination class is used to write batch or streaming data to an Eventhub using the Kafka protocol. This enables Eventhubs to be used as a destination in applications like Delta Live Tables or Databricks Serverless Jobs as the Spark Eventhubs JAR is not supported in these scenarios.</p> <p>Default settings will be specified if not provided in the <code>options</code> parameter:</p> <ul> <li><code>kafka.sasl.mechanism</code> will be set to <code>PLAIN</code></li> <li><code>kafka.security.protocol</code> will be set to <code>SASL_SSL</code></li> <li><code>kafka.request.timeout.ms</code> will be set to <code>60000</code></li> <li><code>kafka.session.timeout.ms</code> will be set to <code>60000</code></li> </ul>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kafka_eventhub/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kafka_eventhub.SparkKafkaEventhubDestination--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.destinations import SparkKafkaEventhubDestination\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconnectionString = Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\n\neventhub_destination = SparkKafkaEventhubDestination(\n    spark=spark,\n    data=df,\n    options={\n        \"kafka.bootstrap.servers\": \"host1:port1,host2:port2\"\n    },\n    connection_string=\"{YOUR-EVENTHUB-CONNECTION-STRING}\",\n    consumer_group=\"{YOUR-EVENTHUB-CONSUMER-GROUP}\",\n    trigger=\"10 seconds\",\n    query_name=\"KafkaEventhubDestination\",\n    query_wait_interval=None\n)\n\neventhub_destination.write_stream()\n\nOR\n\neventhub_destination.write_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session</p> required <code>data</code> <code>DataFrame</code> <p>Any columns not listed in the required schema here will be merged into a single column named \"value\", or ignored if \"value\" is an existing column</p> required <code>connection_string</code> <code>str</code> <p>Eventhubs connection string is required to connect to the Eventhubs service. This must include the Eventhub name as the <code>EntityPath</code> parameter. Example <code>\"Endpoint=sb://test.servicebus.windows.net/;SharedAccessKeyName=test;SharedAccessKey=test_key;EntityPath=test_eventhub\"</code></p> required <code>options</code> <code>dict</code> <p>A dictionary of Kafka configurations (See Attributes tables below)</p> required <code>consumer_group</code> <code>str</code> <p>The Eventhub consumer group to use for the connection</p> required <code>trigger</code> <code>optional str</code> <p>Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds</p> <code>'10 seconds'</code> <code>query_name</code> <code>optional str</code> <p>Unique name for the query in associated SparkSession</p> <code>'KafkaEventhubDestination'</code> <code>query_wait_interval</code> <code>optional int</code> <p>If set, waits for the streaming query to complete before returning. (stream) Default is None</p> <code>None</code> <p>The following are commonly used parameters that may be included in the options dict. kafka.bootstrap.servers is the only required config. A full list of configs can be found here</p> <p>Attributes:</p> Name Type Description <code>kafka.bootstrap.servers</code> <code>A comma-separated list of host\ufe30port</code> <p>The Kafka \"bootstrap.servers\" configuration. (Streaming and Batch)</p> <code>topic</code> <code>string</code> <p>Required if there is no existing topic column in your DataFrame. Sets the topic that all rows will be written to in Kafka. (Streaming and Batch)</p> <code>includeHeaders</code> <code>bool</code> <p>Determines whether to include the Kafka headers in the row; defaults to False. (Streaming and Batch)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kafka_eventhub.py</code> <pre><code>class SparkKafkaEventhubDestination(DestinationInterface):\n    \"\"\"\n    This Spark Destination class is used to write batch or streaming data to an Eventhub using the Kafka protocol. This enables Eventhubs to be used as a destination in applications like Delta Live Tables or Databricks Serverless Jobs as the Spark Eventhubs JAR is not supported in these scenarios.\n\n    Default settings will be specified if not provided in the `options` parameter:\n\n    - `kafka.sasl.mechanism` will be set to `PLAIN`\n    - `kafka.security.protocol` will be set to `SASL_SSL`\n    - `kafka.request.timeout.ms` will be set to `60000`\n    - `kafka.session.timeout.ms` will be set to `60000`\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.destinations import SparkKafkaEventhubDestination\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    connectionString = Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\n\n    eventhub_destination = SparkKafkaEventhubDestination(\n        spark=spark,\n        data=df,\n        options={\n            \"kafka.bootstrap.servers\": \"host1:port1,host2:port2\"\n        },\n        connection_string=\"{YOUR-EVENTHUB-CONNECTION-STRING}\",\n        consumer_group=\"{YOUR-EVENTHUB-CONSUMER-GROUP}\",\n        trigger=\"10 seconds\",\n        query_name=\"KafkaEventhubDestination\",\n        query_wait_interval=None\n    )\n\n    eventhub_destination.write_stream()\n\n    OR\n\n    eventhub_destination.write_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session\n        data (DataFrame): Any columns not listed in the required schema [here](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#writing-data-to-kafka){ target=\"_blank\" } will be merged into a single column named \"value\", or ignored if \"value\" is an existing column\n        connection_string (str): Eventhubs connection string is required to connect to the Eventhubs service. This must include the Eventhub name as the `EntityPath` parameter. Example `\"Endpoint=sb://test.servicebus.windows.net/;SharedAccessKeyName=test;SharedAccessKey=test_key;EntityPath=test_eventhub\"`\n        options (dict): A dictionary of Kafka configurations (See Attributes tables below)\n        consumer_group (str): The Eventhub consumer group to use for the connection\n        trigger (optional str): Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds\n        query_name (optional str): Unique name for the query in associated SparkSession\n        query_wait_interval (optional int): If set, waits for the streaming query to complete before returning. (stream) Default is None\n\n    The following are commonly used parameters that may be included in the options dict. kafka.bootstrap.servers is the only required config. A full list of configs can be found [here](https://kafka.apache.org/documentation/#producerconfigs){ target=\"_blank\" }\n\n    Attributes:\n        kafka.bootstrap.servers (A comma-separated list of host\ufe30port):  The Kafka \"bootstrap.servers\" configuration. (Streaming and Batch)\n        topic (string): Required if there is no existing topic column in your DataFrame. Sets the topic that all rows will be written to in Kafka. (Streaming and Batch)\n        includeHeaders (bool): Determines whether to include the Kafka headers in the row; defaults to False. (Streaming and Batch)\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    connection_string: str\n    options: dict\n    consumer_group: str\n    trigger: str\n    query_name: str\n    connection_string_properties: dict\n    query_wait_interval: int\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n        connection_string: str,\n        options: dict,\n        consumer_group: str,\n        trigger: str = \"10 seconds\",\n        query_name: str = \"KafkaEventhubDestination\",\n        query_wait_interval: int = None,\n    ) -&gt; None:\n        self.spark = spark\n        self.data = data\n        self.connection_string = connection_string\n        self.options = options\n        self.consumer_group = consumer_group\n        self.trigger = trigger\n        self.query_name = query_name\n        self.connection_string_properties = self._parse_connection_string(\n            connection_string\n        )\n        self.options = self._configure_options(options)\n        self.query_wait_interval = query_wait_interval\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        spark_libraries = Libraries()\n        spark_libraries.add_maven_library(get_default_package(\"spark_sql_kafka\"))\n        return spark_libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self) -&gt; bool:\n        return True\n\n    def post_write_validation(self) -&gt; bool:\n        return True\n\n    # Code is from Azure Eventhub Python SDK. Will import the package if possible with Conda in the  conda-forge channel in the future\n    def _parse_connection_string(self, connection_string: str):\n        conn_settings = [s.split(\"=\", 1) for s in connection_string.split(\";\")]\n        if any(len(tup) != 2 for tup in conn_settings):\n            raise ValueError(\"Connection string is either blank or malformed.\")\n        conn_settings = dict(conn_settings)\n        shared_access_signature = None\n        for key, value in conn_settings.items():\n            if key.lower() == \"sharedaccesssignature\":\n                shared_access_signature = value\n        shared_access_key = conn_settings.get(\"SharedAccessKey\")\n        shared_access_key_name = conn_settings.get(\"SharedAccessKeyName\")\n        if any([shared_access_key, shared_access_key_name]) and not all(\n            [shared_access_key, shared_access_key_name]\n        ):\n            raise ValueError(\n                \"Connection string must have both SharedAccessKeyName and SharedAccessKey.\"\n            )\n        if shared_access_signature is not None and shared_access_key is not None:\n            raise ValueError(\n                \"Only one of the SharedAccessKey or SharedAccessSignature must be present.\"\n            )\n        endpoint = conn_settings.get(\"Endpoint\")\n        if not endpoint:\n            raise ValueError(\"Connection string is either blank or malformed.\")\n        parsed = urlparse(endpoint.rstrip(\"/\"))\n        if not parsed.netloc:\n            raise ValueError(\"Invalid Endpoint on the Connection String.\")\n        namespace = parsed.netloc.strip()\n        properties = {\n            \"fully_qualified_namespace\": namespace,\n            \"endpoint\": endpoint,\n            \"eventhub_name\": conn_settings.get(\"EntityPath\"),\n            \"shared_access_signature\": shared_access_signature,\n            \"shared_access_key_name\": shared_access_key_name,\n            \"shared_access_key\": shared_access_key,\n        }\n        return properties\n\n    def _connection_string_builder(self, properties: dict) -&gt; str:\n        connection_string = \"Endpoint=\" + properties.get(\"endpoint\") + \";\"\n\n        if properties.get(\"shared_access_key\"):\n            connection_string += (\n                \"SharedAccessKey=\" + properties.get(\"shared_access_key\") + \";\"\n            )\n\n        if properties.get(\"shared_access_key_name\"):\n            connection_string += (\n                \"SharedAccessKeyName=\" + properties.get(\"shared_access_key_name\") + \";\"\n            )\n\n        if properties.get(\"shared_access_signature\"):\n            connection_string += (\n                \"SharedAccessSignature=\"\n                + properties.get(\"shared_access_signature\")\n                + \";\"\n            )\n        return connection_string\n\n    def _configure_options(self, options: dict) -&gt; dict:\n        if \"topic\" not in options:\n            options[\"topic\"] = self.connection_string_properties.get(\"eventhub_name\")\n\n        if \"kafka.bootstrap.servers\" not in options:\n            options[\"kafka.bootstrap.servers\"] = (\n                self.connection_string_properties.get(\"fully_qualified_namespace\")\n                + \":9093\"\n            )\n\n        if \"kafka.sasl.mechanism\" not in options:\n            options[\"kafka.sasl.mechanism\"] = \"PLAIN\"\n\n        if \"kafka.security.protocol\" not in options:\n            options[\"kafka.security.protocol\"] = \"SASL_SSL\"\n\n        if \"kafka.sasl.jaas.config\" not in options:\n            kafka_package = \"org.apache.kafka.common.security.plain.PlainLoginModule\"\n            if \"DATABRICKS_RUNTIME_VERSION\" in os.environ or (\n                \"_client\" in self.spark.__dict__\n                and \"databricks\" in self.spark.client.host\n            ):\n                kafka_package = \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule\"\n            connection_string = self._connection_string_builder(\n                self.connection_string_properties\n            )\n            options[\"kafka.sasl.jaas.config\"] = (\n                '{} required username=\"$ConnectionString\" password=\"{}\";'.format(\n                    kafka_package, connection_string\n                )\n            )  # NOSONAR\n\n        if \"kafka.request.timeout.ms\" not in options:\n            options[\"kafka.request.timeout.ms\"] = \"60000\"\n\n        if \"kafka.session.timeout.ms\" not in options:\n            options[\"kafka.session.timeout.ms\"] = \"60000\"\n\n        if \"kafka.group.id\" not in options:\n            options[\"kafka.group.id\"] = self.consumer_group\n\n        options[\"includeHeaders\"] = \"true\"\n\n        return options\n\n    def _transform_to_eventhub_schema(self, df: DataFrame) -&gt; DataFrame:\n        column_list = [\"key\", \"headers\", \"topic\", \"partition\"]\n        if \"value\" not in df.columns:\n            df = df.withColumn(\n                \"value\",\n                to_json(\n                    struct(\n                        [\n                            col(column).alias(column)\n                            for column in df.columns\n                            if column not in column_list\n                        ]\n                    )\n                ),\n            )\n        if \"headers\" in df.columns and (\n            df.schema[\"headers\"].dataType.elementType[\"key\"].nullable == True\n            or df.schema[\"headers\"].dataType.elementType[\"value\"].nullable == True\n        ):\n            raise ValueError(\"key and value in the headers column cannot be nullable\")\n\n        return df.select(\n            [\n                column\n                for column in df.columns\n                if column in [\"value\", \"key\", \"headers\", \"topic\", \"partition\"]\n            ]\n        )\n\n    def write_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Reads batch data from Kafka.\n        \"\"\"\n        try:\n            df = self._transform_to_eventhub_schema(self.data)\n            df.write.format(\"kafka\").options(**self.options).save()\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self) -&gt; DataFrame:\n        \"\"\"\n        Reads streaming data from Kafka.\n        \"\"\"\n        try:\n            df = self._transform_to_eventhub_schema(self.data)\n            TRIGGER_OPTION = (\n                {\"availableNow\": True}\n                if self.trigger == \"availableNow\"\n                else {\"processingTime\": self.trigger}\n            )\n            query = (\n                df.writeStream.trigger(**TRIGGER_OPTION)\n                .format(\"kafka\")\n                .options(**self.options)\n                .queryName(self.query_name)\n                .start()\n            )\n\n            if self.query_wait_interval:\n                while query.isActive:\n                    if query.lastProgress:\n                        logging.info(query.lastProgress)\n                    time.sleep(self.query_wait_interval)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kafka_eventhub/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kafka_eventhub.SparkKafkaEventhubDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kafka_eventhub.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kafka_eventhub/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kafka_eventhub.SparkKafkaEventhubDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Reads batch data from Kafka.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kafka_eventhub.py</code> <pre><code>def write_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Reads batch data from Kafka.\n    \"\"\"\n    try:\n        df = self._transform_to_eventhub_schema(self.data)\n        df.write.format(\"kafka\").options(**self.options).save()\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kafka_eventhub/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kafka_eventhub.SparkKafkaEventhubDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Reads streaming data from Kafka.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kafka_eventhub.py</code> <pre><code>def write_stream(self) -&gt; DataFrame:\n    \"\"\"\n    Reads streaming data from Kafka.\n    \"\"\"\n    try:\n        df = self._transform_to_eventhub_schema(self.data)\n        TRIGGER_OPTION = (\n            {\"availableNow\": True}\n            if self.trigger == \"availableNow\"\n            else {\"processingTime\": self.trigger}\n        )\n        query = (\n            df.writeStream.trigger(**TRIGGER_OPTION)\n            .format(\"kafka\")\n            .options(**self.options)\n            .queryName(self.query_name)\n            .start()\n        )\n\n        if self.query_wait_interval:\n            while query.isActive:\n                if query.lastProgress:\n                    logging.info(query.lastProgress)\n                time.sleep(self.query_wait_interval)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kinesis/","title":"Write to Kinesis","text":""},{"location":"sdk/code-reference/pipelines/destinations/spark/kinesis/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kinesis.SparkKinesisDestination","title":"<code>SparkKinesisDestination</code>","text":"<p>               Bases: <code>DestinationInterface</code></p> <p>This Kinesis destination class is used to write batch or streaming data to Kinesis. Kinesis configurations need to be specified as options in a dictionary.</p>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kinesis/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kinesis.SparkKinesisDestination--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.destinations import SparkKinesisDestination\n\nkinesis_destination = SparkKinesisDestination(\n    data=df,\n    options={\n        \"endpointUrl\": \"https://kinesis.{REGION}.amazonaws.com\",\n        \"awsAccessKey\": \"{YOUR-AWS-ACCESS-KEY}\",\n        \"awsSecretKey\": \"{YOUR-AWS-SECRET-KEY}\",\n        \"streamName\": \"{YOUR-STREAM-NAME}\"\n    },\n    mode=\"update\",\n    trigger=\"10 seconds\",\n    query_name=\"KinesisDestination\",\n    query_wait_interval=None\n)\n\nkinesis_destination.write_stream()\n\nOR\n\nkinesis_destination.write_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to be written to Delta</p> required <code>options</code> <code>dict</code> <p>A dictionary of Kinesis configurations (See Attributes table below). All Configuration options for Kinesis can be found here.</p> required <code>mode</code> <code>str</code> <p>Method of writing to Kinesis - append, complete, update</p> <code>'update'</code> <code>trigger</code> <code>optional str</code> <p>Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds</p> <code>'10 seconds'</code> <code>query_name</code> <code>str</code> <p>Unique name for the query in associated SparkSession</p> <code>'KinesisDestination'</code> <code>query_wait_interval</code> <code>optional int</code> <p>If set, waits for the streaming query to complete before returning. (stream) Default is None</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>endpointUrl</code> <code>str</code> <p>Endpoint of the kinesis stream.</p> <code>awsAccessKey</code> <code>str</code> <p>AWS access key.</p> <code>awsSecretKey</code> <code>str</code> <p>AWS secret access key corresponding to the access key.</p> <code>streamName</code> <code>List[str]</code> <p>Name of the streams in Kinesis to write to.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kinesis.py</code> <pre><code>class SparkKinesisDestination(DestinationInterface):\n    \"\"\"\n    This Kinesis destination class is used to write batch or streaming data to Kinesis. Kinesis configurations need to be specified as options in a dictionary.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.destinations import SparkKinesisDestination\n\n    kinesis_destination = SparkKinesisDestination(\n        data=df,\n        options={\n            \"endpointUrl\": \"https://kinesis.{REGION}.amazonaws.com\",\n            \"awsAccessKey\": \"{YOUR-AWS-ACCESS-KEY}\",\n            \"awsSecretKey\": \"{YOUR-AWS-SECRET-KEY}\",\n            \"streamName\": \"{YOUR-STREAM-NAME}\"\n        },\n        mode=\"update\",\n        trigger=\"10 seconds\",\n        query_name=\"KinesisDestination\",\n        query_wait_interval=None\n    )\n\n    kinesis_destination.write_stream()\n\n    OR\n\n    kinesis_destination.write_batch()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe to be written to Delta\n        options (dict): A dictionary of Kinesis configurations (See Attributes table below). All Configuration options for Kinesis can be found [here.](https://github.com/qubole/kinesis-sql#kinesis-sink-configuration){ target=\"_blank\" }\n        mode (str): Method of writing to Kinesis - append, complete, update\n        trigger (optional str): Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds\n        query_name (str): Unique name for the query in associated SparkSession\n        query_wait_interval (optional int): If set, waits for the streaming query to complete before returning. (stream) Default is None\n\n    Attributes:\n        endpointUrl (str): Endpoint of the kinesis stream.\n        awsAccessKey (str): AWS access key.\n        awsSecretKey (str): AWS secret access key corresponding to the access key.\n        streamName (List[str]): Name of the streams in Kinesis to write to.\n    \"\"\"\n\n    data: DataFrame\n    options: dict\n    mode: str\n    trigger: str\n    query_name: str\n    query_wait_interval: int\n\n    def __init__(\n        self,\n        data: DataFrame,\n        options: dict,\n        mode: str = \"update\",\n        trigger: str = \"10 seconds\",\n        query_name=\"KinesisDestination\",\n        query_wait_interval: int = None,\n    ) -&gt; None:\n        self.data = data\n        self.options = options\n        self.mode = mode\n        self.trigger = trigger\n        self.query_name = query_name\n        self.query_wait_interval = query_wait_interval\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK_DATABRICKS\n        \"\"\"\n        return SystemType.PYSPARK_DATABRICKS\n\n    @staticmethod\n    def libraries():\n        spark_libraries = Libraries()\n        return spark_libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def write_batch(self):\n        \"\"\"\n        Writes batch data to Kinesis.\n        \"\"\"\n        try:\n            return self.data.write.format(\"kinesis\").options(**self.options).save()\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self):\n        \"\"\"\n        Writes steaming data to Kinesis.\n        \"\"\"\n        try:\n            TRIGGER_OPTION = (\n                {\"availableNow\": True}\n                if self.trigger == \"availableNow\"\n                else {\"processingTime\": self.trigger}\n            )\n            query = (\n                self.data.writeStream.trigger(**TRIGGER_OPTION)\n                .format(\"kinesis\")\n                .outputMode(self.mode)\n                .options(**self.options)\n                .queryName(self.query_name)\n                .start()\n            )\n\n            if self.query_wait_interval:\n                while query.isActive:\n                    if query.lastProgress:\n                        logging.info(query.lastProgress)\n                    time.sleep(self.query_wait_interval)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kinesis/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kinesis.SparkKinesisDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK_DATABRICKS</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kinesis.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK_DATABRICKS\n    \"\"\"\n    return SystemType.PYSPARK_DATABRICKS\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kinesis/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kinesis.SparkKinesisDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Writes batch data to Kinesis.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kinesis.py</code> <pre><code>def write_batch(self):\n    \"\"\"\n    Writes batch data to Kinesis.\n    \"\"\"\n    try:\n        return self.data.write.format(\"kinesis\").options(**self.options).save()\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/kinesis/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.kinesis.SparkKinesisDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Writes steaming data to Kinesis.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/kinesis.py</code> <pre><code>def write_stream(self):\n    \"\"\"\n    Writes steaming data to Kinesis.\n    \"\"\"\n    try:\n        TRIGGER_OPTION = (\n            {\"availableNow\": True}\n            if self.trigger == \"availableNow\"\n            else {\"processingTime\": self.trigger}\n        )\n        query = (\n            self.data.writeStream.trigger(**TRIGGER_OPTION)\n            .format(\"kinesis\")\n            .outputMode(self.mode)\n            .options(**self.options)\n            .queryName(self.query_name)\n            .start()\n        )\n\n        if self.query_wait_interval:\n            while query.isActive:\n                if query.lastProgress:\n                    logging.info(query.lastProgress)\n                time.sleep(self.query_wait_interval)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/pcdm_latest_to_delta/","title":"Write Process Control Data Model Latest Values to Delta","text":""},{"location":"sdk/code-reference/pipelines/destinations/spark/pcdm_latest_to_delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.pcdm_latest_to_delta.SparkPCDMLatestToDeltaDestination","title":"<code>SparkPCDMLatestToDeltaDestination</code>","text":"<p>               Bases: <code>DestinationInterface</code></p> <p>The Process Control Data Model Latest Values written to Delta.</p>"},{"location":"sdk/code-reference/pipelines/destinations/spark/pcdm_latest_to_delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.pcdm_latest_to_delta.SparkPCDMLatestToDeltaDestination--example","title":"Example","text":"<p><pre><code>#PCDM Latest To Delta Destination for Streaming Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkPCDMLatestToDeltaDestination\n\npcdm_latest_to_delta_destination = SparkPCDMLatestToDeltaDestination(\n    data=df,\n    options={\n        \"checkpointLocation\": \"{/CHECKPOINT-LOCATION/}\"\n    },\n    destination=\"{DELTA_TABLE_PATH}\",\n    mode=\"append\",\n    trigger=\"10 seconds\",\n    query_name=\"PCDMLatestToDeltaDestination\",\n    query_wait_interval=None\n)\n\npcdm_latest_to_delta_destination.write_stream()\n</code></pre> <pre><code>#PCDM Latest To Delta Destination for Batch Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkPCDMLatestToDeltaDestination\n\npcdm_latest_to_delta_destination = SparkPCDMLatestToDeltaDestination(\n    data=df,\n    options={\n        \"maxRecordsPerFile\", \"10000\"\n    },\n    destination=\"{DELTA_TABLE_PATH}\",\n    mode=\"overwrite\",\n    trigger=\"10 seconds\",\n    query_name=\"PCDMLatestToDeltaDestination\",\n    query_wait_interval=None\n)\n\npcdm_latest_to_delta_destination.write_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to be merged into a Delta Table</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for batch and streaming.</p> required <code>destination</code> <code>str</code> <p>Either the name of the Hive Metastore or Unity Catalog Delta Table or the path to the Delta table to store the latest values</p> required <code>mode</code> <code>str</code> <p>Method of writing to Delta Table - append/overwrite (batch), append/complete (stream)</p> <code>None</code> <code>trigger</code> <code>optional str</code> <p>Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds</p> <code>'10 seconds'</code> <code>query_name</code> <code>str</code> <p>Unique name for the query in associated SparkSession</p> <code>'PCDMLatestToDeltaDestination'</code> <code>query_wait_interval</code> <code>optional int</code> <p>If set, waits for the streaming query to complete before returning. (stream) Default is None</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>checkpointLocation</code> <code>str</code> <p>Path to checkpoint files. (Streaming)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/pcdm_latest_to_delta.py</code> <pre><code>class SparkPCDMLatestToDeltaDestination(DestinationInterface):\n    \"\"\"\n    The Process Control Data Model Latest Values written to Delta.\n\n    Example\n    --------\n    ```python\n    #PCDM Latest To Delta Destination for Streaming Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkPCDMLatestToDeltaDestination\n\n    pcdm_latest_to_delta_destination = SparkPCDMLatestToDeltaDestination(\n        data=df,\n        options={\n            \"checkpointLocation\": \"{/CHECKPOINT-LOCATION/}\"\n        },\n        destination=\"{DELTA_TABLE_PATH}\",\n        mode=\"append\",\n        trigger=\"10 seconds\",\n        query_name=\"PCDMLatestToDeltaDestination\",\n        query_wait_interval=None\n    )\n\n    pcdm_latest_to_delta_destination.write_stream()\n    ```\n    ```python\n    #PCDM Latest To Delta Destination for Batch Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkPCDMLatestToDeltaDestination\n\n    pcdm_latest_to_delta_destination = SparkPCDMLatestToDeltaDestination(\n        data=df,\n        options={\n            \"maxRecordsPerFile\", \"10000\"\n        },\n        destination=\"{DELTA_TABLE_PATH}\",\n        mode=\"overwrite\",\n        trigger=\"10 seconds\",\n        query_name=\"PCDMLatestToDeltaDestination\",\n        query_wait_interval=None\n    )\n\n    pcdm_latest_to_delta_destination.write_batch()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe to be merged into a Delta Table\n        options (dict): Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for [batch](https://docs.delta.io/latest/delta-batch.html#write-to-a-table){ target=\"_blank\" } and [streaming](https://docs.delta.io/latest/delta-streaming.html#delta-table-as-a-sink){ target=\"_blank\" }.\n        destination (str): Either the name of the Hive Metastore or Unity Catalog Delta Table **or** the path to the Delta table to store the latest values\n        mode (str): Method of writing to Delta Table - append/overwrite (batch), append/complete (stream)\n        trigger (optional str): Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds\n        query_name (str): Unique name for the query in associated SparkSession\n        query_wait_interval (optional int): If set, waits for the streaming query to complete before returning. (stream) Default is None\n\n    Attributes:\n        checkpointLocation (str): Path to checkpoint files. (Streaming)\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    options: dict\n    destination: str\n    mode: str\n    trigger: str\n    query_name: str\n    query_wait_interval: int\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n        options: dict,\n        destination: str,\n        mode: str = None,\n        trigger=\"10 seconds\",\n        query_name: str = \"PCDMLatestToDeltaDestination\",\n        query_wait_interval: int = None,\n    ) -&gt; None:\n        self.spark = spark\n        self.data = data\n        self.destination = destination\n        self.options = options\n        self.mode = mode\n        self.trigger = trigger\n        self.query_name = query_name\n        self.query_wait_interval = query_wait_interval\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_core\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def _write_latest_to_delta(self, df: DataFrame, epoch_id=None):  # NOSONAR\n        df.persist()\n\n        latest_df = (\n            df.withColumn(\n                \"Latest\",\n                max(struct(\"EventTime\", \"Status\")).over(Window.partitionBy(\"TagName\")),\n            )\n            .withColumn(\n                \"GoodLatest\",\n                when(\n                    col(\"Latest.Status\") == \"Good\",\n                    struct(col(\"EventTime\"), col(\"Value\"), col(\"ValueType\")),\n                ).otherwise(\n                    max(\n                        when(\n                            col(\"Status\") == \"Good\",\n                            struct(\"EventTime\", \"Value\", \"ValueType\"),\n                        )\n                    ).over(Window.partitionBy(\"TagName\"))\n                ),\n            )\n            .filter(col(\"EventTime\") == col(\"Latest.EventTime\"))\n            .drop(\"Latest\")\n            .dropDuplicates([\"TagName\"])\n        )\n\n        when_matched_update_list = [\n            DeltaMergeConditionValues(\n                condition=\"source.EventTime &gt; target.EventTime AND (source.GoodLatest.EventTime IS NULL OR source.GoodLatest.EventTime &lt;= target.GoodEventTime)\",\n                values={\n                    \"EventTime\": \"source.EventTime\",\n                    \"Status\": \"source.Status\",\n                    \"Value\": \"source.Value\",\n                    \"ValueType\": \"source.ValueType\",\n                },\n            ),\n            DeltaMergeConditionValues(\n                condition=\"source.EventTime &gt; target.EventTime AND (source.GoodLatest.EventTime IS NOT NULL AND (source.GoodLatest.EventTime &gt; target.GoodEventTime OR target.GoodEventTime IS NULL))\",\n                values={\n                    \"EventTime\": \"source.EventTime\",\n                    \"Status\": \"source.Status\",\n                    \"Value\": \"source.Value\",\n                    \"ValueType\": \"source.ValueType\",\n                    \"GoodEventTime\": \"source.GoodLatest.EventTime\",\n                    \"GoodValue\": \"source.GoodLatest.Value\",\n                    \"GoodValueType\": \"source.GoodLatest.ValueType\",\n                },\n            ),\n            DeltaMergeConditionValues(\n                condition=\"source.EventTime &lt;= target.EventTime AND (source.GoodLatest.EventTime IS NOT NULL AND (source.GoodLatest.EventTime &gt; target.GoodEventTime OR target.GoodEventTime IS NULL))\",\n                values={\n                    \"GoodEventTime\": \"source.GoodLatest.EventTime\",\n                    \"GoodValue\": \"source.GoodLatest.Value\",\n                    \"GoodValueType\": \"source.GoodLatest.ValueType\",\n                },\n            ),\n        ]\n\n        when_not_matched_insert_list = [\n            DeltaMergeConditionValues(\n                values={\n                    \"TagName\": \"source.TagName\",\n                    \"EventTime\": \"source.EventTime\",\n                    \"Status\": \"source.Status\",\n                    \"Value\": \"source.Value\",\n                    \"ValueType\": \"source.ValueType\",\n                    \"GoodEventTime\": \"source.GoodLatest.EventTime\",\n                    \"GoodValue\": \"source.GoodLatest.Value\",\n                    \"GoodValueType\": \"source.GoodLatest.ValueType\",\n                },\n            )\n        ]\n\n        merge_condition = \"source.TagName = target.TagName\"\n\n        SparkDeltaMergeDestination(\n            spark=self.spark,\n            data=latest_df,\n            destination=self.destination,\n            options=self.options,\n            merge_condition=merge_condition,\n            when_matched_update_list=when_matched_update_list,\n            when_not_matched_insert_list=when_not_matched_insert_list,\n            trigger=self.trigger,\n            query_name=self.query_name,\n        ).write_batch()\n\n        df.unpersist()\n\n    def write_batch(self):\n        \"\"\"\n        Writes Process Control Data Model data to Delta\n        \"\"\"\n        try:\n            self._write_latest_to_delta(self.data)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self):\n        \"\"\"\n        Writes streaming Process Control Data Model data to Delta using foreachBatch\n        \"\"\"\n        try:\n            TRIGGER_OPTION = (\n                {\"availableNow\": True}\n                if self.trigger == \"availableNow\"\n                else {\"processingTime\": self.trigger}\n            )\n\n            query = (\n                self.data.writeStream.trigger(**TRIGGER_OPTION)\n                .format(\"delta\")\n                .foreachBatch(self._write_latest_to_delta)\n                .queryName(self.query_name)\n                .outputMode(\"append\")\n                .options(**self.options)\n                .start()\n            )\n\n            if self.query_wait_interval:\n                while query.isActive:\n                    if query.lastProgress:\n                        logging.info(query.lastProgress)\n                    time.sleep(self.query_wait_interval)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/pcdm_latest_to_delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.pcdm_latest_to_delta.SparkPCDMLatestToDeltaDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/pcdm_latest_to_delta.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/pcdm_latest_to_delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.pcdm_latest_to_delta.SparkPCDMLatestToDeltaDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Writes Process Control Data Model data to Delta</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/pcdm_latest_to_delta.py</code> <pre><code>def write_batch(self):\n    \"\"\"\n    Writes Process Control Data Model data to Delta\n    \"\"\"\n    try:\n        self._write_latest_to_delta(self.data)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/pcdm_latest_to_delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.pcdm_latest_to_delta.SparkPCDMLatestToDeltaDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Writes streaming Process Control Data Model data to Delta using foreachBatch</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/pcdm_latest_to_delta.py</code> <pre><code>def write_stream(self):\n    \"\"\"\n    Writes streaming Process Control Data Model data to Delta using foreachBatch\n    \"\"\"\n    try:\n        TRIGGER_OPTION = (\n            {\"availableNow\": True}\n            if self.trigger == \"availableNow\"\n            else {\"processingTime\": self.trigger}\n        )\n\n        query = (\n            self.data.writeStream.trigger(**TRIGGER_OPTION)\n            .format(\"delta\")\n            .foreachBatch(self._write_latest_to_delta)\n            .queryName(self.query_name)\n            .outputMode(\"append\")\n            .options(**self.options)\n            .start()\n        )\n\n        if self.query_wait_interval:\n            while query.isActive:\n                if query.lastProgress:\n                    logging.info(query.lastProgress)\n                time.sleep(self.query_wait_interval)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/pcdm_to_delta/","title":"Write Process Control Data Model to Delta","text":""},{"location":"sdk/code-reference/pipelines/destinations/spark/pcdm_to_delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.pcdm_to_delta.SparkPCDMToDeltaDestination","title":"<code>SparkPCDMToDeltaDestination</code>","text":"<p>               Bases: <code>DestinationInterface</code></p> <p>The Process Control Data Model written to Delta.</p>"},{"location":"sdk/code-reference/pipelines/destinations/spark/pcdm_to_delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.pcdm_to_delta.SparkPCDMToDeltaDestination--example","title":"Example","text":"<p><pre><code>#PCDM Latest To Delta Destination for Streaming Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkPCDMToDeltaDestination\n\npcdm_to_delta_destination = SparkPCDMToDeltaDestination(\n    data=df,\n    options={\n        \"checkpointLocation\": \"{/CHECKPOINT-LOCATION/}\"\n    },\n    destination_float=\"{DELTA_TABLE_PATH_FLOAT}\",\n    destination_string=\"{DELTA_TABLE_PATH_STRING}\",\n    destination_integer=\"{DELTA_TABLE_PATH_INTEGER}\",\n    mode=\"append\",\n    trigger=\"10 seconds\",\n    query_name=\"PCDMToDeltaDestination\",\n    query_wait_interval=None,\n    merge=True,\n    try_broadcast_join=False,\n    remove_nanoseconds=False,\n    remove_duplicates=True\n)\n\npcdm_to_delta_destination.write_stream()\n</code></pre> <pre><code>#PCDM Latest To Delta Destination for Batch Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkPCDMToDeltaDestination\n\npcdm_to_delta_destination = SparkPCDMToDeltaDestination(\n    data=df,\n    options={\n        \"maxRecordsPerFile\", \"10000\"\n    },\n    destination_float=\"{DELTA_TABLE_PATH_FLOAT}\",\n    destination_string=\"{DELTA_TABLE_PATH_STRING}\",\n    destination_integer=\"{DELTA_TABLE_PATH_INTEGER}\",\n    mode=\"overwrite\",\n    trigger=\"10 seconds\",\n    query_name=\"PCDMToDeltaDestination\",\n    query_wait_interval=None,\n    merge=True,\n    try_broadcast_join=False,\n    remove_nanoseconds=False,\n    remove_duplicates=True\n)\n\npcdm_to_delta_destination.write_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to be merged into a Delta Table</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for batch and streaming.</p> required <code>destination_float</code> <code>str</code> <p>Either the name of the Hive Metastore or Unity Catalog Delta Table or the path to the Delta table to store float values.</p> required <code>destination_string</code> <code>Optional str</code> <p>Either the name of the Hive Metastore or Unity Catalog Delta Table or the path to the Delta table to store string values.</p> <code>None</code> <code>destination_integer</code> <code>Optional str</code> <p>Either the name of the Hive Metastore or Unity Catalog Delta Table or the path to the Delta table to store integer values</p> <code>None</code> <code>mode</code> <code>str</code> <p>Method of writing to Delta Table - append/overwrite (batch), append/complete (stream)</p> <code>None</code> <code>trigger</code> <code>optional str</code> <p>Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds</p> <code>'10 seconds'</code> <code>query_name</code> <code>str</code> <p>Unique name for the query in associated SparkSession</p> <code>'PCDMToDeltaDestination'</code> <code>query_wait_interval</code> <code>optional int</code> <p>If set, waits for the streaming query to complete before returning. (stream) Default is None</p> <code>None</code> <code>merge</code> <code>bool</code> <p>Use Delta Merge to perform inserts, updates and deletes</p> <code>True</code> <code>try_broadcast_join</code> <code>bool</code> <p>Attempts to perform a broadcast join in the merge which can leverage data skipping using partition pruning and file pruning automatically. Can fail if dataframe being merged is large and therefore more suitable for streaming merges than batch merges</p> <code>False</code> <code>remove_nanoseconds</code> <code>bool</code> <p>Removes nanoseconds from the EventTime column and replaces with zeros</p> <code>False</code> <code>remove_duplicates</code> <code>bool</code> <p>Removes duplicates before writing the data</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>checkpointLocation</code> <code>str</code> <p>Path to checkpoint files. (Streaming)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/pcdm_to_delta.py</code> <pre><code>class SparkPCDMToDeltaDestination(DestinationInterface):\n    \"\"\"\n    The Process Control Data Model written to Delta.\n\n    Example\n    --------\n    ```python\n    #PCDM Latest To Delta Destination for Streaming Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkPCDMToDeltaDestination\n\n    pcdm_to_delta_destination = SparkPCDMToDeltaDestination(\n        data=df,\n        options={\n            \"checkpointLocation\": \"{/CHECKPOINT-LOCATION/}\"\n        },\n        destination_float=\"{DELTA_TABLE_PATH_FLOAT}\",\n        destination_string=\"{DELTA_TABLE_PATH_STRING}\",\n        destination_integer=\"{DELTA_TABLE_PATH_INTEGER}\",\n        mode=\"append\",\n        trigger=\"10 seconds\",\n        query_name=\"PCDMToDeltaDestination\",\n        query_wait_interval=None,\n        merge=True,\n        try_broadcast_join=False,\n        remove_nanoseconds=False,\n        remove_duplicates=True\n    )\n\n    pcdm_to_delta_destination.write_stream()\n    ```\n    ```python\n    #PCDM Latest To Delta Destination for Batch Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkPCDMToDeltaDestination\n\n    pcdm_to_delta_destination = SparkPCDMToDeltaDestination(\n        data=df,\n        options={\n            \"maxRecordsPerFile\", \"10000\"\n        },\n        destination_float=\"{DELTA_TABLE_PATH_FLOAT}\",\n        destination_string=\"{DELTA_TABLE_PATH_STRING}\",\n        destination_integer=\"{DELTA_TABLE_PATH_INTEGER}\",\n        mode=\"overwrite\",\n        trigger=\"10 seconds\",\n        query_name=\"PCDMToDeltaDestination\",\n        query_wait_interval=None,\n        merge=True,\n        try_broadcast_join=False,\n        remove_nanoseconds=False,\n        remove_duplicates=True\n    )\n\n    pcdm_to_delta_destination.write_batch()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe to be merged into a Delta Table\n        options (dict): Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for [batch](https://docs.delta.io/latest/delta-batch.html#write-to-a-table){ target=\"_blank\" } and [streaming](https://docs.delta.io/latest/delta-streaming.html#delta-table-as-a-sink){ target=\"_blank\" }.\n        destination_float (str): Either the name of the Hive Metastore or Unity Catalog Delta Table **or** the path to the Delta table to store float values.\n        destination_string (Optional str): Either the name of the Hive Metastore or Unity Catalog Delta Table **or** the path to the Delta table to store string values.\n        destination_integer (Optional str): Either the name of the Hive Metastore or Unity Catalog Delta Table **or** the path to the Delta table to store integer values\n        mode (str): Method of writing to Delta Table - append/overwrite (batch), append/complete (stream)\n        trigger (optional str): Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds\n        query_name (str): Unique name for the query in associated SparkSession\n        query_wait_interval (optional int): If set, waits for the streaming query to complete before returning. (stream) Default is None\n        merge (bool): Use Delta Merge to perform inserts, updates and deletes\n        try_broadcast_join (bool): Attempts to perform a broadcast join in the merge which can leverage data skipping using partition pruning and file pruning automatically. Can fail if dataframe being merged is large and therefore more suitable for streaming merges than batch merges\n        remove_nanoseconds (bool): Removes nanoseconds from the EventTime column and replaces with zeros\n        remove_duplicates (bool): Removes duplicates before writing the data\n\n    Attributes:\n        checkpointLocation (str): Path to checkpoint files. (Streaming)\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    options: dict\n    destination_float: str\n    destination_string: str\n    destination_integer: str\n    mode: str\n    trigger: str\n    query_name: str\n    query_wait_interval: int\n    merge: bool\n    try_broadcast_join: bool\n    remove_nanoseconds: bool\n    remove_duplicates: bool\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n        options: dict,\n        destination_float: str,\n        destination_string: str = None,\n        destination_integer: str = None,\n        mode: str = None,\n        trigger=\"10 seconds\",\n        query_name: str = \"PCDMToDeltaDestination\",\n        query_wait_interval: int = None,\n        merge: bool = True,\n        try_broadcast_join=False,\n        remove_nanoseconds: bool = False,\n        remove_duplicates: bool = True,\n    ) -&gt; None:\n        self.spark = spark\n        self.data = data\n        self.destination_float = destination_float\n        self.destination_string = destination_string\n        self.destination_integer = destination_integer\n        self.options = options\n        self.mode = mode\n        self.trigger = trigger\n        self.query_name = query_name\n        self.query_wait_interval = query_wait_interval\n        self.merge = merge\n        self.try_broadcast_join = try_broadcast_join\n        self.remove_nanoseconds = remove_nanoseconds\n        self.remove_duplicates = remove_duplicates\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_core\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def _get_eventdate_string(self, df: DataFrame) -&gt; str:\n        dates_df = df.select(\"EventDate\").distinct()\n        dates_df = dates_df.select(\n            date_format(\"EventDate\", \"yyyy-MM-dd\").alias(\"EventDate\")\n        )\n        dates_list = list(dates_df.toPandas()[\"EventDate\"])\n        return str(dates_list).replace(\"[\", \"\").replace(\"]\", \"\")\n\n    def _write_delta_merge(self, df: DataFrame, destination: str):\n        df = df.select(\n            \"EventDate\", \"TagName\", \"EventTime\", \"Status\", \"Value\", \"ChangeType\"\n        )\n        when_matched_update_list = [\n            DeltaMergeConditionValues(\n                condition=\"(source.ChangeType IN ('insert', 'update', 'upsert')) AND ((source.Status != target.Status) OR (source.Value != target.Value))\",\n                values={\n                    \"EventDate\": \"source.EventDate\",\n                    \"TagName\": \"source.TagName\",\n                    \"EventTime\": \"source.EventTime\",\n                    \"Status\": \"source.Status\",\n                    \"Value\": \"source.Value\",\n                },\n            )\n        ]\n        when_matched_delete_list = [\n            DeltaMergeCondition(condition=\"source.ChangeType = 'delete'\")\n        ]\n        when_not_matched_insert_list = [\n            DeltaMergeConditionValues(\n                condition=\"(source.ChangeType IN ('insert', 'update', 'upsert'))\",\n                values={\n                    \"EventDate\": \"source.EventDate\",\n                    \"TagName\": \"source.TagName\",\n                    \"EventTime\": \"source.EventTime\",\n                    \"Status\": \"source.Status\",\n                    \"Value\": \"source.Value\",\n                },\n            )\n        ]\n\n        merge_condition = \"source.EventDate = target.EventDate AND source.TagName = target.TagName AND source.EventTime = target.EventTime\"\n\n        perform_merge = True\n        if self.try_broadcast_join != True:\n            eventdate_string = self._get_eventdate_string(df)\n            if eventdate_string == None or eventdate_string == \"\":\n                perform_merge = False\n            else:\n                merge_condition = (\n                    \"target.EventDate in ({}) AND \".format(eventdate_string)\n                    + merge_condition\n                )\n\n        if perform_merge == True:\n            SparkDeltaMergeDestination(\n                spark=self.spark,\n                data=df,\n                destination=destination,\n                options=self.options,\n                merge_condition=merge_condition,\n                when_matched_update_list=when_matched_update_list,\n                when_matched_delete_list=when_matched_delete_list,\n                when_not_matched_insert_list=when_not_matched_insert_list,\n                try_broadcast_join=self.try_broadcast_join,\n                trigger=self.trigger,\n                query_name=self.query_name,\n            ).write_batch()\n\n    def _write_delta_batch(self, df: DataFrame, destination: str):\n        if self.merge == True:\n            if \"EventDate\" not in df.columns:\n                df = df.withColumn(\"EventDate\", date_format(\"EventTime\", \"yyyy-MM-dd\"))\n\n            self._write_delta_merge(\n                df.filter(col(\"ChangeType\").isin(\"insert\", \"update\", \"upsert\")),\n                destination,\n            )\n            self._write_delta_merge(\n                df.filter(col(\"ChangeType\") == \"delete\"), destination\n            )\n        else:\n            df = df.select(\"TagName\", \"EventTime\", \"Status\", \"Value\")\n            SparkDeltaDestination(\n                data=df,\n                destination=destination,\n                options=self.options,\n                mode=self.mode,\n                trigger=self.trigger,\n                query_name=self.query_name,\n            ).write_batch()\n\n    def _write_data_by_type(self, df: DataFrame):\n        if self.merge == True:\n            df = df.withColumn(\n                \"ChangeType\",\n                when(df[\"ChangeType\"].isin(\"insert\", \"update\"), \"upsert\").otherwise(\n                    df[\"ChangeType\"]\n                ),\n            )\n\n        if self.remove_nanoseconds == True:\n            df = df.withColumn(\n                \"EventTime\",\n                (floor(col(\"EventTime\").cast(\"double\") * 1000) / 1000).cast(\n                    \"timestamp\"\n                ),\n            )\n\n        if self.remove_duplicates == True:\n            df = df.drop_duplicates([\"TagName\", \"EventTime\", \"ChangeType\"])\n\n        float_df = df.filter(ValueTypeConstants.FLOAT_VALUE).withColumn(\n            \"Value\", col(\"Value\").cast(\"float\")\n        )\n        self._write_delta_batch(float_df, self.destination_float)\n\n        if self.destination_string != None:\n            string_df = df.filter(ValueTypeConstants.STRING_VALUE)\n            self._write_delta_batch(string_df, self.destination_string)\n\n        if self.destination_integer != None:\n            integer_df = df.filter(ValueTypeConstants.INTEGER_VALUE).withColumn(\n                \"Value\", col(\"Value\").cast(\"integer\")\n            )\n            self._write_delta_batch(integer_df, self.destination_integer)\n\n    def _write_stream_microbatches(self, df: DataFrame, epoch_id=None):  # NOSONAR\n        df.persist()\n        self._write_data_by_type(df)\n        df.unpersist()\n\n    def write_batch(self):\n        \"\"\"\n        Writes Process Control Data Model data to Delta\n        \"\"\"\n        try:\n            if self.try_broadcast_join != True:\n                self.data.persist()\n\n            self._write_data_by_type(self.data)\n\n            if self.try_broadcast_join != True:\n                self.data.unpersist()\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self):\n        \"\"\"\n        Writes streaming Process Control Data Model data to Delta using foreachBatch\n        \"\"\"\n        try:\n            TRIGGER_OPTION = (\n                {\"availableNow\": True}\n                if self.trigger == \"availableNow\"\n                else {\"processingTime\": self.trigger}\n            )\n            if self.merge == True:\n                query = (\n                    self.data.writeStream.trigger(**TRIGGER_OPTION)\n                    .format(\"delta\")\n                    .foreachBatch(self._write_stream_microbatches)\n                    .queryName(self.query_name)\n                    .outputMode(\"update\")\n                    .options(**self.options)\n                    .start()\n                )\n            else:\n                default_checkpoint_location = None\n                float_checkpoint_location = None\n                string_checkpoint_location = None\n                integer_checkpoint_location = None\n\n                append_options = self.options.copy()\n                if \"checkpointLocation\" in self.options:\n                    default_checkpoint_location = self.options[\"checkpointLocation\"]\n                    if default_checkpoint_location[-1] != \"/\":\n                        default_checkpoint_location += \"/\"\n                    float_checkpoint_location = default_checkpoint_location + \"float\"\n                    string_checkpoint_location = default_checkpoint_location + \"string\"\n                    integer_checkpoint_location = (\n                        default_checkpoint_location + \"integer\"\n                    )\n\n                if float_checkpoint_location is not None:\n                    append_options[\"checkpointLocation\"] = float_checkpoint_location\n\n                delta_float = SparkDeltaDestination(\n                    data=self.data.select(\"TagName\", \"EventTime\", \"Status\", \"Value\")\n                    .filter(ValueTypeConstants.FLOAT_VALUE)\n                    .withColumn(\"Value\", col(\"Value\").cast(\"float\")),\n                    destination=self.destination_float,\n                    options=append_options,\n                    mode=self.mode,\n                    trigger=self.trigger,\n                    query_name=self.query_name + \"_float\",\n                )\n\n                delta_float.write_stream()\n\n                if self.destination_string != None:\n                    if string_checkpoint_location is not None:\n                        append_options[\"checkpointLocation\"] = (\n                            string_checkpoint_location\n                        )\n\n                    delta_string = SparkDeltaDestination(\n                        data=self.data.select(\n                            \"TagName\", \"EventTime\", \"Status\", \"Value\"\n                        ).filter(ValueTypeConstants.STRING_VALUE),\n                        destination=self.destination_string,\n                        options=append_options,\n                        mode=self.mode,\n                        trigger=self.trigger,\n                        query_name=self.query_name + \"_string\",\n                    )\n\n                    delta_string.write_stream()\n\n                if self.destination_integer != None:\n                    if integer_checkpoint_location is not None:\n                        append_options[\"checkpointLocation\"] = (\n                            integer_checkpoint_location\n                        )\n\n                    delta_integer = SparkDeltaDestination(\n                        data=self.data.select(\"TagName\", \"EventTime\", \"Status\", \"Value\")\n                        .filter(ValueTypeConstants.INTEGER_VALUE)\n                        .withColumn(\"Value\", col(\"Value\").cast(\"integer\")),\n                        destination=self.destination_integer,\n                        options=append_options,\n                        mode=self.mode,\n                        trigger=self.trigger,\n                        query_name=self.query_name + \"_integer\",\n                    )\n\n                    delta_integer.write_stream()\n\n                if self.query_wait_interval:\n                    while self.spark.streams.active != []:\n                        for query in self.spark.streams.active:\n                            if query.lastProgress:\n                                logging.info(\n                                    \"{}: {}\".format(query.name, query.lastProgress)\n                                )\n                        time.sleep(self.query_wait_interval)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/pcdm_to_delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.pcdm_to_delta.SparkPCDMToDeltaDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/pcdm_to_delta.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/pcdm_to_delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.pcdm_to_delta.SparkPCDMToDeltaDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Writes Process Control Data Model data to Delta</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/pcdm_to_delta.py</code> <pre><code>def write_batch(self):\n    \"\"\"\n    Writes Process Control Data Model data to Delta\n    \"\"\"\n    try:\n        if self.try_broadcast_join != True:\n            self.data.persist()\n\n        self._write_data_by_type(self.data)\n\n        if self.try_broadcast_join != True:\n            self.data.unpersist()\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/pcdm_to_delta/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.pcdm_to_delta.SparkPCDMToDeltaDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Writes streaming Process Control Data Model data to Delta using foreachBatch</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/pcdm_to_delta.py</code> <pre><code>def write_stream(self):\n    \"\"\"\n    Writes streaming Process Control Data Model data to Delta using foreachBatch\n    \"\"\"\n    try:\n        TRIGGER_OPTION = (\n            {\"availableNow\": True}\n            if self.trigger == \"availableNow\"\n            else {\"processingTime\": self.trigger}\n        )\n        if self.merge == True:\n            query = (\n                self.data.writeStream.trigger(**TRIGGER_OPTION)\n                .format(\"delta\")\n                .foreachBatch(self._write_stream_microbatches)\n                .queryName(self.query_name)\n                .outputMode(\"update\")\n                .options(**self.options)\n                .start()\n            )\n        else:\n            default_checkpoint_location = None\n            float_checkpoint_location = None\n            string_checkpoint_location = None\n            integer_checkpoint_location = None\n\n            append_options = self.options.copy()\n            if \"checkpointLocation\" in self.options:\n                default_checkpoint_location = self.options[\"checkpointLocation\"]\n                if default_checkpoint_location[-1] != \"/\":\n                    default_checkpoint_location += \"/\"\n                float_checkpoint_location = default_checkpoint_location + \"float\"\n                string_checkpoint_location = default_checkpoint_location + \"string\"\n                integer_checkpoint_location = (\n                    default_checkpoint_location + \"integer\"\n                )\n\n            if float_checkpoint_location is not None:\n                append_options[\"checkpointLocation\"] = float_checkpoint_location\n\n            delta_float = SparkDeltaDestination(\n                data=self.data.select(\"TagName\", \"EventTime\", \"Status\", \"Value\")\n                .filter(ValueTypeConstants.FLOAT_VALUE)\n                .withColumn(\"Value\", col(\"Value\").cast(\"float\")),\n                destination=self.destination_float,\n                options=append_options,\n                mode=self.mode,\n                trigger=self.trigger,\n                query_name=self.query_name + \"_float\",\n            )\n\n            delta_float.write_stream()\n\n            if self.destination_string != None:\n                if string_checkpoint_location is not None:\n                    append_options[\"checkpointLocation\"] = (\n                        string_checkpoint_location\n                    )\n\n                delta_string = SparkDeltaDestination(\n                    data=self.data.select(\n                        \"TagName\", \"EventTime\", \"Status\", \"Value\"\n                    ).filter(ValueTypeConstants.STRING_VALUE),\n                    destination=self.destination_string,\n                    options=append_options,\n                    mode=self.mode,\n                    trigger=self.trigger,\n                    query_name=self.query_name + \"_string\",\n                )\n\n                delta_string.write_stream()\n\n            if self.destination_integer != None:\n                if integer_checkpoint_location is not None:\n                    append_options[\"checkpointLocation\"] = (\n                        integer_checkpoint_location\n                    )\n\n                delta_integer = SparkDeltaDestination(\n                    data=self.data.select(\"TagName\", \"EventTime\", \"Status\", \"Value\")\n                    .filter(ValueTypeConstants.INTEGER_VALUE)\n                    .withColumn(\"Value\", col(\"Value\").cast(\"integer\")),\n                    destination=self.destination_integer,\n                    options=append_options,\n                    mode=self.mode,\n                    trigger=self.trigger,\n                    query_name=self.query_name + \"_integer\",\n                )\n\n                delta_integer.write_stream()\n\n            if self.query_wait_interval:\n                while self.spark.streams.active != []:\n                    for query in self.spark.streams.active:\n                        if query.lastProgress:\n                            logging.info(\n                                \"{}: {}\".format(query.name, query.lastProgress)\n                            )\n                    time.sleep(self.query_wait_interval)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/rest_api/","title":"Write to Rest API","text":""},{"location":"sdk/code-reference/pipelines/destinations/spark/rest_api/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.rest_api.SparkRestAPIDestination","title":"<code>SparkRestAPIDestination</code>","text":"<p>               Bases: <code>DestinationInterface</code></p> <p>The Spark Rest API Destination is used to write data to a Rest API.</p> <p>The payload sent to the API is constructed by converting each row in the DataFrame to Json.</p> <p>Note</p> <p>While it is possible to use the <code>write_batch</code> method, it is easy to overwhlem a Rest API with large volumes of data. Consider reducing data volumes when writing to a Rest API in Batch mode to prevent API errors including throtting.</p>"},{"location":"sdk/code-reference/pipelines/destinations/spark/rest_api/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.rest_api.SparkRestAPIDestination--example","title":"Example","text":"<p><pre><code>#Rest API Destination for Streaming Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkRestAPIDestination\n\nrest_api_destination = SparkRestAPIDestination(\n    data=df,\n    options={\n        \"checkpointLocation\": \"{/CHECKPOINT-LOCATION/}\"\n    },\n    url=\"{REST-API-URL}\",\n    headers = {\n        'Authorization': 'Bearer {}'.format(\"{TOKEN}\")\n    },\n    batch_size=100,\n    method=\"POST\",\n    parallelism=8,\n    trigger=\"1 minute\",\n    query_name=\"DeltaRestAPIDestination\",\n    query_wait_interval=None\n)\n\nrest_api_destination.write_stream()\n</code></pre> <pre><code>#Rest API Destination for Batch Queries\n\nfrom rtdip_sdk.pipelines.destinations import SparkRestAPIDestination\n\nrest_api_destination = SparkRestAPIDestination(\n    data=df,\n    options={},\n    url=\"{REST-API-URL}\",\n    headers = {\n        'Authorization': 'Bearer {}'.format(\"{TOKEN}\")\n    },\n    batch_size=10,\n    method=\"POST\",\n    parallelism=4,\n    trigger=\"1 minute\",\n    query_name=\"DeltaRestAPIDestination\",\n    query_wait_interval=None\n)\n\nrest_api_destination.write_stream()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to be merged into a Delta Table</p> required <code>options</code> <code>dict</code> <p>A dictionary of options for streaming writes</p> required <code>url</code> <code>str</code> <p>The Rest API Url</p> required <code>headers</code> <code>dict</code> <p>A dictionary of headers to be provided to the Rest API</p> required <code>batch_size</code> <code>int</code> <p>The number of DataFrame rows to be used in each Rest API call</p> required <code>method</code> <code>str</code> <p>The method to be used when calling the Rest API. Allowed values are POST, PATCH and PUT</p> <code>'POST'</code> <code>parallelism</code> <code>int</code> <p>The number of concurrent calls to be made to the Rest API</p> <code>8</code> <code>trigger</code> <code>optional str</code> <p>Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds</p> <code>'1 minutes'</code> <code>query_name</code> <code>str</code> <p>Unique name for the query in associated SparkSession</p> <code>'DeltaRestAPIDestination'</code> <code>query_wait_interval</code> <code>optional int</code> <p>If set, waits for the streaming query to complete before returning. (stream) Default is None</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>checkpointLocation</code> <code>str</code> <p>Path to checkpoint files. (Streaming)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/rest_api.py</code> <pre><code>class SparkRestAPIDestination(DestinationInterface):\n    \"\"\"\n    The Spark Rest API Destination is used to write data to a Rest API.\n\n    The payload sent to the API is constructed by converting each row in the DataFrame to Json.\n\n    !!! Note\n        While it is possible to use the `write_batch` method, it is easy to overwhlem a Rest API with large volumes of data.\n        Consider reducing data volumes when writing to a Rest API in Batch mode to prevent API errors including throtting.\n\n    Example\n    --------\n    ```python\n    #Rest API Destination for Streaming Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkRestAPIDestination\n\n    rest_api_destination = SparkRestAPIDestination(\n        data=df,\n        options={\n            \"checkpointLocation\": \"{/CHECKPOINT-LOCATION/}\"\n        },\n        url=\"{REST-API-URL}\",\n        headers = {\n            'Authorization': 'Bearer {}'.format(\"{TOKEN}\")\n        },\n        batch_size=100,\n        method=\"POST\",\n        parallelism=8,\n        trigger=\"1 minute\",\n        query_name=\"DeltaRestAPIDestination\",\n        query_wait_interval=None\n    )\n\n    rest_api_destination.write_stream()\n    ```\n    ```python\n    #Rest API Destination for Batch Queries\n\n    from rtdip_sdk.pipelines.destinations import SparkRestAPIDestination\n\n    rest_api_destination = SparkRestAPIDestination(\n        data=df,\n        options={},\n        url=\"{REST-API-URL}\",\n        headers = {\n            'Authorization': 'Bearer {}'.format(\"{TOKEN}\")\n        },\n        batch_size=10,\n        method=\"POST\",\n        parallelism=4,\n        trigger=\"1 minute\",\n        query_name=\"DeltaRestAPIDestination\",\n        query_wait_interval=None\n    )\n\n    rest_api_destination.write_stream()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe to be merged into a Delta Table\n        options (dict): A dictionary of options for streaming writes\n        url (str): The Rest API Url\n        headers (dict): A dictionary of headers to be provided to the Rest API\n        batch_size (int): The number of DataFrame rows to be used in each Rest API call\n        method (str): The method to be used when calling the Rest API. Allowed values are POST, PATCH and PUT\n        parallelism (int): The number of concurrent calls to be made to the Rest API\n        trigger (optional str): Frequency of the write operation. Specify \"availableNow\" to execute a trigger once, otherwise specify a time period such as \"30 seconds\", \"5 minutes\". Set to \"0 seconds\" if you do not want to use a trigger. (stream) Default is 10 seconds\n        query_name (str): Unique name for the query in associated SparkSession\n        query_wait_interval (optional int): If set, waits for the streaming query to complete before returning. (stream) Default is None\n\n    Attributes:\n        checkpointLocation (str): Path to checkpoint files. (Streaming)\n    \"\"\"\n\n    data: DataFrame\n    options: dict\n    url: str\n    headers: dict\n    batch_size: int\n    method: str\n    parallelism: int\n    trigger: str\n    query_name: str\n    query_wait_interval: int\n\n    def __init__(\n        self,\n        data: DataFrame,\n        options: dict,\n        url: str,\n        headers: dict,\n        batch_size: int,\n        method: str = \"POST\",\n        parallelism: int = 8,\n        trigger=\"1 minutes\",\n        query_name: str = \"DeltaRestAPIDestination\",\n        query_wait_interval: int = None,\n    ) -&gt; None:\n        self.data = data\n        self.options = options\n        self.url = url\n        self.headers = headers\n        self.batch_size = batch_size\n        self.method = method\n        self.parallelism = parallelism\n        self.trigger = trigger\n        self.query_name = query_name\n        self.query_wait_interval = query_wait_interval\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_pypi_library(get_default_package(\"api_requests\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_write_validation(self):\n        return True\n\n    def post_write_validation(self):\n        return True\n\n    def _pre_batch_records_for_api_call(self, micro_batch_df: DataFrame):\n        batch_count = math.ceil(micro_batch_df.count() / self.batch_size)\n        micro_batch_df = (\n            micro_batch_df.withColumn(\"content\", to_json(struct(col(\"*\"))))\n            .withColumn(\"row_number\", row_number().over(Window().orderBy(lit(\"A\"))))\n            .withColumn(\"batch_id\", col(\"row_number\") % batch_count)\n        )\n        return micro_batch_df.groupBy(\"batch_id\").agg(\n            concat_ws(\",|\", collect_list(\"content\")).alias(\"payload\")\n        )\n\n    def _api_micro_batch(self, micro_batch_df: DataFrame, epoch_id=None):  # NOSONAR\n        url = self.url\n        method = self.method\n        headers = self.headers\n\n        @udf(\"string\")\n        def _rest_api_execute(data):\n            session = requests.Session()\n            adapter = HTTPAdapter(max_retries=3)\n            session.mount(\"http://\", adapter)  # NOSONAR\n            session.mount(\"https://\", adapter)\n\n            if method == \"POST\":\n                response = session.post(url, headers=headers, data=data, verify=False)\n            elif method == \"PATCH\":\n                response = session.patch(url, headers=headers, data=data, verify=False)\n            elif method == \"PUT\":\n                response = session.put(url, headers=headers, data=data, verify=False)\n            else:\n                raise Exception(\"Method {} is not supported\".format(method))  # NOSONAR\n\n            if not (response.status_code == 200 or response.status_code == 201):\n                raise Exception(\n                    \"Response status : {} .Response message : {}\".format(\n                        str(response.status_code), response.text\n                    )\n                )  # NOSONAR\n\n            return str(response.status_code)\n\n        micro_batch_df.persist()\n        micro_batch_df = self._pre_batch_records_for_api_call(micro_batch_df)\n\n        micro_batch_df = micro_batch_df.repartition(self.parallelism)\n\n        (\n            micro_batch_df.withColumn(\n                \"rest_api_response_code\", _rest_api_execute(micro_batch_df[\"payload\"])\n            ).collect()\n        )\n        micro_batch_df.unpersist()\n\n    def write_batch(self):\n        \"\"\"\n        Writes batch data to a Rest API\n        \"\"\"\n        try:\n            return self._api_micro_batch(self.data)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def write_stream(self):\n        \"\"\"\n        Writes streaming data to a Rest API\n        \"\"\"\n        try:\n            TRIGGER_OPTION = (\n                {\"availableNow\": True}\n                if self.trigger == \"availableNow\"\n                else {\"processingTime\": self.trigger}\n            )\n            query = (\n                self.data.writeStream.trigger(**TRIGGER_OPTION)\n                .foreachBatch(self._api_micro_batch)\n                .queryName(self.query_name)\n                .outputMode(\"update\")\n                .options(**self.options)\n                .start()\n            )\n\n            if self.query_wait_interval:\n                while query.isActive:\n                    if query.lastProgress:\n                        logging.info(query.lastProgress)\n                    time.sleep(self.query_wait_interval)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/rest_api/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.rest_api.SparkRestAPIDestination.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/rest_api.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/rest_api/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.rest_api.SparkRestAPIDestination.write_batch","title":"<code>write_batch()</code>","text":"<p>Writes batch data to a Rest API</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/rest_api.py</code> <pre><code>def write_batch(self):\n    \"\"\"\n    Writes batch data to a Rest API\n    \"\"\"\n    try:\n        return self._api_micro_batch(self.data)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/destinations/spark/rest_api/#src.sdk.python.rtdip_sdk.pipelines.destinations.spark.rest_api.SparkRestAPIDestination.write_stream","title":"<code>write_stream()</code>","text":"<p>Writes streaming data to a Rest API</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/destinations/spark/rest_api.py</code> <pre><code>def write_stream(self):\n    \"\"\"\n    Writes streaming data to a Rest API\n    \"\"\"\n    try:\n        TRIGGER_OPTION = (\n            {\"availableNow\": True}\n            if self.trigger == \"availableNow\"\n            else {\"processingTime\": self.trigger}\n        )\n        query = (\n            self.data.writeStream.trigger(**TRIGGER_OPTION)\n            .foreachBatch(self._api_micro_batch)\n            .queryName(self.query_name)\n            .outputMode(\"update\")\n            .options(**self.options)\n            .start()\n        )\n\n        if self.query_wait_interval:\n            while query.isActive:\n                if query.lastProgress:\n                    logging.info(query.lastProgress)\n                time.sleep(self.query_wait_interval)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/forecasting/spark/arima/","title":"Arima","text":""},{"location":"sdk/code-reference/pipelines/forecasting/spark/arima/#src.sdk.python.rtdip_sdk.pipelines.forecasting.spark.arima.ArimaPrediction","title":"<code>ArimaPrediction</code>","text":"<p>               Bases: <code>DataManipulationBaseInterface</code>, <code>InputValidator</code></p> <p>Extends the timeseries data in given DataFrame with forecasted values from an ARIMA model. It forecasts a value column of the given time series dataframe based on the historical data points and constructs full entries based on the preceding timestamps. It is advised to place this step after the missing value imputation to prevent learning on dirty data.</p> <p>It supports dataframes in a source-based format (where each row is an event by a single sensor) and column-based format (where each row is a point in time).</p> <p>The similar component AutoArimaPrediction wraps around this component and needs less manual parameters set.</p> <p>ARIMA-Specific parameters can be viewed at the following statsmodels documentation page: ARIMA Documentation</p>"},{"location":"sdk/code-reference/pipelines/forecasting/spark/arima/#src.sdk.python.rtdip_sdk.pipelines.forecasting.spark.arima.ArimaPrediction--example","title":"Example","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport numpy.random\nimport pandas\nfrom pyspark.sql import SparkSession\n\nfrom rtdip_sdk.pipelines.forecasting.spark.arima import ArimaPrediction\n\nimport rtdip_sdk.pipelines._pipeline_utils.spark as spark_utils\n\nspark_session = SparkSession.builder.master(\"local[2]\").appName(\"test\").getOrCreate()\ndf = pandas.DataFrame()\n\nnumpy.random.seed(0)\narr_len = 250\nh_a_l = int(arr_len / 2)\ndf['Value'] = np.random.rand(arr_len) + np.sin(np.linspace(0, arr_len / 10, num=arr_len))\ndf['Value2'] = np.random.rand(arr_len) + np.cos(np.linspace(0, arr_len / 2, num=arr_len)) + 5\ndf['index'] = np.asarray(pandas.date_range(start='1/1/2024', end='2/1/2024', periods=arr_len))\ndf = df.set_index(pandas.DatetimeIndex(df['index']))\n\nlearn_df = df.head(h_a_l)\n\n# plt.plot(df['Value'])\n# plt.show()\n\ninput_df = spark_session.createDataFrame(\n        learn_df,\n        ['Value', 'Value2', 'index'],\n)\narima_comp = ArimaPrediction(input_df, to_extend_name='Value', number_of_data_points_to_analyze=h_a_l, number_of_data_points_to_predict=h_a_l,\n                     order=(3,0,0), seasonal_order=(3,0,0,62))\nforecasted_df = arima_comp.filter_data().toPandas()\nprint('Done')\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>past_data</code> <code>DataFrame</code> <p>PySpark DataFrame which contains training data</p> required <code>to_extend_name</code> <code>str</code> <p>Column or source to forecast on</p> required <code>past_data_style</code> <code>InputStyle</code> <p>In which format is past_data formatted</p> <code>None</code> <code>value_name</code> <code>str</code> <p>Name of column in source-based format, where values are stored</p> <code>None</code> <code>timestamp_name</code> <code>str</code> <p>Name of column, where event timestamps are stored</p> <code>None</code> <code>source_name</code> <code>str</code> <p>Name of column in source-based format, where source of events are stored</p> <code>None</code> <code>status_name</code> <code>str</code> <p>Name of column in source-based format, where status of events are stored</p> <code>None</code> <code>external_regressor_names</code> <code>List[str]</code> <p>Currently not working. Names of the columns with data to use for prediction, but not extend</p> <code>None</code> <code>number_of_data_points_to_predict</code> <code>int</code> <p>Amount of points to forecast</p> <code>50</code> <code>number_of_data_points_to_analyze</code> <code>int</code> <p>Amount of most recent points to train on</p> <code>None</code> <code>order</code> <code>tuple</code> <p>ARIMA-Specific setting</p> <code>(0, 0, 0)</code> <code>seasonal_order</code> <code>tuple</code> <p>ARIMA-Specific setting</p> <code>(0, 0, 0, 0)</code> <code>trend</code> <code>str</code> <p>ARIMA-Specific setting</p> <code>None</code> <code>enforce_stationarity</code> <code>bool</code> <p>ARIMA-Specific setting</p> <code>True</code> <code>enforce_invertibility</code> <code>bool</code> <p>ARIMA-Specific setting</p> <code>True</code> <code>concentrate_scale</code> <code>bool</code> <p>ARIMA-Specific setting</p> <code>False</code> <code>trend_offset</code> <code>int</code> <p>ARIMA-Specific setting</p> <code>1</code> <code>missing</code> <code>str</code> <p>ARIMA-Specific setting</p> <code>'None'</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/forecasting/spark/arima.py</code> <pre><code>class ArimaPrediction(DataManipulationBaseInterface, InputValidator):\n    \"\"\"\n    Extends the timeseries data in given DataFrame with forecasted values from an ARIMA model.\n    It forecasts a value column of the given time series dataframe based on the historical data points and constructs\n    full entries based on the preceding timestamps. It is advised to place this step after the missing value imputation\n    to prevent learning on dirty data.\n\n    It supports dataframes in a source-based format (where each row is an event by a single sensor) and column-based format (where each row is a point in time).\n\n    The similar component AutoArimaPrediction wraps around this component and needs less manual parameters set.\n\n    ARIMA-Specific parameters can be viewed at the following statsmodels documentation page:\n    [ARIMA Documentation](https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMA.html)\n\n    Example\n    -------\n    ```python\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import numpy.random\n    import pandas\n    from pyspark.sql import SparkSession\n\n    from rtdip_sdk.pipelines.forecasting.spark.arima import ArimaPrediction\n\n    import rtdip_sdk.pipelines._pipeline_utils.spark as spark_utils\n\n    spark_session = SparkSession.builder.master(\"local[2]\").appName(\"test\").getOrCreate()\n    df = pandas.DataFrame()\n\n    numpy.random.seed(0)\n    arr_len = 250\n    h_a_l = int(arr_len / 2)\n    df['Value'] = np.random.rand(arr_len) + np.sin(np.linspace(0, arr_len / 10, num=arr_len))\n    df['Value2'] = np.random.rand(arr_len) + np.cos(np.linspace(0, arr_len / 2, num=arr_len)) + 5\n    df['index'] = np.asarray(pandas.date_range(start='1/1/2024', end='2/1/2024', periods=arr_len))\n    df = df.set_index(pandas.DatetimeIndex(df['index']))\n\n    learn_df = df.head(h_a_l)\n\n    # plt.plot(df['Value'])\n    # plt.show()\n\n    input_df = spark_session.createDataFrame(\n            learn_df,\n            ['Value', 'Value2', 'index'],\n    )\n    arima_comp = ArimaPrediction(input_df, to_extend_name='Value', number_of_data_points_to_analyze=h_a_l, number_of_data_points_to_predict=h_a_l,\n                         order=(3,0,0), seasonal_order=(3,0,0,62))\n    forecasted_df = arima_comp.filter_data().toPandas()\n    print('Done')\n    ```\n\n    Parameters:\n        past_data (PySparkDataFrame): PySpark DataFrame which contains training data\n        to_extend_name (str): Column or source to forecast on\n        past_data_style (InputStyle): In which format is past_data formatted\n        value_name (str): Name of column in source-based format, where values are stored\n        timestamp_name (str): Name of column, where event timestamps are stored\n        source_name (str): Name of column in source-based format, where source of events are stored\n        status_name (str): Name of column in source-based format, where status of events are stored\n        external_regressor_names (List[str]): Currently not working. Names of the columns with data to use for prediction, but not extend\n        number_of_data_points_to_predict (int): Amount of points to forecast\n        number_of_data_points_to_analyze (int): Amount of most recent points to train on\n        order (tuple): ARIMA-Specific setting\n        seasonal_order (tuple): ARIMA-Specific setting\n        trend (str): ARIMA-Specific setting\n        enforce_stationarity (bool): ARIMA-Specific setting\n        enforce_invertibility (bool): ARIMA-Specific setting\n        concentrate_scale (bool): ARIMA-Specific setting\n        trend_offset (int): ARIMA-Specific setting\n        missing (str): ARIMA-Specific setting\n    \"\"\"\n\n    df: PySparkDataFrame = None\n    pd_df: DataFrame = None\n    spark_session: SparkSession\n\n    column_to_predict: str\n    rows_to_predict: int\n    rows_to_analyze: int\n\n    value_name: str\n    timestamp_name: str\n    source_name: str\n    external_regressor_names: List[str]\n\n    class InputStyle(Enum):\n        \"\"\"\n        Used to describe style of a dataframe\n        \"\"\"\n\n        COLUMN_BASED = 1  # Schema: [EventTime, FirstSource, SecondSource, ...]\n        SOURCE_BASED = 2  # Schema: [EventTime, NameSource, Value, OptionalStatus]\n\n    def __init__(\n        self,\n        past_data: PySparkDataFrame,\n        to_extend_name: str,  # either source or column\n        # Metadata about past_date\n        past_data_style: InputStyle = None,\n        value_name: str = None,\n        timestamp_name: str = None,\n        source_name: str = None,\n        status_name: str = None,\n        # Options for ARIMA\n        external_regressor_names: List[str] = None,\n        number_of_data_points_to_predict: int = 50,\n        number_of_data_points_to_analyze: int = None,\n        order: tuple = (0, 0, 0),\n        seasonal_order: tuple = (0, 0, 0, 0),\n        trend=None,\n        enforce_stationarity: bool = True,\n        enforce_invertibility: bool = True,\n        concentrate_scale: bool = False,\n        trend_offset: int = 1,\n        missing: str = \"None\",\n    ) -&gt; None:\n        self.past_data = past_data\n        # Convert dataframe to general column-based format for internal processing\n        self._initialize_self_df(\n            past_data,\n            past_data_style,\n            source_name,\n            status_name,\n            timestamp_name,\n            to_extend_name,\n            value_name,\n        )\n\n        if number_of_data_points_to_analyze &gt; self.df.count():\n            raise ValueError(\n                \"Number of data points to analyze exceeds the number of rows present\"\n            )\n\n        self.spark_session = past_data.sparkSession\n        self.column_to_predict = to_extend_name\n        self.rows_to_predict = number_of_data_points_to_predict\n        self.rows_to_analyze = number_of_data_points_to_analyze or past_data.count()\n        self.order = order\n        self.seasonal_order = seasonal_order\n        self.trend = trend\n        self.enforce_stationarity = enforce_stationarity\n        self.enforce_invertibility = enforce_invertibility\n        self.concentrate_scale = concentrate_scale\n        self.trend_offset = trend_offset\n        self.missing = missing\n        self.external_regressor_names = external_regressor_names\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    @staticmethod\n    def _is_column_type(df, column_name, data_type):\n        \"\"\"\n        Helper method for data type checking\n        \"\"\"\n        type_ = df.schema[column_name]\n\n        return isinstance(type_.dataType, data_type)\n\n    def _initialize_self_df(\n        self,\n        past_data,\n        past_data_style,\n        source_name,\n        status_name,\n        timestamp_name,\n        to_extend_name,\n        value_name,\n    ):\n        # Initialize self.df with meta parameters if not already done by previous constructor\n        if self.df is None:\n            (\n                self.past_data_style,\n                self.value_name,\n                self.timestamp_name,\n                self.source_name,\n                self.status_name,\n            ) = self._constructor_handle_input_metadata(\n                past_data,\n                past_data_style,\n                value_name,\n                timestamp_name,\n                source_name,\n                status_name,\n            )\n\n            if self.past_data_style == self.InputStyle.COLUMN_BASED:\n                self.df = past_data\n            elif self.past_data_style == self.InputStyle.SOURCE_BASED:\n                self.df = (\n                    past_data.groupby(self.timestamp_name)\n                    .pivot(self.source_name)\n                    .agg(F.first(self.value_name))\n                )\n        if not to_extend_name in self.df.columns:\n            raise ValueError(\"{} not found in the DataFrame.\".format(to_extend_name))\n\n    def _constructor_handle_input_metadata(\n        self,\n        past_data: PySparkDataFrame,\n        past_data_style: InputStyle,\n        value_name: str,\n        timestamp_name: str,\n        source_name: str,\n        status_name: str,\n    ) -&gt; Tuple[InputStyle, str, str, str, str]:\n        # Infer names of columns from past_data schema. If nothing is found, leave self parameters at None.\n        if past_data_style is not None:\n            return past_data_style, value_name, timestamp_name, source_name, status_name\n        # Automatic calculation part\n        schema_names = past_data.schema.names.copy()\n\n        assumed_past_data_style = None\n        value_name = None\n        timestamp_name = None\n        source_name = None\n        status_name = None\n\n        def pickout_column(\n            rem_columns: List[str], regex_string: str\n        ) -&gt; (str, List[str]):\n            rgx = regex.compile(regex_string)\n            sus_columns = list(filter(rgx.search, rem_columns))\n            found_column = sus_columns[0] if len(sus_columns) == 1 else None\n            return found_column\n\n        # Is there a status column?\n        status_name = pickout_column(schema_names, r\"(?i)status\")\n        # Is there a source name / tag\n        source_name = pickout_column(schema_names, r\"(?i)tag\")\n        # Is there a timestamp column?\n        timestamp_name = pickout_column(schema_names, r\"(?i)time|index\")\n        # Is there a value column?\n        value_name = pickout_column(schema_names, r\"(?i)value\")\n\n        if source_name is not None:\n            assumed_past_data_style = self.InputStyle.SOURCE_BASED\n        else:\n            assumed_past_data_style = self.InputStyle.COLUMN_BASED\n\n        # if self.past_data_style is None:\n        #    raise ValueError(\n        #        \"Automatic determination of past_data_style failed, must be specified in parameter instead.\")\n        return (\n            assumed_past_data_style,\n            value_name,\n            timestamp_name,\n            source_name,\n            status_name,\n        )\n\n    def filter_data(self) -&gt; PySparkDataFrame:\n        \"\"\"\n        Forecasts a value column of a given time series dataframe based on the historical data points using ARIMA.\n\n        Constructs full entries based on the preceding timestamps. It is advised to place this step after the missing\n        value imputation to prevent learning on dirty data.\n\n        Returns:\n            DataFrame: A PySpark DataFrame with forecasted value entries depending on constructor parameters.\n        \"\"\"\n        # expected_scheme = StructType(\n        #    [\n        #        StructField(\"TagName\", StringType(), True),\n        #        StructField(\"EventTime\", TimestampType(), True),\n        #        StructField(\"Status\", StringType(), True),\n        #        StructField(\"Value\", NumericType(), True),\n        #    ]\n        # )\n        pd_df = self.df.toPandas()\n        pd_df.loc[:, self.timestamp_name] = pd.to_datetime(\n            pd_df[self.timestamp_name], format=\"mixed\"\n        ).astype(\"datetime64[ns]\")\n        pd_df.loc[:, self.column_to_predict] = pd_df.loc[\n            :, self.column_to_predict\n        ].astype(float)\n        pd_df.sort_values(self.timestamp_name, inplace=True)\n        pd_df.reset_index(drop=True, inplace=True)\n        # self.validate(expected_scheme)\n\n        # limit df to specific data points\n        pd_to_train_on = pd_df[pd_df[self.column_to_predict].notna()].tail(\n            self.rows_to_analyze\n        )\n        pd_to_predict_on = pd_df[pd_df[self.column_to_predict].isna()].head(\n            self.rows_to_predict\n        )\n        pd_df = pd.concat([pd_to_train_on, pd_to_predict_on])\n\n        main_signal_df = pd_df[pd_df[self.column_to_predict].notna()]\n\n        input_data = main_signal_df[self.column_to_predict].astype(float)\n        exog_data = None\n        # if self.external_regressor_names is not None:\n        #     exog_data = []\n        #     for column_name in self.external_regressor_names:\n        #         signal_df = pd.concat([pd_to_train_on[column_name], pd_to_predict_on[column_name]])\n        #         exog_data.append(signal_df)\n\n        source_model = ARIMA(\n            endog=input_data,\n            exog=exog_data,\n            order=self.order,\n            seasonal_order=self.seasonal_order,\n            trend=self.trend,\n            enforce_stationarity=self.enforce_stationarity,\n            enforce_invertibility=self.enforce_invertibility,\n            concentrate_scale=self.concentrate_scale,\n            trend_offset=self.trend_offset,\n            missing=self.missing,\n        ).fit()\n\n        forecast = source_model.forecast(steps=self.rows_to_predict)\n        inferred_freq = pd.Timedelta(\n            value=statistics.mode(np.diff(main_signal_df[self.timestamp_name].values))\n        )\n\n        pd_forecast_df = pd.DataFrame(\n            {\n                self.timestamp_name: pd.date_range(\n                    start=main_signal_df[self.timestamp_name].max() + inferred_freq,\n                    periods=self.rows_to_predict,\n                    freq=inferred_freq,\n                ),\n                self.column_to_predict: forecast,\n            }\n        )\n\n        pd_df = pd.concat([pd_df, pd_forecast_df])\n\n        if self.past_data_style == self.InputStyle.COLUMN_BASED:\n            for obj in self.past_data.schema:\n                simple_string_type = obj.dataType.simpleString()\n                if simple_string_type == \"timestamp\":\n                    continue\n                pd_df.loc[:, obj.name] = pd_df.loc[:, obj.name].astype(\n                    simple_string_type\n                )\n            # Workaround needed for PySpark versions &lt;3.4\n            pd_df = _prepare_pandas_to_convert_to_spark(pd_df)\n            predicted_source_pyspark_dataframe = self.spark_session.createDataFrame(\n                pd_df, schema=copy.deepcopy(self.past_data.schema)\n            )\n            return predicted_source_pyspark_dataframe\n        elif self.past_data_style == self.InputStyle.SOURCE_BASED:\n            data_to_add = pd_forecast_df[[self.column_to_predict, self.timestamp_name]]\n            data_to_add = data_to_add.rename(\n                columns={\n                    self.timestamp_name: self.timestamp_name,\n                    self.column_to_predict: self.value_name,\n                }\n            )\n            data_to_add[self.source_name] = self.column_to_predict\n            data_to_add[self.timestamp_name] = data_to_add[\n                self.timestamp_name\n            ].dt.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n\n            pd_df_schema = StructType(\n                [\n                    StructField(self.source_name, StringType(), True),\n                    StructField(self.timestamp_name, StringType(), True),\n                    StructField(self.value_name, StringType(), True),\n                ]\n            )\n\n            # Workaround needed for PySpark versions &lt;3.4\n            data_to_add = _prepare_pandas_to_convert_to_spark(data_to_add)\n\n            predicted_source_pyspark_dataframe = self.spark_session.createDataFrame(\n                _prepare_pandas_to_convert_to_spark(\n                    data_to_add[\n                        [self.source_name, self.timestamp_name, self.value_name]\n                    ]\n                ),\n                schema=pd_df_schema,\n            )\n\n            if self.status_name is not None:\n                predicted_source_pyspark_dataframe = (\n                    predicted_source_pyspark_dataframe.withColumn(\n                        self.status_name, lit(\"Predicted\")\n                    )\n                )\n\n            to_return = self.past_data.unionByName(predicted_source_pyspark_dataframe)\n            return to_return\n\n    def validate(self, schema_dict, df: SparkDataFrame = None):\n        return super().validate(schema_dict, self.past_data)\n</code></pre>"},{"location":"sdk/code-reference/pipelines/forecasting/spark/arima/#src.sdk.python.rtdip_sdk.pipelines.forecasting.spark.arima.ArimaPrediction.InputStyle","title":"<code>InputStyle</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Used to describe style of a dataframe</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/forecasting/spark/arima.py</code> <pre><code>class InputStyle(Enum):\n    \"\"\"\n    Used to describe style of a dataframe\n    \"\"\"\n\n    COLUMN_BASED = 1  # Schema: [EventTime, FirstSource, SecondSource, ...]\n    SOURCE_BASED = 2  # Schema: [EventTime, NameSource, Value, OptionalStatus]\n</code></pre>"},{"location":"sdk/code-reference/pipelines/forecasting/spark/arima/#src.sdk.python.rtdip_sdk.pipelines.forecasting.spark.arima.ArimaPrediction.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/forecasting/spark/arima.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/forecasting/spark/arima/#src.sdk.python.rtdip_sdk.pipelines.forecasting.spark.arima.ArimaPrediction.filter_data","title":"<code>filter_data()</code>","text":"<p>Forecasts a value column of a given time series dataframe based on the historical data points using ARIMA.</p> <p>Constructs full entries based on the preceding timestamps. It is advised to place this step after the missing value imputation to prevent learning on dirty data.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A PySpark DataFrame with forecasted value entries depending on constructor parameters.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/forecasting/spark/arima.py</code> <pre><code>def filter_data(self) -&gt; PySparkDataFrame:\n    \"\"\"\n    Forecasts a value column of a given time series dataframe based on the historical data points using ARIMA.\n\n    Constructs full entries based on the preceding timestamps. It is advised to place this step after the missing\n    value imputation to prevent learning on dirty data.\n\n    Returns:\n        DataFrame: A PySpark DataFrame with forecasted value entries depending on constructor parameters.\n    \"\"\"\n    # expected_scheme = StructType(\n    #    [\n    #        StructField(\"TagName\", StringType(), True),\n    #        StructField(\"EventTime\", TimestampType(), True),\n    #        StructField(\"Status\", StringType(), True),\n    #        StructField(\"Value\", NumericType(), True),\n    #    ]\n    # )\n    pd_df = self.df.toPandas()\n    pd_df.loc[:, self.timestamp_name] = pd.to_datetime(\n        pd_df[self.timestamp_name], format=\"mixed\"\n    ).astype(\"datetime64[ns]\")\n    pd_df.loc[:, self.column_to_predict] = pd_df.loc[\n        :, self.column_to_predict\n    ].astype(float)\n    pd_df.sort_values(self.timestamp_name, inplace=True)\n    pd_df.reset_index(drop=True, inplace=True)\n    # self.validate(expected_scheme)\n\n    # limit df to specific data points\n    pd_to_train_on = pd_df[pd_df[self.column_to_predict].notna()].tail(\n        self.rows_to_analyze\n    )\n    pd_to_predict_on = pd_df[pd_df[self.column_to_predict].isna()].head(\n        self.rows_to_predict\n    )\n    pd_df = pd.concat([pd_to_train_on, pd_to_predict_on])\n\n    main_signal_df = pd_df[pd_df[self.column_to_predict].notna()]\n\n    input_data = main_signal_df[self.column_to_predict].astype(float)\n    exog_data = None\n    # if self.external_regressor_names is not None:\n    #     exog_data = []\n    #     for column_name in self.external_regressor_names:\n    #         signal_df = pd.concat([pd_to_train_on[column_name], pd_to_predict_on[column_name]])\n    #         exog_data.append(signal_df)\n\n    source_model = ARIMA(\n        endog=input_data,\n        exog=exog_data,\n        order=self.order,\n        seasonal_order=self.seasonal_order,\n        trend=self.trend,\n        enforce_stationarity=self.enforce_stationarity,\n        enforce_invertibility=self.enforce_invertibility,\n        concentrate_scale=self.concentrate_scale,\n        trend_offset=self.trend_offset,\n        missing=self.missing,\n    ).fit()\n\n    forecast = source_model.forecast(steps=self.rows_to_predict)\n    inferred_freq = pd.Timedelta(\n        value=statistics.mode(np.diff(main_signal_df[self.timestamp_name].values))\n    )\n\n    pd_forecast_df = pd.DataFrame(\n        {\n            self.timestamp_name: pd.date_range(\n                start=main_signal_df[self.timestamp_name].max() + inferred_freq,\n                periods=self.rows_to_predict,\n                freq=inferred_freq,\n            ),\n            self.column_to_predict: forecast,\n        }\n    )\n\n    pd_df = pd.concat([pd_df, pd_forecast_df])\n\n    if self.past_data_style == self.InputStyle.COLUMN_BASED:\n        for obj in self.past_data.schema:\n            simple_string_type = obj.dataType.simpleString()\n            if simple_string_type == \"timestamp\":\n                continue\n            pd_df.loc[:, obj.name] = pd_df.loc[:, obj.name].astype(\n                simple_string_type\n            )\n        # Workaround needed for PySpark versions &lt;3.4\n        pd_df = _prepare_pandas_to_convert_to_spark(pd_df)\n        predicted_source_pyspark_dataframe = self.spark_session.createDataFrame(\n            pd_df, schema=copy.deepcopy(self.past_data.schema)\n        )\n        return predicted_source_pyspark_dataframe\n    elif self.past_data_style == self.InputStyle.SOURCE_BASED:\n        data_to_add = pd_forecast_df[[self.column_to_predict, self.timestamp_name]]\n        data_to_add = data_to_add.rename(\n            columns={\n                self.timestamp_name: self.timestamp_name,\n                self.column_to_predict: self.value_name,\n            }\n        )\n        data_to_add[self.source_name] = self.column_to_predict\n        data_to_add[self.timestamp_name] = data_to_add[\n            self.timestamp_name\n        ].dt.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n\n        pd_df_schema = StructType(\n            [\n                StructField(self.source_name, StringType(), True),\n                StructField(self.timestamp_name, StringType(), True),\n                StructField(self.value_name, StringType(), True),\n            ]\n        )\n\n        # Workaround needed for PySpark versions &lt;3.4\n        data_to_add = _prepare_pandas_to_convert_to_spark(data_to_add)\n\n        predicted_source_pyspark_dataframe = self.spark_session.createDataFrame(\n            _prepare_pandas_to_convert_to_spark(\n                data_to_add[\n                    [self.source_name, self.timestamp_name, self.value_name]\n                ]\n            ),\n            schema=pd_df_schema,\n        )\n\n        if self.status_name is not None:\n            predicted_source_pyspark_dataframe = (\n                predicted_source_pyspark_dataframe.withColumn(\n                    self.status_name, lit(\"Predicted\")\n                )\n            )\n\n        to_return = self.past_data.unionByName(predicted_source_pyspark_dataframe)\n        return to_return\n</code></pre>"},{"location":"sdk/code-reference/pipelines/forecasting/spark/auto_arima/","title":"Auto Arima","text":""},{"location":"sdk/code-reference/pipelines/forecasting/spark/auto_arima/#src.sdk.python.rtdip_sdk.pipelines.forecasting.spark.auto_arima.ArimaAutoPrediction","title":"<code>ArimaAutoPrediction</code>","text":"<p>               Bases: <code>ArimaPrediction</code></p> <p>A wrapper for ArimaPrediction which uses pmdarima auto_arima for data prediction. It selectively tries various sets of p and q (also P and Q for seasonal models) parameters and selects the model with the minimal AIC.</p>"},{"location":"sdk/code-reference/pipelines/forecasting/spark/auto_arima/#src.sdk.python.rtdip_sdk.pipelines.forecasting.spark.auto_arima.ArimaAutoPrediction--example","title":"Example","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport numpy.random\nimport pandas\nfrom pyspark.sql import SparkSession\n\nfrom rtdip_sdk.pipelines.data_quality.forecasting.spark.arima import ArimaPrediction\n\nimport rtdip_sdk.pipelines._pipeline_utils.spark as spark_utils\nfrom rtdip_sdk.pipelines.data_quality.forecasting.spark.auto_arima import ArimaAutoPrediction\n\nspark_session = SparkSession.builder.master(\"local[2]\").appName(\"test\").getOrCreate()\ndf = pandas.DataFrame()\n\nnumpy.random.seed(0)\narr_len = 250\nh_a_l = int(arr_len / 2)\ndf['Value'] = np.random.rand(arr_len) + np.sin(np.linspace(0, arr_len / 10, num=arr_len))\ndf['Value2'] = np.random.rand(arr_len) + np.cos(np.linspace(0, arr_len / 2, num=arr_len)) + 5\ndf['index'] = np.asarray(pandas.date_range(start='1/1/2024', end='2/1/2024', periods=arr_len))\ndf = df.set_index(pandas.DatetimeIndex(df['index']))\n\nlearn_df = df.head(h_a_l)\n\n# plt.plot(df['Value'])\n# plt.show()\n\ninput_df = spark_session.createDataFrame(\n        learn_df,\n        ['Value', 'Value2', 'index'],\n)\narima_comp = ArimaAutoPrediction(input_df, to_extend_name='Value', number_of_data_points_to_analyze=h_a_l, number_of_data_points_to_predict=h_a_l,\n                     seasonal=True)\nforecasted_df = arima_comp.filter_data().toPandas()\nprint('Done')\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>past_data</code> <code>DataFrame</code> <p>PySpark DataFrame which contains training data</p> required <code>to_extend_name</code> <code>str</code> <p>Column or source to forecast on</p> <code>None</code> <code>past_data_style</code> <code>InputStyle</code> <p>In which format is past_data formatted</p> <code>None</code> <code>value_name</code> <code>str</code> <p>Name of column in source-based format, where values are stored</p> <code>None</code> <code>timestamp_name</code> <code>str</code> <p>Name of column, where event timestamps are stored</p> <code>None</code> <code>source_name</code> <code>str</code> <p>Name of column in source-based format, where source of events are stored</p> <code>None</code> <code>status_name</code> <code>str</code> <p>Name of column in source-based format, where status of events are stored</p> <code>None</code> <code>external_regressor_names</code> <code>List[str]</code> <p>Currently not working. Names of the columns with data to use for prediction, but not extend</p> <code>None</code> <code>number_of_data_points_to_predict</code> <code>int</code> <p>Amount of points to forecast</p> <code>50</code> <code>number_of_data_points_to_analyze</code> <code>int</code> <p>Amount of most recent points to train on</p> <code>None</code> <code>seasonal</code> <code>bool</code> <p>Setting for AutoArima, is past_data seasonal?</p> <code>False</code> <code>enforce_stationarity</code> <code>bool</code> <p>ARIMA-Specific setting</p> <code>True</code> <code>enforce_invertibility</code> <code>bool</code> <p>ARIMA-Specific setting</p> <code>True</code> <code>concentrate_scale</code> <code>bool</code> <p>ARIMA-Specific setting</p> <code>False</code> <code>trend_offset</code> <code>int</code> <p>ARIMA-Specific setting</p> <code>1</code> <code>missing</code> <code>str</code> <p>ARIMA-Specific setting</p> <code>'None'</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/forecasting/spark/auto_arima.py</code> <pre><code>class ArimaAutoPrediction(ArimaPrediction):\n    \"\"\"\n    A wrapper for ArimaPrediction which uses pmdarima auto_arima for data prediction.\n    It selectively tries various sets of p and q (also P and Q for seasonal models) parameters and selects the model with the minimal AIC.\n\n    Example\n    -------\n    ```python\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import numpy.random\n    import pandas\n    from pyspark.sql import SparkSession\n\n    from rtdip_sdk.pipelines.data_quality.forecasting.spark.arima import ArimaPrediction\n\n    import rtdip_sdk.pipelines._pipeline_utils.spark as spark_utils\n    from rtdip_sdk.pipelines.data_quality.forecasting.spark.auto_arima import ArimaAutoPrediction\n\n    spark_session = SparkSession.builder.master(\"local[2]\").appName(\"test\").getOrCreate()\n    df = pandas.DataFrame()\n\n    numpy.random.seed(0)\n    arr_len = 250\n    h_a_l = int(arr_len / 2)\n    df['Value'] = np.random.rand(arr_len) + np.sin(np.linspace(0, arr_len / 10, num=arr_len))\n    df['Value2'] = np.random.rand(arr_len) + np.cos(np.linspace(0, arr_len / 2, num=arr_len)) + 5\n    df['index'] = np.asarray(pandas.date_range(start='1/1/2024', end='2/1/2024', periods=arr_len))\n    df = df.set_index(pandas.DatetimeIndex(df['index']))\n\n    learn_df = df.head(h_a_l)\n\n    # plt.plot(df['Value'])\n    # plt.show()\n\n    input_df = spark_session.createDataFrame(\n            learn_df,\n            ['Value', 'Value2', 'index'],\n    )\n    arima_comp = ArimaAutoPrediction(input_df, to_extend_name='Value', number_of_data_points_to_analyze=h_a_l, number_of_data_points_to_predict=h_a_l,\n                         seasonal=True)\n    forecasted_df = arima_comp.filter_data().toPandas()\n    print('Done')\n    ```\n\n    Parameters:\n        past_data (PySparkDataFrame): PySpark DataFrame which contains training data\n        to_extend_name (str): Column or source to forecast on\n        past_data_style (InputStyle): In which format is past_data formatted\n        value_name (str): Name of column in source-based format, where values are stored\n        timestamp_name (str): Name of column, where event timestamps are stored\n        source_name (str): Name of column in source-based format, where source of events are stored\n        status_name (str): Name of column in source-based format, where status of events are stored\n        external_regressor_names (List[str]): Currently not working. Names of the columns with data to use for prediction, but not extend\n        number_of_data_points_to_predict (int): Amount of points to forecast\n        number_of_data_points_to_analyze (int): Amount of most recent points to train on\n        seasonal (bool): Setting for AutoArima, is past_data seasonal?\n        enforce_stationarity (bool): ARIMA-Specific setting\n        enforce_invertibility (bool): ARIMA-Specific setting\n        concentrate_scale (bool): ARIMA-Specific setting\n        trend_offset (int): ARIMA-Specific setting\n        missing (str): ARIMA-Specific setting\n    \"\"\"\n\n    def __init__(\n        self,\n        past_data: PySparkDataFrame,\n        past_data_style: ArimaPrediction.InputStyle = None,\n        to_extend_name: str = None,\n        value_name: str = None,\n        timestamp_name: str = None,\n        source_name: str = None,\n        status_name: str = None,\n        external_regressor_names: List[str] = None,\n        number_of_data_points_to_predict: int = 50,\n        number_of_data_points_to_analyze: int = None,\n        seasonal: bool = False,\n        enforce_stationarity: bool = True,\n        enforce_invertibility: bool = True,\n        concentrate_scale: bool = False,\n        trend_offset: int = 1,\n        missing: str = \"None\",\n    ) -&gt; None:\n        # Convert source-based dataframe to column-based if necessary\n        self._initialize_self_df(\n            past_data,\n            past_data_style,\n            source_name,\n            status_name,\n            timestamp_name,\n            to_extend_name,\n            value_name,\n        )\n        # Prepare Input data\n        input_data = self.df.toPandas()\n        input_data = input_data[input_data[to_extend_name].notna()].tail(\n            number_of_data_points_to_analyze\n        )[to_extend_name]\n\n        auto_model = auto_arima(\n            y=input_data,\n            seasonal=seasonal,\n            stepwise=True,\n            suppress_warnings=True,\n            trace=False,  # Set to true if to debug\n            error_action=\"ignore\",\n            max_order=None,\n        )\n\n        super().__init__(\n            past_data=past_data,\n            past_data_style=self.past_data_style,\n            to_extend_name=to_extend_name,\n            value_name=self.value_name,\n            timestamp_name=self.timestamp_name,\n            source_name=self.source_name,\n            status_name=self.status_name,\n            external_regressor_names=external_regressor_names,\n            number_of_data_points_to_predict=number_of_data_points_to_predict,\n            number_of_data_points_to_analyze=number_of_data_points_to_analyze,\n            order=auto_model.order,\n            seasonal_order=auto_model.seasonal_order,\n            trend=\"c\" if auto_model.order[1] == 0 else \"t\",\n            enforce_stationarity=enforce_stationarity,\n            enforce_invertibility=enforce_invertibility,\n            concentrate_scale=concentrate_scale,\n            trend_offset=trend_offset,\n            missing=missing,\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/forecasting/spark/data_binning/","title":"Data Binning","text":""},{"location":"sdk/code-reference/pipelines/forecasting/spark/data_binning/#src.sdk.python.rtdip_sdk.pipelines.forecasting.spark.data_binning.DataBinning","title":"<code>DataBinning</code>","text":"<p>               Bases: <code>MachineLearningInterface</code></p> <p>Data binning using clustering methods. This method partitions the data points into a specified number of clusters (bins) based on the specified column. Each data point is assigned to the nearest cluster center.</p>"},{"location":"sdk/code-reference/pipelines/forecasting/spark/data_binning/#src.sdk.python.rtdip_sdk.pipelines.forecasting.spark.data_binning.DataBinning--example","title":"Example","text":"<pre><code>from src.sdk.python.rtdip_sdk.pipelines.forecasting.spark.data_binning import DataBinning\n\ndf = ... # Get a PySpark DataFrame with features column\n\nbinning = DataBinning(\n    column_name=\"features\",\n    bins=3,\n    output_column_name=\"bin\",\n    method=\"kmeans\"\n)\nbinned_df = binning.train(df).predict(df)\nbinned_df.show()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <code>str</code> <p>The name of the input column to be binned (default: \"features\").</p> <code>'features'</code> <code>bins</code> <code>int</code> <p>The number of bins/clusters to create (default: 2).</p> <code>2</code> <code>output_column_name</code> <code>str</code> <p>The name of the output column containing bin assignments (default: \"bin\").</p> <code>'bin'</code> <code>method</code> <code>str</code> <p>The binning method to use. Currently only supports \"kmeans\".</p> <code>'kmeans'</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/forecasting/spark/data_binning.py</code> <pre><code>class DataBinning(MachineLearningInterface):\n    \"\"\"\n    Data binning using clustering methods. This method partitions the data points into a specified number of clusters (bins)\n    based on the specified column. Each data point is assigned to the nearest cluster center.\n\n    Example\n    --------\n    ```python\n    from src.sdk.python.rtdip_sdk.pipelines.forecasting.spark.data_binning import DataBinning\n\n    df = ... # Get a PySpark DataFrame with features column\n\n    binning = DataBinning(\n        column_name=\"features\",\n        bins=3,\n        output_column_name=\"bin\",\n        method=\"kmeans\"\n    )\n    binned_df = binning.train(df).predict(df)\n    binned_df.show()\n    ```\n\n    Parameters:\n        column_name (str): The name of the input column to be binned (default: \"features\").\n        bins (int): The number of bins/clusters to create (default: 2).\n        output_column_name (str): The name of the output column containing bin assignments (default: \"bin\").\n        method (str): The binning method to use. Currently only supports \"kmeans\".\n    \"\"\"\n\n    def __init__(\n        self,\n        column_name: str = \"features\",\n        bins: int = 2,\n        output_column_name: str = \"bin\",\n        method: str = \"kmeans\",\n    ) -&gt; None:\n        self.column_name = column_name\n\n        if method == \"kmeans\":\n            self.method = clustering.KMeans(\n                featuresCol=column_name, predictionCol=output_column_name, k=bins\n            )\n        else:\n            raise ValueError(\"Unknown method: {}\".format(method))\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def train(self, train_df):\n        \"\"\"\n        Filter anomalies based on the k-sigma rule\n        \"\"\"\n        self.model = self.method.fit(train_df)\n        return self\n\n    def predict(self, predict_df):\n        return self.model.transform(predict_df)\n</code></pre>"},{"location":"sdk/code-reference/pipelines/forecasting/spark/data_binning/#src.sdk.python.rtdip_sdk.pipelines.forecasting.spark.data_binning.DataBinning.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/forecasting/spark/data_binning.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/forecasting/spark/data_binning/#src.sdk.python.rtdip_sdk.pipelines.forecasting.spark.data_binning.DataBinning.train","title":"<code>train(train_df)</code>","text":"<p>Filter anomalies based on the k-sigma rule</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/forecasting/spark/data_binning.py</code> <pre><code>def train(self, train_df):\n    \"\"\"\n    Filter anomalies based on the k-sigma rule\n    \"\"\"\n    self.model = self.method.fit(train_df)\n    return self\n</code></pre>"},{"location":"sdk/code-reference/pipelines/forecasting/spark/k_nearest_neighbors/","title":"K Nearest Neighbors","text":""},{"location":"sdk/code-reference/pipelines/forecasting/spark/k_nearest_neighbors/#src.sdk.python.rtdip_sdk.pipelines.forecasting.spark.k_nearest_neighbors.KNearestNeighbors","title":"<code>KNearestNeighbors</code>","text":"<p>               Bases: <code>MachineLearningInterface</code></p> <p>Implements the K-Nearest Neighbors (KNN) algorithm to predict missing values in a dataset. This component is compatible with time series data and supports customizable weighted or unweighted averaging for predictions.</p> <p>Example: <pre><code>from pyspark.ml.feature import StandardScaler, VectorAssembler\nfrom pyspark.sql import SparkSession\nfrom src.sdk.python.rtdip_sdk.pipelines.forecasting.spark.k_nearest_neighbors import KNearestNeighbors\nspark_session = SparkSession.builder.master(\"local[2]\").appName(\"KNN\").getOrCreate()\ndata = [\n    (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 03:49:45.000\", \"Good\", 25.0),\n    (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 07:53:11.000\", \"Good\", -5.0),\n    (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 11:56:42.000\", \"Good\", 50.0),\n    (\"B3TS64V0K.:ZUX09R\", \"2024-01-02 16:00:12.000\", \"Good\", 80.0),\n    (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 20:03:46.000\", \"Good\", 100.0),\n]\ncolumns = [\"TagName\", \"EventTime\", \"Status\", \"Value\"]\nraw_df = = spark.createDataFrame(data, columns)\nassembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\"], outputCol=\"assembled_features\")\ndf = assembler.transform(raw_df)\nscaler = StandardScaler(inputCol=\"assembled_features\", outputCol=\"features\", withStd=True, withMean=True)\nscaled_df = scaler.fit(df).transform(df)\nknn = KNearestNeighbors(\n    features_col=\"features\",\n    label_col=\"label\",\n    timestamp_col=\"timestamp\",\n    k=3,\n    weighted=True,\n    distance_metric=\"combined\",  # Options: \"euclidean\", \"temporal\", \"combined\"\n    temporal_weight=0.3  # Weight for temporal distance when using combined metric\n)\ntrain_df, test_df = knn.randomSplit([0.8, 0.2], seed=42)\nknn.train(train_df)\npredictions = knn.predict(test_df)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>features_col</code> <code>str</code> <p>Name of the column containing the features (the input). Default is 'features'</p> required <code>label_col</code> <code>str</code> <p>Name of the column containing the label (the input). Default is 'label'</p> required <code>timestamp_col</code> <code>str</code> <p>Name of the column containing timestamps</p> <code>None</code> <code>k</code> <code>int</code> <p>The number of neighbors to consider in the KNN algorithm. Default is 3</p> <code>3</code> <code>weighted</code> <code>bool</code> <p>Whether to use weighted averaging based on distance. Default is False (unweighted averaging)</p> <code>False</code> <code>distance_metric</code> <code>str</code> <p>Type of distance calculation (\"euclidean\", \"temporal\", or \"combined\")</p> <code>'euclidean'</code> <code>temporal_weight</code> <code>float</code> <p>Weight for temporal distance in combined metric (0 to 1)</p> <code>0.5</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/forecasting/spark/k_nearest_neighbors.py</code> <pre><code>class KNearestNeighbors(MachineLearningInterface):\n    \"\"\"\n    Implements the K-Nearest Neighbors (KNN) algorithm to predict missing values in a dataset.\n    This component is compatible with time series data and supports customizable weighted or unweighted averaging for predictions.\n\n    Example:\n    ```python\n    from pyspark.ml.feature import StandardScaler, VectorAssembler\n    from pyspark.sql import SparkSession\n    from src.sdk.python.rtdip_sdk.pipelines.forecasting.spark.k_nearest_neighbors import KNearestNeighbors\n    spark_session = SparkSession.builder.master(\"local[2]\").appName(\"KNN\").getOrCreate()\n    data = [\n        (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 03:49:45.000\", \"Good\", 25.0),\n        (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 07:53:11.000\", \"Good\", -5.0),\n        (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 11:56:42.000\", \"Good\", 50.0),\n        (\"B3TS64V0K.:ZUX09R\", \"2024-01-02 16:00:12.000\", \"Good\", 80.0),\n        (\"A2PS64V0J.:ZUX09R\", \"2024-01-02 20:03:46.000\", \"Good\", 100.0),\n    ]\n    columns = [\"TagName\", \"EventTime\", \"Status\", \"Value\"]\n    raw_df = = spark.createDataFrame(data, columns)\n    assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\"], outputCol=\"assembled_features\")\n    df = assembler.transform(raw_df)\n    scaler = StandardScaler(inputCol=\"assembled_features\", outputCol=\"features\", withStd=True, withMean=True)\n    scaled_df = scaler.fit(df).transform(df)\n    knn = KNearestNeighbors(\n        features_col=\"features\",\n        label_col=\"label\",\n        timestamp_col=\"timestamp\",\n        k=3,\n        weighted=True,\n        distance_metric=\"combined\",  # Options: \"euclidean\", \"temporal\", \"combined\"\n        temporal_weight=0.3  # Weight for temporal distance when using combined metric\n    )\n    train_df, test_df = knn.randomSplit([0.8, 0.2], seed=42)\n    knn.train(train_df)\n    predictions = knn.predict(test_df)\n    ```\n\n    Parameters:\n        features_col (str): Name of the column containing the features (the input). Default is 'features'\n        label_col (str): Name of the column containing the label (the input). Default is 'label'\n        timestamp_col (str, optional): Name of the column containing timestamps\n        k (int): The number of neighbors to consider in the KNN algorithm. Default is 3\n        weighted (bool): Whether to use weighted averaging based on distance. Default is False (unweighted averaging)\n        distance_metric (str): Type of distance calculation (\"euclidean\", \"temporal\", or \"combined\")\n        temporal_weight (float): Weight for temporal distance in combined metric (0 to 1)\n    \"\"\"\n\n    def __init__(\n        self,\n        features_col,\n        label_col,\n        timestamp_col=None,\n        k=3,\n        weighted=False,\n        distance_metric=\"euclidean\",\n        temporal_weight=0.5,\n    ):\n        self.features_col = features_col\n        self.label_col = label_col\n        self.timestamp_col = timestamp_col\n        self.k = k\n        self.weighted = weighted\n        self.distance_metric = distance_metric\n        self.temporal_weight = temporal_weight\n        self.train_features = None\n        self.train_labels = None\n        self.train_timestamps = None\n\n        if distance_metric not in [\"euclidean\", \"temporal\", \"combined\"]:\n            raise ValueError(\n                \"distance_metric must be 'euclidean', 'temporal', or 'combined'\"\n            )\n\n        if distance_metric in [\"temporal\", \"combined\"] and timestamp_col is None:\n            raise ValueError(\n                \"timestamp_col must be provided when using temporal or combined distance metrics\"\n            )\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def train(self, train_df: DataFrame):\n        \"\"\"\n        Sets up the training DataFrame including temporal information if specified.\n        \"\"\"\n        if self.timestamp_col:\n            df = train_df.select(\n                self.features_col, self.label_col, self.timestamp_col\n            ).collect()\n            self.train_timestamps = np.array(\n                [row[self.timestamp_col].timestamp() for row in df]\n            )\n        else:\n            df = train_df.select(self.features_col, self.label_col).collect()\n\n        self.train_features = np.array([row[self.features_col] for row in df])\n        self.train_labels = np.array([row[self.label_col] for row in df])\n        return self\n\n    def predict(self, test_df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Predicts labels using the specified distance metric.\n        \"\"\"\n        train_features = self.train_features\n        train_labels = self.train_labels\n        train_timestamps = self.train_timestamps\n        k = self.k\n        weighted = self.weighted\n        distance_metric = self.distance_metric\n        temporal_weight = self.temporal_weight\n\n        def calculate_distances(features, timestamp=None):\n            test_point = np.array(features)\n\n            if distance_metric == \"euclidean\":\n                return np.sqrt(np.sum((train_features - test_point) ** 2, axis=1))\n\n            elif distance_metric == \"temporal\":\n                return np.abs(train_timestamps - timestamp)\n\n            else:  # combined\n                feature_distances = np.sqrt(\n                    np.sum((train_features - test_point) ** 2, axis=1)\n                )\n                temporal_distances = np.abs(train_timestamps - timestamp)\n\n                # Normalize distances to [0, 1] range\n                feature_distances = (feature_distances - feature_distances.min()) / (\n                    feature_distances.max() - feature_distances.min() + 1e-10\n                )\n                temporal_distances = (temporal_distances - temporal_distances.min()) / (\n                    temporal_distances.max() - temporal_distances.min() + 1e-10\n                )\n\n                # Combine distances with weights\n                return (\n                    1 - temporal_weight\n                ) * feature_distances + temporal_weight * temporal_distances\n\n        def knn_predict(features, timestamp=None):\n            distances = calculate_distances(features, timestamp)\n            k_nearest_indices = np.argsort(distances)[:k]\n            k_nearest_labels = train_labels[k_nearest_indices]\n\n            if weighted:\n                k_distances = distances[k_nearest_indices]\n                weights = 1 / (k_distances + 1e-10)\n                weights /= np.sum(weights)\n                unique_labels = np.unique(k_nearest_labels)\n                weighted_votes = {\n                    label: np.sum(weights[k_nearest_labels == label])\n                    for label in unique_labels\n                }\n                return float(max(weighted_votes.items(), key=lambda x: x[1])[0])\n            else:\n                return float(\n                    max(set(k_nearest_labels), key=list(k_nearest_labels).count)\n                )\n\n        if self.distance_metric in [\"temporal\", \"combined\"]:\n            predict_udf = udf(\n                lambda features, timestamp: knn_predict(\n                    features, timestamp.timestamp()\n                ),\n                DoubleType(),\n            )\n            return test_df.withColumn(\n                \"prediction\",\n                predict_udf(col(self.features_col), col(self.timestamp_col)),\n            )\n        else:\n            predict_udf = udf(lambda features: knn_predict(features), DoubleType())\n            return test_df.withColumn(\"prediction\", predict_udf(col(self.features_col)))\n</code></pre>"},{"location":"sdk/code-reference/pipelines/forecasting/spark/k_nearest_neighbors/#src.sdk.python.rtdip_sdk.pipelines.forecasting.spark.k_nearest_neighbors.KNearestNeighbors.train","title":"<code>train(train_df)</code>","text":"<p>Sets up the training DataFrame including temporal information if specified.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/forecasting/spark/k_nearest_neighbors.py</code> <pre><code>def train(self, train_df: DataFrame):\n    \"\"\"\n    Sets up the training DataFrame including temporal information if specified.\n    \"\"\"\n    if self.timestamp_col:\n        df = train_df.select(\n            self.features_col, self.label_col, self.timestamp_col\n        ).collect()\n        self.train_timestamps = np.array(\n            [row[self.timestamp_col].timestamp() for row in df]\n        )\n    else:\n        df = train_df.select(self.features_col, self.label_col).collect()\n\n    self.train_features = np.array([row[self.features_col] for row in df])\n    self.train_labels = np.array([row[self.label_col] for row in df])\n    return self\n</code></pre>"},{"location":"sdk/code-reference/pipelines/forecasting/spark/k_nearest_neighbors/#src.sdk.python.rtdip_sdk.pipelines.forecasting.spark.k_nearest_neighbors.KNearestNeighbors.predict","title":"<code>predict(test_df)</code>","text":"<p>Predicts labels using the specified distance metric.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/forecasting/spark/k_nearest_neighbors.py</code> <pre><code>def predict(self, test_df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Predicts labels using the specified distance metric.\n    \"\"\"\n    train_features = self.train_features\n    train_labels = self.train_labels\n    train_timestamps = self.train_timestamps\n    k = self.k\n    weighted = self.weighted\n    distance_metric = self.distance_metric\n    temporal_weight = self.temporal_weight\n\n    def calculate_distances(features, timestamp=None):\n        test_point = np.array(features)\n\n        if distance_metric == \"euclidean\":\n            return np.sqrt(np.sum((train_features - test_point) ** 2, axis=1))\n\n        elif distance_metric == \"temporal\":\n            return np.abs(train_timestamps - timestamp)\n\n        else:  # combined\n            feature_distances = np.sqrt(\n                np.sum((train_features - test_point) ** 2, axis=1)\n            )\n            temporal_distances = np.abs(train_timestamps - timestamp)\n\n            # Normalize distances to [0, 1] range\n            feature_distances = (feature_distances - feature_distances.min()) / (\n                feature_distances.max() - feature_distances.min() + 1e-10\n            )\n            temporal_distances = (temporal_distances - temporal_distances.min()) / (\n                temporal_distances.max() - temporal_distances.min() + 1e-10\n            )\n\n            # Combine distances with weights\n            return (\n                1 - temporal_weight\n            ) * feature_distances + temporal_weight * temporal_distances\n\n    def knn_predict(features, timestamp=None):\n        distances = calculate_distances(features, timestamp)\n        k_nearest_indices = np.argsort(distances)[:k]\n        k_nearest_labels = train_labels[k_nearest_indices]\n\n        if weighted:\n            k_distances = distances[k_nearest_indices]\n            weights = 1 / (k_distances + 1e-10)\n            weights /= np.sum(weights)\n            unique_labels = np.unique(k_nearest_labels)\n            weighted_votes = {\n                label: np.sum(weights[k_nearest_labels == label])\n                for label in unique_labels\n            }\n            return float(max(weighted_votes.items(), key=lambda x: x[1])[0])\n        else:\n            return float(\n                max(set(k_nearest_labels), key=list(k_nearest_labels).count)\n            )\n\n    if self.distance_metric in [\"temporal\", \"combined\"]:\n        predict_udf = udf(\n            lambda features, timestamp: knn_predict(\n                features, timestamp.timestamp()\n            ),\n            DoubleType(),\n        )\n        return test_df.withColumn(\n            \"prediction\",\n            predict_udf(col(self.features_col), col(self.timestamp_col)),\n        )\n    else:\n        predict_udf = udf(lambda features: knn_predict(features), DoubleType())\n        return test_df.withColumn(\"prediction\", predict_udf(col(self.features_col)))\n</code></pre>"},{"location":"sdk/code-reference/pipelines/forecasting/spark/linear_regression/","title":"Linear Regression","text":""},{"location":"sdk/code-reference/pipelines/forecasting/spark/linear_regression/#src.sdk.python.rtdip_sdk.pipelines.forecasting.spark.linear_regression.LinearRegression","title":"<code>LinearRegression</code>","text":"<p>               Bases: <code>MachineLearningInterface</code></p> <p>This class uses pyspark.ml.LinearRegression to train a linear regression model on time data and then uses the model to predict next values in the time series.</p> <p>Parameters:</p> Name Type Description Default <code>features_col</code> <code>str</code> <p>Name of the column containing the features (the input). Default is 'features'.</p> <code>'features'</code> <code>label_col</code> <code>str</code> <p>Name of the column containing the label (the input). Default is 'label'.</p> <code>'label'</code> <code>prediction_col</code> <code>str</code> <p>Name of the column to which the prediction will be written. Default is 'prediction'.</p> <code>'prediction'</code> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.ml.feature import VectorAssembler\nfrom rtdip_sdk.pipelines.forecasting.spark.linear_regression import LinearRegression\n\nspark = SparkSession.builder.master(\"local[2]\").appName(\"LinearRegressionExample\").getOrCreate()\n\ndata = [\n    (1, 2.0, 3.0),\n    (2, 3.0, 4.0),\n    (3, 4.0, 5.0),\n    (4, 5.0, 6.0),\n    (5, 6.0, 7.0),\n]\ncolumns = [\"id\", \"feature1\", \"label\"]\ndf = spark.createDataFrame(data, columns)\n\nassembler = VectorAssembler(inputCols=[\"feature1\"], outputCol=\"features\")\ndf = assembler.transform(df)\n\nlr = LinearRegression(features_col=\"features\", label_col=\"label\", prediction_col=\"prediction\")\ntrain_df, test_df = lr.split_data(df, train_ratio=0.8)\nlr.train(train_df)\npredictions = lr.predict(test_df)\nrmse, r2 = lr.evaluate(predictions)\nprint(f\"RMSE: {rmse}, R\u00b2: {r2}\")\n</code></pre> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/forecasting/spark/linear_regression.py</code> <pre><code>class LinearRegression(MachineLearningInterface):\n    \"\"\"\n    This class uses pyspark.ml.LinearRegression to train a linear regression model on time data\n    and then uses the model to predict next values in the time series.\n\n    Args:\n        features_col (str): Name of the column containing the features (the input). Default is 'features'.\n        label_col (str): Name of the column containing the label (the input). Default is 'label'.\n        prediction_col (str): Name of the column to which the prediction will be written. Default is 'prediction'.\n\n    Example:\n    --------\n    ```python\n    from pyspark.sql import SparkSession\n    from pyspark.ml.feature import VectorAssembler\n    from rtdip_sdk.pipelines.forecasting.spark.linear_regression import LinearRegression\n\n    spark = SparkSession.builder.master(\"local[2]\").appName(\"LinearRegressionExample\").getOrCreate()\n\n    data = [\n        (1, 2.0, 3.0),\n        (2, 3.0, 4.0),\n        (3, 4.0, 5.0),\n        (4, 5.0, 6.0),\n        (5, 6.0, 7.0),\n    ]\n    columns = [\"id\", \"feature1\", \"label\"]\n    df = spark.createDataFrame(data, columns)\n\n    assembler = VectorAssembler(inputCols=[\"feature1\"], outputCol=\"features\")\n    df = assembler.transform(df)\n\n    lr = LinearRegression(features_col=\"features\", label_col=\"label\", prediction_col=\"prediction\")\n    train_df, test_df = lr.split_data(df, train_ratio=0.8)\n    lr.train(train_df)\n    predictions = lr.predict(test_df)\n    rmse, r2 = lr.evaluate(predictions)\n    print(f\"RMSE: {rmse}, R\u00b2: {r2}\")\n    ```\n\n    \"\"\"\n\n    def __init__(\n        self,\n        features_col: str = \"features\",\n        label_col: str = \"label\",\n        prediction_col: str = \"prediction\",\n    ) -&gt; None:\n        self.features_col = features_col\n        self.label_col = label_col\n        self.prediction_col = prediction_col\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def split_data(\n        self, df: DataFrame, train_ratio: float = 0.8\n    ) -&gt; tuple[DataFrame, DataFrame]:\n        \"\"\"\n        Splits the dataset into training and testing sets.\n\n        Args:\n            train_ratio (float): The ratio of the data to be used for training. Default is 0.8 (80% for training).\n\n        Returns:\n            tuple[DataFrame, DataFrame]: Returns the training and testing datasets.\n        \"\"\"\n        train_df, test_df = df.randomSplit([train_ratio, 1 - train_ratio], seed=42)\n        return train_df, test_df\n\n    def train(self, train_df: DataFrame):\n        \"\"\"\n        Trains a linear regression model on the provided data.\n        \"\"\"\n        linear_regression = ml.regression.LinearRegression(\n            featuresCol=self.features_col,\n            labelCol=self.label_col,\n            predictionCol=self.prediction_col,\n        )\n\n        self.model = linear_regression.fit(train_df)\n        return self\n\n    def predict(self, prediction_df: DataFrame):\n        \"\"\"\n        Predicts the next values in the time series.\n        \"\"\"\n\n        return self.model.transform(\n            prediction_df,\n        )\n\n    def evaluate(self, test_df: DataFrame) -&gt; Optional[float]:\n        \"\"\"\n        Evaluates the trained model using RMSE.\n\n        Args:\n            test_df (DataFrame): The testing dataset to evaluate the model.\n\n        Returns:\n            Optional[float]: The Root Mean Squared Error (RMSE) of the model or None if the prediction columnd doesn't exist.\n        \"\"\"\n\n        if self.prediction_col not in test_df.columns:\n            print(\n                f\"Error: '{self.prediction_col}' column is missing in the test DataFrame.\"\n            )\n            return None\n\n        # Evaluator for RMSE\n        evaluator_rmse = RegressionEvaluator(\n            labelCol=self.label_col,\n            predictionCol=self.prediction_col,\n            metricName=\"rmse\",\n        )\n        rmse = evaluator_rmse.evaluate(test_df)\n\n        # Evaluator for R\u00b2\n        evaluator_r2 = RegressionEvaluator(\n            labelCol=self.label_col, predictionCol=self.prediction_col, metricName=\"r2\"\n        )\n        r2 = evaluator_r2.evaluate(test_df)\n\n        return rmse, r2\n</code></pre>"},{"location":"sdk/code-reference/pipelines/forecasting/spark/linear_regression/#src.sdk.python.rtdip_sdk.pipelines.forecasting.spark.linear_regression.LinearRegression.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/forecasting/spark/linear_regression.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/forecasting/spark/linear_regression/#src.sdk.python.rtdip_sdk.pipelines.forecasting.spark.linear_regression.LinearRegression.split_data","title":"<code>split_data(df, train_ratio=0.8)</code>","text":"<p>Splits the dataset into training and testing sets.</p> <p>Parameters:</p> Name Type Description Default <code>train_ratio</code> <code>float</code> <p>The ratio of the data to be used for training. Default is 0.8 (80% for training).</p> <code>0.8</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>tuple[DataFrame, DataFrame]: Returns the training and testing datasets.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/forecasting/spark/linear_regression.py</code> <pre><code>def split_data(\n    self, df: DataFrame, train_ratio: float = 0.8\n) -&gt; tuple[DataFrame, DataFrame]:\n    \"\"\"\n    Splits the dataset into training and testing sets.\n\n    Args:\n        train_ratio (float): The ratio of the data to be used for training. Default is 0.8 (80% for training).\n\n    Returns:\n        tuple[DataFrame, DataFrame]: Returns the training and testing datasets.\n    \"\"\"\n    train_df, test_df = df.randomSplit([train_ratio, 1 - train_ratio], seed=42)\n    return train_df, test_df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/forecasting/spark/linear_regression/#src.sdk.python.rtdip_sdk.pipelines.forecasting.spark.linear_regression.LinearRegression.train","title":"<code>train(train_df)</code>","text":"<p>Trains a linear regression model on the provided data.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/forecasting/spark/linear_regression.py</code> <pre><code>def train(self, train_df: DataFrame):\n    \"\"\"\n    Trains a linear regression model on the provided data.\n    \"\"\"\n    linear_regression = ml.regression.LinearRegression(\n        featuresCol=self.features_col,\n        labelCol=self.label_col,\n        predictionCol=self.prediction_col,\n    )\n\n    self.model = linear_regression.fit(train_df)\n    return self\n</code></pre>"},{"location":"sdk/code-reference/pipelines/forecasting/spark/linear_regression/#src.sdk.python.rtdip_sdk.pipelines.forecasting.spark.linear_regression.LinearRegression.predict","title":"<code>predict(prediction_df)</code>","text":"<p>Predicts the next values in the time series.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/forecasting/spark/linear_regression.py</code> <pre><code>def predict(self, prediction_df: DataFrame):\n    \"\"\"\n    Predicts the next values in the time series.\n    \"\"\"\n\n    return self.model.transform(\n        prediction_df,\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/forecasting/spark/linear_regression/#src.sdk.python.rtdip_sdk.pipelines.forecasting.spark.linear_regression.LinearRegression.evaluate","title":"<code>evaluate(test_df)</code>","text":"<p>Evaluates the trained model using RMSE.</p> <p>Parameters:</p> Name Type Description Default <code>test_df</code> <code>DataFrame</code> <p>The testing dataset to evaluate the model.</p> required <p>Returns:</p> Type Description <code>Optional[float]</code> <p>Optional[float]: The Root Mean Squared Error (RMSE) of the model or None if the prediction columnd doesn't exist.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/forecasting/spark/linear_regression.py</code> <pre><code>def evaluate(self, test_df: DataFrame) -&gt; Optional[float]:\n    \"\"\"\n    Evaluates the trained model using RMSE.\n\n    Args:\n        test_df (DataFrame): The testing dataset to evaluate the model.\n\n    Returns:\n        Optional[float]: The Root Mean Squared Error (RMSE) of the model or None if the prediction columnd doesn't exist.\n    \"\"\"\n\n    if self.prediction_col not in test_df.columns:\n        print(\n            f\"Error: '{self.prediction_col}' column is missing in the test DataFrame.\"\n        )\n        return None\n\n    # Evaluator for RMSE\n    evaluator_rmse = RegressionEvaluator(\n        labelCol=self.label_col,\n        predictionCol=self.prediction_col,\n        metricName=\"rmse\",\n    )\n    rmse = evaluator_rmse.evaluate(test_df)\n\n    # Evaluator for R\u00b2\n    evaluator_r2 = RegressionEvaluator(\n        labelCol=self.label_col, predictionCol=self.prediction_col, metricName=\"r2\"\n    )\n    r2 = evaluator_r2.evaluate(test_df)\n\n    return rmse, r2\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/azure_key_vault/","title":"Databricks Secret Scope","text":""},{"location":"sdk/code-reference/pipelines/secrets/azure_key_vault/#src.sdk.python.rtdip_sdk.pipelines.secrets.azure_key_vault.AzureKeyVaultSecrets","title":"<code>AzureKeyVaultSecrets</code>","text":"<p>               Bases: <code>SecretsInterface</code></p> <p>Retrieves and creates/updates secrets in Azure Key Vault. For more information about Azure Key Vaults, see here.</p>"},{"location":"sdk/code-reference/pipelines/secrets/azure_key_vault/#src.sdk.python.rtdip_sdk.pipelines.secrets.azure_key_vault.AzureKeyVaultSecrets--example","title":"Example","text":"<p><pre><code># Retrieves Secrets from Azure Key Vault\n\nfrom rtdip_sdk.pipelines.secrets import AzureKeyVaultSecrets\n\nget_key_vault_secret = AzureKeyVaultSecrets(\n    vault=\"https://{YOUR-KEY-VAULT}.azure.net/\",\n    key=\"{KEY}\",\n    secret=None,\n    credential=\"{CREDENTIAL}\",\n    kwargs=None\n)\n\nget_key_vault_secret.get()\n</code></pre> <pre><code># Creates or Updates Secrets in Azure Key Vault\n\nfrom rtdip_sdk.pipelines.secrets import AzureKeyVaultSecrets\n\nset_key_vault_secret = AzureKeyVaultSecrets(\n    vault=\"https://{YOUR-KEY-VAULT}.azure.net/\",\n    key=\"{KEY}\",\n    secret=\"{SECRET-TO-BE-SET}\",\n    credential=\"{CREDENTIAL}\",\n    kwargs=None\n)\n\nset_key_vault_secret.set()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>vault</code> <code>str</code> <p>Azure Key Vault URL</p> required <code>key</code> <code>str</code> <p>Key for the secret</p> required <code>secret</code> <code>str</code> <p>Secret or Password to be set in the Azure Key Vault</p> <code>None</code> <code>credential</code> <code>str</code> <p>Credential for authenticating with Azure Key Vault</p> <code>None</code> <code>kwargs</code> <code>dict</code> <p>List of additional parameters to be passed when creating a Azure Key Vault Client. Please see here for more details on parameters that can be provided to the client</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/azure_key_vault.py</code> <pre><code>class AzureKeyVaultSecrets(SecretsInterface):\n    \"\"\"\n    Retrieves and creates/updates secrets in Azure Key Vault. For more information about Azure Key Vaults, see [here.](https://learn.microsoft.com/en-gb/azure/key-vault/general/overview)\n\n    Example\n    -------\n    ```python\n    # Retrieves Secrets from Azure Key Vault\n\n    from rtdip_sdk.pipelines.secrets import AzureKeyVaultSecrets\n\n    get_key_vault_secret = AzureKeyVaultSecrets(\n        vault=\"https://{YOUR-KEY-VAULT}.azure.net/\",\n        key=\"{KEY}\",\n        secret=None,\n        credential=\"{CREDENTIAL}\",\n        kwargs=None\n    )\n\n    get_key_vault_secret.get()\n\n    ```\n    ```python\n    # Creates or Updates Secrets in Azure Key Vault\n\n    from rtdip_sdk.pipelines.secrets import AzureKeyVaultSecrets\n\n    set_key_vault_secret = AzureKeyVaultSecrets(\n        vault=\"https://{YOUR-KEY-VAULT}.azure.net/\",\n        key=\"{KEY}\",\n        secret=\"{SECRET-TO-BE-SET}\",\n        credential=\"{CREDENTIAL}\",\n        kwargs=None\n    )\n\n    set_key_vault_secret.set()\n    ```\n\n    Parameters:\n        vault (str): Azure Key Vault URL\n        key (str): Key for the secret\n        secret (str): Secret or Password to be set in the Azure Key Vault\n        credential (str): Credential for authenticating with Azure Key Vault\n        kwargs (dict): List of additional parameters to be passed when creating a Azure Key Vault Client. Please see [here](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/keyvault/azure-keyvault-secrets) for more details on parameters that can be provided to the client\n    \"\"\"\n\n    vault: str\n    key: str\n    secret: str\n    credential: str\n    kwargs: dict\n\n    def __init__(\n        self,\n        vault: str,\n        key: str,\n        secret: str = None,\n        credential=None,\n        kwargs: dict = None,\n    ):\n        self.vault = vault\n        self.key = key\n        self.secret = secret\n        self.credential = credential\n        self.kwargs = {} if kwargs is None else kwargs\n        self.client = self._get_akv_client()\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYTHON\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_pypi_library(get_default_package(\"azure_key_vault_secret\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _get_akv_client(self):\n        return SecretClient(\n            vault_url=\"https://{}.vault.azure.net\".format(self.vault),\n            credential=self.credential,\n            **self.kwargs\n        )\n\n    def get(self):\n        \"\"\"\n        Retrieves the secret from the Azure Key Vault\n        \"\"\"\n        response = self.client.get_secret(name=self.key)\n        return response.value\n\n    def set(self):\n        \"\"\"\n        Creates or updates a secret in the Azure Key Vault\n        \"\"\"\n        self.client.set_secret(name=self.key, value=self.secret)\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/azure_key_vault/#src.sdk.python.rtdip_sdk.pipelines.secrets.azure_key_vault.AzureKeyVaultSecrets.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYTHON</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/azure_key_vault.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYTHON\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/azure_key_vault/#src.sdk.python.rtdip_sdk.pipelines.secrets.azure_key_vault.AzureKeyVaultSecrets.get","title":"<code>get()</code>","text":"<p>Retrieves the secret from the Azure Key Vault</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/azure_key_vault.py</code> <pre><code>def get(self):\n    \"\"\"\n    Retrieves the secret from the Azure Key Vault\n    \"\"\"\n    response = self.client.get_secret(name=self.key)\n    return response.value\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/azure_key_vault/#src.sdk.python.rtdip_sdk.pipelines.secrets.azure_key_vault.AzureKeyVaultSecrets.set","title":"<code>set()</code>","text":"<p>Creates or updates a secret in the Azure Key Vault</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/azure_key_vault.py</code> <pre><code>def set(self):\n    \"\"\"\n    Creates or updates a secret in the Azure Key Vault\n    \"\"\"\n    self.client.set_secret(name=self.key, value=self.secret)\n    return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/databricks/","title":"Databricks Secret Scope","text":""},{"location":"sdk/code-reference/pipelines/secrets/databricks/#src.sdk.python.rtdip_sdk.pipelines.secrets.databricks.DatabricksSecrets","title":"<code>DatabricksSecrets</code>","text":"<p>               Bases: <code>SecretsInterface</code></p> <p>Retrieves secrets from Databricks Secret Scopes. For more information about Databricks Secret Scopes, see here.</p>"},{"location":"sdk/code-reference/pipelines/secrets/databricks/#src.sdk.python.rtdip_sdk.pipelines.secrets.databricks.DatabricksSecrets--example","title":"Example","text":"<pre><code># Reads Secrets from Databricks Secret Scopes\n\nfrom rtdip_sdk.pipelines.secrets import DatabricksSecrets\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nget_databricks_secret = DatabricksSecrets(\n    spark=spark,\n    vault=\"{NAME-OF-DATABRICKS-SECRET-SCOPE}\"\n    key=\"{KEY-NAME-OF-SECRET}\",\n)\n\nget_databricks_secret.get()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from a Delta table</p> required <code>vault</code> <code>str</code> <p>Name of the Databricks Secret Scope</p> required <code>key</code> <code>str</code> <p>Name/Key of the secret in the Databricks Secret Scope</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/databricks.py</code> <pre><code>class DatabricksSecrets(SecretsInterface):\n    \"\"\"\n    Retrieves secrets from Databricks Secret Scopes. For more information about Databricks Secret Scopes, see [here.](https://docs.databricks.com/security/secrets/secret-scopes.html)\n\n    Example\n    -------\n    ```python\n    # Reads Secrets from Databricks Secret Scopes\n\n    from rtdip_sdk.pipelines.secrets import DatabricksSecrets\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    get_databricks_secret = DatabricksSecrets(\n        spark=spark,\n        vault=\"{NAME-OF-DATABRICKS-SECRET-SCOPE}\"\n        key=\"{KEY-NAME-OF-SECRET}\",\n    )\n\n    get_databricks_secret.get()\n    ```\n\n    Parameters:\n        spark: Spark Session required to read data from a Delta table\n        vault: Name of the Databricks Secret Scope\n        key: Name/Key of the secret in the Databricks Secret Scope\n    \"\"\"\n\n    spark: SparkSession\n    vault: str\n    key: str\n\n    def __init__(self, spark: SparkSession, vault: str, key: str):\n        self.spark = spark\n        self.vault = vault\n        self.key = key\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK on Databricks\n        \"\"\"\n        return SystemType.PYSPARK_DATABRICKS\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def get(self):\n        \"\"\"\n        Retrieves the secret from the Databricks Secret Scope\n        \"\"\"\n        dbutils = get_dbutils(self.spark)\n        return dbutils.secrets.get(scope=self.vault, key=self.key)\n\n    def set(self):\n        \"\"\"\n        Sets the secret in the Secret Scope\n        Raises:\n            NotImplementedError: Will be implemented at a later point in time\n        \"\"\"\n        return NotImplementedError\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/databricks/#src.sdk.python.rtdip_sdk.pipelines.secrets.databricks.DatabricksSecrets.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK on Databricks</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/databricks.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK on Databricks\n    \"\"\"\n    return SystemType.PYSPARK_DATABRICKS\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/databricks/#src.sdk.python.rtdip_sdk.pipelines.secrets.databricks.DatabricksSecrets.get","title":"<code>get()</code>","text":"<p>Retrieves the secret from the Databricks Secret Scope</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/databricks.py</code> <pre><code>def get(self):\n    \"\"\"\n    Retrieves the secret from the Databricks Secret Scope\n    \"\"\"\n    dbutils = get_dbutils(self.spark)\n    return dbutils.secrets.get(scope=self.vault, key=self.key)\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/databricks/#src.sdk.python.rtdip_sdk.pipelines.secrets.databricks.DatabricksSecrets.set","title":"<code>set()</code>","text":"<p>Sets the secret in the Secret Scope Raises:     NotImplementedError: Will be implemented at a later point in time</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/databricks.py</code> <pre><code>def set(self):\n    \"\"\"\n    Sets the secret in the Secret Scope\n    Raises:\n        NotImplementedError: Will be implemented at a later point in time\n    \"\"\"\n    return NotImplementedError\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/hashicorp_vault/","title":"Databricks Secret Scope","text":""},{"location":"sdk/code-reference/pipelines/secrets/hashicorp_vault/#src.sdk.python.rtdip_sdk.pipelines.secrets.hashicorp_vault.HashiCorpVaultSecrets","title":"<code>HashiCorpVaultSecrets</code>","text":"<p>               Bases: <code>SecretsInterface</code></p> <p>Retrieves and creates/updates secrets in a Hashicorp Vault. For more information about Hashicorp Vaults, see here.</p>"},{"location":"sdk/code-reference/pipelines/secrets/hashicorp_vault/#src.sdk.python.rtdip_sdk.pipelines.secrets.hashicorp_vault.HashiCorpVaultSecrets--example","title":"Example","text":"<p><pre><code># Retrieves Secrets from HashiCorp Vault\n\nfrom rtdip_sdk.pipelines.secrets import HashiCorpVaultSecrets\n\nget_hashicorp_secret = HashiCorpVaultSecrets(\n    vault=\"http://127.0.0.1:8200\",\n    key=\"{KEY}\",\n    secret=None,\n    credential=\"{CREDENTIAL}\",\n    kwargs=None\n)\n\nget_hashicorp_secret.get()\n</code></pre> <pre><code># Creates or Updates Secrets in Hashicorp Vault\n\nfrom rtdip_sdk.pipelines.secrets import HashiCorpVaultSecrets\n\nset_hashicorp_secret = AzureKeyVaultSecrets(\n    vault=\"http://127.0.0.1:8200\",\n    key=\"{KEY}\",\n    secret=\"{SECRET-TO-BE-SET}\",\n    credential=\"{CREDENTIAL}\",\n    kwargs=None\n)\n\nset_hashicorp_secret.set()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>vault</code> <code>str</code> <p>Hashicorp Vault URL</p> required <code>key</code> <code>str</code> <p>Name/Key of the secret in the Hashicorp Vault</p> required <code>secret</code> <code>str</code> <p>Secret or Password to be stored in the Hashicorp Vault</p> <code>None</code> <code>credential</code> <code>str</code> <p>Token for authentication with the Hashicorp Vault</p> <code>None</code> <code>kwargs</code> <code>dict</code> <p>List of additional parameters to be passed when creating a Hashicorp Vault Client. Please see here for more details on parameters that can be provided to the client</p> <code>{}</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/hashicorp_vault.py</code> <pre><code>class HashiCorpVaultSecrets(SecretsInterface):\n    \"\"\"\n    Retrieves and creates/updates secrets in a Hashicorp Vault. For more information about Hashicorp Vaults, see [here.](https://developer.hashicorp.com/vault/docs/get-started/developer-qs)\n\n    Example\n    -------\n    ```python\n    # Retrieves Secrets from HashiCorp Vault\n\n    from rtdip_sdk.pipelines.secrets import HashiCorpVaultSecrets\n\n    get_hashicorp_secret = HashiCorpVaultSecrets(\n        vault=\"http://127.0.0.1:8200\",\n        key=\"{KEY}\",\n        secret=None,\n        credential=\"{CREDENTIAL}\",\n        kwargs=None\n    )\n\n    get_hashicorp_secret.get()\n\n    ```\n    ```python\n    # Creates or Updates Secrets in Hashicorp Vault\n\n    from rtdip_sdk.pipelines.secrets import HashiCorpVaultSecrets\n\n    set_hashicorp_secret = AzureKeyVaultSecrets(\n        vault=\"http://127.0.0.1:8200\",\n        key=\"{KEY}\",\n        secret=\"{SECRET-TO-BE-SET}\",\n        credential=\"{CREDENTIAL}\",\n        kwargs=None\n    )\n\n    set_hashicorp_secret.set()\n    ```\n\n    Parameters:\n        vault (str): Hashicorp Vault URL\n        key (str): Name/Key of the secret in the Hashicorp Vault\n        secret (str): Secret or Password to be stored in the Hashicorp Vault\n        credential (str): Token for authentication with the Hashicorp Vault\n        kwargs (dict): List of additional parameters to be passed when creating a Hashicorp Vault Client. Please see [here](https://hvac.readthedocs.io/en/stable/overview.html#initialize-the-client) for more details on parameters that can be provided to the client\n    \"\"\"\n\n    vault: str\n    key: str\n    secret: str\n    credential: str\n\n    def __init__(\n        self,\n        vault: str,\n        key: str,\n        secret: str = None,\n        credential: str = None,\n        kwargs: dict = {},\n    ):  # NOSONAR\n        self.vault = vault\n        self.key = key\n        self.secret = secret\n        self.credential = credential\n        self.kwargs = kwargs\n        self.client = self._get_hvac_client()\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYTHON\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_pypi_library(get_default_package(\"hashicorp_vault\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _get_hvac_client(self):\n        return hvac.Client(url=self.vault, token=self.credential, **self.kwargs)\n\n    def get(self):\n        \"\"\"\n        Retrieves the secret from the Hashicorp Vault\n        \"\"\"\n        response = self.client.secrets.kv.read_secret_version(path=self.key)\n        return response[\"data\"][\"data\"][\"password\"]\n\n    def set(self):\n        \"\"\"\n        Creates or updates a secret in the Hashicorp Vault\n        \"\"\"\n        self.client.secrets.kv.v2.create_or_update_secret(\n            path=self.key,\n            secret=dict(password=self.secret),\n        )\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/hashicorp_vault/#src.sdk.python.rtdip_sdk.pipelines.secrets.hashicorp_vault.HashiCorpVaultSecrets.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYTHON</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/hashicorp_vault.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYTHON\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/hashicorp_vault/#src.sdk.python.rtdip_sdk.pipelines.secrets.hashicorp_vault.HashiCorpVaultSecrets.get","title":"<code>get()</code>","text":"<p>Retrieves the secret from the Hashicorp Vault</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/hashicorp_vault.py</code> <pre><code>def get(self):\n    \"\"\"\n    Retrieves the secret from the Hashicorp Vault\n    \"\"\"\n    response = self.client.secrets.kv.read_secret_version(path=self.key)\n    return response[\"data\"][\"data\"][\"password\"]\n</code></pre>"},{"location":"sdk/code-reference/pipelines/secrets/hashicorp_vault/#src.sdk.python.rtdip_sdk.pipelines.secrets.hashicorp_vault.HashiCorpVaultSecrets.set","title":"<code>set()</code>","text":"<p>Creates or updates a secret in the Hashicorp Vault</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/secrets/hashicorp_vault.py</code> <pre><code>def set(self):\n    \"\"\"\n    Creates or updates a secret in the Hashicorp Vault\n    \"\"\"\n    self.client.secrets.kv.v2.create_or_update_secret(\n        path=self.key,\n        secret=dict(password=self.secret),\n    )\n    return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/delta/","title":"Read from Delta","text":""},{"location":"sdk/code-reference/pipelines/sources/python/delta/#src.sdk.python.rtdip_sdk.pipelines.sources.python.delta.PythonDeltaSource","title":"<code>PythonDeltaSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>The Python Delta Source is used to read data from a Delta table without using Apache Spark, returning a Polars LazyFrame.</p>"},{"location":"sdk/code-reference/pipelines/sources/python/delta/#src.sdk.python.rtdip_sdk.pipelines.sources.python.delta.PythonDeltaSource--example","title":"Example","text":"AzureAWS <pre><code>from rtdip_sdk.pipelines.sources import PythonDeltaSource\n\npath = \"abfss://{FILE-SYSTEM}@{ACCOUNT-NAME}.dfs.core.windows.net/{PATH}/{FILE-NAME}\n\npython_delta_source = PythonDeltaSource(\n    path=path,\n    version=None,\n    storage_options={\n        \"azure_storage_account_name\": \"{AZURE-STORAGE-ACCOUNT-NAME}\",\n        \"azure_storage_account_key\": \"{AZURE-STORAGE-ACCOUNT-KEY}\"\n    },\n    pyarrow_options=None,\n    without_files=False\n)\n\npython_delta_source.read_batch()\n</code></pre> <pre><code>from rtdip_sdk.pipelines.sources import PythonDeltaSource\n\npath = \"https://s3.{REGION-CODE}.amazonaws.com/{BUCKET-NAME}/{KEY-NAME}\"\n\npython_delta_source = PythonDeltaSource(\n    path=path,\n    version=None,\n    storage_options={\n        \"aws_access_key_id\": \"{AWS-ACCESS-KEY-ID}\",\n        \"aws_secret_access_key\": \"{AWS-SECRET-ACCESS-KEY}\"\n    },\n    pyarrow_options=None,\n    without_files=False\n)\n\npython_delta_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the Delta table. Can be local or in S3/Azure storage</p> required <code>version</code> <code>optional int</code> <p>Specify the Delta table version to read from. Defaults to the latest version</p> <code>None</code> <code>storage_options</code> <code>optional dict</code> <p>Used to read from AWS/Azure storage. For AWS use format {\"aws_access_key_id\": \"&lt;&gt;\", \"aws_secret_access_key\":\"&lt;&gt;\"}. For Azure use format {\"azure_storage_account_name\": \"&lt;&gt;\", \"azure_storage_account_key\": \"&lt;&gt;\"}.</p> <code>None</code> <code>pyarrow_options</code> <code>optional dict</code> <p>Data Access and Efficiency options when reading from Delta. See to_pyarrow_dataset.</p> <code>None</code> <code>without_files</code> <code>optional bool</code> <p>If True loads the table without tracking files</p> <code>False</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/delta.py</code> <pre><code>class PythonDeltaSource(SourceInterface):\n    \"\"\"\n    The Python Delta Source is used to read data from a Delta table without using Apache Spark, returning a Polars LazyFrame.\n\n     Example\n    --------\n    === \"Azure\"\n\n        ```python\n        from rtdip_sdk.pipelines.sources import PythonDeltaSource\n\n        path = \"abfss://{FILE-SYSTEM}@{ACCOUNT-NAME}.dfs.core.windows.net/{PATH}/{FILE-NAME}\n\n        python_delta_source = PythonDeltaSource(\n            path=path,\n            version=None,\n            storage_options={\n                \"azure_storage_account_name\": \"{AZURE-STORAGE-ACCOUNT-NAME}\",\n                \"azure_storage_account_key\": \"{AZURE-STORAGE-ACCOUNT-KEY}\"\n            },\n            pyarrow_options=None,\n            without_files=False\n        )\n\n        python_delta_source.read_batch()\n        ```\n    === \"AWS\"\n\n        ```python\n        from rtdip_sdk.pipelines.sources import PythonDeltaSource\n\n        path = \"https://s3.{REGION-CODE}.amazonaws.com/{BUCKET-NAME}/{KEY-NAME}\"\n\n        python_delta_source = PythonDeltaSource(\n            path=path,\n            version=None,\n            storage_options={\n                \"aws_access_key_id\": \"{AWS-ACCESS-KEY-ID}\",\n                \"aws_secret_access_key\": \"{AWS-SECRET-ACCESS-KEY}\"\n            },\n            pyarrow_options=None,\n            without_files=False\n        )\n\n        python_delta_source.read_batch()\n        ```\n\n    Parameters:\n        path (str): Path to the Delta table. Can be local or in S3/Azure storage\n        version (optional int): Specify the Delta table version to read from. Defaults to the latest version\n        storage_options (optional dict): Used to read from AWS/Azure storage. For AWS use format {\"aws_access_key_id\": \"&lt;&gt;\", \"aws_secret_access_key\":\"&lt;&gt;\"}. For Azure use format {\"azure_storage_account_name\": \"&lt;&gt;\", \"azure_storage_account_key\": \"&lt;&gt;\"}.\n        pyarrow_options (optional dict): Data Access and Efficiency options when reading from Delta. See [to_pyarrow_dataset](https://delta-io.github.io/delta-rs/python/api_reference.html#deltalake.table.DeltaTable.to_pyarrow_dataset){ target=\"_blank\" }.\n        without_files (optional bool): If True loads the table without tracking files\n    \"\"\"\n\n    path: str\n    version: int\n    storage_options: dict\n    pyarrow_options: dict\n    without_files: bool\n\n    def __init__(\n        self,\n        path: str,\n        version: int = None,\n        storage_options: dict = None,\n        pyarrow_options: dict = None,\n        without_files: bool = False,\n    ):\n        self.path = path\n        self.version = version\n        self.storage_options = storage_options\n        self.pyarrow_options = pyarrow_options\n        self.without_files = without_files\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYTHON\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self):\n        return True\n\n    def read_batch(self) -&gt; LazyFrame:\n        \"\"\"\n        Reads data from a Delta table into a Polars LazyFrame\n        \"\"\"\n        without_files_dict = {\"without_files\": self.without_files}\n        lf = pl.scan_delta(\n            source=self.path,\n            version=self.version,\n            storage_options=self.storage_options,\n            delta_table_options=without_files_dict,\n            pyarrow_options=self.pyarrow_options,\n        )\n        return lf\n\n    def read_stream(self):\n        \"\"\"\n        Raises:\n            NotImplementedError: Reading from a Delta table using Python is only possible for batch reads. To perform a streaming read, use the read_stream method of the SparkDeltaSource component.\n        \"\"\"\n        raise NotImplementedError(\n            \"Reading from a Delta table using Python is only possible for batch reads. To perform a streaming read, use the read_stream method of the SparkDeltaSource component\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/delta/#src.sdk.python.rtdip_sdk.pipelines.sources.python.delta.PythonDeltaSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYTHON</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/delta.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYTHON\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/delta/#src.sdk.python.rtdip_sdk.pipelines.sources.python.delta.PythonDeltaSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads data from a Delta table into a Polars LazyFrame</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/delta.py</code> <pre><code>def read_batch(self) -&gt; LazyFrame:\n    \"\"\"\n    Reads data from a Delta table into a Polars LazyFrame\n    \"\"\"\n    without_files_dict = {\"without_files\": self.without_files}\n    lf = pl.scan_delta(\n        source=self.path,\n        version=self.version,\n        storage_options=self.storage_options,\n        delta_table_options=without_files_dict,\n        pyarrow_options=self.pyarrow_options,\n    )\n    return lf\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/delta/#src.sdk.python.rtdip_sdk.pipelines.sources.python.delta.PythonDeltaSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Reading from a Delta table using Python is only possible for batch reads. To perform a streaming read, use the read_stream method of the SparkDeltaSource component.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/delta.py</code> <pre><code>def read_stream(self):\n    \"\"\"\n    Raises:\n        NotImplementedError: Reading from a Delta table using Python is only possible for batch reads. To perform a streaming read, use the read_stream method of the SparkDeltaSource component.\n    \"\"\"\n    raise NotImplementedError(\n        \"Reading from a Delta table using Python is only possible for batch reads. To perform a streaming read, use the read_stream method of the SparkDeltaSource component\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/delta_sharing/","title":"Read from Delta with Delta Sharing","text":""},{"location":"sdk/code-reference/pipelines/sources/python/delta_sharing/#src.sdk.python.rtdip_sdk.pipelines.sources.python.delta_sharing.PythonDeltaSharingSource","title":"<code>PythonDeltaSharingSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>The Python Delta Sharing Source is used to read data from a Delta table with Delta Sharing configured, without using Apache Spark.</p>"},{"location":"sdk/code-reference/pipelines/sources/python/delta_sharing/#src.sdk.python.rtdip_sdk.pipelines.sources.python.delta_sharing.PythonDeltaSharingSource--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import PythonDeltaSharingSource\n\npython_delta_sharing_source = PythonDeltaSharingSource(\n    profile_path=\"{CREDENTIAL-FILE-LOCATION}\",\n    share_name=\"{SHARE-NAME}\",\n    schema_name=\"{SCHEMA-NAME}\",\n    table_name=\"{TABLE-NAME}\"\n)\n\npython_delta_sharing_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>profile_path</code> <code>str</code> <p>Location of the credential file. Can be any URL supported by FSSPEC</p> required <code>share_name</code> <code>str</code> <p>The value of 'share=' for the table</p> required <code>schema_name</code> <code>str</code> <p>The value of 'schema=' for the table</p> required <code>table_name</code> <code>str</code> <p>The value of 'name=' for the table</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/delta_sharing.py</code> <pre><code>class PythonDeltaSharingSource(SourceInterface):\n    \"\"\"\n    The Python Delta Sharing Source is used to read data from a Delta table with Delta Sharing configured, without using Apache Spark.\n\n    Example\n    -------\n    ```python\n    from rtdip_sdk.pipelines.sources import PythonDeltaSharingSource\n\n    python_delta_sharing_source = PythonDeltaSharingSource(\n        profile_path=\"{CREDENTIAL-FILE-LOCATION}\",\n        share_name=\"{SHARE-NAME}\",\n        schema_name=\"{SCHEMA-NAME}\",\n        table_name=\"{TABLE-NAME}\"\n    )\n\n    python_delta_sharing_source.read_batch()\n    ```\n\n    Parameters:\n        profile_path (str): Location of the credential file. Can be any URL supported by [FSSPEC](https://filesystem-spec.readthedocs.io/en/latest/index.html){ target=\"_blank\" }\n        share_name (str): The value of 'share=' for the table\n        schema_name (str): The value of 'schema=' for the table\n        table_name (str): The value of 'name=' for the table\n    \"\"\"\n\n    profile_path: str\n    share_name: str\n    schema_name: str\n    table_name: str\n\n    def __init__(\n        self, profile_path: str, share_name: str, schema_name: str, table_name: str\n    ):\n        self.profile_path = profile_path\n        self.share_name = share_name\n        self.schema_name = schema_name\n        self.table_name = table_name\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYTHON\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self):\n        return True\n\n    def read_batch(self) -&gt; LazyFrame:\n        \"\"\"\n        Reads data from a Delta table with Delta Sharing into a Polars LazyFrame.\n        \"\"\"\n        pandas_df = delta_sharing.load_as_pandas(\n            f\"{self.profile_path}#{self.share_name}.{self.schema_name}.{self.table_name}\"\n        )\n        polars_lazyframe = pl.from_pandas(pandas_df).lazy()\n        return polars_lazyframe\n\n    def read_stream(self):\n        \"\"\"\n        Raises:\n            NotImplementedError: Reading from a Delta table with Delta Sharing using Python is only possible for batch reads.\n        \"\"\"\n        raise NotImplementedError(\n            \"Reading from a Delta table with Delta Sharing using Python is only possible for batch reads.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/delta_sharing/#src.sdk.python.rtdip_sdk.pipelines.sources.python.delta_sharing.PythonDeltaSharingSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYTHON</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/delta_sharing.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYTHON\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/delta_sharing/#src.sdk.python.rtdip_sdk.pipelines.sources.python.delta_sharing.PythonDeltaSharingSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads data from a Delta table with Delta Sharing into a Polars LazyFrame.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/delta_sharing.py</code> <pre><code>def read_batch(self) -&gt; LazyFrame:\n    \"\"\"\n    Reads data from a Delta table with Delta Sharing into a Polars LazyFrame.\n    \"\"\"\n    pandas_df = delta_sharing.load_as_pandas(\n        f\"{self.profile_path}#{self.share_name}.{self.schema_name}.{self.table_name}\"\n    )\n    polars_lazyframe = pl.from_pandas(pandas_df).lazy()\n    return polars_lazyframe\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/delta_sharing/#src.sdk.python.rtdip_sdk.pipelines.sources.python.delta_sharing.PythonDeltaSharingSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Reading from a Delta table with Delta Sharing using Python is only possible for batch reads.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/delta_sharing.py</code> <pre><code>def read_stream(self):\n    \"\"\"\n    Raises:\n        NotImplementedError: Reading from a Delta table with Delta Sharing using Python is only possible for batch reads.\n    \"\"\"\n    raise NotImplementedError(\n        \"Reading from a Delta table with Delta Sharing using Python is only possible for batch reads.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/entsoe/","title":"Read from ENTSO-E API","text":""},{"location":"sdk/code-reference/pipelines/sources/python/entsoe/#src.sdk.python.rtdip_sdk.pipelines.sources.python.entsoe.PythonEntsoeSource","title":"<code>PythonEntsoeSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>The Python ENTSO-E Source is used to read day-ahead prices from ENTSO-E.</p>"},{"location":"sdk/code-reference/pipelines/sources/python/entsoe/#src.sdk.python.rtdip_sdk.pipelines.sources.python.entsoe.PythonEntsoeSource--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import PythonEntsoeSource\n\nentsoe_source = PythonEntsoeSource(\n    api_key={API_KEY},\n    start='20230101',\n    end='20231001',\n    country_code='NL'\n)\n\nentsoe_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>API token for ENTSO-E, to request access see documentation here</p> required <code>start</code> <code>str</code> <p>Start time in the format YYYYMMDD</p> required <code>end</code> <code>str</code> <p>End time in the format YYYYMMDD</p> required <code>country_code</code> <code>str</code> <p>Country code to query from. A full list of country codes can be found here</p> required <code>resolution</code> <code>optional str</code> <p>Frequency of values; '60T' for hourly values, '30T' for half-hourly values or '15T' for quarterly values</p> <code>'60T'</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/entsoe.py</code> <pre><code>class PythonEntsoeSource(SourceInterface):\n    \"\"\"\n    The Python ENTSO-E Source is used to read day-ahead prices from ENTSO-E.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.sources import PythonEntsoeSource\n\n    entsoe_source = PythonEntsoeSource(\n        api_key={API_KEY},\n        start='20230101',\n        end='20231001',\n        country_code='NL'\n    )\n\n    entsoe_source.read_batch()\n    ```\n\n    Args:\n        api_key (str): API token for ENTSO-E, to request access see documentation [here](https://transparency.entsoe.eu/content/static_content/Static%20content/web%20api/Guide.html#_authentication_and_authorisation)\n        start (str): Start time in the format YYYYMMDD\n        end (str): End time in the format YYYYMMDD\n        country_code (str): Country code to query from. A full list of country codes can be found [here](https://github.com/EnergieID/entsoe-py/blob/master/entsoe/mappings.py#L48)\n        resolution (optional str): Frequency of values; '60T' for hourly values, '30T' for half-hourly values or '15T' for quarterly values\n    \"\"\"\n\n    api_key: str\n    start: str\n    end: str\n    country_code: str\n    resolution: str\n\n    def __init__(\n        self, api_key, start, end, country_code, resolution: str = \"60T\"\n    ) -&gt; None:\n        self.key = api_key\n        self.start = pd.Timestamp(start, tz=\"UTC\")\n        self.end = pd.Timestamp(end, tz=\"UTC\")\n        self.country = country_code\n        self.resolution = resolution\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires Python\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self):\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Reads batch from ENTSO-E API.\n        \"\"\"\n        client = EntsoePandasClient(api_key=self.key)\n        df = client.query_day_ahead_prices(self.country, start=self.start, end=self.end)\n        df = pd.DataFrame(df, columns=[\"Price\"])\n        df[\"Name\"] = \"APX\"\n        return df\n\n    def read_stream(self):\n        \"\"\"\n        Raises:\n            NotImplementedError: ENTSO-E connector does not support the stream operation.\n        \"\"\"\n        raise NotImplementedError(\n            \"ENTSO-E connector does not support the stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/entsoe/#src.sdk.python.rtdip_sdk.pipelines.sources.python.entsoe.PythonEntsoeSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires Python</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/entsoe.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires Python\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/entsoe/#src.sdk.python.rtdip_sdk.pipelines.sources.python.entsoe.PythonEntsoeSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads batch from ENTSO-E API.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/entsoe.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Reads batch from ENTSO-E API.\n    \"\"\"\n    client = EntsoePandasClient(api_key=self.key)\n    df = client.query_day_ahead_prices(self.country, start=self.start, end=self.end)\n    df = pd.DataFrame(df, columns=[\"Price\"])\n    df[\"Name\"] = \"APX\"\n    return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/entsoe/#src.sdk.python.rtdip_sdk.pipelines.sources.python.entsoe.PythonEntsoeSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Raises:</p> Type Description <code>NotImplementedError</code> <p>ENTSO-E connector does not support the stream operation.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/entsoe.py</code> <pre><code>def read_stream(self):\n    \"\"\"\n    Raises:\n        NotImplementedError: ENTSO-E connector does not support the stream operation.\n    \"\"\"\n    raise NotImplementedError(\n        \"ENTSO-E connector does not support the stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/mffbas/","title":"Read from MFFBAS API","text":""},{"location":"sdk/code-reference/pipelines/sources/python/mffbas/#src.sdk.python.rtdip_sdk.pipelines.sources.python.mffbas.PythonMFFBASSource","title":"<code>PythonMFFBASSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>The Python MFFBAS Source is used to read the Standaard Jaar Verbruiksprofielen (Standard Consumption Profiles) from the MFFBAS API. More information on the Standard Consumption Profiles can be found here.</p>"},{"location":"sdk/code-reference/pipelines/sources/python/mffbas/#src.sdk.python.rtdip_sdk.pipelines.sources.python.mffbas.PythonMFFBASSource--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import PythonMFFBASSource\n\nsjv_source = PythonMFFBASSource(\n   start=\"2024-01-01\",\n   end=\"2024-01-02\"\n)\n\nsjv_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>str</code> <p>Start date in the format YYYY-MM-DD</p> required <code>end</code> <code>str</code> <p>End date in the format YYYY-MM-DD</p> required <p>Note</p> <p>It is not possible to collect fractions over a period before 2023-04-01 with this API. Requests are limited to a maximum of 31 days at a time.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/mffbas.py</code> <pre><code>class PythonMFFBASSource(SourceInterface):\n    \"\"\"\n    The Python MFFBAS Source is used to read the Standaard Jaar Verbruiksprofielen (Standard Consumption Profiles) from the MFFBAS API. More information on the Standard Consumption Profiles can be found [here](https://www.mffbas.nl/documenten/).\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.sources import PythonMFFBASSource\n\n    sjv_source = PythonMFFBASSource(\n       start=\"2024-01-01\",\n       end=\"2024-01-02\"\n    )\n\n    sjv_source.read_batch()\n    ```\n\n    Args:\n       start (str): Start date in the format YYYY-MM-DD\n       end (str): End date in the format YYYY-MM-DD\n\n    !!! note \"Note\"\n        It is not possible to collect fractions over a period before 2023-04-01 with this API. Requests are limited to a maximum of 31 days at a time.\n\n    \"\"\"\n\n    start: str\n    end: str\n\n    def __init__(self, start, end) -&gt; None:\n        self.start = start\n        self.end = end\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n              SystemType (Environment): Requires Python\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self):\n        return True\n\n    def _pull_data(self):\n        url = \"https://gateway.edsn.nl/energyvalues/profile-fractions-series/v1/profile-fractions\"\n\n        parameters = {\n            \"startdate\": self.start,\n            \"enddate\": self.end,\n            \"pftype\": \"STANDARD\",\n            \"product\": \"023\",\n        }\n\n        response = requests.request(\"GET\", url, params=parameters)\n\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n\n        data = response.json()\n\n        return data\n\n    def _prepare_data(self):\n        data = self._pull_data()\n        df = pd.DataFrame.from_dict(data[\"Detail_SeriesList\"])\n\n        df.rename(columns={\"calendar_date\": \"Versienr\"}, inplace=True)\n        df = df.explode(\"PointList\")\n        df = pd.concat(\n            [df.drop([\"PointList\"], axis=1), df[\"PointList\"].apply(pd.Series)], axis=1\n        )\n        df[\"direction\"] = df[\"direction\"].map({\"E17\": \"A\", \"E18\": \"I\"})\n        df[\"profiles\"] = df[\n            [\"profileCategory\", \"determinedConsumption\", \"direction\"]\n        ].agg(lambda x: \"_\".join(x.dropna()), axis=1)\n        df[\"Versienr\"] = pd.to_datetime(df[\"Versienr\"]) + pd.to_timedelta(\n            df[\"pos\"] * 15, unit=\"min\"\n        )\n        df = df[df[\"pos\"] &lt; 96]\n        drop = [\n            \"direction\",\n            \"pFdate_version\",\n            \"profileCategory\",\n            \"determinedConsumption\",\n            \"pos\",\n            \"resolution\",\n            \"profileStatus_quality\",\n        ]\n        df.drop(columns=drop, axis=1, inplace=True)\n\n        result = df.pivot(index=\"Versienr\", columns=\"profiles\", values=\"qnt\")\n        result[\"year_created\"] = result.index.strftime(\"%Y-%m-%d\")\n\n        return result\n\n    def read_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Reads batch from the MFFBAS API.\n        \"\"\"\n        try:\n            df = self._prepare_data()\n            return df\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self):\n        \"\"\"\n        Raises:\n              NotImplementedError: MFFBAS connector does not support the stream operation.\n        \"\"\"\n        raise NotImplementedError(\n            \"MFFBAS connector does not support the stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/mffbas/#src.sdk.python.rtdip_sdk.pipelines.sources.python.mffbas.PythonMFFBASSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires Python</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/mffbas.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n          SystemType (Environment): Requires Python\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/mffbas/#src.sdk.python.rtdip_sdk.pipelines.sources.python.mffbas.PythonMFFBASSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads batch from the MFFBAS API.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/mffbas.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Reads batch from the MFFBAS API.\n    \"\"\"\n    try:\n        df = self._prepare_data()\n        return df\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/python/mffbas/#src.sdk.python.rtdip_sdk.pipelines.sources.python.mffbas.PythonMFFBASSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Raises:</p> Type Description <code>NotImplementedError</code> <p>MFFBAS connector does not support the stream operation.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/python/mffbas.py</code> <pre><code>def read_stream(self):\n    \"\"\"\n    Raises:\n          NotImplementedError: MFFBAS connector does not support the stream operation.\n    \"\"\"\n    raise NotImplementedError(\n        \"MFFBAS connector does not support the stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/autoloader/","title":"Read from Autoloader","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/autoloader/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.autoloader.DataBricksAutoLoaderSource","title":"<code>DataBricksAutoLoaderSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>The Spark Auto Loader is used to read new data files as they arrive in cloud storage. Further information on Auto Loader is available here</p>"},{"location":"sdk/code-reference/pipelines/sources/spark/autoloader/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.autoloader.DataBricksAutoLoaderSource--example","title":"Example","text":"ADLS Gen2AWS S3GCS <pre><code>from rtdip_sdk.pipelines.sources import DataBricksAutoLoaderSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\noptions = {}\npath = \"abfss://{FILE-SYSTEM}@{ACCOUNT-NAME}.dfs.core.windows.net/{PATH}/{FILE-NAME}\nformat = \"{DESIRED-FILE-FORMAT}\"\n\nDataBricksAutoLoaderSource(spark, options, path, format).read_stream()\n\nOR\n\nDataBricksAutoLoaderSource(spark, options, path, format).read_batch()\n</code></pre> <pre><code>from rtdip_sdk.pipelines.sources import DataBricksAutoLoaderSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\noptions = {}\npath = \"https://s3.{REGION-CODE}.amazonaws.com/{BUCKET-NAME}/{KEY-NAME}\"\nformat = \"{DESIRED-FILE-FORMAT}\"\n\nDataBricksAutoLoaderSource(spark, options, path, format).read_stream()\n\nOR\n\nDataBricksAutoLoaderSource(spark, options, path, format).read_batch()\n</code></pre> <pre><code>from rtdip_sdk.pipelines.sources import DataBricksAutoLoaderSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\noptions = {}\npath = \"gs://{BUCKET-NAME}/{FILE-PATH}\"\nformat = \"{DESIRED-FILE-FORMAT}\"\n\nDataBricksAutoLoaderSource(spark, options, path, format).read_stream()\n\nOR\n\nDataBricksAutoLoaderSource(spark, options, path, format).read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from cloud storage</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for configuring the Auto Loader. Further information on the options available are here</p> required <code>path</code> <code>str</code> <p>The cloud storage path</p> required <code>format</code> <code>str</code> <p>Specifies the file format to be read. Supported formats are available here</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/autoloader.py</code> <pre><code>class DataBricksAutoLoaderSource(SourceInterface):\n    \"\"\"\n    The Spark Auto Loader is used to read new data files as they arrive in cloud storage. Further information on Auto Loader is available [here](https://docs.databricks.com/ingestion/auto-loader/index.html)\n\n    Example\n    --------\n    === \"ADLS Gen2\"\n\n        ```python\n        from rtdip_sdk.pipelines.sources import DataBricksAutoLoaderSource\n        from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n        # Not required if using Databricks\n        spark = SparkSessionUtility(config={}).execute()\n\n        options = {}\n        path = \"abfss://{FILE-SYSTEM}@{ACCOUNT-NAME}.dfs.core.windows.net/{PATH}/{FILE-NAME}\n        format = \"{DESIRED-FILE-FORMAT}\"\n\n        DataBricksAutoLoaderSource(spark, options, path, format).read_stream()\n\n        OR\n\n        DataBricksAutoLoaderSource(spark, options, path, format).read_batch()\n        ```\n    === \"AWS S3\"\n\n        ```python\n        from rtdip_sdk.pipelines.sources import DataBricksAutoLoaderSource\n        from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n        # Not required if using Databricks\n        spark = SparkSessionUtility(config={}).execute()\n\n        options = {}\n        path = \"https://s3.{REGION-CODE}.amazonaws.com/{BUCKET-NAME}/{KEY-NAME}\"\n        format = \"{DESIRED-FILE-FORMAT}\"\n\n        DataBricksAutoLoaderSource(spark, options, path, format).read_stream()\n\n        OR\n\n        DataBricksAutoLoaderSource(spark, options, path, format).read_batch()\n        ```\n    === \"GCS\"\n\n        ```python\n        from rtdip_sdk.pipelines.sources import DataBricksAutoLoaderSource\n        from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n        # Not required if using Databricks\n        spark = SparkSessionUtility(config={}).execute()\n\n        options = {}\n        path = \"gs://{BUCKET-NAME}/{FILE-PATH}\"\n        format = \"{DESIRED-FILE-FORMAT}\"\n\n        DataBricksAutoLoaderSource(spark, options, path, format).read_stream()\n\n        OR\n\n        DataBricksAutoLoaderSource(spark, options, path, format).read_batch()\n        ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to read data from cloud storage\n        options (dict): Options that can be specified for configuring the Auto Loader. Further information on the options available are [here](https://docs.databricks.com/ingestion/auto-loader/options.html)\n        path (str): The cloud storage path\n        format (str): Specifies the file format to be read. Supported formats are available [here](https://docs.databricks.com/ingestion/auto-loader/options.html#file-format-options)\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    path: str\n\n    def __init__(\n        self, spark: SparkSession, options: dict, path: str, format: str\n    ) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.path = path\n        self.options[\"cloudFiles.format\"] = format\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK on Databricks\n        \"\"\"\n        return SystemType.PYSPARK_DATABRICKS\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_core\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self, df: DataFrame):\n        return True\n\n    def read_batch(self):\n        \"\"\"\n        Raises:\n            NotImplementedError: Auto Loader only supports streaming reads. To perform a batch read, use the read_stream method of this component and specify the Trigger on the write_stream to be `availableNow` to perform batch-like reads of cloud storage files.\n        \"\"\"\n        raise NotImplementedError(\n            \"Auto Loader only supports streaming reads. To perform a batch read, use the read_stream method and specify Trigger on the write_stream as `availableNow`\"\n        )\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        Performs streaming reads of files in cloud storage.\n        \"\"\"\n        try:\n            return (\n                self.spark.readStream.format(\"cloudFiles\")\n                .options(**self.options)\n                .load(self.path)\n            )\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/autoloader/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.autoloader.DataBricksAutoLoaderSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK on Databricks</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/autoloader.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK on Databricks\n    \"\"\"\n    return SystemType.PYSPARK_DATABRICKS\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/autoloader/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.autoloader.DataBricksAutoLoaderSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Auto Loader only supports streaming reads. To perform a batch read, use the read_stream method of this component and specify the Trigger on the write_stream to be <code>availableNow</code> to perform batch-like reads of cloud storage files.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/autoloader.py</code> <pre><code>def read_batch(self):\n    \"\"\"\n    Raises:\n        NotImplementedError: Auto Loader only supports streaming reads. To perform a batch read, use the read_stream method of this component and specify the Trigger on the write_stream to be `availableNow` to perform batch-like reads of cloud storage files.\n    \"\"\"\n    raise NotImplementedError(\n        \"Auto Loader only supports streaming reads. To perform a batch read, use the read_stream method and specify Trigger on the write_stream as `availableNow`\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/autoloader/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.autoloader.DataBricksAutoLoaderSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Performs streaming reads of files in cloud storage.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/autoloader.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    Performs streaming reads of files in cloud storage.\n    \"\"\"\n    try:\n        return (\n            self.spark.readStream.format(\"cloudFiles\")\n            .options(**self.options)\n            .load(self.path)\n        )\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta/","title":"Read from Delta","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta.SparkDeltaSource","title":"<code>SparkDeltaSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>The Spark Delta Source is used to read data from a Delta table.</p>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta.SparkDeltaSource--example","title":"Example","text":"<p><pre><code>#Delta Source for Streaming Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkDeltaSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\ndelta_source = SparkDeltaSource(\n    spark=spark,\n    options={\n        \"maxFilesPerTrigger\": 1000,\n        \"ignoreChanges: True,\n        \"startingVersion\": 0\n    },\n    table_name=\"{YOUR-DELTA-TABLE-PATH}\"\n)\n\ndelta_source.read_stream()\n</code></pre> <pre><code>#Delta Source for Batch Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkDeltaSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\ndelta_source = SparkDeltaSource(\n    spark=spark,\n    options={\n        \"versionAsOf\": 0,\n        \"timestampAsOf\": \"yyyy-mm-dd hh:mm:ss[.fffffffff]\"\n    },\n    table_name=\"{YOUR-DELTA-TABLE-PATH}\"\n)\n\ndelta_source.read_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from a Delta table.</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for batch and streaming.</p> required <code>table_name</code> <code>str</code> <p>Name of the Hive Metastore or Unity Catalog Delta Table</p> required <p>Attributes:</p> Name Type Description <code>maxFilesPerTrigger</code> <code>int</code> <p>How many new files to be considered in every micro-batch. The default is 1000. (Streaming)</p> <code>maxBytesPerTrigger</code> <code>int</code> <p>How much data gets processed in each micro-batch. (Streaming)</p> <code>ignoreDeletes</code> <code>bool str</code> <p>Ignore transactions that delete data at partition boundaries. (Streaming)</p> <code>ignoreChanges</code> <code>bool str</code> <p>Pre-process updates if files had to be rewritten in the source table due to a data changing operation. (Streaming)</p> <code>startingVersion</code> <code>int str</code> <p>The Delta Lake version to start from. (Streaming)</p> <code>startingTimestamp</code> <code>datetime str</code> <p>The timestamp to start from. (Streaming)</p> <code>withEventTimeOrder</code> <code>bool str</code> <p>Whether the initial snapshot should be processed with event time order. (Streaming)</p> <code>timestampAsOf</code> <code>datetime str</code> <p>Query the Delta Table from a specific point in time. (Batch)</p> <code>versionAsOf</code> <code>int str</code> <p>Query the Delta Table from a specific version. (Batch)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta.py</code> <pre><code>class SparkDeltaSource(SourceInterface):\n    \"\"\"\n    The Spark Delta Source is used to read data from a Delta table.\n\n    Example\n    --------\n    ```python\n    #Delta Source for Streaming Queries\n\n    from rtdip_sdk.pipelines.sources import SparkDeltaSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    delta_source = SparkDeltaSource(\n        spark=spark,\n        options={\n            \"maxFilesPerTrigger\": 1000,\n            \"ignoreChanges: True,\n            \"startingVersion\": 0\n        },\n        table_name=\"{YOUR-DELTA-TABLE-PATH}\"\n    )\n\n    delta_source.read_stream()\n    ```\n    ```python\n    #Delta Source for Batch Queries\n\n    from rtdip_sdk.pipelines.sources import SparkDeltaSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    delta_source = SparkDeltaSource(\n        spark=spark,\n        options={\n            \"versionAsOf\": 0,\n            \"timestampAsOf\": \"yyyy-mm-dd hh:mm:ss[.fffffffff]\"\n        },\n        table_name=\"{YOUR-DELTA-TABLE-PATH}\"\n    )\n\n    delta_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to read data from a Delta table.\n        options (dict): Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available for [batch](https://docs.delta.io/latest/delta-batch.html#read-a-table){ target=\"_blank\" } and [streaming](https://docs.delta.io/latest/delta-streaming.html#delta-table-as-a-source){ target=\"_blank\" }.\n        table_name (str): Name of the Hive Metastore or Unity Catalog Delta Table\n\n    Attributes:\n        maxFilesPerTrigger (int): How many new files to be considered in every micro-batch. The default is 1000. (Streaming)\n        maxBytesPerTrigger (int): How much data gets processed in each micro-batch. (Streaming)\n        ignoreDeletes (bool str): Ignore transactions that delete data at partition boundaries. (Streaming)\n        ignoreChanges (bool str): Pre-process updates if files had to be rewritten in the source table due to a data changing operation. (Streaming)\n        startingVersion (int str): The Delta Lake version to start from. (Streaming)\n        startingTimestamp (datetime str): The timestamp to start from. (Streaming)\n        withEventTimeOrder (bool str): Whether the initial snapshot should be processed with event time order. (Streaming)\n        timestampAsOf (datetime str): Query the Delta Table from a specific point in time. (Batch)\n        versionAsOf (int str): Query the Delta Table from a specific version. (Batch)\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    table_name: str\n\n    def __init__(self, spark: SparkSession, options: dict, table_name: str) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.table_name = table_name\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_core\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self):\n        return True\n\n    def read_batch(self):\n        \"\"\"\n        Reads batch data from Delta. Most of the options provided by the Apache Spark DataFrame read API are supported for performing batch reads on Delta tables.\n        \"\"\"\n        try:\n            return (\n                self.spark.read.format(\"delta\")\n                .options(**self.options)\n                .table(self.table_name)\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        Reads streaming data from Delta. All of the data in the table is processed as well as any new data that arrives after the stream started. .load() can take table name or path.\n        \"\"\"\n        try:\n            return (\n                self.spark.readStream.format(\"delta\")\n                .options(**self.options)\n                .load(self.table_name)\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta.SparkDeltaSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta.SparkDeltaSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads batch data from Delta. Most of the options provided by the Apache Spark DataFrame read API are supported for performing batch reads on Delta tables.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta.py</code> <pre><code>def read_batch(self):\n    \"\"\"\n    Reads batch data from Delta. Most of the options provided by the Apache Spark DataFrame read API are supported for performing batch reads on Delta tables.\n    \"\"\"\n    try:\n        return (\n            self.spark.read.format(\"delta\")\n            .options(**self.options)\n            .table(self.table_name)\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta.SparkDeltaSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Reads streaming data from Delta. All of the data in the table is processed as well as any new data that arrives after the stream started. .load() can take table name or path.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    Reads streaming data from Delta. All of the data in the table is processed as well as any new data that arrives after the stream started. .load() can take table name or path.\n    \"\"\"\n    try:\n        return (\n            self.spark.readStream.format(\"delta\")\n            .options(**self.options)\n            .load(self.table_name)\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta_sharing/","title":"Read from Delta sharing","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/delta_sharing/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta_sharing.SparkDeltaSharingSource","title":"<code>SparkDeltaSharingSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>The Spark Delta Sharing Source is used to read data from a Delta table where Delta sharing is configured</p>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta_sharing/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta_sharing.SparkDeltaSharingSource--example","title":"Example","text":"<p><pre><code>#Delta Sharing Source for Streaming Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkDeltaSharingSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\ndelta_sharing_source = SparkDeltaSharingSource(\n    spark=spark,\n    options={\n        \"maxFilesPerTrigger\": 1000,\n        \"ignoreChanges: True,\n        \"startingVersion\": 0\n    },\n    table_name=\"{YOUR-DELTA-TABLE-PATH}\"\n)\n\ndelta_sharing_source.read_stream()\n</code></pre> <pre><code>#Delta Sharing Source for Batch Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkDeltaSharingSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\ndelta_sharing_source = SparkDeltaSharingSource(\n    spark=spark,\n    options={\n        \"versionAsOf\": 0,\n        \"timestampAsOf\": \"yyyy-mm-dd hh:mm:ss[.fffffffff]\"\n    },\n    table_name=\"{YOUR-DELTA-TABLE-PATH}\"\n)\n\ndelta_sharing_source.read_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from a Delta table</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available here</p> required <code>table_path</code> <code>str</code> <p>Path to credentials file and Delta table to query</p> required <p>Attributes:</p> Name Type Description <code>ignoreDeletes</code> <code>bool str</code> <p>Ignore transactions that delete data at partition boundaries. (Streaming)</p> <code>ignoreChanges</code> <code>bool str</code> <p>Pre-process updates if files had to be rewritten in the source table due to a data changing operation. (Streaming)</p> <code>startingVersion</code> <code>int str</code> <p>The Delta Lake version to start from. (Streaming)</p> <code>startingTimestamp</code> <code>datetime str</code> <p>The timestamp to start from. (Streaming)</p> <code>maxFilesPerTrigger</code> <code>int</code> <p>How many new files to be considered in every micro-batch. The default is 1000. (Streaming)</p> <code>maxBytesPerTrigger</code> <code>int</code> <p>How much data gets processed in each micro-batch. (Streaming)</p> <code>readChangeFeed</code> <code>bool str</code> <p>Stream read the change data feed of the shared table. (Batch &amp; Streaming)</p> <code>timestampAsOf</code> <code>datetime str</code> <p>Query the Delta Table from a specific point in time. (Batch)</p> <code>versionAsOf</code> <code>int str</code> <p>Query the Delta Table from a specific version. (Batch)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta_sharing.py</code> <pre><code>class SparkDeltaSharingSource(SourceInterface):\n    \"\"\"\n    The Spark Delta Sharing Source is used to read data from a Delta table where Delta sharing is configured\n\n    Example\n    --------\n    ```python\n    #Delta Sharing Source for Streaming Queries\n\n    from rtdip_sdk.pipelines.sources import SparkDeltaSharingSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    delta_sharing_source = SparkDeltaSharingSource(\n        spark=spark,\n        options={\n            \"maxFilesPerTrigger\": 1000,\n            \"ignoreChanges: True,\n            \"startingVersion\": 0\n        },\n        table_name=\"{YOUR-DELTA-TABLE-PATH}\"\n    )\n\n    delta_sharing_source.read_stream()\n    ```\n    ```python\n    #Delta Sharing Source for Batch Queries\n\n    from rtdip_sdk.pipelines.sources import SparkDeltaSharingSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    delta_sharing_source = SparkDeltaSharingSource(\n        spark=spark,\n        options={\n            \"versionAsOf\": 0,\n            \"timestampAsOf\": \"yyyy-mm-dd hh:mm:ss[.fffffffff]\"\n        },\n        table_name=\"{YOUR-DELTA-TABLE-PATH}\"\n    )\n\n    delta_sharing_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to read data from a Delta table\n        options (dict): Options that can be specified for a Delta Table read operation (See Attributes table below). Further information on the options is available [here](https://docs.databricks.com/data-sharing/read-data-open.html#apache-spark-read-shared-data){ target=\"_blank\" }\n        table_path (str): Path to credentials file and Delta table to query\n\n    Attributes:\n        ignoreDeletes (bool str): Ignore transactions that delete data at partition boundaries. (Streaming)\n        ignoreChanges (bool str): Pre-process updates if files had to be rewritten in the source table due to a data changing operation. (Streaming)\n        startingVersion (int str): The Delta Lake version to start from. (Streaming)\n        startingTimestamp (datetime str): The timestamp to start from. (Streaming)\n        maxFilesPerTrigger (int): How many new files to be considered in every micro-batch. The default is 1000. (Streaming)\n        maxBytesPerTrigger (int): How much data gets processed in each micro-batch. (Streaming)\n        readChangeFeed (bool str): Stream read the change data feed of the shared table. (Batch &amp; Streaming)\n        timestampAsOf (datetime str): Query the Delta Table from a specific point in time. (Batch)\n        versionAsOf (int str): Query the Delta Table from a specific version. (Batch)\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    table_path: str\n\n    def __init__(self, spark: SparkSession, options: dict, table_path: str) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.table_path = table_path\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_sharing\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self):\n        return True\n\n    def read_batch(self):\n        \"\"\"\n        Reads batch data from Delta. Most of the options provided by the Apache Spark DataFrame read API are supported for performing batch reads on Delta tables.\n        \"\"\"\n        try:\n            return (\n                self.spark.read.format(\"deltaSharing\")\n                .options(**self.options)\n                .table(self.table_path)\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        Reads streaming data from Delta. All of the data in the table is processed as well as any new data that arrives after the stream started. .load() can take table name or path.\n        \"\"\"\n        try:\n            return (\n                self.spark.readStream.format(\"deltaSharing\")\n                .options(**self.options)\n                .load(self.table_path)\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta_sharing/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta_sharing.SparkDeltaSharingSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta_sharing.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta_sharing/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta_sharing.SparkDeltaSharingSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads batch data from Delta. Most of the options provided by the Apache Spark DataFrame read API are supported for performing batch reads on Delta tables.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta_sharing.py</code> <pre><code>def read_batch(self):\n    \"\"\"\n    Reads batch data from Delta. Most of the options provided by the Apache Spark DataFrame read API are supported for performing batch reads on Delta tables.\n    \"\"\"\n    try:\n        return (\n            self.spark.read.format(\"deltaSharing\")\n            .options(**self.options)\n            .table(self.table_path)\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/delta_sharing/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.delta_sharing.SparkDeltaSharingSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Reads streaming data from Delta. All of the data in the table is processed as well as any new data that arrives after the stream started. .load() can take table name or path.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/delta_sharing.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    Reads streaming data from Delta. All of the data in the table is processed as well as any new data that arrives after the stream started. .load() can take table name or path.\n    \"\"\"\n    try:\n        return (\n            self.spark.readStream.format(\"deltaSharing\")\n            .options(**self.options)\n            .load(self.table_path)\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/eventhub/","title":"Read from an Eventhub","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.eventhub.SparkEventhubSource","title":"<code>SparkEventhubSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>This Spark source class is used to read batch or streaming data from Eventhubs. Eventhub configurations need to be specified as options in a dictionary. Additionally, there are more optional configurations which can be found here. If using startingPosition or endingPosition make sure to check out the Event Position section for more details and examples.</p>"},{"location":"sdk/code-reference/pipelines/sources/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.eventhub.SparkEventhubSource--example","title":"Example","text":"<p><pre><code>#Eventhub Source for Streaming Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkEventhubSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\nimport json\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconnectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n\nstartingEventPosition = {\n\"offset\": -1,\n\"seqNo\": -1,\n\"enqueuedTime\": None,\n\"isInclusive\": True\n}\n\neventhub_source = SparkEventhubSource(\n    spark=spark,\n    options = {\n        \"eventhubs.connectionString\": connectionString,\n        \"eventhubs.consumerGroup\": \"{YOUR-CONSUMER-GROUP}\",\n        \"eventhubs.startingPosition\": json.dumps(startingEventPosition),\n        \"maxEventsPerTrigger\" : 1000\n    }\n)\n\neventhub_source.read_stream()\n</code></pre> <pre><code> #Eventhub Source for Batch Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkEventhubSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\nimport json\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconnectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n\nstartingEventPosition = {\n    \"offset\": -1,\n    \"seqNo\": -1,\n    \"enqueuedTime\": None,\n    \"isInclusive\": True\n}\n\nendingEventPosition = {\n    \"offset\": None,\n    \"seqNo\": -1,\n    \"enqueuedTime\": endTime,\n    \"isInclusive\": True\n}\n\neventhub_source = SparkEventhubSource(\n    spark,\n    options = {\n        \"eventhubs.connectionString\": connectionString,\n        \"eventhubs.consumerGroup\": \"{YOUR-CONSUMER-GROUP}\",\n        \"eventhubs.startingPosition\": json.dumps(startingEventPosition),\n        \"eventhubs.endingPosition\": json.dumps(endingEventPosition)\n    }\n)\n\neventhub_source.read_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session</p> required <code>options</code> <code>dict</code> <p>A dictionary of Eventhub configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>eventhubs.connectionString</code> <code>str</code> <p>Eventhubs connection string is required to connect to the Eventhubs service. (Streaming and Batch)</p> <code>eventhubs.consumerGroup</code> <code>str</code> <p>A consumer group is a view of an entire eventhub. Consumer groups enable multiple consuming applications to each have a separate view of the event stream, and to read the stream independently at their own pace and with their own offsets. (Streaming and Batch)</p> <code>eventhubs.startingPosition</code> <code>JSON str</code> <p>The starting position for your Structured Streaming job. If a specific EventPosition is not set for a partition using startingPositions, then we use the EventPosition set in startingPosition. If nothing is set in either option, we will begin consuming from the end of the partition. (Streaming and Batch)</p> <code>eventhubs.endingPosition</code> <code>JSON str</code> <p>(JSON str): The ending position of a batch query. This works the same as startingPosition. (Batch)</p> <code>maxEventsPerTrigger</code> <code>long</code> <p>Rate limit on maximum number of events processed per trigger interval. The specified total number of events will be proportionally split across partitions of different volume. (Stream)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/eventhub.py</code> <pre><code>class SparkEventhubSource(SourceInterface):\n    \"\"\"\n    This Spark source class is used to read batch or streaming data from Eventhubs. Eventhub configurations need to be specified as options in a dictionary.\n    Additionally, there are more optional configurations which can be found [here.](https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/PySpark/structured-streaming-pyspark.md#event-hubs-configuration){ target=\"_blank\" }\n    If using startingPosition or endingPosition make sure to check out the **Event Position** section for more details and examples.\n\n    Example\n    --------\n    ```python\n    #Eventhub Source for Streaming Queries\n\n    from rtdip_sdk.pipelines.sources import SparkEventhubSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n    import json\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    connectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n\n    startingEventPosition = {\n    \"offset\": -1,\n    \"seqNo\": -1,\n    \"enqueuedTime\": None,\n    \"isInclusive\": True\n    }\n\n    eventhub_source = SparkEventhubSource(\n        spark=spark,\n        options = {\n            \"eventhubs.connectionString\": connectionString,\n            \"eventhubs.consumerGroup\": \"{YOUR-CONSUMER-GROUP}\",\n            \"eventhubs.startingPosition\": json.dumps(startingEventPosition),\n            \"maxEventsPerTrigger\" : 1000\n        }\n    )\n\n    eventhub_source.read_stream()\n    ```\n    ```python\n     #Eventhub Source for Batch Queries\n\n    from rtdip_sdk.pipelines.sources import SparkEventhubSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n    import json\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    connectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n\n    startingEventPosition = {\n        \"offset\": -1,\n        \"seqNo\": -1,\n        \"enqueuedTime\": None,\n        \"isInclusive\": True\n    }\n\n    endingEventPosition = {\n        \"offset\": None,\n        \"seqNo\": -1,\n        \"enqueuedTime\": endTime,\n        \"isInclusive\": True\n    }\n\n    eventhub_source = SparkEventhubSource(\n        spark,\n        options = {\n            \"eventhubs.connectionString\": connectionString,\n            \"eventhubs.consumerGroup\": \"{YOUR-CONSUMER-GROUP}\",\n            \"eventhubs.startingPosition\": json.dumps(startingEventPosition),\n            \"eventhubs.endingPosition\": json.dumps(endingEventPosition)\n        }\n    )\n\n    eventhub_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session\n        options (dict): A dictionary of Eventhub configurations (See Attributes table below)\n\n    Attributes:\n        eventhubs.connectionString (str):  Eventhubs connection string is required to connect to the Eventhubs service. (Streaming and Batch)\n        eventhubs.consumerGroup (str): A consumer group is a view of an entire eventhub. Consumer groups enable multiple consuming applications to each have a separate view of the event stream, and to read the stream independently at their own pace and with their own offsets. (Streaming and Batch)\n        eventhubs.startingPosition (JSON str): The starting position for your Structured Streaming job. If a specific EventPosition is not set for a partition using startingPositions, then we use the EventPosition set in startingPosition. If nothing is set in either option, we will begin consuming from the end of the partition. (Streaming and Batch)\n        eventhubs.endingPosition: (JSON str): The ending position of a batch query. This works the same as startingPosition. (Batch)\n        maxEventsPerTrigger (long): Rate limit on maximum number of events processed per trigger interval. The specified total number of events will be proportionally split across partitions of different volume. (Stream)\n\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.schema = EVENTHUB_SCHEMA\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        spark_libraries = Libraries()\n        spark_libraries.add_maven_library(get_default_package(\"spark_azure_eventhub\"))\n        return spark_libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self) -&gt; bool:\n        return True\n\n    def post_read_validation(self, df: DataFrame) -&gt; bool:\n        assert df.schema == self.schema\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Reads batch data from Eventhubs.\n        \"\"\"\n        eventhub_connection_string = \"eventhubs.connectionString\"\n        try:\n            if eventhub_connection_string in self.options:\n                sc = self.spark.sparkContext\n                self.options[eventhub_connection_string] = (\n                    sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                        self.options[eventhub_connection_string]\n                    )\n                )\n\n            return self.spark.read.format(\"eventhubs\").options(**self.options).load()\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        Reads streaming data from Eventhubs.\n        \"\"\"\n        eventhub_connection_string = \"eventhubs.connectionString\"\n        try:\n            if eventhub_connection_string in self.options:\n                sc = self.spark.sparkContext\n                self.options[eventhub_connection_string] = (\n                    sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                        self.options[eventhub_connection_string]\n                    )\n                )\n\n            return (\n                self.spark.readStream.format(\"eventhubs\").options(**self.options).load()\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.eventhub.SparkEventhubSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/eventhub.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.eventhub.SparkEventhubSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads batch data from Eventhubs.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/eventhub.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Reads batch data from Eventhubs.\n    \"\"\"\n    eventhub_connection_string = \"eventhubs.connectionString\"\n    try:\n        if eventhub_connection_string in self.options:\n            sc = self.spark.sparkContext\n            self.options[eventhub_connection_string] = (\n                sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                    self.options[eventhub_connection_string]\n                )\n            )\n\n        return self.spark.read.format(\"eventhubs\").options(**self.options).load()\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/eventhub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.eventhub.SparkEventhubSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Reads streaming data from Eventhubs.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/eventhub.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    Reads streaming data from Eventhubs.\n    \"\"\"\n    eventhub_connection_string = \"eventhubs.connectionString\"\n    try:\n        if eventhub_connection_string in self.options:\n            sc = self.spark.sparkContext\n            self.options[eventhub_connection_string] = (\n                sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                    self.options[eventhub_connection_string]\n                )\n            )\n\n        return (\n            self.spark.readStream.format(\"eventhubs\").options(**self.options).load()\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iot_hub/","title":"Read from an IoT Hub","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/iot_hub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iot_hub.SparkIoThubSource","title":"<code>SparkIoThubSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>This Spark source class is used to read batch or streaming data from an IoT Hub. IoT Hub configurations need to be specified as options in a dictionary. Additionally, there are more optional configurations which can be found here. If using startingPosition or endingPosition make sure to check out the Event Position section for more details and examples.</p>"},{"location":"sdk/code-reference/pipelines/sources/spark/iot_hub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iot_hub.SparkIoThubSource--example","title":"Example","text":"<p><pre><code>#IoT Hub Source for Streaming Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkIoThubSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\nimport json\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconnectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n\nstartingEventPosition = {\n\"offset\": -1,\n\"seqNo\": -1,\n\"enqueuedTime\": None,\n\"isInclusive\": True\n}\n\niot_hub_source = SparkIoThubSource(\n    spark=spark,\n    options = {\n        \"eventhubs.connectionString\": connectionString,\n        \"eventhubs.consumerGroup\": \"{YOUR-CONSUMER-GROUP}\",\n        \"eventhubs.startingPosition\": json.dumps(startingEventPosition),\n        \"maxEventsPerTrigger\" : 1000\n    }\n)\n\niot_hub_source.read_stream()\n</code></pre> <pre><code> #IoT Hub Source for Batch Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkIoThubSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\nimport json\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconnectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n\nstartingEventPosition = {\n    \"offset\": -1,\n    \"seqNo\": -1,\n    \"enqueuedTime\": None,\n    \"isInclusive\": True\n}\n\nendingEventPosition = {\n    \"offset\": None,\n    \"seqNo\": -1,\n    \"enqueuedTime\": endTime,\n    \"isInclusive\": True\n}\n\niot_hub_source = SparkIoThubSource(\n    spark,\n    options = {\n        \"eventhubs.connectionString\": connectionString,\n        \"eventhubs.consumerGroup\": \"{YOUR-CONSUMER-GROUP}\",\n        \"eventhubs.startingPosition\": json.dumps(startingEventPosition),\n        \"eventhubs.endingPosition\": json.dumps(endingEventPosition)\n    }\n)\n\niot_hub_source.read_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session</p> required <code>options</code> <code>dict</code> <p>A dictionary of IoT Hub configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>eventhubs.connectionString</code> <code>str</code> <p>IoT Hub connection string is required to connect to the Eventhubs service. (Streaming and Batch)</p> <code>eventhubs.consumerGroup</code> <code>str</code> <p>A consumer group is a view of an entire IoT Hub. Consumer groups enable multiple consuming applications to each have a separate view of the event stream, and to read the stream independently at their own pace and with their own offsets. (Streaming and Batch)</p> <code>eventhubs.startingPosition</code> <code>JSON str</code> <p>The starting position for your Structured Streaming job. If a specific EventPosition is not set for a partition using startingPositions, then we use the EventPosition set in startingPosition. If nothing is set in either option, we will begin consuming from the end of the partition. (Streaming and Batch)</p> <code>eventhubs.endingPosition</code> <code>JSON str</code> <p>(JSON str): The ending position of a batch query. This works the same as startingPosition. (Batch)</p> <code>maxEventsPerTrigger</code> <code>long</code> <p>Rate limit on maximum number of events processed per trigger interval. The specified total number of events will be proportionally split across partitions of different volume. (Stream)</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iot_hub.py</code> <pre><code>class SparkIoThubSource(SourceInterface):\n    \"\"\"\n    This Spark source class is used to read batch or streaming data from an IoT Hub. IoT Hub configurations need to be specified as options in a dictionary.\n    Additionally, there are more optional configurations which can be found [here.](https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/PySpark/structured-streaming-pyspark.md#event-hubs-configuration){ target=\"_blank\" }\n    If using startingPosition or endingPosition make sure to check out the **Event Position** section for more details and examples.\n\n    Example\n    --------\n    ```python\n    #IoT Hub Source for Streaming Queries\n\n    from rtdip_sdk.pipelines.sources import SparkIoThubSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n    import json\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    connectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n\n    startingEventPosition = {\n    \"offset\": -1,\n    \"seqNo\": -1,\n    \"enqueuedTime\": None,\n    \"isInclusive\": True\n    }\n\n    iot_hub_source = SparkIoThubSource(\n        spark=spark,\n        options = {\n            \"eventhubs.connectionString\": connectionString,\n            \"eventhubs.consumerGroup\": \"{YOUR-CONSUMER-GROUP}\",\n            \"eventhubs.startingPosition\": json.dumps(startingEventPosition),\n            \"maxEventsPerTrigger\" : 1000\n        }\n    )\n\n    iot_hub_source.read_stream()\n    ```\n    ```python\n     #IoT Hub Source for Batch Queries\n\n    from rtdip_sdk.pipelines.sources import SparkIoThubSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n    import json\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    connectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n\n    startingEventPosition = {\n        \"offset\": -1,\n        \"seqNo\": -1,\n        \"enqueuedTime\": None,\n        \"isInclusive\": True\n    }\n\n    endingEventPosition = {\n        \"offset\": None,\n        \"seqNo\": -1,\n        \"enqueuedTime\": endTime,\n        \"isInclusive\": True\n    }\n\n    iot_hub_source = SparkIoThubSource(\n        spark,\n        options = {\n            \"eventhubs.connectionString\": connectionString,\n            \"eventhubs.consumerGroup\": \"{YOUR-CONSUMER-GROUP}\",\n            \"eventhubs.startingPosition\": json.dumps(startingEventPosition),\n            \"eventhubs.endingPosition\": json.dumps(endingEventPosition)\n        }\n    )\n\n    iot_hub_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session\n        options (dict): A dictionary of IoT Hub configurations (See Attributes table below)\n\n    Attributes:\n        eventhubs.connectionString (str):  IoT Hub connection string is required to connect to the Eventhubs service. (Streaming and Batch)\n        eventhubs.consumerGroup (str): A consumer group is a view of an entire IoT Hub. Consumer groups enable multiple consuming applications to each have a separate view of the event stream, and to read the stream independently at their own pace and with their own offsets. (Streaming and Batch)\n        eventhubs.startingPosition (JSON str): The starting position for your Structured Streaming job. If a specific EventPosition is not set for a partition using startingPositions, then we use the EventPosition set in startingPosition. If nothing is set in either option, we will begin consuming from the end of the partition. (Streaming and Batch)\n        eventhubs.endingPosition: (JSON str): The ending position of a batch query. This works the same as startingPosition. (Batch)\n        maxEventsPerTrigger (long): Rate limit on maximum number of events processed per trigger interval. The specified total number of events will be proportionally split across partitions of different volume. (Stream)\n\n    \"\"\"\n\n    options: dict\n    spark: SparkSession\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.schema = EVENTHUB_SCHEMA\n        self.options = options\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    @staticmethod\n    def libraries():\n        spark_libraries = Libraries()\n        spark_libraries.add_maven_library(get_default_package(\"spark_azure_eventhub\"))\n        return spark_libraries\n\n    def pre_read_validation(self) -&gt; bool:\n        return True\n\n    def post_read_validation(self, df: DataFrame) -&gt; bool:\n        assert df.schema == self.schema\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Reads batch data from IoT Hubs.\n        \"\"\"\n        iothub_connection_string = \"eventhubs.connectionString\"\n        try:\n            if iothub_connection_string in self.options:\n                sc = self.spark.sparkContext\n                self.options[iothub_connection_string] = (\n                    sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                        self.options[iothub_connection_string]\n                    )\n                )\n\n            return self.spark.read.format(\"eventhubs\").options(**self.options).load()\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        Reads streaming data from IoT Hubs.\n        \"\"\"\n        iothub_connection_string = \"eventhubs.connectionString\"\n        try:\n            if iothub_connection_string in self.options:\n                sc = self.spark.sparkContext\n                self.options[iothub_connection_string] = (\n                    sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                        self.options[iothub_connection_string]\n                    )\n                )\n\n            return (\n                self.spark.readStream.format(\"eventhubs\").options(**self.options).load()\n            )\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iot_hub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iot_hub.SparkIoThubSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iot_hub.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iot_hub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iot_hub.SparkIoThubSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads batch data from IoT Hubs.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iot_hub.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Reads batch data from IoT Hubs.\n    \"\"\"\n    iothub_connection_string = \"eventhubs.connectionString\"\n    try:\n        if iothub_connection_string in self.options:\n            sc = self.spark.sparkContext\n            self.options[iothub_connection_string] = (\n                sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                    self.options[iothub_connection_string]\n                )\n            )\n\n        return self.spark.read.format(\"eventhubs\").options(**self.options).load()\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iot_hub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iot_hub.SparkIoThubSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Reads streaming data from IoT Hubs.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iot_hub.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    Reads streaming data from IoT Hubs.\n    \"\"\"\n    iothub_connection_string = \"eventhubs.connectionString\"\n    try:\n        if iothub_connection_string in self.options:\n            sc = self.spark.sparkContext\n            self.options[iothub_connection_string] = (\n                sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                    self.options[iothub_connection_string]\n                )\n            )\n\n        return (\n            self.spark.readStream.format(\"eventhubs\").options(**self.options).load()\n        )\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/kafka/","title":"Read from Kafka","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/kafka/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kafka.SparkKafkaSource","title":"<code>SparkKafkaSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>This Spark source class is used to read batch or streaming data from Kafka. Required and optional configurations can be found in the Attributes tables below.</p> <p>Additionally, there are more optional configurations which can be found here.</p>"},{"location":"sdk/code-reference/pipelines/sources/spark/kafka/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kafka.SparkKafkaSource--example","title":"Example","text":"<p><pre><code> #Kafka Source for Streaming Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkKafkaSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nkafka_source = SparkKafkaSource(\n    spark=spark,\n    options={\n        \"kafka.bootstrap.servers\": \"{HOST_1}:{PORT_1},{HOST_2}:{PORT_2}\",\n        \"subscribe\": \"{TOPIC_1},{TOPIC_2}\",\n        \"includeHeaders\", \"true\"\n    }\n)\n\nkafka_source.read_stream()\n</code></pre> <pre><code> #Kafka Source for Batch Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkKafkaSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nkafka_source = SparkKafkaSource(\n    spark=spark,\n    options={\n        \"kafka.bootstrap.servers\": \"{HOST_1}:{PORT_1},{HOST_2}:{PORT_2}\",\n        \"subscribe\": \"{TOPIC_1},{TOPIC_2}\",\n        \"startingOffsets\": \"earliest\",\n        \"endingOffsets\": \"latest\"\n    }\n)\n\nkafka_source.read_batch()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session</p> required <code>options</code> <code>dict</code> <p>A dictionary of Kafka configurations (See Attributes tables below). For more information on configuration options see here</p> required <p>The following attributes are the most common configurations for Kafka.</p> <p>The only configuration that must be set for the Kafka source for both batch and streaming queries is listed below.</p> <p>Attributes:</p> Name Type Description <code>kafka.bootstrap.servers</code> <code>A comma-separated list of host\ufe30port</code> <p>The Kafka \"bootstrap.servers\" configuration. (Streaming and Batch)</p> <p>There are multiple ways of specifying which topics to subscribe to. You should provide only one of these attributes:</p> <p>Attributes:</p> Name Type Description <code>assign</code> <code>json string {\"topicA\"\ufe30[0,1],\"topicB\"\ufe30[2,4]}</code> <p>Specific TopicPartitions to consume. Only one of \"assign\", \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)</p> <code>subscribe</code> <code>A comma-separated list of topics</code> <p>The topic list to subscribe. Only one of \"assign\", \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)</p> <code>subscribePattern</code> <code>Java regex string</code> <p>The pattern used to subscribe to topic(s). Only one of \"assign, \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)</p> <p>The following configurations are optional:</p> <p>Attributes:</p> Name Type Description <code>startingTimestamp</code> <code>timestamp str</code> <p>The start point of timestamp when a query is started, a string specifying a starting timestamp for all partitions in topics being subscribed. Please refer the note on starting timestamp offset options below. (Streaming and Batch)</p> <code>startingOffsetsByTimestamp</code> <code>JSON str</code> <p>The start point of timestamp when a query is started, a json string specifying a starting timestamp for each TopicPartition. Please refer the note on starting timestamp offset options below. (Streaming and Batch)</p> <code>startingOffsets</code> <code>\"earliest\", \"latest\" (streaming only), or JSON string</code> <p>The start point when a query is started, either \"earliest\" which is from the earliest offsets, \"latest\" which is just from the latest offsets, or a json string specifying a starting offset for each TopicPartition. In the json, -2 as an offset can be used to refer to earliest, -1 to latest.</p> <code>endingTimestamp</code> <code>timestamp str</code> <p>The end point when a batch query is ended, a json string specifying an ending timestamp for all partitions in topics being subscribed. Please refer the note on ending timestamp offset options below. (Batch)</p> <code>endingOffsetsByTimestamp</code> <code>JSON str</code> <p>The end point when a batch query is ended, a json string specifying an ending timestamp for each TopicPartition. Please refer the note on ending timestamp offset options below. (Batch)</p> <code>endingOffsets</code> <code>latest or JSON str</code> <p>The end point when a batch query is ended, either \"latest\" which is just referred to the latest, or a json string specifying an ending offset for each TopicPartition. In the json, -1 as an offset can be used to refer to latest, and -2 (earliest) as an offset is not allowed. (Batch)</p> <code>maxOffsetsPerTrigger</code> <code>long</code> <p>Rate limit on maximum number of offsets processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume. (Streaming)</p> <code>minOffsetsPerTrigger</code> <code>long</code> <p>Minimum number of offsets to be processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume. (Streaming)</p> <code>failOnDataLoss</code> <code>bool</code> <p>Whether to fail the query when it's possible that data is lost (e.g., topics are deleted, or offsets are out of range). This may be a false alarm. You can disable it when it doesn't work as you expected.</p> <code>minPartitions</code> <code>int</code> <p>Desired minimum number of partitions to read from Kafka. By default, Spark has a 1-1 mapping of topicPartitions to Spark partitions consuming from Kafka. (Streaming and Batch)</p> <code>includeHeaders</code> <code>bool</code> <p>Whether to include the Kafka headers in the row. (Streaming and Batch)</p> <p>Starting Timestamp Offset Note</p> <p>If Kafka doesn't return the matched offset, the behavior will follow to the value of the option <code>startingOffsetsByTimestampStrategy</code>.</p> <p><code>startingTimestamp</code> takes precedence over <code>startingOffsetsByTimestamp</code> and startingOffsets.</p> <p>For streaming queries, this only applies when a new query is started, and that resuming will always pick up from where the query left off. Newly discovered partitions during a query will start at earliest.</p> <p>Ending Timestamp Offset Note</p> <p>If Kafka doesn't return the matched offset, the offset will be set to latest.</p> <p><code>endingOffsetsByTimestamp</code> takes precedence over <code>endingOffsets</code>.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kafka.py</code> <pre><code>class SparkKafkaSource(SourceInterface):\n    \"\"\"\n    This Spark source class is used to read batch or streaming data from Kafka. Required and optional configurations can be found in the Attributes tables below.\n\n    Additionally, there are more optional configurations which can be found [here.](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html){ target=\"_blank\" }\n\n    Example\n    --------\n    ```python\n     #Kafka Source for Streaming Queries\n\n    from rtdip_sdk.pipelines.sources import SparkKafkaSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    kafka_source = SparkKafkaSource(\n        spark=spark,\n        options={\n            \"kafka.bootstrap.servers\": \"{HOST_1}:{PORT_1},{HOST_2}:{PORT_2}\",\n            \"subscribe\": \"{TOPIC_1},{TOPIC_2}\",\n            \"includeHeaders\", \"true\"\n        }\n    )\n\n    kafka_source.read_stream()\n    ```\n    ```python\n     #Kafka Source for Batch Queries\n\n    from rtdip_sdk.pipelines.sources import SparkKafkaSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    kafka_source = SparkKafkaSource(\n        spark=spark,\n        options={\n            \"kafka.bootstrap.servers\": \"{HOST_1}:{PORT_1},{HOST_2}:{PORT_2}\",\n            \"subscribe\": \"{TOPIC_1},{TOPIC_2}\",\n            \"startingOffsets\": \"earliest\",\n            \"endingOffsets\": \"latest\"\n        }\n    )\n\n    kafka_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session\n        options (dict): A dictionary of Kafka configurations (See Attributes tables below). For more information on configuration options see [here](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html){ target=\"_blank\" }\n\n    The following attributes are the most common configurations for Kafka.\n\n    The only configuration that must be set for the Kafka source for both batch and streaming queries is listed below.\n\n    Attributes:\n        kafka.bootstrap.servers (A comma-separated list of host\ufe30port):  The Kafka \"bootstrap.servers\" configuration. (Streaming and Batch)\n\n    There are multiple ways of specifying which topics to subscribe to. You should provide only one of these attributes:\n\n    Attributes:\n        assign (json string {\"topicA\"\ufe30[0,1],\"topicB\"\ufe30[2,4]}):  Specific TopicPartitions to consume. Only one of \"assign\", \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)\n        subscribe (A comma-separated list of topics): The topic list to subscribe. Only one of \"assign\", \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)\n        subscribePattern (Java regex string): The pattern used to subscribe to topic(s). Only one of \"assign, \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)\n\n    The following configurations are optional:\n\n    Attributes:\n        startingTimestamp (timestamp str): The start point of timestamp when a query is started, a string specifying a starting timestamp for all partitions in topics being subscribed. Please refer the note on starting timestamp offset options below. (Streaming and Batch)\n        startingOffsetsByTimestamp (JSON str): The start point of timestamp when a query is started, a json string specifying a starting timestamp for each TopicPartition. Please refer the note on starting timestamp offset options below. (Streaming and Batch)\n        startingOffsets (\"earliest\", \"latest\" (streaming only), or JSON string): The start point when a query is started, either \"earliest\" which is from the earliest offsets, \"latest\" which is just from the latest offsets, or a json string specifying a starting offset for each TopicPartition. In the json, -2 as an offset can be used to refer to earliest, -1 to latest.\n        endingTimestamp (timestamp str): The end point when a batch query is ended, a json string specifying an ending timestamp for all partitions in topics being subscribed. Please refer the note on ending timestamp offset options below. (Batch)\n        endingOffsetsByTimestamp (JSON str): The end point when a batch query is ended, a json string specifying an ending timestamp for each TopicPartition. Please refer the note on ending timestamp offset options below. (Batch)\n        endingOffsets (latest or JSON str): The end point when a batch query is ended, either \"latest\" which is just referred to the latest, or a json string specifying an ending offset for each TopicPartition. In the json, -1 as an offset can be used to refer to latest, and -2 (earliest) as an offset is not allowed. (Batch)\n        maxOffsetsPerTrigger (long): Rate limit on maximum number of offsets processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume. (Streaming)\n        minOffsetsPerTrigger (long): Minimum number of offsets to be processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume. (Streaming)\n        failOnDataLoss (bool): Whether to fail the query when it's possible that data is lost (e.g., topics are deleted, or offsets are out of range). This may be a false alarm. You can disable it when it doesn't work as you expected.\n        minPartitions (int): Desired minimum number of partitions to read from Kafka. By default, Spark has a 1-1 mapping of topicPartitions to Spark partitions consuming from Kafka. (Streaming and Batch)\n        includeHeaders (bool): Whether to include the Kafka headers in the row. (Streaming and Batch)\n\n    !!! note \"Starting Timestamp Offset Note\"\n        If Kafka doesn't return the matched offset, the behavior will follow to the value of the option &lt;code&gt;startingOffsetsByTimestampStrategy&lt;/code&gt;.\n\n        &lt;code&gt;startingTimestamp&lt;/code&gt; takes precedence over &lt;code&gt;startingOffsetsByTimestamp&lt;/code&gt; and &lt;/code&gt;startingOffsets&lt;/code&gt;.\n\n        For streaming queries, this only applies when a new query is started, and that resuming will always pick up from where the query left off. Newly discovered partitions during a query will start at earliest.\n\n    !!! note \"Ending Timestamp Offset Note\"\n        If Kafka doesn't return the matched offset, the offset will be set to latest.\n\n        &lt;code&gt;endingOffsetsByTimestamp&lt;/code&gt; takes precedence over &lt;code&gt;endingOffsets&lt;/code&gt;.\n\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.schema = KAFKA_SCHEMA\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        spark_libraries = Libraries()\n        spark_libraries.add_maven_library(get_default_package(\"spark_sql_kafka\"))\n        return spark_libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self) -&gt; bool:\n        return True\n\n    def post_read_validation(self, df: DataFrame) -&gt; bool:\n        assert df.schema == self.schema\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Reads batch data from Kafka.\n        \"\"\"\n        try:\n            return self.spark.read.format(\"kafka\").options(**self.options).load()\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        Reads streaming data from Kafka.\n        \"\"\"\n        try:\n            return self.spark.readStream.format(\"kafka\").options(**self.options).load()\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/kafka/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kafka.SparkKafkaSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kafka.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/kafka/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kafka.SparkKafkaSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads batch data from Kafka.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kafka.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Reads batch data from Kafka.\n    \"\"\"\n    try:\n        return self.spark.read.format(\"kafka\").options(**self.options).load()\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/kafka/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kafka.SparkKafkaSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Reads streaming data from Kafka.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kafka.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    Reads streaming data from Kafka.\n    \"\"\"\n    try:\n        return self.spark.readStream.format(\"kafka\").options(**self.options).load()\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/kafka_eventhub/","title":"Read from Kafka","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/kafka_eventhub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kafka_eventhub.SparkKafkaEventhubSource","title":"<code>SparkKafkaEventhubSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>This Spark source class is used to read batch or streaming data from an Eventhub using the Kafka protocol. This enables Eventhubs to be used as a source in applications like Delta Live Tables or Databricks Serverless Jobs as the Spark Eventhubs JAR is not supported in these scenarios.</p> <p>The dataframe returned is transformed to ensure the schema is as close to the Eventhub Spark source as possible. There are some minor differences:</p> <ul> <li><code>offset</code> is dependent on <code>x-opt-offset</code> being populated in the headers provided. If this is not found in the headers, the value will be null</li> <li><code>publisher</code> is dependent on <code>x-opt-publisher</code> being populated in the headers provided. If this is not found in the headers, the value will be null</li> <li><code>partitionKey</code> is dependent on <code>x-opt-partition-key</code> being populated in the headers provided. If this is not found in the headers, the value will be null</li> <li><code>systemProperties</code> are identified according to the list provided in the Eventhub documentation and IoT Hub documentation</li> </ul> <p>Default settings will be specified if not provided in the <code>options</code> parameter:</p> <ul> <li><code>kafka.sasl.mechanism</code> will be set to <code>PLAIN</code></li> <li><code>kafka.security.protocol</code> will be set to <code>SASL_SSL</code></li> <li><code>kafka.request.timeout.ms</code> will be set to <code>60000</code></li> <li><code>kafka.session.timeout.ms</code> will be set to <code>60000</code></li> </ul>"},{"location":"sdk/code-reference/pipelines/sources/spark/kafka_eventhub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kafka_eventhub.SparkKafkaEventhubSource--examples","title":"Examples","text":"<p><pre><code>#Kafka Source for Streaming Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkKafkaEventhubSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconnectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\nconsumerGroup = \"{YOUR-CONSUMER-GROUP}\"\n\nkafka_eventhub_source = SparkKafkaEventhubSource(\n    spark=spark,\n    options={\n        \"startingOffsets\": \"earliest\",\n        \"maxOffsetsPerTrigger\": 10000,\n        \"failOnDataLoss\": \"false\",\n    },\n    connection_string=connectionString,\n    consumer_group=\"consumerGroup\"\n)\n\nkafka_eventhub_source.read_stream()\n</code></pre> <pre><code>#Kafka Source for Batch Queries\n\nfrom rtdip_sdk.pipelines.sources import SparkKafkaEventhubSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconnectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\nconsumerGroup = \"{YOUR-CONSUMER-GROUP}\"\n\nkafka_eventhub_source = SparkKafkaEventhubSource(\n    spark=spark,\n    options={\n        \"startingOffsets\": \"earliest\",\n        \"endingOffsets\": \"latest\",\n        \"failOnDataLoss\": \"false\"\n    },\n    connection_string=connectionString,\n    consumer_group=\"consumerGroup\"\n)\n\nkafka_eventhub_source.read_batch()\n</code></pre></p> <p>Required and optional configurations can be found in the Attributes and Parameter tables below. Additionally, there are more optional configurations which can be found here.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session</p> required <code>options</code> <code>dict</code> <p>A dictionary of Kafka configurations (See Attributes tables below). For more information on configuration options see here</p> required <code>connection_string</code> <code>str</code> <p>Eventhubs connection string is required to connect to the Eventhubs service. This must include the Eventhub name as the <code>EntityPath</code> parameter. Example <code>\"Endpoint=sb://test.servicebus.windows.net/;SharedAccessKeyName=test;SharedAccessKey=test_key;EntityPath=test_eventhub\"</code></p> required <code>consumer_group</code> <code>str</code> <p>The Eventhub consumer group to use for the connection</p> required <code>decode_kafka_headers_to_amqp_properties</code> <code>optional bool</code> <p>Perform decoding of Kafka headers into their AMQP properties. Default is True</p> <code>True</code> <p>The only configuration that must be set for the Kafka source for both batch and streaming queries is listed below.</p> <p>Attributes:</p> Name Type Description <code>kafka.bootstrap.servers</code> <code>A comma-separated list of host\ufe30port</code> <p>The Kafka \"bootstrap.servers\" configuration. (Streaming and Batch)</p> <p>There are multiple ways of specifying which topics to subscribe to. You should provide only one of these parameters:</p> <p>Attributes:</p> Name Type Description <code>assign</code> <code>json string {\"topicA\"\ufe30[0,1],\"topicB\"\ufe30[2,4]}</code> <p>Specific TopicPartitions to consume. Only one of \"assign\", \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)</p> <code>subscribe</code> <code>A comma-separated list of topics</code> <p>The topic list to subscribe. Only one of \"assign\", \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)</p> <code>subscribePattern</code> <code>Java regex string</code> <p>The pattern used to subscribe to topic(s). Only one of \"assign, \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)</p> <p>The following configurations are optional:</p> <p>Attributes:</p> Name Type Description <code>startingTimestamp</code> <code>timestamp str</code> <p>The start point of timestamp when a query is started, a string specifying a starting timestamp for all partitions in topics being subscribed. Please refer the note on starting timestamp offset options below. (Streaming and Batch)</p> <code>startingOffsetsByTimestamp</code> <code>JSON str</code> <p>The start point of timestamp when a query is started, a json string specifying a starting timestamp for each TopicPartition. Please refer the note on starting timestamp offset options below. (Streaming and Batch)</p> <code>startingOffsets</code> <code>\"earliest\", \"latest\" (streaming only), or JSON string</code> <p>The start point when a query is started, either \"earliest\" which is from the earliest offsets, \"latest\" which is just from the latest offsets, or a json string specifying a starting offset for each TopicPartition. In the json, -2 as an offset can be used to refer to earliest, -1 to latest.</p> <code>endingTimestamp</code> <code>timestamp str</code> <p>The end point when a batch query is ended, a json string specifying an ending timestamp for all partitions in topics being subscribed. Please refer the note on ending timestamp offset options below. (Batch)</p> <code>endingOffsetsByTimestamp</code> <code>JSON str</code> <p>The end point when a batch query is ended, a json string specifying an ending timestamp for each TopicPartition. Please refer the note on ending timestamp offset options below. (Batch)</p> <code>endingOffsets</code> <code>latest or JSON str</code> <p>The end point when a batch query is ended, either \"latest\" which is just referred to the latest, or a json string specifying an ending offset for each TopicPartition. In the json, -1 as an offset can be used to refer to latest, and -2 (earliest) as an offset is not allowed. (Batch)</p> <code>maxOffsetsPerTrigger</code> <code>long</code> <p>Rate limit on maximum number of offsets processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume. (Streaming)</p> <code>minOffsetsPerTrigger</code> <code>long</code> <p>Minimum number of offsets to be processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume. (Streaming)</p> <code>failOnDataLoss</code> <code>bool</code> <p>Whether to fail the query when it's possible that data is lost (e.g., topics are deleted, or offsets are out of range). This may be a false alarm. You can disable it when it doesn't work as you expected.</p> <code>minPartitions</code> <code>int</code> <p>Desired minimum number of partitions to read from Kafka. By default, Spark has a 1-1 mapping of topicPartitions to Spark partitions consuming from Kafka. (Streaming and Batch)</p> <code>includeHeaders</code> <code>bool</code> <p>Whether to include the Kafka headers in the row. (Streaming and Batch)</p> <p>Starting Timestamp Offset Note</p> <p>If Kafka doesn't return the matched offset, the behavior will follow to the value of the option <code>startingOffsetsByTimestampStrategy</code>.</p> <p><code>startingTimestamp</code> takes precedence over <code>startingOffsetsByTimestamp</code> and startingOffsets.</p> <p>For streaming queries, this only applies when a new query is started, and that resuming will always pick up from where the query left off. Newly discovered partitions during a query will start at earliest.</p> <p>Ending Timestamp Offset Note</p> <p>If Kafka doesn't return the matched offset, the offset will be set to latest.</p> <p><code>endingOffsetsByTimestamp</code> takes precedence over <code>endingOffsets</code>.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kafka_eventhub.py</code> <pre><code>class SparkKafkaEventhubSource(SourceInterface):\n    \"\"\"\n    This Spark source class is used to read batch or streaming data from an Eventhub using the Kafka protocol. This enables Eventhubs to be used as a source in applications like Delta Live Tables or Databricks Serverless Jobs as the Spark Eventhubs JAR is not supported in these scenarios.\n\n    The dataframe returned is transformed to ensure the schema is as close to the Eventhub Spark source as possible. There are some minor differences:\n\n    - `offset` is dependent on `x-opt-offset` being populated in the headers provided. If this is not found in the headers, the value will be null\n    - `publisher` is dependent on `x-opt-publisher` being populated in the headers provided. If this is not found in the headers, the value will be null\n    - `partitionKey` is dependent on `x-opt-partition-key` being populated in the headers provided. If this is not found in the headers, the value will be null\n    - `systemProperties` are identified according to the list provided in the [Eventhub documentation](https://learn.microsoft.com/en-us/azure/data-explorer/ingest-data-event-hub-overview#event-system-properties-mapping){ target=\"_blank\" } and [IoT Hub documentation](https://learn.microsoft.com/en-us/azure/data-explorer/ingest-data-iot-hub-overview#event-system-properties-mapping){ target=\"_blank\" }\n\n    Default settings will be specified if not provided in the `options` parameter:\n\n    - `kafka.sasl.mechanism` will be set to `PLAIN`\n    - `kafka.security.protocol` will be set to `SASL_SSL`\n    - `kafka.request.timeout.ms` will be set to `60000`\n    - `kafka.session.timeout.ms` will be set to `60000`\n\n    Examples\n    --------\n    ```python\n    #Kafka Source for Streaming Queries\n\n    from rtdip_sdk.pipelines.sources import SparkKafkaEventhubSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    connectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n    consumerGroup = \"{YOUR-CONSUMER-GROUP}\"\n\n    kafka_eventhub_source = SparkKafkaEventhubSource(\n        spark=spark,\n        options={\n            \"startingOffsets\": \"earliest\",\n            \"maxOffsetsPerTrigger\": 10000,\n            \"failOnDataLoss\": \"false\",\n        },\n        connection_string=connectionString,\n        consumer_group=\"consumerGroup\"\n    )\n\n    kafka_eventhub_source.read_stream()\n    ```\n    ```python\n    #Kafka Source for Batch Queries\n\n    from rtdip_sdk.pipelines.sources import SparkKafkaEventhubSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    connectionString = \"Endpoint=sb://{NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={ACCESS_KEY_NAME};SharedAccessKey={ACCESS_KEY}=;EntityPath={EVENT_HUB_NAME}\"\n    consumerGroup = \"{YOUR-CONSUMER-GROUP}\"\n\n    kafka_eventhub_source = SparkKafkaEventhubSource(\n        spark=spark,\n        options={\n            \"startingOffsets\": \"earliest\",\n            \"endingOffsets\": \"latest\",\n            \"failOnDataLoss\": \"false\"\n        },\n        connection_string=connectionString,\n        consumer_group=\"consumerGroup\"\n    )\n\n    kafka_eventhub_source.read_batch()\n    ```\n\n    Required and optional configurations can be found in the Attributes and Parameter tables below.\n    Additionally, there are more optional configurations which can be found [here.](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html){ target=\"_blank\" }\n\n    Parameters:\n        spark (SparkSession): Spark Session\n        options (dict): A dictionary of Kafka configurations (See Attributes tables below). For more information on configuration options see [here](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html){ target=\"_blank\" }\n        connection_string (str): Eventhubs connection string is required to connect to the Eventhubs service. This must include the Eventhub name as the `EntityPath` parameter. Example `\"Endpoint=sb://test.servicebus.windows.net/;SharedAccessKeyName=test;SharedAccessKey=test_key;EntityPath=test_eventhub\"`\n        consumer_group (str): The Eventhub consumer group to use for the connection\n        decode_kafka_headers_to_amqp_properties (optional bool): Perform decoding of Kafka headers into their AMQP properties. Default is True\n\n    The only configuration that must be set for the Kafka source for both batch and streaming queries is listed below.\n\n    Attributes:\n        kafka.bootstrap.servers (A comma-separated list of host\ufe30port):  The Kafka \"bootstrap.servers\" configuration. (Streaming and Batch)\n\n    There are multiple ways of specifying which topics to subscribe to. You should provide only one of these parameters:\n\n    Attributes:\n        assign (json string {\"topicA\"\ufe30[0,1],\"topicB\"\ufe30[2,4]}):  Specific TopicPartitions to consume. Only one of \"assign\", \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)\n        subscribe (A comma-separated list of topics): The topic list to subscribe. Only one of \"assign\", \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)\n        subscribePattern (Java regex string): The pattern used to subscribe to topic(s). Only one of \"assign, \"subscribe\" or \"subscribePattern\" options can be specified for Kafka source. (Streaming and Batch)\n\n    The following configurations are optional:\n\n    Attributes:\n        startingTimestamp (timestamp str): The start point of timestamp when a query is started, a string specifying a starting timestamp for all partitions in topics being subscribed. Please refer the note on starting timestamp offset options below. (Streaming and Batch)\n        startingOffsetsByTimestamp (JSON str): The start point of timestamp when a query is started, a json string specifying a starting timestamp for each TopicPartition. Please refer the note on starting timestamp offset options below. (Streaming and Batch)\n        startingOffsets (\"earliest\", \"latest\" (streaming only), or JSON string): The start point when a query is started, either \"earliest\" which is from the earliest offsets, \"latest\" which is just from the latest offsets, or a json string specifying a starting offset for each TopicPartition. In the json, -2 as an offset can be used to refer to earliest, -1 to latest.\n        endingTimestamp (timestamp str): The end point when a batch query is ended, a json string specifying an ending timestamp for all partitions in topics being subscribed. Please refer the note on ending timestamp offset options below. (Batch)\n        endingOffsetsByTimestamp (JSON str): The end point when a batch query is ended, a json string specifying an ending timestamp for each TopicPartition. Please refer the note on ending timestamp offset options below. (Batch)\n        endingOffsets (latest or JSON str): The end point when a batch query is ended, either \"latest\" which is just referred to the latest, or a json string specifying an ending offset for each TopicPartition. In the json, -1 as an offset can be used to refer to latest, and -2 (earliest) as an offset is not allowed. (Batch)\n        maxOffsetsPerTrigger (long): Rate limit on maximum number of offsets processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume. (Streaming)\n        minOffsetsPerTrigger (long): Minimum number of offsets to be processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume. (Streaming)\n        failOnDataLoss (bool): Whether to fail the query when it's possible that data is lost (e.g., topics are deleted, or offsets are out of range). This may be a false alarm. You can disable it when it doesn't work as you expected.\n        minPartitions (int): Desired minimum number of partitions to read from Kafka. By default, Spark has a 1-1 mapping of topicPartitions to Spark partitions consuming from Kafka. (Streaming and Batch)\n        includeHeaders (bool): Whether to include the Kafka headers in the row. (Streaming and Batch)\n\n    !!! note \"Starting Timestamp Offset Note\"\n        If Kafka doesn't return the matched offset, the behavior will follow to the value of the option &lt;code&gt;startingOffsetsByTimestampStrategy&lt;/code&gt;.\n\n        &lt;code&gt;startingTimestamp&lt;/code&gt; takes precedence over &lt;code&gt;startingOffsetsByTimestamp&lt;/code&gt; and &lt;/code&gt;startingOffsets&lt;/code&gt;.\n\n        For streaming queries, this only applies when a new query is started, and that resuming will always pick up from where the query left off. Newly discovered partitions during a query will start at earliest.\n\n    !!! note \"Ending Timestamp Offset Note\"\n        If Kafka doesn't return the matched offset, the offset will be set to latest.\n\n        &lt;code&gt;endingOffsetsByTimestamp&lt;/code&gt; takes precedence over &lt;code&gt;endingOffsets&lt;/code&gt;.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        options: dict,\n        connection_string: str,\n        consumer_group: str,\n        decode_kafka_headers_to_amqp_properties: bool = True,\n    ) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.connection_string = connection_string\n        self.consumer_group = consumer_group\n        self.decode_kafka_headers_to_amqp_properties = (\n            decode_kafka_headers_to_amqp_properties\n        )\n        self.connection_string_properties = self._parse_connection_string(\n            connection_string\n        )\n        self.schema = KAFKA_EVENTHUB_SCHEMA\n        self.options = self._configure_options(options)\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        spark_libraries = Libraries()\n        spark_libraries.add_maven_library(get_default_package(\"spark_sql_kafka\"))\n        return spark_libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self) -&gt; bool:\n        return True\n\n    def post_read_validation(self, df: DataFrame) -&gt; bool:\n        assert df.schema == self.schema\n        return True\n\n    # Code is from Azure Eventhub Python SDK. Will import the package if possible with Conda in the  conda-forge channel in the future\n    def _parse_connection_string(self, connection_string: str):\n        conn_settings = [s.split(\"=\", 1) for s in connection_string.split(\";\")]\n        if any(len(tup) != 2 for tup in conn_settings):\n            raise ValueError(\"Connection string is either blank or malformed.\")\n        conn_settings = dict(conn_settings)\n        shared_access_signature = None\n        for key, value in conn_settings.items():\n            if key.lower() == \"sharedaccesssignature\":\n                shared_access_signature = value\n        shared_access_key = conn_settings.get(\"SharedAccessKey\")\n        shared_access_key_name = conn_settings.get(\"SharedAccessKeyName\")\n        if any([shared_access_key, shared_access_key_name]) and not all(\n            [shared_access_key, shared_access_key_name]\n        ):\n            raise ValueError(\n                \"Connection string must have both SharedAccessKeyName and SharedAccessKey.\"\n            )\n        if shared_access_signature is not None and shared_access_key is not None:\n            raise ValueError(\n                \"Only one of the SharedAccessKey or SharedAccessSignature must be present.\"\n            )\n        endpoint = conn_settings.get(\"Endpoint\")\n        if not endpoint:\n            raise ValueError(\"Connection string is either blank or malformed.\")\n        parsed = urlparse(endpoint.rstrip(\"/\"))\n        if not parsed.netloc:\n            raise ValueError(\"Invalid Endpoint on the Connection String.\")\n        namespace = parsed.netloc.strip()\n        properties = {\n            \"fully_qualified_namespace\": namespace,\n            \"endpoint\": endpoint,\n            \"eventhub_name\": conn_settings.get(\"EntityPath\"),\n            \"shared_access_signature\": shared_access_signature,\n            \"shared_access_key_name\": shared_access_key_name,\n            \"shared_access_key\": shared_access_key,\n        }\n        return properties\n\n    def _connection_string_builder(self, properties: dict) -&gt; str:\n        connection_string = \"Endpoint=\" + properties.get(\"endpoint\") + \";\"\n\n        if properties.get(\"shared_access_key\"):\n            connection_string += (\n                \"SharedAccessKey=\" + properties.get(\"shared_access_key\") + \";\"\n            )\n\n        if properties.get(\"shared_access_key_name\"):\n            connection_string += (\n                \"SharedAccessKeyName=\" + properties.get(\"shared_access_key_name\") + \";\"\n            )\n\n        if properties.get(\"shared_access_signature\"):\n            connection_string += (\n                \"SharedAccessSignature=\"\n                + properties.get(\"shared_access_signature\")\n                + \";\"\n            )\n        return connection_string\n\n    def _configure_options(self, options: dict) -&gt; dict:\n        if \"subscribe\" not in options:\n            options[\"subscribe\"] = self.connection_string_properties.get(\n                \"eventhub_name\"\n            )\n\n        if \"kafka.bootstrap.servers\" not in options:\n            options[\"kafka.bootstrap.servers\"] = (\n                self.connection_string_properties.get(\"fully_qualified_namespace\")\n                + \":9093\"\n            )\n\n        if \"kafka.sasl.mechanism\" not in options:\n            options[\"kafka.sasl.mechanism\"] = \"PLAIN\"\n\n        if \"kafka.security.protocol\" not in options:\n            options[\"kafka.security.protocol\"] = \"SASL_SSL\"\n\n        if \"kafka.sasl.jaas.config\" not in options:\n            kafka_package = \"org.apache.kafka.common.security.plain.PlainLoginModule\"\n            if \"DATABRICKS_RUNTIME_VERSION\" in os.environ or (\n                \"_client\" in self.spark.__dict__\n                and \"databricks\" in self.spark.client.host\n            ):\n                kafka_package = \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule\"\n            connection_string = self._connection_string_builder(\n                self.connection_string_properties\n            )\n            options[\"kafka.sasl.jaas.config\"] = (\n                '{} required username=\"$ConnectionString\" password=\"{}\";'.format(\n                    kafka_package, connection_string\n                )\n            )  # NOSONAR\n\n        if \"kafka.request.timeout.ms\" not in options:\n            options[\"kafka.request.timeout.ms\"] = \"60000\"\n\n        if \"kafka.session.timeout.ms\" not in options:\n            options[\"kafka.session.timeout.ms\"] = \"60000\"\n\n        if \"kafka.group.id\" not in options:\n            options[\"kafka.group.id\"] = self.consumer_group\n\n        options[\"includeHeaders\"] = \"true\"\n\n        return options\n\n    def _transform_to_eventhub_schema(self, df: DataFrame) -&gt; DataFrame:\n        return (\n            df.withColumn(\"headers\", map_from_entries(col(\"headers\")))\n            .select(\n                col(\"value\").alias(\"body\"),\n                col(\"partition\").cast(\"string\"),\n                col(\"offset\").alias(\"sequenceNumber\"),\n                col(\"timestamp\").alias(\"enqueuedTime\"),\n                (\n                    decode_kafka_headers_to_amqp_properties(col(\"headers\")).alias(\n                        \"properties\"\n                    )\n                    if self.decode_kafka_headers_to_amqp_properties\n                    else create_map().cast(\"map&lt;string,string&gt;\").alias(\"properties\")\n                ),\n            )\n            .withColumn(\"offset\", col(\"properties\").getItem(\"x-opt-offset\"))\n            .withColumn(\"publisher\", col(\"properties\").getItem(\"x-opt-publisher\"))\n            .withColumn(\n                \"partitionKey\", col(\"properties\").getItem(\"x-opt-partition-key\")\n            )\n            .withColumn(\n                \"systemProperties\",\n                map_filter(\n                    col(\"properties\"), lambda k, _: k.isin(eventhub_system_properties)\n                ),\n            )\n            .withColumn(\n                \"properties\",\n                map_filter(\n                    col(\"properties\"), lambda k, _: ~k.isin(eventhub_system_properties)\n                ),\n            )\n            .select(\n                col(\"body\"),\n                col(\"partition\"),\n                col(\"offset\"),\n                col(\"sequenceNumber\"),\n                col(\"enqueuedTime\"),\n                col(\"publisher\"),\n                col(\"partitionKey\"),\n                col(\"properties\"),\n                col(\"systemProperties\"),\n            )\n        )\n\n    def read_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Reads batch data from Kafka.\n        \"\"\"\n        try:\n            df = self.spark.read.format(\"kafka\").options(**self.options).load()\n            return self._transform_to_eventhub_schema(df)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        Reads streaming data from Kafka.\n        \"\"\"\n        try:\n            df = self.spark.readStream.format(\"kafka\").options(**self.options).load()\n            return self._transform_to_eventhub_schema(df)\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/kafka_eventhub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kafka_eventhub.SparkKafkaEventhubSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kafka_eventhub.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/kafka_eventhub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kafka_eventhub.SparkKafkaEventhubSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Reads batch data from Kafka.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kafka_eventhub.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Reads batch data from Kafka.\n    \"\"\"\n    try:\n        df = self.spark.read.format(\"kafka\").options(**self.options).load()\n        return self._transform_to_eventhub_schema(df)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/kafka_eventhub/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kafka_eventhub.SparkKafkaEventhubSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Reads streaming data from Kafka.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kafka_eventhub.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    Reads streaming data from Kafka.\n    \"\"\"\n    try:\n        df = self.spark.readStream.format(\"kafka\").options(**self.options).load()\n        return self._transform_to_eventhub_schema(df)\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/kinesis/","title":"Read from Amazon Kinesis Data Streams","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/kinesis/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kinesis.SparkKinesisSource","title":"<code>SparkKinesisSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>The Spark Kinesis Source is used to read data from Kinesis in a Databricks environment. Structured streaming from Kinesis is not supported in open source Spark.</p>"},{"location":"sdk/code-reference/pipelines/sources/spark/kinesis/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kinesis.SparkKinesisSource--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import SparkKinesisSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nkinesis_source = SparkKinesisSource(\n    spark=spark,\n    options={\n        \"awsAccessKey\": \"{AWS-ACCESS-KEY}\",\n        \"awsSecretKey\": \"{AWS-SECRET-KEY}\",\n        \"streamName\": \"{STREAM-NAME}\",\n        \"region\": \"{REGION}\",\n        \"endpoint\": \"https://kinesis.{REGION}.amazonaws.com\",\n        \"initialPosition\": \"earliest\"\n    }\n)\n\nkinesis_source.read_stream()\n\nOR\n\nkinesis_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from Kinesis</p> required <code>options</code> <code>dict</code> <p>Options that can be specified for a Kinesis read operation (See Attributes table below). Further information on the options is available here</p> required <p>Attributes:</p> Name Type Description <code>awsAccessKey</code> <code>str</code> <p>AWS access key.</p> <code>awsSecretKey</code> <code>str</code> <p>AWS secret access key corresponding to the access key.</p> <code>streamName</code> <code>List[str]</code> <p>The stream names to subscribe to.</p> <code>region</code> <code>str</code> <p>The region the streams are defined in.</p> <code>endpoint</code> <code>str</code> <p>The regional endpoint for Kinesis Data Streams.</p> <code>initialPosition</code> <code>str</code> <p>The point to start reading from; earliest, latest, or at_timestamp.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kinesis.py</code> <pre><code>class SparkKinesisSource(SourceInterface):\n    \"\"\"\n    The Spark Kinesis Source is used to read data from Kinesis in a Databricks environment.\n    Structured streaming from Kinesis is **not** supported in open source Spark.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.sources import SparkKinesisSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    kinesis_source = SparkKinesisSource(\n        spark=spark,\n        options={\n            \"awsAccessKey\": \"{AWS-ACCESS-KEY}\",\n            \"awsSecretKey\": \"{AWS-SECRET-KEY}\",\n            \"streamName\": \"{STREAM-NAME}\",\n            \"region\": \"{REGION}\",\n            \"endpoint\": \"https://kinesis.{REGION}.amazonaws.com\",\n            \"initialPosition\": \"earliest\"\n        }\n    )\n\n    kinesis_source.read_stream()\n\n    OR\n\n    kinesis_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to read data from Kinesis\n        options (dict): Options that can be specified for a Kinesis read operation (See Attributes table below). Further information on the options is available [here](https://docs.databricks.com/structured-streaming/kinesis.html#configuration){ target=\"_blank\" }\n\n    Attributes:\n        awsAccessKey (str): AWS access key.\n        awsSecretKey (str): AWS secret access key corresponding to the access key.\n        streamName (List[str]): The stream names to subscribe to.\n        region (str): The region the streams are defined in.\n        endpoint (str): The regional endpoint for Kinesis Data Streams.\n        initialPosition (str): The point to start reading from; earliest, latest, or at_timestamp.\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.schema = KINESIS_SCHEMA\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK_DATABRICKS\n        \"\"\"\n        return SystemType.PYSPARK_DATABRICKS\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self, df: DataFrame) -&gt; bool:\n        assert df.schema == self.schema\n        return True\n\n    def read_batch(self):\n        \"\"\"\n        Raises:\n            NotImplementedError: Kinesis only supports streaming reads. To perform a batch read, use the read_stream method of this component and specify the Trigger on the write_stream to be `availableNow=True` to perform batch-like reads of cloud storage files.\n        \"\"\"\n        raise NotImplementedError(\n            \"Kinesis only supports streaming reads. To perform a batch read, use the read_stream method and specify Trigger on the write_stream as `availableNow=True`\"\n        )\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        Reads streaming data from Kinesis. All of the data in the table is processed as well as any new data that arrives after the stream started.\n        \"\"\"\n        try:\n            return (\n                self.spark.readStream.format(\"kinesis\").options(**self.options).load()\n            )\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/kinesis/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kinesis.SparkKinesisSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK_DATABRICKS</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kinesis.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK_DATABRICKS\n    \"\"\"\n    return SystemType.PYSPARK_DATABRICKS\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/kinesis/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kinesis.SparkKinesisSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Kinesis only supports streaming reads. To perform a batch read, use the read_stream method of this component and specify the Trigger on the write_stream to be <code>availableNow=True</code> to perform batch-like reads of cloud storage files.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kinesis.py</code> <pre><code>def read_batch(self):\n    \"\"\"\n    Raises:\n        NotImplementedError: Kinesis only supports streaming reads. To perform a batch read, use the read_stream method of this component and specify the Trigger on the write_stream to be `availableNow=True` to perform batch-like reads of cloud storage files.\n    \"\"\"\n    raise NotImplementedError(\n        \"Kinesis only supports streaming reads. To perform a batch read, use the read_stream method and specify Trigger on the write_stream as `availableNow=True`\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/kinesis/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.kinesis.SparkKinesisSource.read_stream","title":"<code>read_stream()</code>","text":"<p>Reads streaming data from Kinesis. All of the data in the table is processed as well as any new data that arrives after the stream started.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/kinesis.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    Reads streaming data from Kinesis. All of the data in the table is processed as well as any new data that arrives after the stream started.\n    \"\"\"\n    try:\n        return (\n            self.spark.readStream.format(\"kinesis\").options(**self.options).load()\n        )\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/ecmwf/base_mars/","title":"Base MARS","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/ecmwf/base_mars/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.ecmwf.base_mars.SparkECMWFBaseMarsSource","title":"<code>SparkECMWFBaseMarsSource</code>","text":"<p>Download nc files from ECMWF MARS server using the ECMWF python API. Data is downloaded in parallel using joblib from ECMWF MARS server using the ECMWF python API.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str</code> <p>Path to local directory where the nc files will be stored, in format \"yyyy-mm-dd_HH.nc\"</p> required <code>date_start</code> <code>str</code> <p>Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format</p> required <code>date_end</code> <code>str</code> <p>End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format</p> required <code>ecmwf_api_key</code> <code>str</code> <p>API key for ECMWF MARS server</p> required <code>ecmwf_api_email</code> <code>str</code> <p>Email for ECMWF MARS server</p> required <code>ecmwf_api_url</code> <code>str</code> <p>URL for ECMWF MARS server</p> <code>'https://api.ecmwf.int/v1'</code> <code>run_frequency</code> <code>str</code> <p>Frequency format of runs to download, e.g. \"H\"</p> <code>'H'</code> <code>run_interval</code> <code>str</code> <p>Interval of runs, e.g. a run_frequency of \"H\" and run_interval of \"12\" will extract the data of the 00 and 12 run for each day.</p> <code>'12'</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/ecmwf/base_mars.py</code> <pre><code>class SparkECMWFBaseMarsSource:\n    \"\"\"\n    Download nc files from ECMWF MARS server using the ECMWF python API.\n    Data is downloaded in parallel using joblib from ECMWF MARS server using the ECMWF python API.\n\n    Parameters:\n        save_path (str): Path to local directory where the nc files will be stored, in format \"yyyy-mm-dd_HH.nc\"\n        date_start (str): Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format\n        date_end (str): End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format\n        ecmwf_api_key (str): API key for ECMWF MARS server\n        ecmwf_api_email (str): Email for ECMWF MARS server\n        ecmwf_api_url (str): URL for ECMWF MARS server\n        run_frequency (str):Frequency format of runs to download, e.g. \"H\"\n        run_interval (str): Interval of runs, e.g. a run_frequency of \"H\" and run_interval of \"12\" will extract the data of the 00 and 12 run for each day.\n    \"\"\"\n\n    def __init__(\n        self,\n        date_start: str,\n        date_end: str,\n        save_path: str,\n        ecmwf_api_key: str,\n        ecmwf_api_email: str,\n        ecmwf_api_url: str = \"https://api.ecmwf.int/v1\",\n        run_interval: str = \"12\",\n        run_frequency: str = \"H\",\n    ):\n        self.retrieve_ran = False\n        self.date_start = date_start\n        self.date_end = date_end\n        self.save_path = save_path\n        self.format = format\n        self.run_interval = run_interval\n        self.run_frequency = run_frequency\n        self.ecmwf_api_key = ecmwf_api_key\n        self.ecmwf_api_url = ecmwf_api_url\n        self.ecmwf_api_email = ecmwf_api_email\n\n        # Pandas date_list (info best retrieved per forecast day)\n        self.dates = pd.date_range(\n            start=date_start, end=date_end, freq=run_interval + run_frequency\n        )\n\n    def retrieve(\n        self,\n        mars_dict: dict,\n        n_jobs=None,\n        backend=\"loky\",\n        tries=5,\n        cost=False,\n    ):\n        \"\"\"Retrieve the data from the server.\n\n        Function will use the ecmwf api to download the data from the server.\n        Note that mars has a max of two active requests per user and 20 queued\n        requests.\n        Data is downloaded in parallel using joblib from ECMWF MARS server using the ECMWF python API.\n\n\n        Parameters:\n            mars_dict (dict): Dictionary of mars parameters.\n            n_jobs (int, optional): Download in parallel? by default None, i.e. no parallelization\n            backend (str, optional): Specify the parallelization backend implementation in joblib, by default \"loky\"\n            tries (int, optional): Number of tries for each request if it fails, by default 5\n            cost (bool, optional):  Pass a cost request to mars to estimate the size and efficiency of your request, but not actually download the data. Can be useful for defining requests, by default False.\n        \"\"\"\n        chk = [\"date\", \"target\", \"time\", \"format\", \"output\"]\n        for i in chk:\n            if i in mars_dict.keys():\n                raise ValueError(f\"don't include {i} in the mars_dict\")\n\n        parallel = Parallel(n_jobs=n_jobs, backend=backend)\n\n        def _retrieve_datetime(i, j, cost=cost):\n            i_dict = {\"date\": i, \"time\": j}\n\n            if cost:\n                filename = f\"{i}_{j}.txt\"  # NOSONAR\n            else:\n                filename = f\"{i}_{j}.nc\"\n                i_dict[\"format\"] = \"netcdf\"  # NOSONAR\n\n            target = os.path.join(self.save_path, filename)\n            msg = f\"retrieving mars data --- {filename}\"\n\n            req_dict = {**i_dict, **mars_dict}\n            for k, v in req_dict.items():\n                if isinstance(v, (list, tuple)):\n                    req_dict[k] = \"/\".join([str(x) for x in v])  # NOSONAR\n\n            req_dict = [\"{}={}\".format(k, v) for k, v in req_dict.items()]\n            if cost:\n                req_dict = \"list,output=cost,{}\".format(\",\".join(req_dict))  # NOSONAR\n            else:\n                req_dict = \"retrieve,{}\".format(\",\".join(req_dict))  # NOSONAR\n\n            for j in range(tries):\n                try:\n                    print(msg)\n                    server = ECMWFService(\n                        \"mars\",\n                        url=self.ecmwf_api_url,\n                        email=self.ecmwf_api_email,\n                        key=self.ecmwf_api_key,\n                    )\n                    server.execute(req_dict, target)\n                    return 1  # NOSONAR\n                except:  # NOSONAR\n                    if j &lt; tries - 1:\n                        continue  # NOSONAR\n                    else:\n                        return 0  # NOSONAR\n\n        self.success = parallel(\n            delayed(_retrieve_datetime)(str(k.date()), f\"{k.hour:02}\")\n            for k in self.dates\n        )\n        self.retrieve_ran = True\n\n        return self\n\n    def info(self) -&gt; pd.Series:\n        \"\"\"\n        Return info on each ECMWF request.\n\n        Returns:\n            pd.Series: Successful request for each run == 1.\n        \"\"\"\n        if not self.retrieve_ran:\n            raise ValueError(\n                \"Before using self.info(), prepare the request using \"\n                + \"self.retrieve()\"\n            )\n        y = pd.Series(self.success, index=self.dates, name=\"success\", dtype=bool)\n\n        return y\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/ecmwf/base_mars/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.ecmwf.base_mars.SparkECMWFBaseMarsSource.retrieve","title":"<code>retrieve(mars_dict, n_jobs=None, backend='loky', tries=5, cost=False)</code>","text":"<p>Retrieve the data from the server.</p> <p>Function will use the ecmwf api to download the data from the server. Note that mars has a max of two active requests per user and 20 queued requests. Data is downloaded in parallel using joblib from ECMWF MARS server using the ECMWF python API.</p> <p>Parameters:</p> Name Type Description Default <code>mars_dict</code> <code>dict</code> <p>Dictionary of mars parameters.</p> required <code>n_jobs</code> <code>int</code> <p>Download in parallel? by default None, i.e. no parallelization</p> <code>None</code> <code>backend</code> <code>str</code> <p>Specify the parallelization backend implementation in joblib, by default \"loky\"</p> <code>'loky'</code> <code>tries</code> <code>int</code> <p>Number of tries for each request if it fails, by default 5</p> <code>5</code> <code>cost</code> <code>bool</code> <p>Pass a cost request to mars to estimate the size and efficiency of your request, but not actually download the data. Can be useful for defining requests, by default False.</p> <code>False</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/ecmwf/base_mars.py</code> <pre><code>def retrieve(\n    self,\n    mars_dict: dict,\n    n_jobs=None,\n    backend=\"loky\",\n    tries=5,\n    cost=False,\n):\n    \"\"\"Retrieve the data from the server.\n\n    Function will use the ecmwf api to download the data from the server.\n    Note that mars has a max of two active requests per user and 20 queued\n    requests.\n    Data is downloaded in parallel using joblib from ECMWF MARS server using the ECMWF python API.\n\n\n    Parameters:\n        mars_dict (dict): Dictionary of mars parameters.\n        n_jobs (int, optional): Download in parallel? by default None, i.e. no parallelization\n        backend (str, optional): Specify the parallelization backend implementation in joblib, by default \"loky\"\n        tries (int, optional): Number of tries for each request if it fails, by default 5\n        cost (bool, optional):  Pass a cost request to mars to estimate the size and efficiency of your request, but not actually download the data. Can be useful for defining requests, by default False.\n    \"\"\"\n    chk = [\"date\", \"target\", \"time\", \"format\", \"output\"]\n    for i in chk:\n        if i in mars_dict.keys():\n            raise ValueError(f\"don't include {i} in the mars_dict\")\n\n    parallel = Parallel(n_jobs=n_jobs, backend=backend)\n\n    def _retrieve_datetime(i, j, cost=cost):\n        i_dict = {\"date\": i, \"time\": j}\n\n        if cost:\n            filename = f\"{i}_{j}.txt\"  # NOSONAR\n        else:\n            filename = f\"{i}_{j}.nc\"\n            i_dict[\"format\"] = \"netcdf\"  # NOSONAR\n\n        target = os.path.join(self.save_path, filename)\n        msg = f\"retrieving mars data --- {filename}\"\n\n        req_dict = {**i_dict, **mars_dict}\n        for k, v in req_dict.items():\n            if isinstance(v, (list, tuple)):\n                req_dict[k] = \"/\".join([str(x) for x in v])  # NOSONAR\n\n        req_dict = [\"{}={}\".format(k, v) for k, v in req_dict.items()]\n        if cost:\n            req_dict = \"list,output=cost,{}\".format(\",\".join(req_dict))  # NOSONAR\n        else:\n            req_dict = \"retrieve,{}\".format(\",\".join(req_dict))  # NOSONAR\n\n        for j in range(tries):\n            try:\n                print(msg)\n                server = ECMWFService(\n                    \"mars\",\n                    url=self.ecmwf_api_url,\n                    email=self.ecmwf_api_email,\n                    key=self.ecmwf_api_key,\n                )\n                server.execute(req_dict, target)\n                return 1  # NOSONAR\n            except:  # NOSONAR\n                if j &lt; tries - 1:\n                    continue  # NOSONAR\n                else:\n                    return 0  # NOSONAR\n\n    self.success = parallel(\n        delayed(_retrieve_datetime)(str(k.date()), f\"{k.hour:02}\")\n        for k in self.dates\n    )\n    self.retrieve_ran = True\n\n    return self\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/ecmwf/base_mars/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.ecmwf.base_mars.SparkECMWFBaseMarsSource.info","title":"<code>info()</code>","text":"<p>Return info on each ECMWF request.</p> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Successful request for each run == 1.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/ecmwf/base_mars.py</code> <pre><code>def info(self) -&gt; pd.Series:\n    \"\"\"\n    Return info on each ECMWF request.\n\n    Returns:\n        pd.Series: Successful request for each run == 1.\n    \"\"\"\n    if not self.retrieve_ran:\n        raise ValueError(\n            \"Before using self.info(), prepare the request using \"\n            + \"self.retrieve()\"\n        )\n    y = pd.Series(self.success, index=self.dates, name=\"success\", dtype=bool)\n\n    return y\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/ecmwf/weather_forecast/","title":"Weather Forecast","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/ecmwf/weather_forecast/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.ecmwf.weather_forecast.SparkECMWFWeatherForecastSource","title":"<code>SparkECMWFWeatherForecastSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>The Weather Forecast API V1 Source class to doownload nc files from ECMWF MARS server using the ECMWF python API.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>save_path</code> <code>str</code> <p>Path to local directory where the nc files will be stored, in format \"yyyy-mm-dd_HH.nc\"</p> required <code>date_start</code> <code>str</code> <p>Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format    date_end:str,</p> required <code>date_end</code> <code>str</code> <p>End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format</p> required <code>ecmwf_class</code> <code>str</code> <p>ecmwf classification of data</p> required <code>stream</code> <code>str</code> <p>Operational model stream</p> required <code>expver</code> <code>str</code> <p>Version of data</p> required <code>leveltype</code> <code>str</code> <p>Surface level forecasts</p> required <code>ec_vars</code> <code>list</code> <p>Variables of forecast measurements.</p> required <code>forecast_area</code> <code>list</code> <p>N/W/S/E coordinates of the forecast area</p> required <code>ecmwf_api_key</code> <code>str</code> <p>API key for ECMWF API</p> required <code>ecmwf_api_email</code> <code>str</code> <p>Email for ECMWF API</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/ecmwf/weather_forecast.py</code> <pre><code>class SparkECMWFWeatherForecastSource(SourceInterface):\n    \"\"\"\n    The Weather Forecast API V1 Source class to doownload nc files from ECMWF MARS server using the ECMWF python API.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        save_path (str): Path to local directory where the nc files will be stored, in format \"yyyy-mm-dd_HH.nc\"\n        date_start (str): Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format    date_end:str,\n        date_end (str): End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format\n        ecmwf_class (str): ecmwf classification of data\n        stream (str): Operational model stream\n        expver (str): Version of data\n        leveltype (str): Surface level forecasts\n        ec_vars (list): Variables of forecast measurements.\n        forecast_area (list): N/W/S/E coordinates of the forecast area\n        ecmwf_api_key (str): API key for ECMWF API\n        ecmwf_api_email (str): Email for ECMWF API\n    \"\"\"\n\n    spark: SparkSession\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        save_path: str,\n        date_start: str,\n        date_end: str,\n        ecmwf_class: str,\n        stream: str,\n        expver: str,\n        leveltype: str,\n        ec_vars: list,\n        forecast_area: list,\n        ecmwf_api_key: str,\n        ecmwf_api_email: str,\n    ) -&gt; None:\n        self.spark = spark\n        self.save_path = save_path\n        self.date_start = date_start\n        self.date_end = date_end\n        self.ecmwf_class = ecmwf_class\n        self.stream = stream  # operational model\n        self.expver = expver  # experiment version of data\n        self.leveltype = leveltype  # surface level forecasts\n        self.ec_vars = ec_vars  # variables\n        self.forecast_area = forecast_area  # N/W/S/E\n        self.ecmwf_api_key = ecmwf_api_key\n        self.ecmwf_api_email = ecmwf_api_email\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_read_validation(self):\n        return True\n\n    def post_read_validation(self):\n        return True\n\n    def read_stream(self):\n        return True\n\n    @classmethod\n    def _get_lead_time(cls):\n        \"\"\"\n        Lead time for the forecast data.\n        90 hours - 1 Hour Interval\n        90-146 - 3 Hour interval\n        146 -246 - 6 Hour interval\n\n        Returns:\n            lead_times: Lead times in an array format.\n        \"\"\"\n        lead_times = [*range(91), *range(93, 146, 3), *range(150, 246, 6)]\n        np.array(lead_times)\n\n        return lead_times\n\n    def _get_api_params(self, lead_times):\n        \"\"\"\n        API parameters for the forecast data.\n\n        Returns:\n            params (dict): API parameters for the forecast data.\n        \"\"\"\n\n        params = {\n            \"class\": self.ecmwf_class,  # ecmwf classification of data\n            \"stream\": self.stream,  # operational model\n            \"expver\": self.expver,  # experiment version of data\n            \"levtype\": self.leveltype,  # surface level forecasts\n            \"type\": \"fc\",  # forecasts\n            \"param\": self.ec_vars,  # variables\n            \"step\": lead_times,  # which lead times to download\n            \"area\": self.forecast_area,  # N/W/S/E\n            \"grid\": [0.1, 0.1],  # grid res of output\n        }\n\n        return params\n\n    def read_batch(self):\n        \"\"\"\n        Pulls data from the Weather API and returns as .nc files.\n\n        \"\"\"\n        lead_times = self._get_lead_time()\n        para = self._get_api_params(lead_times=lead_times)\n\n        ec_conn = SparkECMWFBaseMarsSource(\n            date_start=self.date_start,\n            date_end=self.date_end,\n            save_path=self.save_path,\n            run_interval=\"12\",\n            run_frequency=\"H\",\n            ecmwf_api_key=self.ecmwf_api_key,\n            ecmwf_api_email=self.ecmwf_api_email,\n            ecmwf_api_url=\"https://api.ecmwf.int/v1\",\n        )\n\n        ec_conn.retrieve(\n            mars_dict=para,\n            tries=5,\n            n_jobs=-1,  # maximum of 20 queued requests per user (only two allowed active)\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/ecmwf/weather_forecast/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.ecmwf.weather_forecast.SparkECMWFWeatherForecastSource.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/ecmwf/weather_forecast.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/ecmwf/weather_forecast/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.ecmwf.weather_forecast.SparkECMWFWeatherForecastSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Pulls data from the Weather API and returns as .nc files.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/ecmwf/weather_forecast.py</code> <pre><code>def read_batch(self):\n    \"\"\"\n    Pulls data from the Weather API and returns as .nc files.\n\n    \"\"\"\n    lead_times = self._get_lead_time()\n    para = self._get_api_params(lead_times=lead_times)\n\n    ec_conn = SparkECMWFBaseMarsSource(\n        date_start=self.date_start,\n        date_end=self.date_end,\n        save_path=self.save_path,\n        run_interval=\"12\",\n        run_frequency=\"H\",\n        ecmwf_api_key=self.ecmwf_api_key,\n        ecmwf_api_email=self.ecmwf_api_email,\n        ecmwf_api_url=\"https://api.ecmwf.int/v1\",\n    )\n\n    ec_conn.retrieve(\n        mars_dict=para,\n        tries=5,\n        n_jobs=-1,  # maximum of 20 queued requests per user (only two allowed active)\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/caiso_daily_load_iso/","title":"CAISO Daily Load","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/iso/caiso_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.caiso_daily_load_iso.CAISODailyLoadISOSource","title":"<code>CAISODailyLoadISOSource</code>","text":"<p>               Bases: <code>BaseISOSource</code></p> <p>The CAISO Daily Load ISO Source is used to read daily load data from CAISO API. It supports multiple types of data. Check the <code>load_types</code> attribute.</p> <p>To read more about the available reports from CAISO API, download the file -  Interface Specification</p> <p>From the list of reports in the file, it pulls the report named <code>CAISO Demand Forecast</code> in the file.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>load_types</code> <code>list</code> <p>Must be a subset of [<code>Demand Forecast 7-Day Ahead</code>, <code>Demand Forecast 2-Day Ahead</code>, <code>Demand Forecast Day Ahead</code>, <code>RTM 15Min Load Forecast</code>, <code>RTM 5Min Load Forecast</code>, <code>Total Actual Hourly Integrated Load</code>].  Default Value - <code>[Total Actual Hourly Integrated Load]</code>.</p> <code>date</code> <code>str</code> <p>Must be in <code>YYYY-MM-DD</code> format.</p> <p>Please check the BaseISOSource for available methods.</p> BaseISOSource Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/caiso_daily_load_iso.py</code> <pre><code>class CAISODailyLoadISOSource(BaseISOSource):\n    \"\"\"\n    The CAISO Daily Load ISO Source is used to read daily load data from CAISO API.\n    It supports multiple types of data. Check the `load_types` attribute.\n\n    To read more about the available reports from CAISO API, download the file -\n     [Interface Specification](https://www.caiso.com/Documents/OASISAPISpecification.pdf)\n\n    From the list of reports in the file, it pulls the report named `CAISO Demand Forecast` in the file.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations (See Attributes table below)\n\n    Attributes:\n        load_types (list): Must be a subset of [`Demand Forecast 7-Day Ahead`, `Demand Forecast 2-Day Ahead`, `Demand Forecast Day Ahead`, `RTM 15Min Load Forecast`, `RTM 5Min Load Forecast`, `Total Actual Hourly Integrated Load`]. &lt;br&gt; Default Value - `[Total Actual Hourly Integrated Load]`.\n        date (str): Must be in `YYYY-MM-DD` format.\n\n    Please check the BaseISOSource for available methods.\n\n    BaseISOSource:\n        ::: src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://oasis.caiso.com/oasisapi/SingleZip\"\n    query_datetime_format: str = \"%Y%m%dT00:00-0000\"\n    required_options = [\"load_types\", \"date\"]\n    spark_schema = CAISO_SCHEMA\n    default_query_timezone = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super().__init__(spark, options)\n        self.spark = spark\n        self.options = options\n        self.load_types = self.options.get(\n            \"load_types\", [\"Total Actual Hourly Integrated Load\"]\n        )\n        self.date = self.options.get(\"date\", \"\").strip()\n        self.user_datetime_format = \"%Y-%m-%d\"\n\n        # The following to fix the Security Check Error as the CAISO API is timing out with HTTPS protocol.\n        self.iso_url = self.iso_url.replace(\"s://\", \"://\")\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Pulls data from the CAISO API and parses the zip files for CSV data.\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n\n        logging.info(f\"Getting {self.load_types} data for date {self.date}\")\n        start_date = datetime.strptime(self.date, self.user_datetime_format)\n        end_date = start_date + timedelta(days=1)\n        return self._fetch_and_parse_zip(start_date, end_date)\n\n    def _fetch_and_parse_zip(\n        self, start_date: datetime, end_date: datetime\n    ) -&gt; pd.DataFrame:\n        suffix = (\n            f\"?resultformat=6&amp;\"\n            f\"queryname=SLD_FCST&amp;\"\n            \"version=1&amp;\"\n            f\"startdatetime={start_date.strftime(self.query_datetime_format)}&amp;\"\n            f\"enddatetime={end_date.strftime(self.query_datetime_format)}\"\n        )\n\n        content = self._fetch_from_url(suffix)\n        if not content:\n            raise HTTPError(\"Empty Response was returned\")\n        logging.info(\"Unzipping the file\")\n\n        zf = ZipFile(BytesIO(content))\n\n        csvs = list(filter(lambda name: \".csv\" in name, zf.namelist()))\n        if len(csvs) == 0:\n            raise ValueError(\"No data was found in the specified interval\")\n\n        df = pd.read_csv(zf.open(csvs[0]))\n        return df\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        date_cols = [\"INTERVALSTARTTIME_GMT\", \"INTERVALENDTIME_GMT\"]\n        for date_col in date_cols:\n            df[date_col] = df[date_col].apply(\n                lambda data: datetime.strptime(str(data)[:19], \"%Y-%m-%dT%H:%M:%S\")\n            )\n\n        df = df.rename(\n            columns={\n                \"INTERVALSTARTTIME_GMT\": \"StartTime\",\n                \"INTERVALENDTIME_GMT\": \"EndTime\",\n                \"LOAD_TYPE\": \"LoadType\",\n                \"OPR_DT\": \"OprDt\",\n                \"OPR_HR\": \"OprHr\",\n                \"OPR_INTERVAL\": \"OprInterval\",\n                \"MARKET_RUN_ID\": \"MarketRunId\",\n                \"TAC_AREA_NAME\": \"TacAreaName\",\n                \"LABEL\": \"Label\",\n                \"XML_DATA_ITEM\": \"XmlDataItem\",\n                \"POS\": \"Pos\",\n                \"MW\": \"Load\",\n                \"EXECUTION_TYPE\": \"ExecutionType\",\n                \"GROUP\": \"Group\",\n            }\n        )\n\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        df = df[df[\"Label\"].isin(self.load_types)]\n        return df\n\n    def _validate_options(self) -&gt; bool:\n        try:\n            datetime.strptime(self.date, self.user_datetime_format)\n        except ValueError:\n            raise ValueError(\n                f\"Unable to parse date. Please specify in {self.user_datetime_format} format.\"\n            )\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/caiso_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource","title":"<code>BaseISOSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>class BaseISOSource(SourceInterface):\n    \"\"\"\n    Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options: list = []\n    spark_schema = StructType([StructField(\"id\", IntegerType(), True)])\n    default_query_timezone: str = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.query_timezone = pytz.timezone(\n            self.options.get(\"query_timezone\", self.default_query_timezone)\n        )\n        self.current_date = datetime.now(timezone.utc).astimezone(self.query_timezone)\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n        \"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.iso_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - {url}\")\n\n        response = requests.get(url)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n\n        return response.content\n\n    def _get_localized_datetime(self, datetime_str: str) -&gt; datetime:\n        \"\"\"\n        Converts string datetime into Python datetime object with configured format and timezone.\n        Args:\n            datetime_str: String to be converted into datetime.\n\n        Returns: Timezone aware datetime object.\n\n        \"\"\"\n        parsed_dt = datetime.strptime(datetime_str, self.query_datetime_format)\n        parsed_dt = parsed_dt.replace(tzinfo=self.query_timezone)\n        return parsed_dt\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Hits the fetch_from_url method with certain parameters to get raw data from API.\n\n        All the children ISO classes must override this method and call the fetch_url method\n        in it.\n\n        Returns:\n             Raw DataFrame from API.\n        \"\"\"\n\n        return pd.read_csv(BytesIO(self._fetch_from_url(\"\")))\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Performs all the basic transformations to prepare data for further processing.\n        All the children ISO classes must override this method.\n\n        Args:\n            df: Raw DataFrame, received from the API.\n\n        Returns:\n             Modified DataFrame, ready for basic use.\n\n        \"\"\"\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Another data transformation helper method to be called after prepare data.\n        Used for advance data processing such as cleaning, filtering, restructuring.\n        All the children ISO classes must override this method if there is any post-processing required.\n\n        Args:\n            df: Initial modified version of DataFrame, received after preparing the data.\n\n        Returns:\n             Final version of data after all the fixes and modifications.\n\n        \"\"\"\n        return df\n\n    def _get_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Entrypoint method to return the final version of DataFrame.\n\n        Returns:\n            Modified form of data for specific use case.\n\n        \"\"\"\n        df = self._pull_data()\n        df = self._prepare_data(df)\n        df = self._sanitize_data(df)\n\n        # Reorder columns to keep the data consistent\n        df = df[self.spark_schema.names]\n\n        return df\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Performs all the options checks. Raises exception in case of any invalid value.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        return True\n\n    def pre_read_validation(self) -&gt; bool:\n        \"\"\"\n        Ensures all the required options are provided and performs other validations.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        for key in self.required_options:\n            if key not in self.options:\n                raise ValueError(f\"Required option `{key}` is missing.\")\n\n        return self._validate_options()\n\n    def post_read_validation(self) -&gt; bool:\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n        Returns:\n             Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n        \"\"\"\n\n        try:\n            self.pre_read_validation()\n            pdf = self._get_data()\n            pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n            # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n            pd.DataFrame.iteritems = pd.DataFrame.items\n            df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n            return df\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n        Returns:\n             Final Spark DataFrame after all the processing.\n\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/caiso_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.pre_read_validation","title":"<code>pre_read_validation()</code>","text":"<p>Ensures all the required options are provided and performs other validations. Returns:      True if all checks are passed.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def pre_read_validation(self) -&gt; bool:\n    \"\"\"\n    Ensures all the required options are provided and performs other validations.\n    Returns:\n         True if all checks are passed.\n\n    \"\"\"\n    for key in self.required_options:\n        if key not in self.options:\n            raise ValueError(f\"Required option `{key}` is missing.\")\n\n    return self._validate_options()\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/caiso_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data. Returns:      Final Spark DataFrame converted from Pandas DataFrame post-execution.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n    Returns:\n         Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n    \"\"\"\n\n    try:\n        self.pre_read_validation()\n        pdf = self._get_data()\n        pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n        # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n        pd.DataFrame.iteritems = pd.DataFrame.items\n        df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n        return df\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/caiso_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_stream","title":"<code>read_stream()</code>","text":"<p>By default, the streaming operation is not supported but child classes can override if ISO supports streaming.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Final Spark DataFrame after all the processing.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n    Returns:\n         Final Spark DataFrame after all the processing.\n\n    \"\"\"\n\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/caiso_historical_load_iso/","title":"CAISO Historical Load","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/iso/caiso_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.caiso_historical_load_iso.CAISOHistoricalLoadISOSource","title":"<code>CAISOHistoricalLoadISOSource</code>","text":"<p>               Bases: <code>CAISODailyLoadISOSource</code></p> <p>The CAISO Historical Load ISO Source is used to read load data for an interval of dates  between start_date and end_date inclusive from CAISO API. It supports multiple types of data. Check the <code>load_types</code> attribute.</p> <p>To read more about the available reports from CAISO API, download the file -  Interface Specification</p> <p>From the list of reports in the file, it pulls the report named <code>CAISO Demand Forecast</code> in the file.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>load_types</code> <code>list</code> <p>Must be a subset of [<code>Demand Forecast 7-Day Ahead</code>, <code>Demand Forecast 2-Day Ahead</code>, <code>Demand Forecast Day Ahead</code>, <code>RTM 15Min Load Forecast</code>, <code>RTM 5Min Load Forecast</code>, <code>Total Actual Hourly Integrated Load</code>].  Default Value - <code>[Total Actual Hourly Integrated Load]</code>.</p> <code>start_date</code> <code>str</code> <p>Must be in <code>YYYY-MM-DD</code> format.</p> <code>end_date</code> <code>str</code> <p>Must be in <code>YYYY-MM-DD</code> format.</p> <p>Please check the BaseISOSource for available methods.</p> BaseISOSource Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/caiso_historical_load_iso.py</code> <pre><code>class CAISOHistoricalLoadISOSource(CAISODailyLoadISOSource):\n    \"\"\"\n    The CAISO Historical Load ISO Source is used to read load data for an interval of dates\n     between start_date and end_date inclusive from CAISO API.\n    It supports multiple types of data. Check the `load_types` attribute.\n\n    To read more about the available reports from CAISO API, download the file -\n     [Interface Specification](https://www.caiso.com/Documents/OASISAPISpecification.pdf)\n\n    From the list of reports in the file, it pulls the report named `CAISO Demand Forecast` in the file.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations (See Attributes table below)\n\n    Attributes:\n        load_types (list): Must be a subset of [`Demand Forecast 7-Day Ahead`, `Demand Forecast 2-Day Ahead`, `Demand Forecast Day Ahead`, `RTM 15Min Load Forecast`, `RTM 5Min Load Forecast`, `Total Actual Hourly Integrated Load`]. &lt;br&gt; Default Value - `[Total Actual Hourly Integrated Load]`.\n        start_date (str): Must be in `YYYY-MM-DD` format.\n        end_date (str): Must be in `YYYY-MM-DD` format.\n\n    Please check the BaseISOSource for available methods.\n\n    BaseISOSource:\n        ::: src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    required_options = [\"load_types\", \"start_date\", \"end_date\"]\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super().__init__(spark, options)\n        self.spark = spark\n        self.options = options\n        self.load_types = self.options.get(\n            \"load_types\", [\"Total Actual Hourly Integrated Load\"]\n        )\n        self.start_date = self.options.get(\"start_date\", \"\").strip()\n        self.end_date = self.options.get(\"end_date\", \"\").strip()\n        self.user_datetime_format = \"%Y-%m-%d\"\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Pulls data from the CAISO API and parses the zip files for CSV data.\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n\n        logging.info(\n            f\"Getting {self.load_types} data from {self.start_date} to {self.end_date}\"\n        )\n        start_date = datetime.strptime(self.start_date, self.user_datetime_format)\n        end_date = datetime.strptime(self.end_date, self.user_datetime_format)\n        end_date = end_date + timedelta(days=1)\n        generated_days_ranges = []\n        dates = pd.date_range(start_date, end_date, freq=\"30D\", inclusive=\"left\")\n\n        for date in dates:\n            py_date = date.to_pydatetime()\n            date_last = py_date + timedelta(days=30)\n            date_last = min(date_last, end_date)\n            generated_days_ranges.append((py_date, date_last))\n\n        logging.info(f\"Generated date ranges are {generated_days_ranges}\")\n\n        dfs = []\n        for idx, date_range in enumerate(generated_days_ranges):\n            start_date_str, end_date_str = date_range\n            df = self._fetch_and_parse_zip(start_date_str, end_date_str)\n\n            dfs.append(df)\n\n        return pd.concat(dfs)\n\n    def _validate_options(self) -&gt; bool:\n        try:\n            datetime.strptime(self.start_date, self.user_datetime_format)\n        except ValueError:\n            raise ValueError(\n                f\"Unable to parse start_date. Please specify in {self.user_datetime_format} format.\"\n            )\n\n        try:\n            datetime.strptime(self.end_date, self.user_datetime_format)\n        except ValueError:\n            raise ValueError(\n                f\"Unable to parse end_date. Please specify in {self.user_datetime_format} format.\"\n            )\n\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/caiso_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource","title":"<code>BaseISOSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>class BaseISOSource(SourceInterface):\n    \"\"\"\n    Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options: list = []\n    spark_schema = StructType([StructField(\"id\", IntegerType(), True)])\n    default_query_timezone: str = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.query_timezone = pytz.timezone(\n            self.options.get(\"query_timezone\", self.default_query_timezone)\n        )\n        self.current_date = datetime.now(timezone.utc).astimezone(self.query_timezone)\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n        \"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.iso_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - {url}\")\n\n        response = requests.get(url)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n\n        return response.content\n\n    def _get_localized_datetime(self, datetime_str: str) -&gt; datetime:\n        \"\"\"\n        Converts string datetime into Python datetime object with configured format and timezone.\n        Args:\n            datetime_str: String to be converted into datetime.\n\n        Returns: Timezone aware datetime object.\n\n        \"\"\"\n        parsed_dt = datetime.strptime(datetime_str, self.query_datetime_format)\n        parsed_dt = parsed_dt.replace(tzinfo=self.query_timezone)\n        return parsed_dt\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Hits the fetch_from_url method with certain parameters to get raw data from API.\n\n        All the children ISO classes must override this method and call the fetch_url method\n        in it.\n\n        Returns:\n             Raw DataFrame from API.\n        \"\"\"\n\n        return pd.read_csv(BytesIO(self._fetch_from_url(\"\")))\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Performs all the basic transformations to prepare data for further processing.\n        All the children ISO classes must override this method.\n\n        Args:\n            df: Raw DataFrame, received from the API.\n\n        Returns:\n             Modified DataFrame, ready for basic use.\n\n        \"\"\"\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Another data transformation helper method to be called after prepare data.\n        Used for advance data processing such as cleaning, filtering, restructuring.\n        All the children ISO classes must override this method if there is any post-processing required.\n\n        Args:\n            df: Initial modified version of DataFrame, received after preparing the data.\n\n        Returns:\n             Final version of data after all the fixes and modifications.\n\n        \"\"\"\n        return df\n\n    def _get_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Entrypoint method to return the final version of DataFrame.\n\n        Returns:\n            Modified form of data for specific use case.\n\n        \"\"\"\n        df = self._pull_data()\n        df = self._prepare_data(df)\n        df = self._sanitize_data(df)\n\n        # Reorder columns to keep the data consistent\n        df = df[self.spark_schema.names]\n\n        return df\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Performs all the options checks. Raises exception in case of any invalid value.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        return True\n\n    def pre_read_validation(self) -&gt; bool:\n        \"\"\"\n        Ensures all the required options are provided and performs other validations.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        for key in self.required_options:\n            if key not in self.options:\n                raise ValueError(f\"Required option `{key}` is missing.\")\n\n        return self._validate_options()\n\n    def post_read_validation(self) -&gt; bool:\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n        Returns:\n             Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n        \"\"\"\n\n        try:\n            self.pre_read_validation()\n            pdf = self._get_data()\n            pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n            # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n            pd.DataFrame.iteritems = pd.DataFrame.items\n            df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n            return df\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n        Returns:\n             Final Spark DataFrame after all the processing.\n\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/caiso_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.pre_read_validation","title":"<code>pre_read_validation()</code>","text":"<p>Ensures all the required options are provided and performs other validations. Returns:      True if all checks are passed.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def pre_read_validation(self) -&gt; bool:\n    \"\"\"\n    Ensures all the required options are provided and performs other validations.\n    Returns:\n         True if all checks are passed.\n\n    \"\"\"\n    for key in self.required_options:\n        if key not in self.options:\n            raise ValueError(f\"Required option `{key}` is missing.\")\n\n    return self._validate_options()\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/caiso_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data. Returns:      Final Spark DataFrame converted from Pandas DataFrame post-execution.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n    Returns:\n         Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n    \"\"\"\n\n    try:\n        self.pre_read_validation()\n        pdf = self._get_data()\n        pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n        # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n        pd.DataFrame.iteritems = pd.DataFrame.items\n        df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n        return df\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/caiso_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_stream","title":"<code>read_stream()</code>","text":"<p>By default, the streaming operation is not supported but child classes can override if ISO supports streaming.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Final Spark DataFrame after all the processing.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n    Returns:\n         Final Spark DataFrame after all the processing.\n\n    \"\"\"\n\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/ercot_daily_load_iso/","title":"ERCOT Daily Load","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/iso/ercot_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.ercot_daily_load_iso.ERCOTDailyLoadISOSource","title":"<code>ERCOTDailyLoadISOSource</code>","text":"<p>               Bases: <code>BaseISOSource</code></p> <p>The ERCOT Daily Load ISO Source is used to read daily load data from ERCOT using WebScrapping. It supports actual and forecast data. To read more about the reports, visit the following URLs (The urls are only accessible if the requester/client is in US)-</p> <p>For load type <code>actual</code>: Actual System Load by Weather Zone  For load type <code>forecast</code>: Seven-Day Load Forecast by Weather Zone</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>load_type</code> <code>list</code> <p>Must be one of <code>actual</code> or <code>forecast</code>.</p> <code>date</code> <code>str</code> <p>Must be in <code>YYYY-MM-DD</code> format.</p> <code>certificate_pfx_key</code> <code>str</code> <p>The certificate key data or password received from ERCOT.</p> <code>certificate_pfx_key_contents</code> <code>str</code> <p>The certificate data received from ERCOT, it could be base64 encoded.</p> <p>Please check the BaseISOSource for available methods.</p> BaseISOSource Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/ercot_daily_load_iso.py</code> <pre><code>class ERCOTDailyLoadISOSource(BaseISOSource):\n    \"\"\"\n    The ERCOT Daily Load ISO Source is used to read daily load data from ERCOT using WebScrapping.\n    It supports actual and forecast data. To read more about the reports, visit the following URLs\n    (The urls are only accessible if the requester/client is in US)-\n\n    For load type `actual`: [Actual System Load by Weather Zone](https://www.ercot.com/mp/data-products/\n    data-product-details?id=NP6-345-CD)\n    &lt;br&gt;\n    For load type `forecast`: [Seven-Day Load Forecast by Weather Zone](https://www.ercot.com/mp/data-products/\n    data-product-details?id=NP3-561-CD)\n\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations (See Attributes table below)\n\n    Attributes:\n        load_type (list): Must be one of `actual` or `forecast`.\n        date (str): Must be in `YYYY-MM-DD` format.\n        certificate_pfx_key (str): The certificate key data or password received from ERCOT.\n        certificate_pfx_key_contents (str): The certificate data received from ERCOT, it could be base64 encoded.\n\n    Please check the BaseISOSource for available methods.\n\n    BaseISOSource:\n        ::: src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    url_forecast: str = \"https://mis.ercot.com/misapp/GetReports.do?reportTypeId=12312\"\n    url_actual: str = \"https://mis.ercot.com/misapp/GetReports.do?reportTypeId=13101\"\n    url_prefix: str = \"https://mis.ercot.com\"\n    query_datetime_format: str = \"%Y-%m-%d\"\n    required_options = [\n        \"load_type\",\n        \"date\",\n        \"certificate_pfx_key\",\n        \"certificate_pfx_key_contents\",\n    ]\n    spark_schema = ERCOT_SCHEMA\n    default_query_timezone = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super().__init__(spark, options)\n        self.spark = spark\n        self.options = options\n        self.load_type = self.options.get(\"load_type\", \"actual\")\n        self.date = self.options.get(\"date\", \"\").strip()\n        self.certificate_pfx_key = self.options.get(\"certificate_pfx_key\", \"\").strip()\n        self.certificate_pfx_key_contents = self.options.get(\n            \"certificate_pfx_key_contents\", \"\"\n        ).strip()\n\n    def generate_temp_client_cert_files_from_pfx(self):\n        password = self.certificate_pfx_key.encode()\n        pfx: bytes = base64.b64decode(self.certificate_pfx_key_contents)\n\n        if base64.b64encode(pfx) != self.certificate_pfx_key_contents.encode():\n            pfx = self.certificate_pfx_key_contents\n\n        key, cert, _ = pkcs12.load_key_and_certificates(data=pfx, password=password)\n        key_bytes = key.private_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PrivateFormat.TraditionalOpenSSL,\n            encryption_algorithm=serialization.NoEncryption(),\n        )\n\n        cert_bytes = cert.public_bytes(encoding=serialization.Encoding.PEM)\n        return TempCertFiles(cert_bytes, key_bytes)\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Pulls data from the ERCOT API and parses the zip files for CSV data.\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n\n        logging.info(f\"Getting {self.load_type} data for date {self.date}\")\n        url = self.url_forecast\n        req_date = datetime.strptime(self.date, self.query_datetime_format)\n\n        if self.load_type == \"actual\":\n            req_date = req_date + timedelta(days=1)\n            url = self.url_actual\n\n        url_lists, files = self.generate_urls_for_zip(url, req_date)\n        dfs = []\n        logging.info(f\"Generated {len(url_lists)} URLs - {url_lists}\")\n        logging.info(f\"Requesting files - {files}\")\n\n        for url in url_lists:\n            df = self.download_zip(url)\n            dfs.append(df)\n        final_df = pd.concat(dfs)\n        return final_df\n\n    def download_zip(self, url) -&gt; pd.DataFrame:\n        logging.info(f\"Downloading zip using {url}\")\n        with self.generate_temp_client_cert_files_from_pfx() as cert:\n            response = requests.get(url, cert=cert)\n\n        if not response.content:\n            raise HTTPError(\"Empty Response was returned\")\n\n        logging.info(\"Unzipping the file\")\n        zf = ZipFile(BytesIO(response.content))\n        csvs = [s for s in zf.namelist() if \".csv\" in s]\n\n        if len(csvs) == 0:\n            raise ValueError(\"No data was found in the specified interval\")\n\n        df = pd.read_csv(zf.open(csvs[0]))\n        return df\n\n    def generate_urls_for_zip(self, url: str, date: datetime) -&gt; (List[str], List[str]):\n        logging.info(f\"Finding urls list for date {date}\")\n        with self.generate_temp_client_cert_files_from_pfx() as cert:\n            page_response = requests.get(url, timeout=5, cert=cert)\n\n        page_content = BeautifulSoup(page_response.content, \"html.parser\")\n        zip_info = []\n        length = len(page_content.find_all(\"td\", {\"class\": \"labelOptional_ind\"}))\n\n        for i in range(0, length):\n            zip_name = page_content.find_all(\"td\", {\"class\": \"labelOptional_ind\"})[\n                i\n            ].text\n            zip_link = page_content.find_all(\"a\")[i].get(\"href\")\n            zip_info.append((zip_name, zip_link))\n\n        date_str = date.strftime(\"%Y%m%d\")\n        zip_info = list(\n            filter(\n                lambda f_info: f_info[0].endswith(\"csv.zip\") and date_str in f_info[0],\n                zip_info,\n            )\n        )\n\n        urls = []\n        files = []\n\n        if len(zip_info) == 0:\n            raise ValueError(f\"No file was found for date - {date_str}\")\n\n        # As Forecast is generated every hour, pick the latest one.\n        zip_info = sorted(zip_info, key=lambda item: item[0], reverse=True)\n        zip_info_item = zip_info[0]\n\n        file_name, file_url = zip_info_item\n        urls.append(self.url_prefix + file_url)\n        files.append(file_name)\n\n        return urls, files\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        if self.load_type == \"actual\":\n            df[\"Date\"] = pd.to_datetime(df[\"OperDay\"], format=\"%m/%d/%Y\")\n\n            df = df.rename(\n                columns={\n                    \"COAST\": \"Coast\",\n                    \"EAST\": \"East\",\n                    \"FAR_WEST\": \"FarWest\",\n                    \"NORTH\": \"North\",\n                    \"NORTH_C\": \"NorthCentral\",\n                    \"SOUTH_C\": \"SouthCentral\",\n                    \"SOUTHERN\": \"Southern\",\n                    \"WEST\": \"West\",\n                    \"TOTAL\": \"SystemTotal\",\n                    \"DSTFlag\": \"DstFlag\",\n                }\n            )\n\n        else:\n            df = df.rename(columns={\"DSTFlag\": \"DstFlag\"})\n\n            df[\"Date\"] = pd.to_datetime(df[\"DeliveryDate\"], format=\"%m/%d/%Y\")\n\n        return df\n\n    def _validate_options(self) -&gt; bool:\n        try:\n            datetime.strptime(self.date, self.query_datetime_format)\n        except ValueError:\n            raise ValueError(\n                f\"Unable to parse date. Please specify in {self.query_datetime_format} format.\"\n            )\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/ercot_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource","title":"<code>BaseISOSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>class BaseISOSource(SourceInterface):\n    \"\"\"\n    Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options: list = []\n    spark_schema = StructType([StructField(\"id\", IntegerType(), True)])\n    default_query_timezone: str = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.query_timezone = pytz.timezone(\n            self.options.get(\"query_timezone\", self.default_query_timezone)\n        )\n        self.current_date = datetime.now(timezone.utc).astimezone(self.query_timezone)\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n        \"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.iso_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - {url}\")\n\n        response = requests.get(url)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n\n        return response.content\n\n    def _get_localized_datetime(self, datetime_str: str) -&gt; datetime:\n        \"\"\"\n        Converts string datetime into Python datetime object with configured format and timezone.\n        Args:\n            datetime_str: String to be converted into datetime.\n\n        Returns: Timezone aware datetime object.\n\n        \"\"\"\n        parsed_dt = datetime.strptime(datetime_str, self.query_datetime_format)\n        parsed_dt = parsed_dt.replace(tzinfo=self.query_timezone)\n        return parsed_dt\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Hits the fetch_from_url method with certain parameters to get raw data from API.\n\n        All the children ISO classes must override this method and call the fetch_url method\n        in it.\n\n        Returns:\n             Raw DataFrame from API.\n        \"\"\"\n\n        return pd.read_csv(BytesIO(self._fetch_from_url(\"\")))\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Performs all the basic transformations to prepare data for further processing.\n        All the children ISO classes must override this method.\n\n        Args:\n            df: Raw DataFrame, received from the API.\n\n        Returns:\n             Modified DataFrame, ready for basic use.\n\n        \"\"\"\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Another data transformation helper method to be called after prepare data.\n        Used for advance data processing such as cleaning, filtering, restructuring.\n        All the children ISO classes must override this method if there is any post-processing required.\n\n        Args:\n            df: Initial modified version of DataFrame, received after preparing the data.\n\n        Returns:\n             Final version of data after all the fixes and modifications.\n\n        \"\"\"\n        return df\n\n    def _get_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Entrypoint method to return the final version of DataFrame.\n\n        Returns:\n            Modified form of data for specific use case.\n\n        \"\"\"\n        df = self._pull_data()\n        df = self._prepare_data(df)\n        df = self._sanitize_data(df)\n\n        # Reorder columns to keep the data consistent\n        df = df[self.spark_schema.names]\n\n        return df\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Performs all the options checks. Raises exception in case of any invalid value.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        return True\n\n    def pre_read_validation(self) -&gt; bool:\n        \"\"\"\n        Ensures all the required options are provided and performs other validations.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        for key in self.required_options:\n            if key not in self.options:\n                raise ValueError(f\"Required option `{key}` is missing.\")\n\n        return self._validate_options()\n\n    def post_read_validation(self) -&gt; bool:\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n        Returns:\n             Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n        \"\"\"\n\n        try:\n            self.pre_read_validation()\n            pdf = self._get_data()\n            pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n            # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n            pd.DataFrame.iteritems = pd.DataFrame.items\n            df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n            return df\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n        Returns:\n             Final Spark DataFrame after all the processing.\n\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/ercot_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.pre_read_validation","title":"<code>pre_read_validation()</code>","text":"<p>Ensures all the required options are provided and performs other validations. Returns:      True if all checks are passed.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def pre_read_validation(self) -&gt; bool:\n    \"\"\"\n    Ensures all the required options are provided and performs other validations.\n    Returns:\n         True if all checks are passed.\n\n    \"\"\"\n    for key in self.required_options:\n        if key not in self.options:\n            raise ValueError(f\"Required option `{key}` is missing.\")\n\n    return self._validate_options()\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/ercot_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data. Returns:      Final Spark DataFrame converted from Pandas DataFrame post-execution.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n    Returns:\n         Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n    \"\"\"\n\n    try:\n        self.pre_read_validation()\n        pdf = self._get_data()\n        pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n        # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n        pd.DataFrame.iteritems = pd.DataFrame.items\n        df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n        return df\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/ercot_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_stream","title":"<code>read_stream()</code>","text":"<p>By default, the streaming operation is not supported but child classes can override if ISO supports streaming.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Final Spark DataFrame after all the processing.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n    Returns:\n         Final Spark DataFrame after all the processing.\n\n    \"\"\"\n\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_daily_load_iso/","title":"MISO Daily Load","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.miso_daily_load_iso.MISODailyLoadISOSource","title":"<code>MISODailyLoadISOSource</code>","text":"<p>               Bases: <code>BaseISOSource</code></p> <p>The MISO Daily Load ISO Source is used to read daily load data from MISO API. It supports both Actual and Forecast data.</p> <p>To read more about the available reports from MISO API, download the file - Market Reports</p> <p>From the list of reports in the file, it pulls the report named <code>Daily Forecast and Actual Load by Local Resource Zone</code>.</p> <p>Actual data is available for one day minus from the given date.</p> <p>Forecast data is available for next 6 day (inclusive of given date).</p>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.miso_daily_load_iso.MISODailyLoadISOSource--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import MISODailyLoadISOSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nmiso_source = MISODailyLoadISOSource(\n    spark=spark,\n    options={\n        \"load_type\": \"actual\",\n        \"date\": \"20230520\",\n    }\n)\n\nmiso_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>load_type</code> <code>str</code> <p>Must be one of <code>actual</code> or <code>forecast</code></p> <code>date</code> <code>str</code> <p>Must be in <code>YYYYMMDD</code> format.</p> <p>Please check the BaseISOSource for available methods.</p> BaseISOSource Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/miso_daily_load_iso.py</code> <pre><code>class MISODailyLoadISOSource(BaseISOSource):\n    \"\"\"\n    The MISO Daily Load ISO Source is used to read daily load data from MISO API. It supports both Actual and Forecast data.\n\n    To read more about the available reports from MISO API, download the file -\n    [Market Reports](https://cdn.misoenergy.org/Market%20Reports%20Directory115139.xlsx)\n\n    From the list of reports in the file, it pulls the report named\n    `Daily Forecast and Actual Load by Local Resource Zone`.\n\n    Actual data is available for one day minus from the given date.\n\n    Forecast data is available for next 6 day (inclusive of given date).\n\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.sources import MISODailyLoadISOSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    miso_source = MISODailyLoadISOSource(\n        spark=spark,\n        options={\n            \"load_type\": \"actual\",\n            \"date\": \"20230520\",\n        }\n    )\n\n    miso_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations (See Attributes table below)\n\n    Attributes:\n        load_type (str): Must be one of `actual` or `forecast`\n        date (str): Must be in `YYYYMMDD` format.\n\n    Please check the BaseISOSource for available methods.\n\n    BaseISOSource:\n        ::: src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://docs.misoenergy.org/marketreports/\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options = [\"load_type\", \"date\"]\n    spark_schema = MISO_SCHEMA\n    default_query_timezone = \"US/Central\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super().__init__(spark, options)\n        self.spark = spark\n        self.options = options\n        self.load_type = self.options.get(\"load_type\", \"actual\")\n        self.date = self.options.get(\"date\", \"\").strip()\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Pulls data from the MISO API and parses the Excel file.\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n\n        logging.info(f\"Getting {self.load_type} data for date {self.date}\")\n        df = pd.read_excel(self._fetch_from_url(f\"{self.date}_df_al.xls\"), skiprows=4)\n\n        return df\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Creates a new `date_time` column and removes null values.\n\n        Args:\n            df: Raw form of data received from the API.\n\n        Returns:\n            Data after basic transformations.\n\n        \"\"\"\n\n        df.drop(\n            df.index[(df[\"HourEnding\"] == \"HourEnding\") | df[\"MISO MTLF (MWh)\"].isna()],\n            inplace=True,\n        )\n        df.rename(columns={\"Market Day\": \"date\"}, inplace=True)\n\n        df[\"date_time\"] = pd.to_datetime(df[\"date\"]) + pd.to_timedelta(\n            df[\"HourEnding\"].astype(int) - 1, \"h\"\n        )\n        df.drop([\"HourEnding\", \"date\"], axis=1, inplace=True)\n\n        data_cols = df.columns[df.columns != \"date_time\"]\n        df[data_cols] = df[data_cols].astype(float)\n\n        df.reset_index(inplace=True, drop=True)\n\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Filter outs Actual or Forecast data based on `load_type`.\n        Args:\n            df: Data received after preparation.\n\n        Returns:\n            Final data either containing Actual or Forecast values.\n\n        \"\"\"\n\n        skip_col_suffix = \"\"\n\n        if self.load_type == \"actual\":\n            skip_col_suffix = \"MTLF (MWh)\"\n\n        elif self.load_type == \"forecast\":\n            skip_col_suffix = \"ActualLoad (MWh)\"\n\n        df = df[[x for x in df.columns if not x.endswith(skip_col_suffix)]]\n        df = df.dropna()\n        df.columns = [str(x.split(\" \")[0]).upper() for x in df.columns]\n\n        rename_cols = {\n            \"LRZ1\": \"Lrz1\",\n            \"LRZ2_7\": \"Lrz2_7\",\n            \"LRZ3_5\": \"Lrz3_5\",\n            \"LRZ4\": \"Lrz4\",\n            \"LRZ6\": \"Lrz6\",\n            \"LRZ8_9_10\": \"Lrz8_9_10\",\n            \"MISO\": \"Miso\",\n            \"DATE_TIME\": \"Datetime\",\n        }\n\n        df = df.rename(columns=rename_cols)\n\n        return df\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Validates the following options:\n            - `date` must be in the correct format.\n            - `load_type` must be valid.\n\n        Returns:\n            True if all looks good otherwise raises Exception.\n\n        \"\"\"\n\n        try:\n            date = self._get_localized_datetime(self.date)\n        except ValueError:\n            raise ValueError(\"Unable to parse Date. Please specify in YYYYMMDD format.\")\n\n        if date &gt; self.current_date:\n            raise ValueError(\"Query date can't be in future.\")\n\n        valid_load_types = [\"actual\", \"forecast\"]\n\n        if self.load_type not in valid_load_types:\n            raise ValueError(\n                f\"Invalid load_type `{self.load_type}` given. Supported values are {valid_load_types}.\"\n            )\n\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource","title":"<code>BaseISOSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>class BaseISOSource(SourceInterface):\n    \"\"\"\n    Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options: list = []\n    spark_schema = StructType([StructField(\"id\", IntegerType(), True)])\n    default_query_timezone: str = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.query_timezone = pytz.timezone(\n            self.options.get(\"query_timezone\", self.default_query_timezone)\n        )\n        self.current_date = datetime.now(timezone.utc).astimezone(self.query_timezone)\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n        \"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.iso_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - {url}\")\n\n        response = requests.get(url)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n\n        return response.content\n\n    def _get_localized_datetime(self, datetime_str: str) -&gt; datetime:\n        \"\"\"\n        Converts string datetime into Python datetime object with configured format and timezone.\n        Args:\n            datetime_str: String to be converted into datetime.\n\n        Returns: Timezone aware datetime object.\n\n        \"\"\"\n        parsed_dt = datetime.strptime(datetime_str, self.query_datetime_format)\n        parsed_dt = parsed_dt.replace(tzinfo=self.query_timezone)\n        return parsed_dt\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Hits the fetch_from_url method with certain parameters to get raw data from API.\n\n        All the children ISO classes must override this method and call the fetch_url method\n        in it.\n\n        Returns:\n             Raw DataFrame from API.\n        \"\"\"\n\n        return pd.read_csv(BytesIO(self._fetch_from_url(\"\")))\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Performs all the basic transformations to prepare data for further processing.\n        All the children ISO classes must override this method.\n\n        Args:\n            df: Raw DataFrame, received from the API.\n\n        Returns:\n             Modified DataFrame, ready for basic use.\n\n        \"\"\"\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Another data transformation helper method to be called after prepare data.\n        Used for advance data processing such as cleaning, filtering, restructuring.\n        All the children ISO classes must override this method if there is any post-processing required.\n\n        Args:\n            df: Initial modified version of DataFrame, received after preparing the data.\n\n        Returns:\n             Final version of data after all the fixes and modifications.\n\n        \"\"\"\n        return df\n\n    def _get_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Entrypoint method to return the final version of DataFrame.\n\n        Returns:\n            Modified form of data for specific use case.\n\n        \"\"\"\n        df = self._pull_data()\n        df = self._prepare_data(df)\n        df = self._sanitize_data(df)\n\n        # Reorder columns to keep the data consistent\n        df = df[self.spark_schema.names]\n\n        return df\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Performs all the options checks. Raises exception in case of any invalid value.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        return True\n\n    def pre_read_validation(self) -&gt; bool:\n        \"\"\"\n        Ensures all the required options are provided and performs other validations.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        for key in self.required_options:\n            if key not in self.options:\n                raise ValueError(f\"Required option `{key}` is missing.\")\n\n        return self._validate_options()\n\n    def post_read_validation(self) -&gt; bool:\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n        Returns:\n             Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n        \"\"\"\n\n        try:\n            self.pre_read_validation()\n            pdf = self._get_data()\n            pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n            # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n            pd.DataFrame.iteritems = pd.DataFrame.items\n            df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n            return df\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n        Returns:\n             Final Spark DataFrame after all the processing.\n\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.pre_read_validation","title":"<code>pre_read_validation()</code>","text":"<p>Ensures all the required options are provided and performs other validations. Returns:      True if all checks are passed.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def pre_read_validation(self) -&gt; bool:\n    \"\"\"\n    Ensures all the required options are provided and performs other validations.\n    Returns:\n         True if all checks are passed.\n\n    \"\"\"\n    for key in self.required_options:\n        if key not in self.options:\n            raise ValueError(f\"Required option `{key}` is missing.\")\n\n    return self._validate_options()\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data. Returns:      Final Spark DataFrame converted from Pandas DataFrame post-execution.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n    Returns:\n         Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n    \"\"\"\n\n    try:\n        self.pre_read_validation()\n        pdf = self._get_data()\n        pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n        # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n        pd.DataFrame.iteritems = pd.DataFrame.items\n        df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n        return df\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_stream","title":"<code>read_stream()</code>","text":"<p>By default, the streaming operation is not supported but child classes can override if ISO supports streaming.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Final Spark DataFrame after all the processing.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n    Returns:\n         Final Spark DataFrame after all the processing.\n\n    \"\"\"\n\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_historical_load_iso/","title":"MISO Historical Load","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.miso_historical_load_iso.MISOHistoricalLoadISOSource","title":"<code>MISOHistoricalLoadISOSource</code>","text":"<p>               Bases: <code>MISODailyLoadISOSource</code></p> <p>The MISO Historical Load ISO Source is used to read historical load data from MISO API.</p> <p>To read more about the available reports from MISO API, download the file -  Market Reports</p> <p>From the list of reports in the file, it pulls the report named  <code>Historical Daily Forecast and Actual Load by Local Resource Zone</code>.</p>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.miso_historical_load_iso.MISOHistoricalLoadISOSource--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import MISOHistoricalLoadISOSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nmiso_source = MISOHistoricalLoadISOSource(\n    spark=spark,\n    options={\n        \"start_date\": \"20230510\",\n        \"end_date\": \"20230520\",\n    }\n)\n\nmiso_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>start_date</code> <code>str</code> <p>Must be in <code>YYYYMMDD</code> format.</p> <code>end_date</code> <code>str</code> <p>Must be in <code>YYYYMMDD</code> format.</p> <code>fill_missing</code> <code>str</code> <p>Set to <code>\"true\"</code> to fill missing Actual load with Forecast load. Default - <code>true</code>.</p> <p>Please check the BaseISOSource for available methods.</p> BaseISOSource Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/miso_historical_load_iso.py</code> <pre><code>class MISOHistoricalLoadISOSource(MISODailyLoadISOSource):\n    \"\"\"\n    The MISO Historical Load ISO Source is used to read historical load data from MISO API.\n\n    To read more about the available reports from MISO API, download the file -\n     [Market Reports](https://cdn.misoenergy.org/Market%20Reports%20Directory115139.xlsx)\n\n    From the list of reports in the file, it pulls the report named\n     `Historical Daily Forecast and Actual Load by Local Resource Zone`.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.sources import MISOHistoricalLoadISOSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    miso_source = MISOHistoricalLoadISOSource(\n        spark=spark,\n        options={\n            \"start_date\": \"20230510\",\n            \"end_date\": \"20230520\",\n        }\n    )\n\n    miso_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations (See Attributes table below)\n\n    Attributes:\n        start_date (str): Must be in `YYYYMMDD` format.\n        end_date (str): Must be in `YYYYMMDD` format.\n        fill_missing (str): Set to `\"true\"` to fill missing Actual load with Forecast load. Default - `true`.\n\n    Please check the BaseISOSource for available methods.\n\n    BaseISOSource:\n        ::: src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    required_options = [\"start_date\", \"end_date\"]\n\n    def __init__(self, spark: SparkSession, options: dict):\n        super().__init__(spark, options)\n        self.start_date = self.options.get(\"start_date\", \"\")\n        self.end_date = self.options.get(\"end_date\", \"\")\n        self.fill_missing = bool(self.options.get(\"fill_missing\", \"true\") == \"true\")\n\n    def _get_historical_data_for_date(self, date: datetime) -&gt; pd.DataFrame:\n        logging.info(f\"Getting historical data for date {date}\")\n        df = pd.read_excel(\n            self._fetch_from_url(\n                f\"{date.strftime(self.query_datetime_format)}_dfal_HIST.xls\"\n            ),\n            skiprows=5,\n        )\n\n        if date.month == 12 and date.day == 31:\n            expected_year_rows = (\n                pd.Timestamp(date.year, 12, 31).dayofyear * 24 * 7\n            )  # Every hour has 7 zones.\n            received_year_rows = (\n                len(df[df[\"MarketDay\"] != \"MarketDay\"]) - 2\n            )  # Last 2 rows are invalid.\n\n            if expected_year_rows != received_year_rows:\n                logging.warning(\n                    f\"Didn't receive full year historical data for year {date.year}.\"\n                    f\" Expected {expected_year_rows} but Received {received_year_rows}\"\n                )\n\n        return df\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Pulls data from the MISO API and parses the Excel file.\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n\n        logging.info(\n            f\"Historical load requested from {self.start_date} to {self.end_date}\"\n        )\n\n        start_date = self._get_localized_datetime(self.start_date)\n        end_date = self._get_localized_datetime(self.end_date)\n\n        dates = pd.date_range(\n            start_date, end_date + timedelta(days=365), freq=\"Y\", inclusive=\"left\"\n        )\n        logging.info(f\"Generated date ranges are - {dates}\")\n\n        # Collect all historical data on yearly basis.\n        df = pd.concat(\n            [\n                self._get_historical_data_for_date(min(date, self.current_date))\n                for date in dates\n            ],\n            sort=False,\n        )\n\n        return df\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Creates a new `Datetime` column, removes null values and pivots the data.\n\n        Args:\n            df: Raw form of data received from the API.\n\n        Returns:\n            Data after basic transformations and pivoting.\n\n        \"\"\"\n\n        df = df[df[\"MarketDay\"] != \"MarketDay\"]\n\n        # Fill missing actual values with the forecast values to avoid gaps.\n        if self.fill_missing:\n            df = df.fillna({\"ActualLoad (MWh)\": df[\"MTLF (MWh)\"]})\n\n        df = df.rename(\n            columns={\n                \"MarketDay\": \"date\",\n                \"HourEnding\": \"hour\",\n                \"ActualLoad (MWh)\": \"load\",\n                \"LoadResource Zone\": \"zone\",\n            }\n        )\n        df = df.dropna()\n\n        df[\"date_time\"] = pd.to_datetime(df[\"date\"]) + pd.to_timedelta(\n            df[\"hour\"].astype(int) - 1, \"h\"\n        )\n\n        df.drop([\"hour\", \"date\"], axis=1, inplace=True)\n        df[\"load\"] = df[\"load\"].astype(float)\n\n        df = df.pivot_table(\n            index=\"date_time\", values=\"load\", columns=\"zone\"\n        ).reset_index()\n\n        df.columns = [str(x.split(\" \")[0]).upper() for x in df.columns]\n\n        rename_cols = {\n            \"LRZ1\": \"Lrz1\",\n            \"LRZ2_7\": \"Lrz2_7\",\n            \"LRZ3_5\": \"Lrz3_5\",\n            \"LRZ4\": \"Lrz4\",\n            \"LRZ6\": \"Lrz6\",\n            \"LRZ8_9_10\": \"Lrz8_9_10\",\n            \"MISO\": \"Miso\",\n            \"DATE_TIME\": \"Datetime\",\n        }\n\n        df = df.rename(columns=rename_cols)\n\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Filter outs data outside the requested date range.\n\n        Args:\n            df: Data received after preparation.\n\n        Returns:\n            Final data after all the transformations.\n\n        \"\"\"\n\n        start_date = self._get_localized_datetime(self.start_date)\n        end_date = self._get_localized_datetime(self.end_date).replace(\n            hour=23, minute=59, second=59\n        )\n\n        df = df[\n            (df[\"Datetime\"] &gt;= start_date.replace(tzinfo=None))\n            &amp; (df[\"Datetime\"] &lt;= end_date.replace(tzinfo=None))\n        ]\n\n        df = df.sort_values(by=\"Datetime\", ascending=True).reset_index(drop=True)\n\n        expected_rows = ((min(end_date, self.current_date) - start_date).days + 1) * 24\n\n        actual_rows = len(df)\n\n        logging.info(f\"Rows Expected = {expected_rows}, Rows Found = {actual_rows}\")\n\n        return df\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Validates the following options:\n            - `start_date` &amp; `end_data` must be in the correct format.\n            - `start_date` must be behind `end_data`.\n            - `start_date` must not be in the future (UTC).\n\n        Returns:\n            True if all looks good otherwise raises Exception.\n\n        \"\"\"\n\n        try:\n            start_date = self._get_localized_datetime(self.start_date)\n        except ValueError:\n            raise ValueError(\n                \"Unable to parse Start date. Please specify in YYYYMMDD format.\"\n            )\n\n        try:\n            end_date = self._get_localized_datetime(self.end_date)\n        except ValueError:\n            raise ValueError(\n                \"Unable to parse End date. Please specify in YYYYMMDD format.\"\n            )\n\n        if start_date &gt; self.current_date:\n            raise ValueError(\"Start date can't be in future.\")\n\n        if start_date &gt; end_date:\n            raise ValueError(\"Start date can't be ahead of End date.\")\n\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource","title":"<code>BaseISOSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>class BaseISOSource(SourceInterface):\n    \"\"\"\n    Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options: list = []\n    spark_schema = StructType([StructField(\"id\", IntegerType(), True)])\n    default_query_timezone: str = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.query_timezone = pytz.timezone(\n            self.options.get(\"query_timezone\", self.default_query_timezone)\n        )\n        self.current_date = datetime.now(timezone.utc).astimezone(self.query_timezone)\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n        \"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.iso_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - {url}\")\n\n        response = requests.get(url)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n\n        return response.content\n\n    def _get_localized_datetime(self, datetime_str: str) -&gt; datetime:\n        \"\"\"\n        Converts string datetime into Python datetime object with configured format and timezone.\n        Args:\n            datetime_str: String to be converted into datetime.\n\n        Returns: Timezone aware datetime object.\n\n        \"\"\"\n        parsed_dt = datetime.strptime(datetime_str, self.query_datetime_format)\n        parsed_dt = parsed_dt.replace(tzinfo=self.query_timezone)\n        return parsed_dt\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Hits the fetch_from_url method with certain parameters to get raw data from API.\n\n        All the children ISO classes must override this method and call the fetch_url method\n        in it.\n\n        Returns:\n             Raw DataFrame from API.\n        \"\"\"\n\n        return pd.read_csv(BytesIO(self._fetch_from_url(\"\")))\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Performs all the basic transformations to prepare data for further processing.\n        All the children ISO classes must override this method.\n\n        Args:\n            df: Raw DataFrame, received from the API.\n\n        Returns:\n             Modified DataFrame, ready for basic use.\n\n        \"\"\"\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Another data transformation helper method to be called after prepare data.\n        Used for advance data processing such as cleaning, filtering, restructuring.\n        All the children ISO classes must override this method if there is any post-processing required.\n\n        Args:\n            df: Initial modified version of DataFrame, received after preparing the data.\n\n        Returns:\n             Final version of data after all the fixes and modifications.\n\n        \"\"\"\n        return df\n\n    def _get_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Entrypoint method to return the final version of DataFrame.\n\n        Returns:\n            Modified form of data for specific use case.\n\n        \"\"\"\n        df = self._pull_data()\n        df = self._prepare_data(df)\n        df = self._sanitize_data(df)\n\n        # Reorder columns to keep the data consistent\n        df = df[self.spark_schema.names]\n\n        return df\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Performs all the options checks. Raises exception in case of any invalid value.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        return True\n\n    def pre_read_validation(self) -&gt; bool:\n        \"\"\"\n        Ensures all the required options are provided and performs other validations.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        for key in self.required_options:\n            if key not in self.options:\n                raise ValueError(f\"Required option `{key}` is missing.\")\n\n        return self._validate_options()\n\n    def post_read_validation(self) -&gt; bool:\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n        Returns:\n             Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n        \"\"\"\n\n        try:\n            self.pre_read_validation()\n            pdf = self._get_data()\n            pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n            # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n            pd.DataFrame.iteritems = pd.DataFrame.items\n            df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n            return df\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n        Returns:\n             Final Spark DataFrame after all the processing.\n\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.pre_read_validation","title":"<code>pre_read_validation()</code>","text":"<p>Ensures all the required options are provided and performs other validations. Returns:      True if all checks are passed.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def pre_read_validation(self) -&gt; bool:\n    \"\"\"\n    Ensures all the required options are provided and performs other validations.\n    Returns:\n         True if all checks are passed.\n\n    \"\"\"\n    for key in self.required_options:\n        if key not in self.options:\n            raise ValueError(f\"Required option `{key}` is missing.\")\n\n    return self._validate_options()\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data. Returns:      Final Spark DataFrame converted from Pandas DataFrame post-execution.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n    Returns:\n         Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n    \"\"\"\n\n    try:\n        self.pre_read_validation()\n        pdf = self._get_data()\n        pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n        # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n        pd.DataFrame.iteritems = pd.DataFrame.items\n        df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n        return df\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/miso_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_stream","title":"<code>read_stream()</code>","text":"<p>By default, the streaming operation is not supported but child classes can override if ISO supports streaming.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Final Spark DataFrame after all the processing.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n    Returns:\n         Final Spark DataFrame after all the processing.\n\n    \"\"\"\n\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_daily_load_iso/","title":"PJM Daily Load","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.pjm_daily_load_iso.PJMDailyLoadISOSource","title":"<code>PJMDailyLoadISOSource</code>","text":"<p>               Bases: <code>BaseISOSource</code></p> <p>The PJM Daily Load ISO Source is used to read daily load data from PJM API. It supports both Actual and Forecast data. Actual will return 1 day, Forecast will return 7 days.</p> <p>To read more about the reports, visit the following URLs -  Actual doc:    ops_sum_prev_period  Forecast doc:  load_frcstd_7_day</p>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.pjm_daily_load_iso.PJMDailyLoadISOSource--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import PJMDailyLoadISOSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\npjm_source = PJMDailyLoadISOSource(\n    spark=spark,\n    options={\n        \"api_key\": \"{api_key}\",\n        \"load_type\": \"actual\"\n    }\n)\n\npjm_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>Must be a valid key from PJM, see api url</p> <code>load_type</code> <code>str</code> <p>Must be one of <code>actual</code> or <code>forecast</code></p> <p>Please check the BaseISOSource for available methods.</p> BaseISOSource Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/pjm_daily_load_iso.py</code> <pre><code>class PJMDailyLoadISOSource(BaseISOSource):\n    \"\"\"\n    The PJM Daily Load ISO Source is used to read daily load data from PJM API.\n    It supports both Actual and Forecast data. Actual will return 1 day, Forecast will return 7 days.\n\n    To read more about the reports, visit the following URLs -\n    &lt;br&gt;\n    Actual doc:    [ops_sum_prev_period](https://dataminer2.pjm.com/feed/ops_sum_prev_period/definition)\n    &lt;br&gt;\n    Forecast doc:  [load_frcstd_7_day](https://dataminer2.pjm.com/feed/load_frcstd_7_day/definition)\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.sources import PJMDailyLoadISOSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    pjm_source = PJMDailyLoadISOSource(\n        spark=spark,\n        options={\n            \"api_key\": \"{api_key}\",\n            \"load_type\": \"actual\"\n        }\n    )\n\n    pjm_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations (See Attributes table below)\n\n    Attributes:\n        api_key (str): Must be a valid key from PJM, see api url\n        load_type (str): Must be one of `actual` or `forecast`\n\n    Please check the BaseISOSource for available methods.\n\n    BaseISOSource:\n        ::: src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso\n    \"\"\"\n\n    spark: SparkSession\n    spark_schema = PJM_SCHEMA\n    options: dict\n    iso_url: str = \"https://api.pjm.com/api/v1/\"\n    query_datetime_format: str = \"%Y-%m-%d %H:%M\"\n    required_options = [\"api_key\", \"load_type\"]\n    default_query_timezone = \"US/Eastern\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super().__init__(spark, options)\n        self.spark: SparkSession = spark\n        self.options: dict = options\n        self.load_type: str = self.options.get(\"load_type\", \"\").strip()\n        self.api_key: str = self.options.get(\"api_key\", \"\").strip()\n        self.days: int = self.options.get(\"days\", 7)\n\n    def _fetch_from_url(self, url_suffix: str, start_date: str, end_date: str) -&gt; bytes:\n        \"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n        \"\"\"\n\n        url = f\"{self.iso_url}{url_suffix}\"\n        headers = {\"Ocp-Apim-Subscription-Key\": self.api_key}\n        logging.info(\n            f\"Requesting URL - {url}, start_date={start_date}, end_date={end_date}, load_type={self.load_type}\"\n        )\n        load_key = (\n            \"datetime_beginning_ept\"\n            if self.load_type != \"forecast\"\n            else \"forecast_datetime_beginning_ept\"\n        )\n        feed = (\n            \"ops_sum_prev_period\"\n            if self.load_type != \"forecast\"\n            else \"load_frcstd_7_day\"\n        )\n        query = {\n            \"startRow\": \"1\",\n            load_key: f\"{start_date}to{end_date}\",\n            \"format\": \"csv\",\n            \"download\": \"true\",\n        }\n        query_s = \"&amp;\".join([\"=\".join([k, v]) for k, v in query.items()])\n        new_url = f\"{url}{feed}?{query_s}\"\n        response = requests.get(new_url, headers=headers)\n        code = response.status_code\n\n        if code != 200:\n            raise requests.HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n        return response.content\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Pulls data from the PJM API and parses the return.\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n        start_date = self.current_date - timedelta(days=1)\n        start_date = start_date.replace(hour=0, minute=0)\n        end_date = (start_date + timedelta(days=self.days)).replace(hour=23)\n        start_date_str = start_date.strftime(self.query_datetime_format)\n        end_date_str = end_date.strftime(self.query_datetime_format)\n        df = pd.read_csv(\n            BytesIO(self._fetch_from_url(\"\", start_date_str, end_date_str))\n        )\n\n        return df\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Creates a new date time column and removes null values. Renames columns\n\n        Args:\n            df: Raw form of data received from the API.\n\n        Returns:\n            Data after basic transformations.\n\n        \"\"\"\n\n        if self.load_type == \"forecast\":\n            df = df.rename(\n                columns={\n                    \"forecast_datetime_beginning_utc\": \"start_time\",\n                    \"forecast_area\": \"zone\",\n                    \"forecast_datetime_ending_utc\": \"end_time\",\n                    \"forecast_load_mw\": \"load\",\n                }\n            )\n        else:\n            df = df.rename(\n                columns={\n                    \"datetime_beginning_utc\": \"start_time\",\n                    \"area\": \"zone\",\n                    \"datetime_ending_utc\": \"end_time\",\n                    \"actual_load\": \"load\",\n                }\n            )\n\n        df = df[[\"start_time\", \"end_time\", \"zone\", \"load\"]]\n        df = df.replace({np.nan: None, \"\": None})\n\n        date_cols = [\"start_time\", \"end_time\"]\n        for col in date_cols:\n            df[col] = pd.to_datetime(df[col], format=\"%m/%d/%Y %I:%M:%S %p\")\n\n        df[\"load\"] = df[\"load\"].astype(float)\n        df = df.replace({np.nan: None, \"\": None})\n        df.columns = list(map(lambda x: x.upper(), df.columns))\n\n        rename_cols = {\n            \"START_TIME\": \"StartTime\",\n            \"END_TIME\": \"EndTime\",\n            \"ZONE\": \"Zone\",\n            \"LOAD\": \"Load\",\n        }\n\n        df = df.rename(columns=rename_cols)\n\n        df.reset_index(inplace=True, drop=True)\n\n        return df\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Validates the following options:\n            - `load_type` must be valid.\n\n        Returns:\n            True if all looks good otherwise raises Exception.\n        \"\"\"\n\n        valid_load_types = [\"actual\", \"forecast\"]\n\n        if self.load_type not in valid_load_types:\n            raise ValueError(\n                f\"Invalid load_type `{self.load_type}` given. Supported values are {valid_load_types}.\"\n            )\n\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource","title":"<code>BaseISOSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>class BaseISOSource(SourceInterface):\n    \"\"\"\n    Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options: list = []\n    spark_schema = StructType([StructField(\"id\", IntegerType(), True)])\n    default_query_timezone: str = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.query_timezone = pytz.timezone(\n            self.options.get(\"query_timezone\", self.default_query_timezone)\n        )\n        self.current_date = datetime.now(timezone.utc).astimezone(self.query_timezone)\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n        \"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.iso_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - {url}\")\n\n        response = requests.get(url)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n\n        return response.content\n\n    def _get_localized_datetime(self, datetime_str: str) -&gt; datetime:\n        \"\"\"\n        Converts string datetime into Python datetime object with configured format and timezone.\n        Args:\n            datetime_str: String to be converted into datetime.\n\n        Returns: Timezone aware datetime object.\n\n        \"\"\"\n        parsed_dt = datetime.strptime(datetime_str, self.query_datetime_format)\n        parsed_dt = parsed_dt.replace(tzinfo=self.query_timezone)\n        return parsed_dt\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Hits the fetch_from_url method with certain parameters to get raw data from API.\n\n        All the children ISO classes must override this method and call the fetch_url method\n        in it.\n\n        Returns:\n             Raw DataFrame from API.\n        \"\"\"\n\n        return pd.read_csv(BytesIO(self._fetch_from_url(\"\")))\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Performs all the basic transformations to prepare data for further processing.\n        All the children ISO classes must override this method.\n\n        Args:\n            df: Raw DataFrame, received from the API.\n\n        Returns:\n             Modified DataFrame, ready for basic use.\n\n        \"\"\"\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Another data transformation helper method to be called after prepare data.\n        Used for advance data processing such as cleaning, filtering, restructuring.\n        All the children ISO classes must override this method if there is any post-processing required.\n\n        Args:\n            df: Initial modified version of DataFrame, received after preparing the data.\n\n        Returns:\n             Final version of data after all the fixes and modifications.\n\n        \"\"\"\n        return df\n\n    def _get_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Entrypoint method to return the final version of DataFrame.\n\n        Returns:\n            Modified form of data for specific use case.\n\n        \"\"\"\n        df = self._pull_data()\n        df = self._prepare_data(df)\n        df = self._sanitize_data(df)\n\n        # Reorder columns to keep the data consistent\n        df = df[self.spark_schema.names]\n\n        return df\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Performs all the options checks. Raises exception in case of any invalid value.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        return True\n\n    def pre_read_validation(self) -&gt; bool:\n        \"\"\"\n        Ensures all the required options are provided and performs other validations.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        for key in self.required_options:\n            if key not in self.options:\n                raise ValueError(f\"Required option `{key}` is missing.\")\n\n        return self._validate_options()\n\n    def post_read_validation(self) -&gt; bool:\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n        Returns:\n             Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n        \"\"\"\n\n        try:\n            self.pre_read_validation()\n            pdf = self._get_data()\n            pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n            # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n            pd.DataFrame.iteritems = pd.DataFrame.items\n            df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n            return df\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n        Returns:\n             Final Spark DataFrame after all the processing.\n\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.pre_read_validation","title":"<code>pre_read_validation()</code>","text":"<p>Ensures all the required options are provided and performs other validations. Returns:      True if all checks are passed.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def pre_read_validation(self) -&gt; bool:\n    \"\"\"\n    Ensures all the required options are provided and performs other validations.\n    Returns:\n         True if all checks are passed.\n\n    \"\"\"\n    for key in self.required_options:\n        if key not in self.options:\n            raise ValueError(f\"Required option `{key}` is missing.\")\n\n    return self._validate_options()\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data. Returns:      Final Spark DataFrame converted from Pandas DataFrame post-execution.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n    Returns:\n         Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n    \"\"\"\n\n    try:\n        self.pre_read_validation()\n        pdf = self._get_data()\n        pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n        # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n        pd.DataFrame.iteritems = pd.DataFrame.items\n        df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n        return df\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_daily_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_stream","title":"<code>read_stream()</code>","text":"<p>By default, the streaming operation is not supported but child classes can override if ISO supports streaming.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Final Spark DataFrame after all the processing.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n    Returns:\n         Final Spark DataFrame after all the processing.\n\n    \"\"\"\n\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_daily_pricing_iso/","title":"PJM Daily Pricing Load","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_daily_pricing_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.pjm_daily_pricing_iso.PJMDailyPricingISOSource","title":"<code>PJMDailyPricingISOSource</code>","text":"<p>               Bases: <code>BaseISOSource</code></p> <p>The PJM Daily Pricing ISO Source is used to retrieve Real-Time and Day-Ahead hourly data from PJM API. Real-Time will return data for T - 3 to T days and Day-Ahead will return T - 3 to T + 1 days data.</p> <p>API:             https://api.pjm.com/api/v1/  (must be a valid apy key from PJM)</p> <p>Real-Time doc:    https://dataminer2.pjm.com/feed/rt_hrl_lmps/definition</p> <p>Day-Ahead doc:    https://dataminer2.pjm.com/feed/da_hrl_lmps/definition</p>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_daily_pricing_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.pjm_daily_pricing_iso.PJMDailyPricingISOSource--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import PJMDailyPricingISOSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\npjm_source = PJMDailyPricingISOSource(\n    spark=spark,\n    options={\n        \"api_key\": \"{api_key}\",\n        \"load_type\": \"real_time\"\n    }\n)\n\npjm_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>Must be a valid key from PJM, see api url</p> <code>load_type</code> <code>str</code> <p>Must be one of <code>real_time</code> or <code>day_ahead</code></p> <p>Please check the BaseISOSource for available methods.</p> BaseISOSource Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/pjm_daily_pricing_iso.py</code> <pre><code>class PJMDailyPricingISOSource(BaseISOSource):\n    \"\"\"\n    The PJM Daily Pricing ISO Source is used to retrieve Real-Time and Day-Ahead hourly data from PJM API.\n    Real-Time will return data for T - 3 to T days and Day-Ahead will return T - 3 to T + 1 days data.\n\n    API:             &lt;a href=\"https://api.pjm.com/api/v1/\"&gt;https://api.pjm.com/api/v1/&lt;/a&gt;  (must be a valid apy key from PJM)\n\n    Real-Time doc:    &lt;a href=\"https://dataminer2.pjm.com/feed/rt_hrl_lmps/definition\"&gt;https://dataminer2.pjm.com/feed/rt_hrl_lmps/definition&lt;/a&gt;\n\n    Day-Ahead doc:    &lt;a href=\"https://dataminer2.pjm.com/feed/da_hrl_lmps/definition\"&gt;https://dataminer2.pjm.com/feed/da_hrl_lmps/definition&lt;/a&gt;\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.sources import PJMDailyPricingISOSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    pjm_source = PJMDailyPricingISOSource(\n        spark=spark,\n        options={\n            \"api_key\": \"{api_key}\",\n            \"load_type\": \"real_time\"\n        }\n    )\n\n    pjm_source.read_batch()\n    ```\n\n    Parameters:\n       spark (SparkSession): Spark Session instance\n       options (dict): A dictionary of ISO Source specific configurations (See Attributes table below)\n\n    Attributes:\n        api_key (str): Must be a valid key from PJM, see api url\n        load_type (str): Must be one of `real_time` or `day_ahead`\n\n    Please check the BaseISOSource for available methods.\n\n    BaseISOSource:\n        ::: src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso\n    \"\"\"\n\n    spark: SparkSession\n    spark_schema = PJM_PRICING_SCHEMA\n    options: dict\n    iso_url: str = \"https://api.pjm.com/api/v1/\"\n    query_datetime_format: str = \"%Y-%m-%d %H:%M\"\n    required_options = [\"api_key\", \"load_type\"]\n    default_query_timezone = \"US/Eastern\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super().__init__(spark, options)\n        self.spark: SparkSession = spark\n        self.options: dict = options\n        self.load_type: str = self.options.get(\"load_type\", \"\").strip()\n        self.api_key: str = self.options.get(\"api_key\", \"\").strip()\n        self.days: int = self.options.get(\"days\", 3)\n\n    def _fetch_paginated_data(\n        self, url_suffix: str, start_date: str, end_date: str\n    ) -&gt; bytes:\n        \"\"\"\n        Fetches data from the PJM API with pagination support.\n\n        Args:\n            url_suffix: String to be used as suffix to ISO URL.\n            start_date: Start date for the data retrieval.\n            end_date: End date for the data retrieval.\n\n        Returns:\n            Raw content of the data received.\n        \"\"\"\n        headers = {\"Ocp-Apim-Subscription-Key\": self.api_key}\n        items = []\n        query = {\n            \"startRow\": \"1\",\n            \"rowCount\": \"5\",\n            \"datetime_beginning_ept\": f\"{start_date}to{end_date}\",\n        }\n        query_s = \"&amp;\".join([\"=\".join([k, v]) for k, v in query.items()])\n        base_url = f\"{self.iso_url}{url_suffix}?{query_s}\"\n\n        next_page = base_url\n\n        logging.info(\n            f\"Requesting URL - {base_url}, start_date={start_date}, end_date={end_date}, load_type={self.load_type}\"\n        )\n\n        while next_page:\n            now = datetime.now()\n            logging.info(f\"Timestamp: {now}\")\n            response = requests.get(next_page, headers=headers)\n            code = response.status_code\n\n            if code != 200:\n                raise requests.HTTPError(\n                    f\"Unable to access URL `{next_page}`.\"\n                    f\" Received status code {code} with message {response.content}\"\n                )\n\n            data = response.json()\n\n            logging.info(f\"Data for page {next_page}:\")\n            items.extend(data[\"items\"])\n            next_urls = list(filter(lambda item: item[\"rel\"] == \"next\", data[\"links\"]))\n            next_page = next_urls[0][\"href\"] if next_urls else None\n            time.sleep(10)\n\n        return items\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Pulls data from the PJM API and parses the return.\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n        start_date = self.current_date - timedelta(self.days)\n        start_date = start_date.replace(hour=0, minute=0)\n        end_date = (start_date + timedelta(days=self.days)).replace(hour=23)\n        start_date_str = start_date.strftime(self.query_datetime_format)\n        end_date_str = end_date.strftime(self.query_datetime_format)\n\n        if self.load_type == \"day_ahead\":\n            url_suffix = \"da_hrl_lmps\"\n        else:\n            url_suffix = \"rt_hrl_lmps\"\n\n        data = self._fetch_paginated_data(url_suffix, start_date_str, end_date_str)\n\n        df = pd.DataFrame(data)\n        logging.info(f\"Data fetched successfully: {len(df)} rows\")\n\n        return df\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Creates a new date time column and removes null values. Renames columns\n\n        Args:\n            df: Raw form of data received from the API.\n\n        Returns:\n            Data after basic transformations.\n\n        \"\"\"\n\n        if self.load_type == \"day_ahead\":\n            df = df.rename(\n                columns={\n                    \"datetime_beginning_utc\": \"StartTime\",\n                    \"pnode_id\": \"PnodeId\",\n                    \"pnode_name\": \"PnodeName\",\n                    \"voltage\": \"Voltage\",\n                    \"equipment\": \"Equipment\",\n                    \"type\": \"Type\",\n                    \"zone\": \"Zone\",\n                    \"system_energy_price_da\": \"SystemEnergyPrice\",\n                    \"total_lmp_da\": \"TotalLmp\",\n                    \"congestion_price_da\": \"CongestionPrice\",\n                    \"marginal_loss_price_da\": \"MarginalLossPrice\",\n                    \"version_nbr\": \"VersionNbr\",\n                }\n            )\n        else:\n            df = df.rename(\n                columns={\n                    \"datetime_beginning_utc\": \"StartTime\",\n                    \"pnode_id\": \"PnodeId\",\n                    \"pnode_name\": \"PnodeName\",\n                    \"voltage\": \"Voltage\",\n                    \"equipment\": \"Equipment\",\n                    \"type\": \"Type\",\n                    \"zone\": \"Zone\",\n                    \"system_energy_price_rt\": \"SystemEnergyPrice\",\n                    \"total_lmp_rt\": \"TotalLmp\",\n                    \"congestion_price_rt\": \"CongestionPrice\",\n                    \"marginal_loss_price_rt\": \"MarginalLossPrice\",\n                    \"version_nbr\": \"VersionNbr\",\n                }\n            )\n\n        df = df[\n            [\n                \"StartTime\",\n                \"PnodeId\",\n                \"PnodeName\",\n                \"Voltage\",\n                \"Equipment\",\n                \"Type\",\n                \"Zone\",\n                \"SystemEnergyPrice\",\n                \"TotalLmp\",\n                \"CongestionPrice\",\n                \"MarginalLossPrice\",\n                \"VersionNbr\",\n            ]\n        ]\n\n        df = df.replace({np.nan: None, \"\": None})\n\n        df[\"StartTime\"] = pd.to_datetime(df[\"StartTime\"])\n        df = df.replace({np.nan: None, \"\": None})\n\n        df.reset_index(inplace=True, drop=True)\n\n        return df\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Validates the following options:\n            - `load_type` must be valid.\n\n        Returns:\n            True if all looks good otherwise raises Exception.\n        \"\"\"\n\n        valid_load_types = [\"real_time\", \"day_ahead\"]\n\n        if self.load_type not in valid_load_types:\n            raise ValueError(\n                f\"Invalid load_type `{self.load_type}` given. Supported values are {valid_load_types}.\"\n            )\n\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_daily_pricing_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource","title":"<code>BaseISOSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>class BaseISOSource(SourceInterface):\n    \"\"\"\n    Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options: list = []\n    spark_schema = StructType([StructField(\"id\", IntegerType(), True)])\n    default_query_timezone: str = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.query_timezone = pytz.timezone(\n            self.options.get(\"query_timezone\", self.default_query_timezone)\n        )\n        self.current_date = datetime.now(timezone.utc).astimezone(self.query_timezone)\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n        \"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.iso_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - {url}\")\n\n        response = requests.get(url)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n\n        return response.content\n\n    def _get_localized_datetime(self, datetime_str: str) -&gt; datetime:\n        \"\"\"\n        Converts string datetime into Python datetime object with configured format and timezone.\n        Args:\n            datetime_str: String to be converted into datetime.\n\n        Returns: Timezone aware datetime object.\n\n        \"\"\"\n        parsed_dt = datetime.strptime(datetime_str, self.query_datetime_format)\n        parsed_dt = parsed_dt.replace(tzinfo=self.query_timezone)\n        return parsed_dt\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Hits the fetch_from_url method with certain parameters to get raw data from API.\n\n        All the children ISO classes must override this method and call the fetch_url method\n        in it.\n\n        Returns:\n             Raw DataFrame from API.\n        \"\"\"\n\n        return pd.read_csv(BytesIO(self._fetch_from_url(\"\")))\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Performs all the basic transformations to prepare data for further processing.\n        All the children ISO classes must override this method.\n\n        Args:\n            df: Raw DataFrame, received from the API.\n\n        Returns:\n             Modified DataFrame, ready for basic use.\n\n        \"\"\"\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Another data transformation helper method to be called after prepare data.\n        Used for advance data processing such as cleaning, filtering, restructuring.\n        All the children ISO classes must override this method if there is any post-processing required.\n\n        Args:\n            df: Initial modified version of DataFrame, received after preparing the data.\n\n        Returns:\n             Final version of data after all the fixes and modifications.\n\n        \"\"\"\n        return df\n\n    def _get_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Entrypoint method to return the final version of DataFrame.\n\n        Returns:\n            Modified form of data for specific use case.\n\n        \"\"\"\n        df = self._pull_data()\n        df = self._prepare_data(df)\n        df = self._sanitize_data(df)\n\n        # Reorder columns to keep the data consistent\n        df = df[self.spark_schema.names]\n\n        return df\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Performs all the options checks. Raises exception in case of any invalid value.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        return True\n\n    def pre_read_validation(self) -&gt; bool:\n        \"\"\"\n        Ensures all the required options are provided and performs other validations.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        for key in self.required_options:\n            if key not in self.options:\n                raise ValueError(f\"Required option `{key}` is missing.\")\n\n        return self._validate_options()\n\n    def post_read_validation(self) -&gt; bool:\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n        Returns:\n             Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n        \"\"\"\n\n        try:\n            self.pre_read_validation()\n            pdf = self._get_data()\n            pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n            # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n            pd.DataFrame.iteritems = pd.DataFrame.items\n            df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n            return df\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n        Returns:\n             Final Spark DataFrame after all the processing.\n\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_daily_pricing_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.pre_read_validation","title":"<code>pre_read_validation()</code>","text":"<p>Ensures all the required options are provided and performs other validations. Returns:      True if all checks are passed.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def pre_read_validation(self) -&gt; bool:\n    \"\"\"\n    Ensures all the required options are provided and performs other validations.\n    Returns:\n         True if all checks are passed.\n\n    \"\"\"\n    for key in self.required_options:\n        if key not in self.options:\n            raise ValueError(f\"Required option `{key}` is missing.\")\n\n    return self._validate_options()\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_daily_pricing_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data. Returns:      Final Spark DataFrame converted from Pandas DataFrame post-execution.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n    Returns:\n         Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n    \"\"\"\n\n    try:\n        self.pre_read_validation()\n        pdf = self._get_data()\n        pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n        # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n        pd.DataFrame.iteritems = pd.DataFrame.items\n        df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n        return df\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_daily_pricing_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_stream","title":"<code>read_stream()</code>","text":"<p>By default, the streaming operation is not supported but child classes can override if ISO supports streaming.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Final Spark DataFrame after all the processing.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n    Returns:\n         Final Spark DataFrame after all the processing.\n\n    \"\"\"\n\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_historical_load_iso/","title":"PJM Historical Load","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.pjm_historical_load_iso.PJMHistoricalLoadISOSource","title":"<code>PJMHistoricalLoadISOSource</code>","text":"<p>               Bases: <code>PJMDailyLoadISOSource</code></p> <p>The PJM Historical Load ISO Source is used to read historical load data from PJM API.</p> <p>To read more about the reports, visit the following URLs -  Actual doc:    ops_sum_prev_period  Forecast doc:  load_frcstd_7_day</p> <p>Historical is the same PJM endpoint as Actual, but is called repeatedly within a range established by the start_date &amp; end_date attributes</p>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.pjm_historical_load_iso.PJMHistoricalLoadISOSource--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import PJMHistoricalLoadISOSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\npjm_source = PJMHistoricalLoadISOSource(\n    spark=spark,\n    options={\n        \"api_key\": \"{api_key}\",\n        \"start_date\": \"20230510\",\n        \"end_date\": \"20230520\",\n    }\n)\n\npjm_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>Must be a valid key from PJM, see PJM documentation</p> <code>start_date</code> <code>str</code> <p>Must be in <code>YYYY-MM-DD</code> format.</p> <code>end_date</code> <code>str</code> <p>Must be in <code>YYYY-MM-DD</code> format.</p> <code>query_batch_days</code> <code>int</code> <p>(optional) Number of days must be &lt; 160 as per PJM &amp; is defaulted to <code>120</code></p> <code>sleep_duration</code> <code>int</code> <p>(optional) Number of seconds to sleep between request, defaulted to <code>5</code> seconds, used to manage requests to PJM endpoint</p> <code>request_count</code> <code>int</code> <p>(optional) Number of requests made to PJM endpoint before sleep_duration, currently defaulted to <code>1</code></p> <p>Please check the BaseISOSource for available methods.</p> BaseISOSource Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/pjm_historical_load_iso.py</code> <pre><code>class PJMHistoricalLoadISOSource(PJMDailyLoadISOSource):\n    \"\"\"\n    The PJM Historical Load ISO Source is used to read historical load data from PJM API.\n\n    To read more about the reports, visit the following URLs -\n    &lt;br&gt;\n    Actual doc:    [ops_sum_prev_period](https://dataminer2.pjm.com/feed/ops_sum_prev_period/definition)\n    &lt;br&gt;\n    Forecast doc:  [load_frcstd_7_day](https://dataminer2.pjm.com/feed/load_frcstd_7_day/definition)\n\n    Historical is the same PJM endpoint as Actual, but is called repeatedly within a range established by the\n    start_date &amp; end_date attributes\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.sources import PJMHistoricalLoadISOSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    pjm_source = PJMHistoricalLoadISOSource(\n        spark=spark,\n        options={\n            \"api_key\": \"{api_key}\",\n            \"start_date\": \"20230510\",\n            \"end_date\": \"20230520\",\n        }\n    )\n\n    pjm_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations (See Attributes table below)\n\n    Attributes:\n        api_key (str): Must be a valid key from PJM, see PJM documentation\n        start_date (str): Must be in `YYYY-MM-DD` format.\n        end_date (str): Must be in `YYYY-MM-DD` format.\n\n        query_batch_days (int): (optional) Number of days must be &lt; 160 as per PJM &amp; is defaulted to `120`\n        sleep_duration (int): (optional) Number of seconds to sleep between request, defaulted to `5` seconds, used to manage requests to PJM endpoint\n        request_count (int): (optional) Number of requests made to PJM endpoint before sleep_duration, currently defaulted to `1`\n\n    Please check the BaseISOSource for available methods.\n\n    BaseISOSource:\n        ::: src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso\"\"\"\n\n    spark: SparkSession\n    options: dict\n    required_options = [\"api_key\", \"start_date\", \"end_date\"]\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super().__init__(spark, options)\n        self.spark: SparkSession = spark\n        self.options: dict = options\n        self.api_key: str = self.options.get(\"api_key\", \"\").strip()\n        self.start_date: str = self.options.get(\"start_date\", \"\")\n        self.end_date: str = self.options.get(\"end_date\", \"\")\n        self.query_batch_days: int = self.options.get(\"query_batch_days\", 120)\n        self.sleep_duration: int = self.options.get(\"sleep_duration\", 5)\n        self.request_count: int = self.options.get(\"request_count\", 1)\n        self.load_type: str = \"actual\"\n        self.user_datetime_format = \"%Y-%m-%d\"\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Pulls data from the PJM API and parses the return including date ranges.\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n\n        logging.info(\n            f\"Historical load requested from {self.start_date} to {self.end_date}\"\n        )\n        start_date = datetime.strptime(self.start_date, self.user_datetime_format)\n        end_date = datetime.strptime(self.end_date, self.user_datetime_format).replace(\n            hour=23\n        )\n\n        days_diff = (end_date - start_date).days\n        logging.info(f\"Expected hours for a single zone = {(days_diff + 1) * 24}\")\n        generated_days_ranges = []\n        dates = pd.date_range(\n            start_date, end_date, freq=pd.DateOffset(days=self.query_batch_days)\n        )\n\n        for date in dates:\n            py_date = date.to_pydatetime()\n            date_last = (py_date + timedelta(days=self.query_batch_days - 1)).replace(\n                hour=23\n            )\n            date_last = min(date_last, end_date)\n            generated_days_ranges.append((py_date, date_last))\n\n        logging.info(\n            f\"Generated date ranges for batch days {self.query_batch_days} are {generated_days_ranges}\"\n        )\n\n        # Collect all historical data on yearly basis.\n        dfs = []\n        for idx, date_range in enumerate(generated_days_ranges):\n            start_date_str = date_range[0].strftime(self.query_datetime_format)\n            end_date_str = date_range[1].strftime(self.query_datetime_format)\n\n            df = pd.read_csv(\n                BytesIO(self._fetch_from_url(\"\", start_date_str, end_date_str))\n            )\n            dfs.append(df)\n\n            if idx &gt; 0 and idx % self.request_count == 0:\n                logging.info(f\"Going to sleep for {self.sleep_duration} seconds\")\n                time.sleep(self.sleep_duration)\n\n        df = pd.concat(dfs, sort=False)\n        df = df.reset_index(drop=True)\n        return df\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Validates all parameters including the following examples:\n            - `start_date` &amp; `end_data` must be in the correct format.\n            - `start_date` must be behind `end_data`.\n            - `start_date` must not be in the future (UTC).\n\n        Returns:\n            True if all looks good otherwise raises Exception.\n\n        \"\"\"\n\n        try:\n            start_date = datetime.strptime(self.start_date, self.user_datetime_format)\n        except ValueError:\n            raise ValueError(\n                f\"Unable to parse Start date. Please specify in {self.user_datetime_format} format.\"\n            )\n\n        try:\n            end_date = datetime.strptime(self.end_date, self.user_datetime_format)\n        except ValueError:\n            raise ValueError(\n                f\"Unable to parse End date. Please specify in {self.user_datetime_format} format.\"\n            )\n\n        if start_date &gt; datetime.now(timezone.utc).replace(tzinfo=None) - timedelta(\n            days=1\n        ):\n            raise ValueError(\"Start date can't be in future.\")\n\n        if start_date &gt; end_date:\n            raise ValueError(\"Start date can't be ahead of End date.\")\n\n        if end_date &gt; datetime.now(timezone.utc).replace(tzinfo=None) - timedelta(\n            days=1\n        ):\n            raise ValueError(\"End date can't be in future.\")\n\n        if self.sleep_duration &lt; 0:\n            raise ValueError(\"Sleep duration can't be negative.\")\n\n        if self.request_count &lt; 0:\n            raise ValueError(\"Request count can't be negative.\")\n\n        if self.query_batch_days &lt; 0:\n            raise ValueError(\"Query batch days count can't be negative.\")\n\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource","title":"<code>BaseISOSource</code>","text":"<p>               Bases: <code>SourceInterface</code></p> <p>Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>class BaseISOSource(SourceInterface):\n    \"\"\"\n    Base class for all the ISO Sources. It provides common functionality and helps in reducing the code redundancy.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    iso_url: str = \"https://\"\n    query_datetime_format: str = \"%Y%m%d\"\n    required_options: list = []\n    spark_schema = StructType([StructField(\"id\", IntegerType(), True)])\n    default_query_timezone: str = \"UTC\"\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        self.spark = spark\n        self.options = options\n        self.query_timezone = pytz.timezone(\n            self.options.get(\"query_timezone\", self.default_query_timezone)\n        )\n        self.current_date = datetime.now(timezone.utc).astimezone(self.query_timezone)\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n        \"\"\"\n        Gets data from external ISO API.\n\n        Args:\n            url_suffix: String to be used as suffix to iso url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.iso_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - {url}\")\n\n        response = requests.get(url)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n\n        return response.content\n\n    def _get_localized_datetime(self, datetime_str: str) -&gt; datetime:\n        \"\"\"\n        Converts string datetime into Python datetime object with configured format and timezone.\n        Args:\n            datetime_str: String to be converted into datetime.\n\n        Returns: Timezone aware datetime object.\n\n        \"\"\"\n        parsed_dt = datetime.strptime(datetime_str, self.query_datetime_format)\n        parsed_dt = parsed_dt.replace(tzinfo=self.query_timezone)\n        return parsed_dt\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Hits the fetch_from_url method with certain parameters to get raw data from API.\n\n        All the children ISO classes must override this method and call the fetch_url method\n        in it.\n\n        Returns:\n             Raw DataFrame from API.\n        \"\"\"\n\n        return pd.read_csv(BytesIO(self._fetch_from_url(\"\")))\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Performs all the basic transformations to prepare data for further processing.\n        All the children ISO classes must override this method.\n\n        Args:\n            df: Raw DataFrame, received from the API.\n\n        Returns:\n             Modified DataFrame, ready for basic use.\n\n        \"\"\"\n        return df\n\n    def _sanitize_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Another data transformation helper method to be called after prepare data.\n        Used for advance data processing such as cleaning, filtering, restructuring.\n        All the children ISO classes must override this method if there is any post-processing required.\n\n        Args:\n            df: Initial modified version of DataFrame, received after preparing the data.\n\n        Returns:\n             Final version of data after all the fixes and modifications.\n\n        \"\"\"\n        return df\n\n    def _get_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Entrypoint method to return the final version of DataFrame.\n\n        Returns:\n            Modified form of data for specific use case.\n\n        \"\"\"\n        df = self._pull_data()\n        df = self._prepare_data(df)\n        df = self._sanitize_data(df)\n\n        # Reorder columns to keep the data consistent\n        df = df[self.spark_schema.names]\n\n        return df\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Performs all the options checks. Raises exception in case of any invalid value.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        return True\n\n    def pre_read_validation(self) -&gt; bool:\n        \"\"\"\n        Ensures all the required options are provided and performs other validations.\n        Returns:\n             True if all checks are passed.\n\n        \"\"\"\n        for key in self.required_options:\n            if key not in self.options:\n                raise ValueError(f\"Required option `{key}` is missing.\")\n\n        return self._validate_options()\n\n    def post_read_validation(self) -&gt; bool:\n        return True\n\n    def read_batch(self) -&gt; DataFrame:\n        \"\"\"\n        Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n        Returns:\n             Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n        \"\"\"\n\n        try:\n            self.pre_read_validation()\n            pdf = self._get_data()\n            pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n            # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n            pd.DataFrame.iteritems = pd.DataFrame.items\n            df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n            return df\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n\n    def read_stream(self) -&gt; DataFrame:\n        \"\"\"\n        By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n        Returns:\n             Final Spark DataFrame after all the processing.\n\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.pre_read_validation","title":"<code>pre_read_validation()</code>","text":"<p>Ensures all the required options are provided and performs other validations. Returns:      True if all checks are passed.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def pre_read_validation(self) -&gt; bool:\n    \"\"\"\n    Ensures all the required options are provided and performs other validations.\n    Returns:\n         True if all checks are passed.\n\n    \"\"\"\n    for key in self.required_options:\n        if key not in self.options:\n            raise ValueError(f\"Required option `{key}` is missing.\")\n\n    return self._validate_options()\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_batch","title":"<code>read_batch()</code>","text":"<p>Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data. Returns:      Final Spark DataFrame converted from Pandas DataFrame post-execution.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_batch(self) -&gt; DataFrame:\n    \"\"\"\n    Spark entrypoint, It executes the entire process of pulling, transforming &amp; fixing data.\n    Returns:\n         Final Spark DataFrame converted from Pandas DataFrame post-execution.\n\n    \"\"\"\n\n    try:\n        self.pre_read_validation()\n        pdf = self._get_data()\n        pdf = _prepare_pandas_to_convert_to_spark(pdf)\n\n        # The below is to fix the compatibility issues between Pandas 2.0 and PySpark.\n        pd.DataFrame.iteritems = pd.DataFrame.items\n        df = self.spark.createDataFrame(data=pdf, schema=self.spark_schema)\n        return df\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_historical_load_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso.BaseISOSource.read_stream","title":"<code>read_stream()</code>","text":"<p>By default, the streaming operation is not supported but child classes can override if ISO supports streaming.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Final Spark DataFrame after all the processing.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/base_iso.py</code> <pre><code>def read_stream(self) -&gt; DataFrame:\n    \"\"\"\n    By default, the streaming operation is not supported but child classes can override if ISO supports streaming.\n\n    Returns:\n         Final Spark DataFrame after all the processing.\n\n    \"\"\"\n\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} connector doesn't support stream operation.\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_historical_pricing_iso/","title":"PJM Historical Pricing Load","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_historical_pricing_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.pjm_historical_pricing_iso.PJMHistoricalPricingISOSource","title":"<code>PJMHistoricalPricingISOSource</code>","text":"<p>               Bases: <code>PJMDailyPricingISOSource</code></p> <p>The PJM Historical Pricing ISO Source is used to retrieve historical Real-Time and Day-Ahead hourly data from the PJM API.</p> <p>API:             https://api.pjm.com/api/v1/  (must be a valid apy key from PJM)</p> <p>Real-Time doc:    https://dataminer2.pjm.com/feed/rt_hrl_lmps/definition</p> <p>Day-Ahead doc:    https://dataminer2.pjm.com/feed/da_hrl_lmps/definition</p> <p>The PJM Historical Pricing ISO Source accesses the same PJM endpoints as the daily pricing source but is tailored for retrieving data within a specified historical range defined by the <code>start_date</code> and <code>end_date</code> attributes.</p>"},{"location":"sdk/code-reference/pipelines/sources/spark/iso/pjm_historical_pricing_iso/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.pjm_historical_pricing_iso.PJMHistoricalPricingISOSource--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import PJMHistoricalPricingISOSource\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\npjm_source = PJMHistoricalPricingISOSource(\n    spark=spark,\n    options={\n        \"api_key\": \"{api_key}\",\n        \"start_date\": \"2023-05-10\",\n        \"end_date\": \"2023-05-20\",\n    }\n)\n\npjm_source.read_batch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>The Spark Session instance.</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations.</p> required <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>A valid key from PJM required for authentication.</p> <code>load_type</code> <code>str</code> <p>The type of data to retrieve, either <code>real_time</code> or <code>day_ahead</code>.</p> <code>start_date</code> <code>str</code> <p>Must be in <code>YYYY-MM-DD</code> format.</p> <code>end_date</code> <code>str</code> <p>Must be in <code>YYYY-MM-DD</code> format.</p> <p>Please refer to the BaseISOSource for available methods and further details.</p> <p>BaseISOSource: ::: src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/iso/pjm_historical_pricing_iso.py</code> <pre><code>class PJMHistoricalPricingISOSource(PJMDailyPricingISOSource):\n    \"\"\"\n    The PJM Historical Pricing ISO Source is used to retrieve historical Real-Time and Day-Ahead hourly data from the PJM API.\n\n    API:             &lt;a href=\"https://api.pjm.com/api/v1/\"&gt;https://api.pjm.com/api/v1/&lt;/a&gt;  (must be a valid apy key from PJM)\n\n    Real-Time doc:    &lt;a href=\"https://dataminer2.pjm.com/feed/rt_hrl_lmps/definition\"&gt;https://dataminer2.pjm.com/feed/rt_hrl_lmps/definition&lt;/a&gt;\n\n    Day-Ahead doc:    &lt;a href=\"https://dataminer2.pjm.com/feed/da_hrl_lmps/definition\"&gt;https://dataminer2.pjm.com/feed/da_hrl_lmps/definition&lt;/a&gt;\n\n    The PJM Historical Pricing ISO Source accesses the same PJM endpoints as the daily pricing source but is tailored for retrieving data within a specified historical range defined by the `start_date` and `end_date` attributes.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.sources import PJMHistoricalPricingISOSource\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    pjm_source = PJMHistoricalPricingISOSource(\n        spark=spark,\n        options={\n            \"api_key\": \"{api_key}\",\n            \"start_date\": \"2023-05-10\",\n            \"end_date\": \"2023-05-20\",\n        }\n    )\n\n    pjm_source.read_batch()\n    ```\n\n    Parameters:\n        spark (SparkSession): The Spark Session instance.\n        options (dict): A dictionary of ISO Source specific configurations.\n\n    Attributes:\n        api_key (str): A valid key from PJM required for authentication.\n        load_type (str): The type of data to retrieve, either `real_time` or `day_ahead`.\n        start_date (str): Must be in `YYYY-MM-DD` format.\n        end_date (str): Must be in `YYYY-MM-DD` format.\n\n    Please refer to the BaseISOSource for available methods and further details.\n\n    BaseISOSource: ::: src.sdk.python.rtdip_sdk.pipelines.sources.spark.iso.base_iso\"\"\"\n\n    spark: SparkSession\n    options: dict\n    required_options = [\"api_key\", \"load_type\", \"start_date\", \"end_date\"]\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super().__init__(spark, options)\n        self.spark: SparkSession = spark\n        self.options: dict = options\n        self.start_date: str = self.options.get(\"start_date\", \"\")\n        self.end_date: str = self.options.get(\"end_date\", \"\")\n        self.user_datetime_format = \"%Y-%m-%d\"\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Pulls historical pricing data from the PJM API within the specified date range.\n\n        Returns:\n            pd.DataFrame: A DataFrame containing the raw historical pricing data retrieved from the PJM API.\n        \"\"\"\n\n        logging.info(\n            f\"Historical data requested from {self.start_date} to {self.end_date}\"\n        )\n\n        start_date_str = datetime.strptime(\n            self.start_date, self.user_datetime_format\n        ).replace(hour=0, minute=0)\n        end_date_str = datetime.strptime(\n            self.end_date, self.user_datetime_format\n        ).replace(hour=23)\n\n        if self.load_type == \"day_ahead\":\n            url_suffix = \"da_hrl_lmps\"\n        else:\n            url_suffix = \"rt_hrl_lmps\"\n\n        data = self._fetch_paginated_data(url_suffix, start_date_str, end_date_str)\n\n        df = pd.DataFrame(data)\n        logging.info(f\"Data fetched successfully: {len(df)} rows\")\n\n        return df\n\n    def _validate_options(self) -&gt; bool:\n        \"\"\"\n        Validates all parameters including the following examples:\n            - `start_date` &amp; `end_data` must be in the correct format.\n            - `start_date` must be behind `end_data`.\n            - `start_date` must not be in the future (UTC).\n\n        Returns:\n            True if all looks good otherwise raises Exception.\n\n        \"\"\"\n        super()._validate_options()\n        try:\n            start_date = datetime.strptime(self.start_date, self.user_datetime_format)\n        except ValueError:\n            raise ValueError(\n                f\"Unable to parse Start date. Please specify in {self.user_datetime_format} format.\"\n            )\n\n        try:\n            end_date = datetime.strptime(self.end_date, self.user_datetime_format)\n        except ValueError:\n            raise ValueError(\n                f\"Unable to parse End date. Please specify in {self.user_datetime_format} format.\"\n            )\n\n        if start_date &gt; datetime.now(timezone.utc).replace(tzinfo=None) - timedelta(\n            days=1\n        ):\n            raise ValueError(\"Start date can't be in future.\")\n\n        if start_date &gt; end_date:\n            raise ValueError(\"Start date can't be ahead of End date.\")\n\n        if end_date &gt; datetime.now(timezone.utc).replace(tzinfo=None) - timedelta(\n            days=1\n        ):\n            raise ValueError(\"End date can't be in future.\")\n\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/the_weather_company/base_weather/","title":"Base Weather","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/the_weather_company/base_weather/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.the_weather_company.base_weather.SparkWeatherCompanyBaseWeatherSource","title":"<code>SparkWeatherCompanyBaseWeatherSource</code>","text":"<p>               Bases: <code>BaseISOSource</code></p> <p>Base class for all the Weather related sources. Provides common functionality.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of Weather Source specific configurations.</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/the_weather_company/base_weather.py</code> <pre><code>class SparkWeatherCompanyBaseWeatherSource(BaseISOSource):\n    \"\"\"\n    Base class for all the Weather related sources. Provides common functionality.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of Weather Source specific configurations.\n\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    weather_url: str = \"https://\"\n    api_params: dict = {}\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super(SparkWeatherCompanyBaseWeatherSource, self).__init__(spark, options)\n        self.spark = spark\n        self.options = options\n\n    def _get_api_params(self) -&gt; dict:\n        return self.api_params\n\n    def _fetch_weather_from_url(self, url_suffix: str, params: dict) -&gt; bytes:\n        \"\"\"\n        Gets data from external Weather Forecast API.\n\n        Args:\n            url_suffix: String to be used as suffix to weather url.\n\n        Returns:\n            Raw content of the data received.\n\n        \"\"\"\n        url = f\"{self.weather_url}{url_suffix}\"\n        logging.info(f\"Requesting URL - `{url}` with params - {params}\")\n\n        response = requests.get(url, params)\n        code = response.status_code\n\n        if code != 200:\n            raise HTTPError(\n                f\"Unable to access URL `{url}`.\"\n                f\" Received status code {code} with message {response.content}\"\n            )\n        return response.content\n\n    def _fetch_from_url(self, url_suffix: str) -&gt; bytes:\n        return self._fetch_weather_from_url(url_suffix, self._get_api_params())\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/the_weather_company/weather_forecast_api_v1/","title":"Weather Forecast API V1","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/the_weather_company/weather_forecast_api_v1/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.the_weather_company.weather_forecast_api_v1.SparkWeatherCompanyForecastAPIV1Source","title":"<code>SparkWeatherCompanyForecastAPIV1Source</code>","text":"<p>               Bases: <code>SparkWeatherCompanyBaseWeatherSource</code></p> <p>The Weather Forecast API V1 Source is used to read 15 days forecast from the Weather API.</p> <p>URL:  https://api.weather.com/v1/geocode/32.3667/-95.4/forecast/hourly/360hour.json</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below).</p> required <p>Attributes:</p> Name Type Description <code>lat</code> <code>str</code> <p>Latitude of the Weather Station.</p> <code>lon</code> <code>str</code> <p>Longitude of the Weather Station.</p> <code>api_key</code> <code>str</code> <p>Weather API key.</p> <code>language</code> <code>str</code> <p>API response language. Defaults to <code>en-US</code>.</p> <code>units</code> <code>str</code> <p>Unit of measurements. Defaults to <code>e</code>.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/the_weather_company/weather_forecast_api_v1.py</code> <pre><code>class SparkWeatherCompanyForecastAPIV1Source(SparkWeatherCompanyBaseWeatherSource):\n    \"\"\"\n    The Weather Forecast API V1 Source is used to read 15 days forecast from the Weather API.\n\n    URL: &lt;a href=\"https://api.weather.com/v1/geocode/32.3667/-95.4/forecast/hourly/360hour.json\"&gt;\n    https://api.weather.com/v1/geocode/32.3667/-95.4/forecast/hourly/360hour.json&lt;/a&gt;\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations (See Attributes table below).\n\n    Attributes:\n        lat (str): Latitude of the Weather Station.\n        lon (str): Longitude of the Weather Station.\n        api_key (str): Weather API key.\n        language (str): API response language. Defaults to `en-US`.\n        units (str): Unit of measurements. Defaults to `e`.\n    \"\"\"\n\n    spark: SparkSession\n    spark_schema = WEATHER_FORECAST_SCHEMA\n    options: dict\n    weather_url: str = \"https://api.weather.com/v1/geocode/\"\n    required_options = [\"lat\", \"lon\", \"api_key\"]\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super(SparkWeatherCompanyForecastAPIV1Source, self).__init__(spark, options)\n        self.spark = spark\n        self.options = options\n        self.lat = self.options.get(\"lat\", \"\").strip()\n        self.lon = self.options.get(\"lon\", \"\").strip()\n        self.api_key = self.options.get(\"api_key\", \"\").strip()\n        self.language = self.options.get(\"language\", \"en-US\").strip()\n        self.units = self.options.get(\"units\", \"e\").strip()\n\n    def _prepare_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Prepares weather data for the use.\n\n        Args:\n            df: Data received after preparation.\n\n        Returns:\n            Final data after all the transformations.\n\n        \"\"\"\n\n        rename_cols = {\n            \"latitude\": \"Latitude\",\n            \"longitude\": \"Longitude\",\n            \"class\": \"Class\",\n            \"expire_time_gmt\": \"ExpireTimeGmt\",\n            \"fcst_valid\": \"FcstValid\",\n            \"fcst_valid_local\": \"FcstValidLocal\",\n            \"num\": \"Num\",\n            \"day_ind\": \"DayInd\",\n            \"temp\": \"Temp\",\n            \"dewpt\": \"Dewpt\",\n            \"hi\": \"Hi\",\n            \"wc\": \"Wc\",\n            \"feels_like\": \"FeelsLike\",\n            \"icon_extd\": \"IconExtd\",\n            \"wxman\": \"Wxman\",\n            \"icon_code\": \"IconCode\",\n            \"dow\": \"Dow\",\n            \"phrase_12char\": \"Phrase12Char\",\n            \"phrase_22char\": \"Phrase22Char\",\n            \"phrase_32char\": \"Phrase32Char\",\n            \"subphrase_pt1\": \"SubphrasePt1\",\n            \"subphrase_pt2\": \"SubphrasePt2\",\n            \"subphrase_pt3\": \"SubphrasePt3\",\n            \"pop\": \"Pop\",\n            \"precip_type\": \"PrecipType\",\n            \"qpf\": \"Qpf\",\n            \"snow_qpf\": \"SnowQpf\",\n            \"rh\": \"Rh\",\n            \"wspd\": \"Wspd\",\n            \"wdir\": \"Wdir\",\n            \"wdir_cardinal\": \"WdirCardinal\",\n            \"gust\": \"Gust\",\n            \"clds\": \"Clds\",\n            \"vis\": \"Vis\",\n            \"mslp\": \"Mslp\",\n            \"uv_index_raw\": \"UvIndexRaw\",\n            \"uv_index\": \"UvIndex\",\n            \"uv_warning\": \"UvWarning\",\n            \"uv_desc\": \"UvDesc\",\n            \"golf_index\": \"GolfIndex\",\n            \"golf_category\": \"GolfCategory\",\n            \"severity\": \"Severity\",\n        }\n\n        df = df.rename(columns=rename_cols)\n\n        fields = self.spark_schema.fields\n\n        str_cols = list(\n            map(\n                lambda x: x.name,\n                filter(lambda x: isinstance(x.dataType, StringType), fields),\n            )\n        )\n        double_cols = list(\n            map(\n                lambda x: x.name,\n                filter(lambda x: isinstance(x.dataType, DoubleType), fields),\n            )\n        )\n        int_cols = list(\n            map(\n                lambda x: x.name,\n                filter(lambda x: isinstance(x.dataType, IntegerType), fields),\n            )\n        )\n\n        df[str_cols] = df[str_cols].astype(str)\n        df[double_cols] = df[double_cols].astype(float)\n        df[int_cols] = df[int_cols].astype(int)\n\n        df.reset_index(inplace=True, drop=True)\n\n        return df\n\n    def _get_api_params(self):\n        params = {\n            \"language\": self.language,\n            \"units\": self.units,\n            \"apiKey\": self.api_key,\n        }\n        return params\n\n    def _pull_for_weather_station(self, lat: str, lon: str) -&gt; pd.DataFrame:\n        response = json.loads(\n            self._fetch_from_url(f\"{lat}/{lon}/forecast/hourly/360hour.json\").decode(\n                \"utf-8\"\n            )\n        )\n        return pd.DataFrame(response[\"forecasts\"])\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Pulls data from the Weather API and parses the JSON file.\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n\n        df = self._pull_for_weather_station(self.lat, self.lon)\n        df[\"latitude\"] = self.lat\n        df[\"longitude\"] = self.lon\n\n        return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/sources/spark/the_weather_company/weather_forecast_api_v1_multi/","title":"Weather Forecast V1 Multi","text":""},{"location":"sdk/code-reference/pipelines/sources/spark/the_weather_company/weather_forecast_api_v1_multi/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.the_weather_company.weather_forecast_api_v1_multi.SparkWeatherCompanyForecastAPIV1MultiSource","title":"<code>SparkWeatherCompanyForecastAPIV1MultiSource</code>","text":"<p>               Bases: <code>SparkWeatherCompanyForecastAPIV1Source</code></p> <p>The Weather Forecast API V1 Multi Source is used to read 15 days forecast from the Weather API. It allows to pull weather data for multiple stations and returns all of them in a single DataFrame.</p> <p>URL for one station:  https://api.weather.com/v1/geocode/32.3667/-95.4/forecast/hourly/360hour.json</p> <p>It takes a list of Weather Stations. Each station item must contain comma separated Latitude &amp; Longitude.</p>"},{"location":"sdk/code-reference/pipelines/sources/spark/the_weather_company/weather_forecast_api_v1_multi/#src.sdk.python.rtdip_sdk.pipelines.sources.spark.the_weather_company.weather_forecast_api_v1_multi.SparkWeatherCompanyForecastAPIV1MultiSource--examples","title":"Examples","text":"<p><code>[\"32.3667,-95.4\", \"51.52,-0.11\"]</code></p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance</p> required <code>options</code> <code>dict</code> <p>A dictionary of ISO Source specific configurations (See Attributes table below).</p> required <p>Attributes:</p> Name Type Description <code>stations</code> <code>list[str]</code> <p>List of Weather Stations.</p> <code>api_key</code> <code>str</code> <p>Weather API key.</p> <code>language</code> <code>str</code> <p>API response language. Defaults to <code>en-US</code>.</p> <code>units</code> <code>str</code> <p>Unit of measurements. Defaults to <code>e</code>.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/sources/spark/the_weather_company/weather_forecast_api_v1_multi.py</code> <pre><code>class SparkWeatherCompanyForecastAPIV1MultiSource(\n    SparkWeatherCompanyForecastAPIV1Source\n):\n    \"\"\"\n    The Weather Forecast API V1 Multi Source is used to read 15 days forecast from the Weather API. It allows to\n    pull weather data for multiple stations and returns all of them in a single DataFrame.\n\n    URL for one station: &lt;a href=\"https://api.weather.com/v1/geocode/32.3667/-95.4/forecast/hourly/360hour.json\"&gt;\n    https://api.weather.com/v1/geocode/32.3667/-95.4/forecast/hourly/360hour.json&lt;/a&gt;\n\n    It takes a list of Weather Stations. Each station item must contain comma separated Latitude &amp; Longitude.\n\n    Examples\n    --------\n    `[\"32.3667,-95.4\", \"51.52,-0.11\"]`\n\n    Parameters:\n        spark (SparkSession): Spark Session instance\n        options (dict): A dictionary of ISO Source specific configurations (See Attributes table below).\n\n    Attributes:\n        stations (list[str]): List of Weather Stations.\n        api_key (str): Weather API key.\n        language (str): API response language. Defaults to `en-US`.\n        units (str): Unit of measurements. Defaults to `e`.\n    \"\"\"\n\n    spark: SparkSession\n    options: dict\n    spark_schema = WEATHER_FORECAST_SCHEMA\n    required_options = [\"stations\", \"api_key\"]\n\n    def __init__(self, spark: SparkSession, options: dict) -&gt; None:\n        super(SparkWeatherCompanyForecastAPIV1MultiSource, self).__init__(\n            spark, options\n        )\n        self.spark = spark\n        self.options = options\n        self.stations = self.options.get(\"stations\", [])\n        self.api_key = self.options.get(\"api_key\", \"\").strip()\n        self.language = self.options.get(\"language\", \"en-US\").strip()\n        self.units = self.options.get(\"units\", \"e\").strip()\n\n    def _pull_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Pulls data from the Weather API and parses the JSON file for multiple stations\n\n        Returns:\n            Raw form of data.\n        \"\"\"\n\n        result_df = None\n        for station in self.stations:\n            parts = station.split(\",\")\n            lat, lon = parts\n\n            df = self._pull_for_weather_station(lat, lon)\n            df[\"latitude\"] = lat\n            df[\"longitude\"] = lon\n\n            if result_df is not None:\n                result_df = pd.concat([result_df, df])\n            else:\n                result_df = df\n\n        return result_df\n\n    def _validate_options(self) -&gt; bool:\n        for station in self.stations:\n            parts = station.split(\",\")\n\n            if len(parts) != 2 or parts[0].strip() == \"\" or parts[1].strip() == \"\":\n                raise ValueError(\n                    f\"Each station item must contain comma separated Latitude &amp; Longitude. Eg: 10.23:45.2\"\n                )\n\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/base_raw_to_mdm/","title":"Base raw to mdm","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/base_raw_to_mdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.base_raw_to_mdm.BaseRawToMDMTransformer","title":"<code>BaseRawToMDMTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Base class for all the Raw to Meters Data Model Transformers.</p> Meters Data Model requires two outputs <ul> <li><code>UsageData</code> : To store measurement(value) as timeseries data.</li> <li><code>MetaData</code> : To store meters related meta information.</li> </ul> <p>It supports the generation of both the outputs as they share some common properties.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance.</p> required <code>data</code> <code>DataFrame</code> <p>Dataframe containing the raw MISO data.</p> required <code>output_type</code> <code>str</code> <p>Must be one of <code>usage</code> or <code>meta</code>.</p> required <code>name</code> <code>str</code> <p>Set this to override default <code>name</code> column.</p> <code>None</code> <code>description</code> <code>str</code> <p>Set this to override default <code>description</code> column.</p> <code>None</code> <code>value_type</code> <code>ValueType</code> <p>Set this to override default <code>value_type</code> column.</p> <code>None</code> <code>version</code> <code>str</code> <p>Set this to override default <code>version</code> column.</p> <code>None</code> <code>series_id</code> <code>str</code> <p>Set this to override default <code>series_id</code> column.</p> <code>None</code> <code>series_parent_id</code> <code>str</code> <p>Set this to override default <code>series_parent_id</code> column.</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/base_raw_to_mdm.py</code> <pre><code>class BaseRawToMDMTransformer(TransformerInterface):\n    \"\"\"\n    Base class for all the Raw to Meters Data Model Transformers.\n\n    Meters Data Model requires two outputs:\n        - `UsageData` : To store measurement(value) as timeseries data.\n        - `MetaData` : To store meters related meta information.\n\n    It supports the generation of both the outputs as they share some common properties.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance.\n        data (DataFrame): Dataframe containing the raw MISO data.\n        output_type (str): Must be one of `usage` or `meta`.\n        name (str): Set this to override default `name` column.\n        description (str): Set this to override default `description` column.\n        value_type (ValueType): Set this to override default `value_type` column.\n        version (str): Set this to override default `version` column.\n        series_id (str): Set this to override default `series_id` column.\n        series_parent_id (str): Set this to override default `series_parent_id` column.\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    output_type: str\n    input_schema: StructType\n    target_schema: StructType\n    uid_col: str\n    series_id_col: str\n    timestamp_col: str\n    interval_timestamp_col: str\n    value_col: str\n    series_parent_id_col: str\n    name_col: str\n    uom_col: str\n    description_col: str\n    timestamp_start_col: str\n    timestamp_end_col: str\n    time_zone_col: str\n    version_col: str\n    series_type: SeriesType\n    model_type: ModelType\n    value_type: ValueType\n    properties_col: str\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n        output_type: str,\n        name: str = None,\n        description: str = None,\n        value_type: ValueType = None,\n        version: str = None,\n        series_id: str = None,\n        series_parent_id: str = None,\n    ):\n        self.spark = spark\n        self.data = data\n        self.output_type = output_type\n        self.name = name if name is not None else self.name_col\n        self.description = (\n            description if description is not None else self.description_col\n        )\n        self.value_type = value_type if value_type is not None else self.value_type\n        self.version = version if version is not None else self.version_col\n        self.series_id = series_id if series_id is not None else self.series_id_col\n        self.series_parent_id = (\n            series_parent_id\n            if series_parent_id is not None\n            else self.series_parent_id_col\n        )\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self) -&gt; bool:\n        valid_output_types = [\"usage\", \"meta\"]\n        if self.output_type not in valid_output_types:\n            raise ValueError(\n                f\"Invalid output_type `{self.output_type}` given. Must be one of {valid_output_types}\"\n            )\n\n        assert str(self.data.schema) == str(self.input_schema)\n        assert type(self.series_type).__name__ == SeriesType.__name__\n        assert type(self.model_type).__name__ == ModelType.__name__\n        assert type(self.value_type).__name__ == ValueType.__name__\n        return True\n\n    def post_transform_validation(self) -&gt; bool:\n        assert str(self.data.schema) == str(self.target_schema)\n        return True\n\n    def _get_transformed_df(self) -&gt; DataFrame:\n        if self.output_type == \"usage\":\n            self.target_schema = MDM_USAGE_SCHEMA\n            return self._get_usage_transformed_df()\n        else:\n            self.target_schema = MDM_META_SCHEMA\n            return self._get_meta_transformed_df()\n\n    def _convert_into_target_schema(self) -&gt; None:\n        \"\"\"\n        Converts a Spark DataFrame structure into new structure based on the Target Schema.\n\n        Returns: Nothing.\n\n        \"\"\"\n\n        df: DataFrame = self.data\n        df = df.select(self.target_schema.names)\n\n        for field in self.target_schema.fields:\n            df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n\n        self.data = self.spark.createDataFrame(df.rdd, self.target_schema)\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the raw data converted into MDM.\n        \"\"\"\n\n        self.pre_transform_validation()\n        self.data = self._get_transformed_df()\n        self._convert_into_target_schema()\n        self.post_transform_validation()\n\n        return self.data\n\n    def _add_uid_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Uid\", expr(self.uid_col))\n\n    def _add_series_id_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"SeriesId\", expr(self.series_id))\n\n    def _add_timestamp_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Timestamp\", expr(self.timestamp_col))\n\n    def _add_interval_timestamp_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"IntervalTimestamp\", expr(self.interval_timestamp_col))\n\n    def _add_value_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Value\", expr(self.value_col))\n\n    def _add_series_parent_id_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"SeriesParentId\", expr(self.series_parent_id))\n\n    def _add_name_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Name\", expr(self.name))\n\n    def _add_uom_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Uom\", expr(self.uom_col))\n\n    def _add_description_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Description\", expr(self.description))\n\n    def _add_timestamp_start_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"TimestampStart\", expr(self.timestamp_start_col))\n\n    def _add_timestamp_end_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"TimestampEnd\", expr(self.timestamp_end_col))\n\n    def _add_time_zone_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Timezone\", expr(self.time_zone_col))\n\n    def _add_version_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Version\", expr(self.version))\n\n    def _add_series_type_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"SeriesType\", lit(self.series_type.value))\n\n    def _add_model_type_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"ModelType\", lit(self.model_type.value))\n\n    def _add_value_type_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"ValueType\", lit(self.value_type.value))\n\n    def _add_properties_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Properties\", expr(self.properties_col))\n\n    def _pre_process(self) -&gt; DataFrame:\n        return self.data\n\n    @staticmethod\n    def _post_process(df: DataFrame) -&gt; DataFrame:\n        return df\n\n    def _get_usage_transformed_df(self) -&gt; DataFrame:\n        df = self._pre_process()\n\n        df = self._add_uid_column(df)\n        df = self._add_series_id_column(df)\n        df = self._add_timestamp_column(df)\n        df = self._add_interval_timestamp_column(df)\n        df = self._add_value_column(df)\n\n        df = self._post_process(df)\n\n        return df\n\n    def _get_meta_transformed_df(self) -&gt; DataFrame:\n        df = self._pre_process()\n\n        df = self._add_uid_column(df)\n        df = self._add_series_id_column(df)\n        df = self._add_series_parent_id_column(df)\n        df = self._add_name_column(df)\n        df = self._add_uom_column(df)\n        df = self._add_description_column(df)\n        df = self._add_timestamp_start_column(df)\n        df = self._add_timestamp_end_column(df)\n        df = self._add_time_zone_column(df)\n        df = self._add_version_column(df)\n        df = self._add_series_type_column(df)\n        df = self._add_model_type_column(df)\n        df = self._add_value_type_column(df)\n        df = self._add_properties_column(df)\n\n        df = self._post_process(df)\n\n        return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/base_raw_to_mdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.base_raw_to_mdm.BaseRawToMDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the raw data converted into MDM.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/base_raw_to_mdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the raw data converted into MDM.\n    \"\"\"\n\n    self.pre_transform_validation()\n    self.data = self._get_transformed_df()\n    self._convert_into_target_schema()\n    self.post_transform_validation()\n\n    return self.data\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/binary_to_string/","title":"Convert Eventhub Body Column from Binary to String","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/binary_to_string/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.binary_to_string.BinaryToStringTransformer","title":"<code>BinaryToStringTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a dataframe body column from a binary to a string.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/binary_to_string/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.binary_to_string.BinaryToStringTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import BinaryToStringTransformer\n\nbinary_to_string_transformer = BinaryToStringTransformer(\n    data=df,\n    souce_column_name=\"body\",\n    target_column_name=\"body\"\n)\n\nresult = binary_to_string_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to be transformed</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the Binary data</p> required <code>target_column_name</code> <code>str</code> <p>Spark Dataframe column name to be used for the String data</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/binary_to_string.py</code> <pre><code>class BinaryToStringTransformer(TransformerInterface):\n    \"\"\"\n    Converts a dataframe body column from a binary to a string.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import BinaryToStringTransformer\n\n    binary_to_string_transformer = BinaryToStringTransformer(\n        data=df,\n        souce_column_name=\"body\",\n        target_column_name=\"body\"\n    )\n\n    result = binary_to_string_transformer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe to be transformed\n        source_column_name (str): Spark Dataframe column containing the Binary data\n        target_column_name (str): Spark Dataframe column name to be used for the String data\n    \"\"\"\n\n    data: DataFrame\n    source_column_name: str\n    target_column_name: str\n\n    def __init__(\n        self, data: DataFrame, source_column_name: str, target_column_name: str\n    ) -&gt; None:\n        self.data = data\n        self.source_column_name = source_column_name\n        self.target_column_name = target_column_name\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the body column converted to string.\n        \"\"\"\n        return self.data.withColumn(\n            self.target_column_name, self.data[self.source_column_name].cast(\"string\")\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/binary_to_string/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.binary_to_string.BinaryToStringTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/binary_to_string.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/binary_to_string/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.binary_to_string.BinaryToStringTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the body column converted to string.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/binary_to_string.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the body column converted to string.\n    \"\"\"\n    return self.data.withColumn(\n        self.target_column_name, self.data[self.source_column_name].cast(\"string\")\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/edgex_opcua_json_to_pcdm/","title":"Convert Edge Xpert Json to Process Control Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/edgex_opcua_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.edgex_opcua_json_to_pcdm.EdgeXOPCUAJsonToPCDMTransformer","title":"<code>EdgeXOPCUAJsonToPCDMTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a Spark Dataframe column containing a json string created by EdgeX to the Process Control Data Model.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/edgex_opcua_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.edgex_opcua_json_to_pcdm.EdgeXOPCUAJsonToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import EdgeXOPCUAJsonToPCDMTransformer\n\nedge_opcua_json_to_pcdm_transformer = EdgeXOPCUAJsonToPCDMTransformer(\n    data=df,\n    souce_column_name=\"body\",\n    status_null_value=\"Good\",\n    change_type_value=\"insert\"\n)\n\nresult = edge_opcua_json_to_pcdm_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe containing the column with EdgeX data</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the OPC Publisher Json OPC UA data</p> required <code>status_null_value</code> <code>optional str</code> <p>If populated, will replace 'Good' in the Status column with the specified value.</p> <code>'Good'</code> <code>change_type_value</code> <code>optional str</code> <p>If populated, will replace 'insert' in the ChangeType column with the specified value.</p> <code>'insert'</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/edgex_opcua_json_to_pcdm.py</code> <pre><code>class EdgeXOPCUAJsonToPCDMTransformer(TransformerInterface):\n    \"\"\"\n    Converts a Spark Dataframe column containing a json string created by EdgeX to the Process Control Data Model.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import EdgeXOPCUAJsonToPCDMTransformer\n\n    edge_opcua_json_to_pcdm_transformer = EdgeXOPCUAJsonToPCDMTransformer(\n        data=df,\n        souce_column_name=\"body\",\n        status_null_value=\"Good\",\n        change_type_value=\"insert\"\n    )\n\n    result = edge_opcua_json_to_pcdm_transformer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe containing the column with EdgeX data\n        source_column_name (str): Spark Dataframe column containing the OPC Publisher Json OPC UA data\n        status_null_value (optional str): If populated, will replace 'Good' in the Status column with the specified value.\n        change_type_value (optional str): If populated, will replace 'insert' in the ChangeType column with the specified value.\n    \"\"\"\n\n    data: DataFrame\n    source_column_name: str\n    status_null_value: str\n    change_type_value: str\n    tagname_field: str\n\n    def __init__(\n        self,\n        data: DataFrame,\n        source_column_name: str,\n        status_null_value: str = \"Good\",\n        change_type_value: str = \"insert\",\n        tagname_field=\"resourceName\",\n    ) -&gt; None:\n        self.data = data\n        self.source_column_name = source_column_name\n        self.status_null_value = status_null_value\n        self.change_type_value = change_type_value\n        self.tagname_field = tagname_field\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the specified column converted to PCDM\n        \"\"\"\n        df = (\n            self.data.withColumn(\n                self.source_column_name,\n                from_json(self.source_column_name, EDGEX_SCHEMA),\n            )\n            .select(\"*\", explode(\"{}.readings\".format(self.source_column_name)))\n            .selectExpr(\n                \"explode({}.readings.{}) as TagName\".format(\n                    self.source_column_name, self.tagname_field\n                ),\n                \"to_utc_timestamp(to_timestamp((col.origin / 1000000000)), current_timezone()) as EventTime\",\n                \"col.value as Value\",\n                \"col.valueType as ValueType\",\n            )\n            .withColumn(\"Status\", lit(self.status_null_value))\n            .withColumn(\"ChangeType\", lit(self.change_type_value))\n            .withColumn(\n                \"ValueType\",\n                (\n                    when(col(\"ValueType\") == \"Int8\", \"integer\")\n                    .when(col(\"ValueType\") == \"Int16\", \"integer\")\n                    .when(col(\"ValueType\") == \"Int32\", \"integer\")\n                    .when(col(\"ValueType\") == \"Int64\", \"integer\")\n                    .when(col(\"ValueType\") == \"Uint8\", \"integer\")\n                    .when(col(\"ValueType\") == \"Uint16\", \"integer\")\n                    .when(col(\"ValueType\") == \"Uint32\", \"integer\")\n                    .when(col(\"ValueType\") == \"Uint64\", \"integer\")\n                    .when(col(\"ValueType\") == \"Float32\", \"float\")\n                    .when(col(\"ValueType\") == \"Float64\", \"float\")\n                    .when(col(\"ValueType\") == \"Bool\", \"bool\")\n                    .otherwise(\"string\")\n                ),\n            )\n        )\n\n        return df.select(\n            \"TagName\", \"EventTime\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/edgex_opcua_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.edgex_opcua_json_to_pcdm.EdgeXOPCUAJsonToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/edgex_opcua_json_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/edgex_opcua_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.edgex_opcua_json_to_pcdm.EdgeXOPCUAJsonToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the specified column converted to PCDM</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/edgex_opcua_json_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the specified column converted to PCDM\n    \"\"\"\n    df = (\n        self.data.withColumn(\n            self.source_column_name,\n            from_json(self.source_column_name, EDGEX_SCHEMA),\n        )\n        .select(\"*\", explode(\"{}.readings\".format(self.source_column_name)))\n        .selectExpr(\n            \"explode({}.readings.{}) as TagName\".format(\n                self.source_column_name, self.tagname_field\n            ),\n            \"to_utc_timestamp(to_timestamp((col.origin / 1000000000)), current_timezone()) as EventTime\",\n            \"col.value as Value\",\n            \"col.valueType as ValueType\",\n        )\n        .withColumn(\"Status\", lit(self.status_null_value))\n        .withColumn(\"ChangeType\", lit(self.change_type_value))\n        .withColumn(\n            \"ValueType\",\n            (\n                when(col(\"ValueType\") == \"Int8\", \"integer\")\n                .when(col(\"ValueType\") == \"Int16\", \"integer\")\n                .when(col(\"ValueType\") == \"Int32\", \"integer\")\n                .when(col(\"ValueType\") == \"Int64\", \"integer\")\n                .when(col(\"ValueType\") == \"Uint8\", \"integer\")\n                .when(col(\"ValueType\") == \"Uint16\", \"integer\")\n                .when(col(\"ValueType\") == \"Uint32\", \"integer\")\n                .when(col(\"ValueType\") == \"Uint64\", \"integer\")\n                .when(col(\"ValueType\") == \"Float32\", \"float\")\n                .when(col(\"ValueType\") == \"Float64\", \"float\")\n                .when(col(\"ValueType\") == \"Bool\", \"bool\")\n                .otherwise(\"string\")\n            ),\n        )\n    )\n\n    return df.select(\n        \"TagName\", \"EventTime\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/fledge_opcua_json_to_pcdm/","title":"Convert Fledge Json to Process Control Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/fledge_opcua_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.fledge_opcua_json_to_pcdm.FledgeOPCUAJsonToPCDMTransformer","title":"<code>FledgeOPCUAJsonToPCDMTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a Spark Dataframe column containing a json string created by Fledge to the Process Control Data Model.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/fledge_opcua_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.fledge_opcua_json_to_pcdm.FledgeOPCUAJsonToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import FledgeOPCUAJsonToPCDMTransformer\n\nfledge_opcua_json_to_pcdm_transfromer = FledgeOPCUAJsonToPCDMTransformer(\n    data=df,\n    souce_column_name=\"body\",\n    status_null_value=\"Good\",\n    change_type_value=\"insert\",\n    timestamp_formats=[\n        \"yyyy-MM-dd'T'HH:mm:ss.SSSX\",\n        \"yyyy-MM-dd'T'HH:mm:ssX\",\n    ]\n)\n\nresult = fledge_opcua_json_to_pcdm_transfromer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe containing the column with Json Fledge data</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the OPC Publisher Json OPC UA data</p> required <code>status_null_value</code> <code>str</code> <p>If populated, will replace 'Good' in the Status column with the specified value.</p> <code>'Good'</code> <code>change_type_value</code> <code>optional str</code> <p>If populated, will replace 'insert' in the ChangeType column with the specified value.</p> <code>'insert'</code> <code>timestamp_formats</code> <code>list[str]</code> <p>Specifies the timestamp formats to be used for converting the timestamp string to a Timestamp Type. For more information on formats, refer to this documentation.</p> <code>[\"yyyy-MM-dd'T'HH:mm:ss.SSSX\", \"yyyy-MM-dd'T'HH:mm:ssX\"]</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/fledge_opcua_json_to_pcdm.py</code> <pre><code>class FledgeOPCUAJsonToPCDMTransformer(TransformerInterface):\n    \"\"\"\n    Converts a Spark Dataframe column containing a json string created by Fledge to the Process Control Data Model.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import FledgeOPCUAJsonToPCDMTransformer\n\n    fledge_opcua_json_to_pcdm_transfromer = FledgeOPCUAJsonToPCDMTransformer(\n        data=df,\n        souce_column_name=\"body\",\n        status_null_value=\"Good\",\n        change_type_value=\"insert\",\n        timestamp_formats=[\n            \"yyyy-MM-dd'T'HH:mm:ss.SSSX\",\n            \"yyyy-MM-dd'T'HH:mm:ssX\",\n        ]\n    )\n\n    result = fledge_opcua_json_to_pcdm_transfromer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe containing the column with Json Fledge data\n        source_column_name (str): Spark Dataframe column containing the OPC Publisher Json OPC UA data\n        status_null_value (str): If populated, will replace 'Good' in the Status column with the specified value.\n        change_type_value (optional str): If populated, will replace 'insert' in the ChangeType column with the specified value.\n        timestamp_formats (list[str]): Specifies the timestamp formats to be used for converting the timestamp string to a Timestamp Type. For more information on formats, refer to this [documentation.](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html)\n    \"\"\"\n\n    data: DataFrame\n    source_column_name: str\n    status_null_value: str\n    change_type_value: str\n    timestamp_formats: list\n\n    def __init__(\n        self,\n        data: DataFrame,\n        source_column_name: str,\n        status_null_value: str = \"Good\",\n        change_type_value: str = \"insert\",\n        timestamp_formats: list = [\n            \"yyyy-MM-dd'T'HH:mm:ss.SSSX\",\n            \"yyyy-MM-dd'T'HH:mm:ssX\",\n        ],\n    ) -&gt; None:  # NOSONAR\n        self.data = data\n        self.source_column_name = source_column_name\n        self.status_null_value = status_null_value\n        self.change_type_value = change_type_value\n        self.timestamp_formats = timestamp_formats\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the specified column converted to PCDM\n        \"\"\"\n        df = (\n            self.data.withColumn(\n                self.source_column_name,\n                from_json(self.source_column_name, FLEDGE_SCHEMA),\n            )\n            .selectExpr(\"inline({})\".format(self.source_column_name))\n            .select(explode(\"readings\"), \"timestamp\")\n            .withColumn(\n                \"EventTime\",\n                coalesce(\n                    *[to_timestamp(col(\"timestamp\"), f) for f in self.timestamp_formats]\n                ),\n            )\n            .withColumnRenamed(\"key\", \"TagName\")\n            .withColumnRenamed(\"value\", \"Value\")\n            .withColumn(\"Status\", lit(self.status_null_value))\n            .withColumn(\n                \"ValueType\",\n                when(col(\"value\").cast(\"float\").isNotNull(), \"float\").when(\n                    col(\"value\").cast(\"float\").isNull(), \"string\"\n                ),\n            )\n            .withColumn(\"ChangeType\", lit(self.change_type_value))\n        )\n\n        return df.select(\n            \"TagName\", \"EventTime\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/fledge_opcua_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.fledge_opcua_json_to_pcdm.FledgeOPCUAJsonToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/fledge_opcua_json_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/fledge_opcua_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.fledge_opcua_json_to_pcdm.FledgeOPCUAJsonToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the specified column converted to PCDM</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/fledge_opcua_json_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the specified column converted to PCDM\n    \"\"\"\n    df = (\n        self.data.withColumn(\n            self.source_column_name,\n            from_json(self.source_column_name, FLEDGE_SCHEMA),\n        )\n        .selectExpr(\"inline({})\".format(self.source_column_name))\n        .select(explode(\"readings\"), \"timestamp\")\n        .withColumn(\n            \"EventTime\",\n            coalesce(\n                *[to_timestamp(col(\"timestamp\"), f) for f in self.timestamp_formats]\n            ),\n        )\n        .withColumnRenamed(\"key\", \"TagName\")\n        .withColumnRenamed(\"value\", \"Value\")\n        .withColumn(\"Status\", lit(self.status_null_value))\n        .withColumn(\n            \"ValueType\",\n            when(col(\"value\").cast(\"float\").isNotNull(), \"float\").when(\n                col(\"value\").cast(\"float\").isNull(), \"string\"\n            ),\n        )\n        .withColumn(\"ChangeType\", lit(self.change_type_value))\n    )\n\n    return df.select(\n        \"TagName\", \"EventTime\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/honeywell_apm_to_pcdm/","title":"Convert Honeywell APM Json to Process Control Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/honeywell_apm_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.honeywell_apm_to_pcdm.HoneywellAPMJsonToPCDMTransformer","title":"<code>HoneywellAPMJsonToPCDMTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a Spark Dataframe column containing a json string created by Honeywell APM to the Process Control Data Model.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/honeywell_apm_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.honeywell_apm_to_pcdm.HoneywellAPMJsonToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import HoneywellAPMJsonToPCDMTransformer\n\nhoneywell_apm_json_to_pcdm_transformer = HoneywellAPMJsonToPCDMTransformer(\n    data=df,\n    souce_column_name=\"body\",\n    status_null_value=\"Good\",\n    change_type_value=\"insert\"\n)\n\nresult = honeywell_apm_json_to_pcdm_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe containing the column with EdgeX data</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the OPC Publisher Json OPC UA data</p> required <code>status_null_value</code> <code>optional str</code> <p>If populated, will replace 'Good' in the Status column with the specified value.</p> <code>'Good'</code> <code>change_type_value</code> <code>optional str</code> <p>If populated, will replace 'insert' in the ChangeType column with the specified value.</p> <code>'insert'</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/honeywell_apm_to_pcdm.py</code> <pre><code>class HoneywellAPMJsonToPCDMTransformer(TransformerInterface):\n    \"\"\"\n    Converts a Spark Dataframe column containing a json string created by Honeywell APM to the Process Control Data Model.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import HoneywellAPMJsonToPCDMTransformer\n\n    honeywell_apm_json_to_pcdm_transformer = HoneywellAPMJsonToPCDMTransformer(\n        data=df,\n        souce_column_name=\"body\",\n        status_null_value=\"Good\",\n        change_type_value=\"insert\"\n    )\n\n    result = honeywell_apm_json_to_pcdm_transformer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe containing the column with EdgeX data\n        source_column_name (str): Spark Dataframe column containing the OPC Publisher Json OPC UA data\n        status_null_value (optional str): If populated, will replace 'Good' in the Status column with the specified value.\n        change_type_value (optional str): If populated, will replace 'insert' in the ChangeType column with the specified value.\n    \"\"\"\n\n    data: DataFrame\n    source_column_name: str\n    status_null_value: str\n    change_type_value: str\n\n    def __init__(\n        self,\n        data: DataFrame,\n        source_column_name: str,\n        status_null_value: str = \"Good\",\n        change_type_value: str = \"insert\",\n    ) -&gt; None:\n        self.data = data\n        self.source_column_name = source_column_name\n        self.status_null_value = status_null_value\n        self.change_type_value = change_type_value\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the specified column converted to PCDM\n        \"\"\"\n        df = (\n            self.data.withColumn(\"body\", from_json(self.source_column_name, APM_SCHEMA))\n            .select(explode(\"body.SystemTimeSeries.Samples\"))\n            .selectExpr(\"*\", \"to_timestamp(col.Time) as EventTime\")\n            .withColumn(\"TagName\", col(\"col.Itemname\"))\n            .withColumn(\"Status\", lit(self.status_null_value))\n            .withColumn(\"Value\", col(\"col.Value\"))\n            .withColumn(\n                \"ValueType\",\n                when(col(\"value\").cast(\"float\").isNotNull(), \"float\").when(\n                    col(\"value\").cast(\"float\").isNull(), \"string\"\n                ),\n            )\n            .withColumn(\"ChangeType\", lit(self.change_type_value))\n        )\n\n        return df.select(\n            \"TagName\", \"EventTime\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/honeywell_apm_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.honeywell_apm_to_pcdm.HoneywellAPMJsonToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/honeywell_apm_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/honeywell_apm_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.honeywell_apm_to_pcdm.HoneywellAPMJsonToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the specified column converted to PCDM</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/honeywell_apm_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the specified column converted to PCDM\n    \"\"\"\n    df = (\n        self.data.withColumn(\"body\", from_json(self.source_column_name, APM_SCHEMA))\n        .select(explode(\"body.SystemTimeSeries.Samples\"))\n        .selectExpr(\"*\", \"to_timestamp(col.Time) as EventTime\")\n        .withColumn(\"TagName\", col(\"col.Itemname\"))\n        .withColumn(\"Status\", lit(self.status_null_value))\n        .withColumn(\"Value\", col(\"col.Value\"))\n        .withColumn(\n            \"ValueType\",\n            when(col(\"value\").cast(\"float\").isNotNull(), \"float\").when(\n                col(\"value\").cast(\"float\").isNull(), \"string\"\n            ),\n        )\n        .withColumn(\"ChangeType\", lit(self.change_type_value))\n    )\n\n    return df.select(\n        \"TagName\", \"EventTime\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/mirico_json_to_pcdm/","title":"Convert Mirico Json to Process Control Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/mirico_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.mirico_json_to_pcdm.MiricoJsonToPCDMTransformer","title":"<code>MiricoJsonToPCDMTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a Spark Dataframe column containing a json string created from Mirico to the Process Control Data Model.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/mirico_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.mirico_json_to_pcdm.MiricoJsonToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import MiricoJsonToPCDMTransformer\n\nmirico_json_to_pcdm_transformer = MiricoJsonToPCDMTransformer(\n    data=df\n    source_column_name=\"body\",\n    status_null_value=\"Good\",\n    change_type_value=\"insert\"\n    tagname_field=\"test\"\n)\n\nresult = mirico_json_to_pcdm_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe containing the column with Mirico data</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the Json Mirico data</p> required <code>status_null_value</code> <code>optional str</code> <p>If populated, will replace 'Good' in the Status column with the specified value.</p> <code>'Good'</code> <code>change_type_value</code> <code>optional str</code> <p>If populated, will replace 'insert' in the ChangeType column with the specified value.</p> <code>'insert'</code> <code>tagname_field</code> <code>optional str</code> <p>If populated, will add the specified field to the TagName column.</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/mirico_json_to_pcdm.py</code> <pre><code>class MiricoJsonToPCDMTransformer(TransformerInterface):\n    \"\"\"\n    Converts a Spark Dataframe column containing a json string created from Mirico to the Process Control Data Model.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import MiricoJsonToPCDMTransformer\n\n    mirico_json_to_pcdm_transformer = MiricoJsonToPCDMTransformer(\n        data=df\n        source_column_name=\"body\",\n        status_null_value=\"Good\",\n        change_type_value=\"insert\"\n        tagname_field=\"test\"\n    )\n\n    result = mirico_json_to_pcdm_transformer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe containing the column with Mirico data\n        source_column_name (str): Spark Dataframe column containing the Json Mirico data\n        status_null_value (optional str): If populated, will replace 'Good' in the Status column with the specified value.\n        change_type_value (optional str): If populated, will replace 'insert' in the ChangeType column with the specified value.\n        tagname_field (optional str): If populated, will add the specified field to the TagName column.\n    \"\"\"\n\n    data: DataFrame\n    source_column_name: str\n    status_null_value: str\n    change_type_value: str\n\n    def __init__(\n        self,\n        data: DataFrame,\n        source_column_name: str,\n        status_null_value: str = \"Good\",\n        change_type_value: str = \"insert\",\n        tagname_field: str = None,\n    ) -&gt; None:\n        _package_version_meets_minimum(\"pyspark\", \"3.4.0\")\n        self.data = data\n        self.source_column_name = source_column_name\n        self.status_null_value = status_null_value\n        self.change_type_value = change_type_value\n        self.tagname_field = tagname_field\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the specified column converted to PCDM\n        \"\"\"\n\n        mapping = mirico_field_mappings.MIRICO_FIELD_MAPPINGS\n        df = (\n            self.data.withColumn(\n                self.source_column_name,\n                from_json(self.source_column_name, \"map&lt;string,string&gt;\"),\n            )\n            .withColumn(\"TagName\", map_keys(\"body\"))\n            .withColumn(\"Value\", map_values(\"body\"))\n            .select(\n                map_from_arrays(\"TagName\", \"Value\").alias(\"x\"),\n                to_timestamp(col(\"x.timeStamp\")).alias(\"EventTime\"),\n                col(\"x.siteName\").alias(\"siteName\"),\n                col(\"x.gasType\").alias(\"gasType\"),\n                col(\"x.retroName\").alias(\"retroName\"),\n            )\n            .select(\"EventTime\", \"siteName\", \"gasType\", \"retroName\", posexplode(\"x\"))\n            .withColumn(\n                \"ValueType\", udf(lambda row: mapping[row][\"ValueType\"])(col(\"pos\"))\n            )\n            .withColumn(\"Status\", lit(\"Good\"))\n            .withColumn(\"ChangeType\", lit(\"insert\"))\n            .withColumn(\n                \"TagName\",\n                when(\n                    lit(self.tagname_field).isNotNull(),\n                    concat_ws(\n                        \":\",\n                        *[\n                            upper(lit(self.tagname_field)),\n                            concat_ws(\n                                \"_\",\n                                *[\n                                    upper(col(\"siteName\")),\n                                    upper(col(\"retroName\")),\n                                    when(\n                                        upper(col(\"key\")) == \"GASPPM\",\n                                        concat_ws(\n                                            \"_\",\n                                            *[upper(col(\"key\")), upper(col(\"gasType\"))]\n                                        ),\n                                    ).otherwise(upper(col(\"key\"))),\n                                ]\n                            ),\n                        ]\n                    ),\n                ).otherwise(\n                    concat_ws(\n                        \"_\",\n                        *[\n                            upper(col(\"siteName\")),\n                            upper(col(\"retroName\")),\n                            when(\n                                upper(col(\"key\")) == \"GASPPM\",\n                                concat_ws(\n                                    \"_\", *[upper(col(\"key\")), upper(col(\"gasType\"))]\n                                ),\n                            ).otherwise(upper(col(\"key\"))),\n                        ]\n                    )\n                ),\n            )\n            .filter(\n                ~col(\"key\").isin(\n                    \"timeStamp\",\n                    \"gasType\",\n                    \"retroLongitude\",\n                    \"retroLatitude\",\n                    \"retroAltitude\",\n                    \"sensorLongitude\",\n                    \"sensorLatitude\",\n                    \"sensorAltitude\",\n                    \"siteName\",\n                    \"siteKey\",\n                    \"retroName\",\n                )\n            )\n        )\n        return df.select(\n            \"EventTime\", \"TagName\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/mirico_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.mirico_json_to_pcdm.MiricoJsonToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/mirico_json_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/mirico_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.mirico_json_to_pcdm.MiricoJsonToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the specified column converted to PCDM</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/mirico_json_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the specified column converted to PCDM\n    \"\"\"\n\n    mapping = mirico_field_mappings.MIRICO_FIELD_MAPPINGS\n    df = (\n        self.data.withColumn(\n            self.source_column_name,\n            from_json(self.source_column_name, \"map&lt;string,string&gt;\"),\n        )\n        .withColumn(\"TagName\", map_keys(\"body\"))\n        .withColumn(\"Value\", map_values(\"body\"))\n        .select(\n            map_from_arrays(\"TagName\", \"Value\").alias(\"x\"),\n            to_timestamp(col(\"x.timeStamp\")).alias(\"EventTime\"),\n            col(\"x.siteName\").alias(\"siteName\"),\n            col(\"x.gasType\").alias(\"gasType\"),\n            col(\"x.retroName\").alias(\"retroName\"),\n        )\n        .select(\"EventTime\", \"siteName\", \"gasType\", \"retroName\", posexplode(\"x\"))\n        .withColumn(\n            \"ValueType\", udf(lambda row: mapping[row][\"ValueType\"])(col(\"pos\"))\n        )\n        .withColumn(\"Status\", lit(\"Good\"))\n        .withColumn(\"ChangeType\", lit(\"insert\"))\n        .withColumn(\n            \"TagName\",\n            when(\n                lit(self.tagname_field).isNotNull(),\n                concat_ws(\n                    \":\",\n                    *[\n                        upper(lit(self.tagname_field)),\n                        concat_ws(\n                            \"_\",\n                            *[\n                                upper(col(\"siteName\")),\n                                upper(col(\"retroName\")),\n                                when(\n                                    upper(col(\"key\")) == \"GASPPM\",\n                                    concat_ws(\n                                        \"_\",\n                                        *[upper(col(\"key\")), upper(col(\"gasType\"))]\n                                    ),\n                                ).otherwise(upper(col(\"key\"))),\n                            ]\n                        ),\n                    ]\n                ),\n            ).otherwise(\n                concat_ws(\n                    \"_\",\n                    *[\n                        upper(col(\"siteName\")),\n                        upper(col(\"retroName\")),\n                        when(\n                            upper(col(\"key\")) == \"GASPPM\",\n                            concat_ws(\n                                \"_\", *[upper(col(\"key\")), upper(col(\"gasType\"))]\n                            ),\n                        ).otherwise(upper(col(\"key\"))),\n                    ]\n                )\n            ),\n        )\n        .filter(\n            ~col(\"key\").isin(\n                \"timeStamp\",\n                \"gasType\",\n                \"retroLongitude\",\n                \"retroLatitude\",\n                \"retroAltitude\",\n                \"sensorLongitude\",\n                \"sensorLatitude\",\n                \"sensorAltitude\",\n                \"siteName\",\n                \"siteKey\",\n                \"retroName\",\n            )\n        )\n    )\n    return df.select(\n        \"EventTime\", \"TagName\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/opc_publisher_opcae_json_to_pcdm/","title":"Convert OPC Publisher Json for A&amp;E(Alarm &amp; Events) Data to Process Control Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/opc_publisher_opcae_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.opc_publisher_opcae_json_to_pcdm.OPCPublisherOPCAEJsonToPCDMTransformer","title":"<code>OPCPublisherOPCAEJsonToPCDMTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a Spark Dataframe column containing a json string created by OPC Publisher for A&amp;E(Alarm &amp;Events) data to the Process Control Data Model.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/opc_publisher_opcae_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.opc_publisher_opcae_json_to_pcdm.OPCPublisherOPCAEJsonToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import OPCPublisherOPCAEJsonToPCDMTransformer\n\nopc_publisher_opcae_json_to_pcdm_transformer = OPCPublisherOPCAEJsonToPCDMTransformer(\n    data=df,\n    souce_column_name=\"body\",\n    timestamp_formats=[\n        \"yyyy-MM-dd'T'HH:mm:ss.SSSX\",\n        \"yyyy-MM-dd'T'HH:mm:ssX\"\n    ],\n    filter=None\n)\n\nresult = opc_publisher_opcae_json_to_pcdm_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe containing the column with Json OPC AE data</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the OPC Publisher Json OPC AE data</p> required <code>timestamp_formats</code> <code>optional list[str]</code> <p>Specifies the timestamp formats to be used for converting the timestamp string to a Timestamp Type. For more information on formats, refer to this documentation.</p> <code>None</code> <code>filter</code> <code>optional str</code> <p>Enables providing a filter to the data which can be required in certain scenarios. For example, it would be possible to filter on IoT Hub Device Id and Module by providing a filter in SQL format such as <code>systemProperties.iothub-connection-device-id = \"&lt;Device Id&gt;\" AND systemProperties.iothub-connection-module-id = \"&lt;Module&gt;\"</code></p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/opc_publisher_opcae_json_to_pcdm.py</code> <pre><code>class OPCPublisherOPCAEJsonToPCDMTransformer(TransformerInterface):\n    \"\"\"\n    Converts a Spark Dataframe column containing a json string created by OPC Publisher for A&amp;E(Alarm &amp;Events) data to the Process Control Data Model.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import OPCPublisherOPCAEJsonToPCDMTransformer\n\n    opc_publisher_opcae_json_to_pcdm_transformer = OPCPublisherOPCAEJsonToPCDMTransformer(\n        data=df,\n        souce_column_name=\"body\",\n        timestamp_formats=[\n            \"yyyy-MM-dd'T'HH:mm:ss.SSSX\",\n            \"yyyy-MM-dd'T'HH:mm:ssX\"\n        ],\n        filter=None\n    )\n\n    result = opc_publisher_opcae_json_to_pcdm_transformer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe containing the column with Json OPC AE data\n        source_column_name (str): Spark Dataframe column containing the OPC Publisher Json OPC AE data\n        timestamp_formats (optional list[str]): Specifies the timestamp formats to be used for converting the timestamp string to a Timestamp Type. For more information on formats, refer to this [documentation.](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html)\n        filter (optional str): Enables providing a filter to the data which can be required in certain scenarios. For example, it would be possible to filter on IoT Hub Device Id and Module by providing a filter in SQL format such as `systemProperties.iothub-connection-device-id = \"&lt;Device Id&gt;\" AND systemProperties.iothub-connection-module-id = \"&lt;Module&gt;\"`\n    \"\"\"\n\n    data: DataFrame\n    source_column_name: str\n    timestamp_formats: list\n    filter: str\n\n    def __init__(\n        self,\n        data: DataFrame,\n        source_column_name: str,\n        timestamp_formats=None,\n        filter: str = None,\n    ) -&gt; None:  # NOSONAR\n        self.data = data\n        self.source_column_name = source_column_name\n        self.timestamp_formats = timestamp_formats or [\n            \"yyyy-MM-dd'T'HH:mm:ss.SSSX\",\n            \"yyyy-MM-dd'T'HH:mm:ssX\",\n        ]\n        self.filter = filter\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the OPC Publisher A&amp;E data converted to the Process Control Data Model\n        \"\"\"\n\n        df = self.data.withColumn(\n            self.source_column_name,\n            from_json(col(self.source_column_name), ArrayType(StringType())),\n        ).withColumn(self.source_column_name, explode(self.source_column_name))\n\n        if self.filter != None:\n            df = df.where(self.filter)\n\n        df = df.withColumn(\n            \"OPCAE\", from_json(col(self.source_column_name), OPC_PUBLISHER_AE_SCHEMA)\n        )\n\n        df = df.select(\n            col(\"OPCAE.NodeId\"),\n            col(\"OPCAE.DisplayName\"),\n            col(\"OPCAE.Value.ConditionId.Value\").alias(\"ConditionId\"),\n            col(\"OPCAE.Value.AckedState.Value\").alias(\"AckedState\"),\n            col(\"OPCAE.Value.AckedState/FalseState.Value\").alias(\n                \"AckedState/FalseState\"\n            ),\n            col(\"OPCAE.Value.AckedState/Id.Value\").alias(\"AckedState/Id\"),\n            col(\"OPCAE.Value.AckedState/TrueState.Value\").alias(\"AckedState/TrueState\"),\n            col(\"OPCAE.Value.ActiveState.Value\").alias(\"ActiveState\"),\n            col(\"OPCAE.Value.ActiveState/FalseState.Value\").alias(\n                \"ActiveState/FalseState\"\n            ),\n            col(\"OPCAE.Value.ActiveState/Id.Value\").alias(\"ActiveState/Id\"),\n            col(\"OPCAE.Value.ActiveState/TrueState.Value\").alias(\n                \"ActiveState/TrueState\"\n            ),\n            col(\"OPCAE.Value.EnabledState.Value\").alias(\"EnabledState\"),\n            col(\"OPCAE.Value.EnabledState/FalseState.Value\").alias(\n                \"EnabledState/FalseState\"\n            ),\n            col(\"OPCAE.Value.EnabledState/Id.Value\").alias(\"EnabledState/Id\"),\n            col(\"OPCAE.Value.EnabledState/TrueState.Value\").alias(\n                \"EnabledState/TrueState\"\n            ),\n            col(\"OPCAE.Value.EventId.Value\").alias(\"EventId\"),\n            col(\"OPCAE.Value.EventType.Value\").alias(\"EventType\"),\n            col(\"OPCAE.Value.HighHighLimit.Value\").alias(\"HighHighLimit\"),\n            col(\"OPCAE.Value.HighLimit.Value\").alias(\"HighLimit\"),\n            col(\"OPCAE.Value.InputNode.Value\").alias(\"InputNode\"),\n            col(\"OPCAE.Value.LowLimit.Value\").alias(\"LowLimit\"),\n            col(\"OPCAE.Value.LowLowLimit.Value\").alias(\"LowLowLimit\"),\n            col(\"OPCAE.Value.Message.Value\").alias(\"Message\"),\n            col(\"OPCAE.Value.Quality.Value\").alias(\"Quality\"),\n            col(\"OPCAE.Value.ReceiveTime.Value\").alias(\"ReceiveTime\"),\n            col(\"OPCAE.Value.Retain.Value\").alias(\"Retain\"),\n            col(\"OPCAE.Value.Severity.Value\").alias(\"Severity\"),\n            col(\"OPCAE.Value.SourceName.Value\").alias(\"SourceName\"),\n            col(\"OPCAE.Value.SourceNode.Value\").alias(\"SourceNode\"),\n            col(\"OPCAE.Value.Time.Value\").alias(\"EventTime\"),\n        )\n\n        df = df.withColumn(\n            \"EventTime\",\n            coalesce(\n                *[to_timestamp(col(\"EventTime\"), f) for f in self.timestamp_formats]\n            ),\n        )\n\n        return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/opc_publisher_opcae_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.opc_publisher_opcae_json_to_pcdm.OPCPublisherOPCAEJsonToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/opc_publisher_opcae_json_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/opc_publisher_opcae_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.opc_publisher_opcae_json_to_pcdm.OPCPublisherOPCAEJsonToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the OPC Publisher A&amp;E data converted to the Process Control Data Model</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/opc_publisher_opcae_json_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the OPC Publisher A&amp;E data converted to the Process Control Data Model\n    \"\"\"\n\n    df = self.data.withColumn(\n        self.source_column_name,\n        from_json(col(self.source_column_name), ArrayType(StringType())),\n    ).withColumn(self.source_column_name, explode(self.source_column_name))\n\n    if self.filter != None:\n        df = df.where(self.filter)\n\n    df = df.withColumn(\n        \"OPCAE\", from_json(col(self.source_column_name), OPC_PUBLISHER_AE_SCHEMA)\n    )\n\n    df = df.select(\n        col(\"OPCAE.NodeId\"),\n        col(\"OPCAE.DisplayName\"),\n        col(\"OPCAE.Value.ConditionId.Value\").alias(\"ConditionId\"),\n        col(\"OPCAE.Value.AckedState.Value\").alias(\"AckedState\"),\n        col(\"OPCAE.Value.AckedState/FalseState.Value\").alias(\n            \"AckedState/FalseState\"\n        ),\n        col(\"OPCAE.Value.AckedState/Id.Value\").alias(\"AckedState/Id\"),\n        col(\"OPCAE.Value.AckedState/TrueState.Value\").alias(\"AckedState/TrueState\"),\n        col(\"OPCAE.Value.ActiveState.Value\").alias(\"ActiveState\"),\n        col(\"OPCAE.Value.ActiveState/FalseState.Value\").alias(\n            \"ActiveState/FalseState\"\n        ),\n        col(\"OPCAE.Value.ActiveState/Id.Value\").alias(\"ActiveState/Id\"),\n        col(\"OPCAE.Value.ActiveState/TrueState.Value\").alias(\n            \"ActiveState/TrueState\"\n        ),\n        col(\"OPCAE.Value.EnabledState.Value\").alias(\"EnabledState\"),\n        col(\"OPCAE.Value.EnabledState/FalseState.Value\").alias(\n            \"EnabledState/FalseState\"\n        ),\n        col(\"OPCAE.Value.EnabledState/Id.Value\").alias(\"EnabledState/Id\"),\n        col(\"OPCAE.Value.EnabledState/TrueState.Value\").alias(\n            \"EnabledState/TrueState\"\n        ),\n        col(\"OPCAE.Value.EventId.Value\").alias(\"EventId\"),\n        col(\"OPCAE.Value.EventType.Value\").alias(\"EventType\"),\n        col(\"OPCAE.Value.HighHighLimit.Value\").alias(\"HighHighLimit\"),\n        col(\"OPCAE.Value.HighLimit.Value\").alias(\"HighLimit\"),\n        col(\"OPCAE.Value.InputNode.Value\").alias(\"InputNode\"),\n        col(\"OPCAE.Value.LowLimit.Value\").alias(\"LowLimit\"),\n        col(\"OPCAE.Value.LowLowLimit.Value\").alias(\"LowLowLimit\"),\n        col(\"OPCAE.Value.Message.Value\").alias(\"Message\"),\n        col(\"OPCAE.Value.Quality.Value\").alias(\"Quality\"),\n        col(\"OPCAE.Value.ReceiveTime.Value\").alias(\"ReceiveTime\"),\n        col(\"OPCAE.Value.Retain.Value\").alias(\"Retain\"),\n        col(\"OPCAE.Value.Severity.Value\").alias(\"Severity\"),\n        col(\"OPCAE.Value.SourceName.Value\").alias(\"SourceName\"),\n        col(\"OPCAE.Value.SourceNode.Value\").alias(\"SourceNode\"),\n        col(\"OPCAE.Value.Time.Value\").alias(\"EventTime\"),\n    )\n\n    df = df.withColumn(\n        \"EventTime\",\n        coalesce(\n            *[to_timestamp(col(\"EventTime\"), f) for f in self.timestamp_formats]\n        ),\n    )\n\n    return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/opc_publisher_opcua_json_to_pcdm/","title":"Convert OPC Publisher Json to Process Control Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/opc_publisher_opcua_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.opc_publisher_opcua_json_to_pcdm.OPCPublisherOPCUAJsonToPCDMTransformer","title":"<code>OPCPublisherOPCUAJsonToPCDMTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a Spark Dataframe column containing a json string created by OPC Publisher to the Process Control Data Model.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/opc_publisher_opcua_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.opc_publisher_opcua_json_to_pcdm.OPCPublisherOPCUAJsonToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import OPCPublisherOPCUAJsonToPCDMTransformer\n\nopc_publisher_opcua_json_to_pcdm_transformer = OPCPublisherOPCUAJsonToPCDMTransformer(\n    data=df,\n    souce_column_name=\"body\",\n    multiple_rows_per_message=True,\n    status_null_value=\"Good\",\n    change_type_value=\"insert\",\n    timestamp_formats=[\n        \"yyyy-MM-dd'T'HH:mm:ss.SSSX\",\n        \"yyyy-MM-dd'T'HH:mm:ssX\"\n    ],\n    filter=None\n)\n\nresult = opc_publisher_opcua_json_to_pcdm_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe containing the column with Json OPC UA data</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the OPC Publisher Json OPC UA data</p> required <code>multiple_rows_per_message</code> <code>optional bool</code> <p>Each Dataframe Row contains an array of/multiple OPC UA messages. The list of Json will be exploded into rows in the Dataframe.</p> <code>True</code> <code>status_null_value</code> <code>optional str</code> <p>If populated, will replace null values in the Status column with the specified value.</p> <code>None</code> <code>change_type_value</code> <code>optional str</code> <p>If populated, will replace 'insert' in the ChangeType column with the specified value.</p> <code>'insert'</code> <code>timestamp_formats</code> <code>optional list[str]</code> <p>Specifies the timestamp formats to be used for converting the timestamp string to a Timestamp Type. For more information on formats, refer to this documentation.</p> <code>[\"yyyy-MM-dd'T'HH:mm:ss.SSSX\", \"yyyy-MM-dd'T'HH:mm:ssX\"]</code> <code>filter</code> <code>optional str</code> <p>Enables providing a filter to the data which can be required in certain scenarios. For example, it would be possible to filter on IoT Hub Device Id and Module by providing a filter in SQL format such as <code>systemProperties.iothub-connection-device-id = \"&lt;Device Id&gt;\" AND systemProperties.iothub-connection-module-id = \"&lt;Module&gt;\"</code></p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/opc_publisher_opcua_json_to_pcdm.py</code> <pre><code>class OPCPublisherOPCUAJsonToPCDMTransformer(TransformerInterface):\n    \"\"\"\n    Converts a Spark Dataframe column containing a json string created by OPC Publisher to the Process Control Data Model.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import OPCPublisherOPCUAJsonToPCDMTransformer\n\n    opc_publisher_opcua_json_to_pcdm_transformer = OPCPublisherOPCUAJsonToPCDMTransformer(\n        data=df,\n        souce_column_name=\"body\",\n        multiple_rows_per_message=True,\n        status_null_value=\"Good\",\n        change_type_value=\"insert\",\n        timestamp_formats=[\n            \"yyyy-MM-dd'T'HH:mm:ss.SSSX\",\n            \"yyyy-MM-dd'T'HH:mm:ssX\"\n        ],\n        filter=None\n    )\n\n    result = opc_publisher_opcua_json_to_pcdm_transformer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe containing the column with Json OPC UA data\n        source_column_name (str): Spark Dataframe column containing the OPC Publisher Json OPC UA data\n        multiple_rows_per_message (optional bool): Each Dataframe Row contains an array of/multiple OPC UA messages. The list of Json will be exploded into rows in the Dataframe.\n        status_null_value (optional str): If populated, will replace null values in the Status column with the specified value.\n        change_type_value (optional str): If populated, will replace 'insert' in the ChangeType column with the specified value.\n        timestamp_formats (optional list[str]): Specifies the timestamp formats to be used for converting the timestamp string to a Timestamp Type. For more information on formats, refer to this [documentation.](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html)\n        filter (optional str): Enables providing a filter to the data which can be required in certain scenarios. For example, it would be possible to filter on IoT Hub Device Id and Module by providing a filter in SQL format such as `systemProperties.iothub-connection-device-id = \"&lt;Device Id&gt;\" AND systemProperties.iothub-connection-module-id = \"&lt;Module&gt;\"`\n    \"\"\"\n\n    data: DataFrame\n    source_column_name: str\n    multiple_rows_per_message: bool\n    tagname_field: str\n    status_null_value: str\n    change_type_value: str\n    timestamp_formats: list\n    filter: str\n\n    def __init__(\n        self,\n        data: DataFrame,\n        source_column_name: str,\n        multiple_rows_per_message: bool = True,\n        tagname_field: str = \"DisplayName\",\n        status_null_value: str = None,\n        change_type_value: str = \"insert\",\n        timestamp_formats: list = [\n            \"yyyy-MM-dd'T'HH:mm:ss.SSSX\",\n            \"yyyy-MM-dd'T'HH:mm:ssX\",\n        ],\n        filter: str = None,\n    ) -&gt; None:  # NOSONAR\n        self.data = data\n        self.source_column_name = source_column_name\n        self.multiple_rows_per_message = multiple_rows_per_message\n        self.tagname_field = tagname_field\n        self.status_null_value = status_null_value\n        self.change_type_value = change_type_value\n        self.timestamp_formats = timestamp_formats\n        self.filter = filter\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the specified column converted to PCDM\n        \"\"\"\n        if self.multiple_rows_per_message:\n            df = self.data.withColumn(\n                self.source_column_name,\n                from_json(col(self.source_column_name), ArrayType(StringType())),\n            ).withColumn(self.source_column_name, explode(self.source_column_name))\n        else:\n            df = self.data.withColumn(\n                self.source_column_name,\n                from_json(col(self.source_column_name), StringType()),\n            )\n\n        if self.filter != None:\n            df = df.where(self.filter)\n\n        df = (\n            df.withColumn(\n                \"OPCUA\", from_json(col(self.source_column_name), OPC_PUBLISHER_SCHEMA)\n            )\n            .withColumn(\"TagName\", (col(\"OPCUA.{}\".format(self.tagname_field))))\n            .withColumn(\n                \"EventTime\",\n                coalesce(\n                    *[\n                        to_timestamp(col(\"OPCUA.Value.SourceTimestamp\"), f)\n                        for f in self.timestamp_formats\n                    ]\n                ),\n            )\n            .withColumn(\"Value\", col(\"OPCUA.Value.Value\"))\n            .withColumn(\n                \"ValueType\",\n                when(col(\"Value\").cast(\"float\").isNotNull(), \"float\")\n                .when(col(\"Value\").cast(\"float\").isNull(), \"string\")\n                .otherwise(\"unknown\"),\n            )\n            .withColumn(\"ChangeType\", lit(self.change_type_value))\n        )\n\n        status_col_name = \"OPCUA.Value.StatusCode.Symbol\"\n        if self.status_null_value != None:\n            df = df.withColumn(\n                \"Status\",\n                when(col(status_col_name).isNotNull(), col(status_col_name)).otherwise(\n                    lit(self.status_null_value)\n                ),\n            )\n        else:\n            df = df.withColumn(\"Status\", col(status_col_name))\n\n        return df.select(\n            \"TagName\", \"EventTime\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/opc_publisher_opcua_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.opc_publisher_opcua_json_to_pcdm.OPCPublisherOPCUAJsonToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/opc_publisher_opcua_json_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/opc_publisher_opcua_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.opc_publisher_opcua_json_to_pcdm.OPCPublisherOPCUAJsonToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the specified column converted to PCDM</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/opc_publisher_opcua_json_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the specified column converted to PCDM\n    \"\"\"\n    if self.multiple_rows_per_message:\n        df = self.data.withColumn(\n            self.source_column_name,\n            from_json(col(self.source_column_name), ArrayType(StringType())),\n        ).withColumn(self.source_column_name, explode(self.source_column_name))\n    else:\n        df = self.data.withColumn(\n            self.source_column_name,\n            from_json(col(self.source_column_name), StringType()),\n        )\n\n    if self.filter != None:\n        df = df.where(self.filter)\n\n    df = (\n        df.withColumn(\n            \"OPCUA\", from_json(col(self.source_column_name), OPC_PUBLISHER_SCHEMA)\n        )\n        .withColumn(\"TagName\", (col(\"OPCUA.{}\".format(self.tagname_field))))\n        .withColumn(\n            \"EventTime\",\n            coalesce(\n                *[\n                    to_timestamp(col(\"OPCUA.Value.SourceTimestamp\"), f)\n                    for f in self.timestamp_formats\n                ]\n            ),\n        )\n        .withColumn(\"Value\", col(\"OPCUA.Value.Value\"))\n        .withColumn(\n            \"ValueType\",\n            when(col(\"Value\").cast(\"float\").isNotNull(), \"float\")\n            .when(col(\"Value\").cast(\"float\").isNull(), \"string\")\n            .otherwise(\"unknown\"),\n        )\n        .withColumn(\"ChangeType\", lit(self.change_type_value))\n    )\n\n    status_col_name = \"OPCUA.Value.StatusCode.Symbol\"\n    if self.status_null_value != None:\n        df = df.withColumn(\n            \"Status\",\n            when(col(status_col_name).isNotNull(), col(status_col_name)).otherwise(\n                lit(self.status_null_value)\n            ),\n        )\n    else:\n        df = df.withColumn(\"Status\", col(status_col_name))\n\n    return df.select(\n        \"TagName\", \"EventTime\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/pandas_to_pyspark/","title":"Pandas to PySpark DataFrame Conversion","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/pandas_to_pyspark/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.pandas_to_pyspark.PandasToPySparkTransformer","title":"<code>PandasToPySparkTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a Pandas DataFrame to a PySpark DataFrame.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/pandas_to_pyspark/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.pandas_to_pyspark.PandasToPySparkTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import PandasToPySparkTransformer\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\npandas_to_pyspark = PandasToPySparkTransformer(\n    spark=spark,\n    df=df,\n)\n\nresult = pandas_to_pyspark.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to convert DataFrame</p> required <code>df</code> <code>DataFrame</code> <p>Pandas DataFrame to be converted</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/pandas_to_pyspark.py</code> <pre><code>class PandasToPySparkTransformer(TransformerInterface):\n    \"\"\"\n    Converts a Pandas DataFrame to a PySpark DataFrame.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import PandasToPySparkTransformer\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    pandas_to_pyspark = PandasToPySparkTransformer(\n        spark=spark,\n        df=df,\n    )\n\n    result = pandas_to_pyspark.transform()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to convert DataFrame\n        df (DataFrame): Pandas DataFrame to be converted\n    \"\"\"\n\n    spark: SparkSession\n    df: PandasDataFrame\n\n    def __init__(self, spark: SparkSession, df: PandasDataFrame) -&gt; None:\n        self.spark = spark\n        self.df = df\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; PySparkDataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A PySpark dataframe converted from a Pandas DataFrame.\n        \"\"\"\n\n        self.df = _prepare_pandas_to_convert_to_spark(self.df)\n        df = self.spark.createDataFrame(self.df)\n\n        return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/pandas_to_pyspark/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.pandas_to_pyspark.PandasToPySparkTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/pandas_to_pyspark.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/pandas_to_pyspark/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.pandas_to_pyspark.PandasToPySparkTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A PySpark dataframe converted from a Pandas DataFrame.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/pandas_to_pyspark.py</code> <pre><code>def transform(self) -&gt; PySparkDataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A PySpark dataframe converted from a Pandas DataFrame.\n    \"\"\"\n\n    self.df = _prepare_pandas_to_convert_to_spark(self.df)\n    df = self.spark.createDataFrame(self.df)\n\n    return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/pcdm_to_honeywell_apm/","title":"Convert Process Control Data Model to Honeywell APM Json","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/pcdm_to_honeywell_apm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.pcdm_to_honeywell_apm.PCDMToHoneywellAPMTransformer","title":"<code>PCDMToHoneywellAPMTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a Spark Dataframe in PCDM format to Honeywell APM format.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/pcdm_to_honeywell_apm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.pcdm_to_honeywell_apm.PCDMToHoneywellAPMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import PCDMToHoneywellAPMTransformer\n\npcdm_to_honeywell_apm_transformer = PCDMToHoneywellAPMTransformer(\n    data=df,\n    quality=\"Good\",\n    history_samples_per_message=1,\n    compress_payload=True\n)\n\nresult = pcdm_to_honeywell_apm_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dataframe</code> <p>Spark Dataframe in PCDM format</p> required <code>quality</code> <code>str</code> <p>Value for quality inside HistorySamples</p> <code>'Good'</code> <code>history_samples_per_message</code> <code>int</code> <p>The number of HistorySamples for each row in the DataFrame (Batch Only)</p> <code>1</code> <code>compress_payload</code> <code>bool</code> <p>If True compresses CloudPlatformEvent with gzip compression</p> <code>True</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/pcdm_to_honeywell_apm.py</code> <pre><code>class PCDMToHoneywellAPMTransformer(TransformerInterface):\n    \"\"\"\n    Converts a Spark Dataframe in PCDM format to Honeywell APM format.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import PCDMToHoneywellAPMTransformer\n\n    pcdm_to_honeywell_apm_transformer = PCDMToHoneywellAPMTransformer(\n        data=df,\n        quality=\"Good\",\n        history_samples_per_message=1,\n        compress_payload=True\n    )\n\n    result = pcdm_to_honeywell_apm_transformer.transform()\n    ```\n\n    Parameters:\n        data (Dataframe): Spark Dataframe in PCDM format\n        quality (str): Value for quality inside HistorySamples\n        history_samples_per_message (int): The number of HistorySamples for each row in the DataFrame (Batch Only)\n        compress_payload (bool): If True compresses CloudPlatformEvent with gzip compression\n    \"\"\"\n\n    data: DataFrame\n    quality: str\n    history_samples_per_message: int\n    compress_payload: bool\n\n    def __init__(\n        self,\n        data: DataFrame,\n        quality: str = \"Good\",\n        history_samples_per_message: int = 1,\n        compress_payload: bool = True,\n    ) -&gt; None:\n        self.data = data\n        self.quality = quality\n        self.history_samples_per_message = history_samples_per_message\n        self.compress_payload = compress_payload\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with with rows in Honeywell APM format\n        \"\"\"\n\n        @udf(\"string\")\n        def _compress_payload(data):\n            compressed_data = gzip.compress(data.encode(\"utf-8\"))\n            encoded_data = base64.b64encode(compressed_data).decode(\"utf-8\")\n            return encoded_data\n\n        if self.data.isStreaming == False and self.history_samples_per_message &gt; 1:\n            w = Window.partitionBy(\"TagName\").orderBy(\"TagName\")\n            cleaned_pcdm_df = (\n                self.data.withColumn(\n                    \"index\",\n                    floor(\n                        (row_number().over(w) - 0.01) / self.history_samples_per_message\n                    ),\n                )\n                .withColumn(\n                    \"HistorySamples\",\n                    struct(\n                        col(\"TagName\").alias(\"ItemName\"),\n                        lit(self.quality).alias(\"Quality\"),\n                        col(\"EventTime\").alias(\"Time\"),\n                        col(\"Value\").alias(\"Value\"),\n                    ).alias(\"HistorySamples\"),\n                )\n                .groupBy(\"TagName\", \"index\")\n                .agg(collect_list(\"HistorySamples\").alias(\"HistorySamples\"))\n                .withColumn(\"guid\", sha2(col(\"TagName\"), 256).cast(\"string\"))\n                .withColumn(\n                    \"value\",\n                    struct(\n                        col(\"guid\").alias(\"SystemGuid\"), col(\"HistorySamples\")\n                    ).alias(\"value\"),\n                )\n            )\n        else:\n            cleaned_pcdm_df = self.data.withColumn(\n                \"guid\", sha2(col(\"TagName\"), 256).cast(\"string\")\n            ).withColumn(\n                \"value\",\n                struct(\n                    col(\"guid\").alias(\"SystemGuid\"),\n                    array(\n                        struct(\n                            col(\"TagName\").alias(\"ItemName\"),\n                            lit(self.quality).alias(\"Quality\"),\n                            col(\"EventTime\").alias(\"Time\"),\n                            col(\"Value\").alias(\"Value\"),\n                        ),\n                    ).alias(\"HistorySamples\"),\n                ),\n            )\n\n        df = (\n            cleaned_pcdm_df.withColumn(\n                \"CloudPlatformEvent\",\n                struct(\n                    lit(datetime.now(tz=pytz.UTC)).alias(\"CreatedTime\"),\n                    lit(expr(\"uuid()\")).alias(\"Id\"),\n                    col(\"guid\").alias(\"CreatorId\"),\n                    lit(\"CloudPlatformSystem\").alias(\"CreatorType\"),\n                    lit(None).alias(\"GeneratorId\"),\n                    lit(\"CloudPlatformTenant\").alias(\"GeneratorType\"),\n                    col(\"guid\").alias(\"TargetId\"),\n                    lit(\"CloudPlatformTenant\").alias(\"TargetType\"),\n                    lit(None).alias(\"TargetContext\"),\n                    struct(\n                        lit(\"TextualBody\").alias(\"type\"),\n                        to_json(col(\"value\")).alias(\"value\"),\n                        lit(\"application/json\").alias(\"format\"),\n                    ).alias(\"Body\"),\n                    array(\n                        struct(\n                            lit(\"SystemType\").alias(\"Key\"),\n                            lit(\"apm-system\").alias(\"Value\"),\n                        ),\n                        struct(\n                            lit(\"SystemGuid\").alias(\"Key\"), col(\"guid\").alias(\"Value\")\n                        ),\n                    ).alias(\"BodyProperties\"),\n                    lit(\"DataChange.Update\").alias(\"EventType\"),\n                ),\n            )\n            .withColumn(\"AnnotationStreamIds\", lit(\",\"))\n            .withColumn(\"partitionKey\", col(\"guid\"))\n        )\n        if self.compress_payload:\n            return df.select(\n                _compress_payload(to_json(\"CloudPlatformEvent\")).alias(\n                    \"CloudPlatformEvent\"\n                ),\n                \"AnnotationStreamIds\",\n                \"partitionKey\",\n            )\n        else:\n            return df.select(\n                \"CloudPlatformEvent\", \"AnnotationStreamIds\", \"partitionKey\"\n            )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/pcdm_to_honeywell_apm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.pcdm_to_honeywell_apm.PCDMToHoneywellAPMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/pcdm_to_honeywell_apm.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/pcdm_to_honeywell_apm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.pcdm_to_honeywell_apm.PCDMToHoneywellAPMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with with rows in Honeywell APM format</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/pcdm_to_honeywell_apm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with with rows in Honeywell APM format\n    \"\"\"\n\n    @udf(\"string\")\n    def _compress_payload(data):\n        compressed_data = gzip.compress(data.encode(\"utf-8\"))\n        encoded_data = base64.b64encode(compressed_data).decode(\"utf-8\")\n        return encoded_data\n\n    if self.data.isStreaming == False and self.history_samples_per_message &gt; 1:\n        w = Window.partitionBy(\"TagName\").orderBy(\"TagName\")\n        cleaned_pcdm_df = (\n            self.data.withColumn(\n                \"index\",\n                floor(\n                    (row_number().over(w) - 0.01) / self.history_samples_per_message\n                ),\n            )\n            .withColumn(\n                \"HistorySamples\",\n                struct(\n                    col(\"TagName\").alias(\"ItemName\"),\n                    lit(self.quality).alias(\"Quality\"),\n                    col(\"EventTime\").alias(\"Time\"),\n                    col(\"Value\").alias(\"Value\"),\n                ).alias(\"HistorySamples\"),\n            )\n            .groupBy(\"TagName\", \"index\")\n            .agg(collect_list(\"HistorySamples\").alias(\"HistorySamples\"))\n            .withColumn(\"guid\", sha2(col(\"TagName\"), 256).cast(\"string\"))\n            .withColumn(\n                \"value\",\n                struct(\n                    col(\"guid\").alias(\"SystemGuid\"), col(\"HistorySamples\")\n                ).alias(\"value\"),\n            )\n        )\n    else:\n        cleaned_pcdm_df = self.data.withColumn(\n            \"guid\", sha2(col(\"TagName\"), 256).cast(\"string\")\n        ).withColumn(\n            \"value\",\n            struct(\n                col(\"guid\").alias(\"SystemGuid\"),\n                array(\n                    struct(\n                        col(\"TagName\").alias(\"ItemName\"),\n                        lit(self.quality).alias(\"Quality\"),\n                        col(\"EventTime\").alias(\"Time\"),\n                        col(\"Value\").alias(\"Value\"),\n                    ),\n                ).alias(\"HistorySamples\"),\n            ),\n        )\n\n    df = (\n        cleaned_pcdm_df.withColumn(\n            \"CloudPlatformEvent\",\n            struct(\n                lit(datetime.now(tz=pytz.UTC)).alias(\"CreatedTime\"),\n                lit(expr(\"uuid()\")).alias(\"Id\"),\n                col(\"guid\").alias(\"CreatorId\"),\n                lit(\"CloudPlatformSystem\").alias(\"CreatorType\"),\n                lit(None).alias(\"GeneratorId\"),\n                lit(\"CloudPlatformTenant\").alias(\"GeneratorType\"),\n                col(\"guid\").alias(\"TargetId\"),\n                lit(\"CloudPlatformTenant\").alias(\"TargetType\"),\n                lit(None).alias(\"TargetContext\"),\n                struct(\n                    lit(\"TextualBody\").alias(\"type\"),\n                    to_json(col(\"value\")).alias(\"value\"),\n                    lit(\"application/json\").alias(\"format\"),\n                ).alias(\"Body\"),\n                array(\n                    struct(\n                        lit(\"SystemType\").alias(\"Key\"),\n                        lit(\"apm-system\").alias(\"Value\"),\n                    ),\n                    struct(\n                        lit(\"SystemGuid\").alias(\"Key\"), col(\"guid\").alias(\"Value\")\n                    ),\n                ).alias(\"BodyProperties\"),\n                lit(\"DataChange.Update\").alias(\"EventType\"),\n            ),\n        )\n        .withColumn(\"AnnotationStreamIds\", lit(\",\"))\n        .withColumn(\"partitionKey\", col(\"guid\"))\n    )\n    if self.compress_payload:\n        return df.select(\n            _compress_payload(to_json(\"CloudPlatformEvent\")).alias(\n                \"CloudPlatformEvent\"\n            ),\n            \"AnnotationStreamIds\",\n            \"partitionKey\",\n        )\n    else:\n        return df.select(\n            \"CloudPlatformEvent\", \"AnnotationStreamIds\", \"partitionKey\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/pyspark_to_pandas/","title":"PySpark to Pandas DataFrame Conversion","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/pyspark_to_pandas/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.pyspark_to_pandas.PySparkToPandasTransformer","title":"<code>PySparkToPandasTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a PySpark DataFrame to a Pandas DataFrame.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/pyspark_to_pandas/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.pyspark_to_pandas.PySparkToPandasTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import PySparkToPandasTransformer\n\npyspark_to_pandas = PySparkToPandasTransformer(\n    df=df\n)\n\nresult = pyspark_to_pandas.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>PySpark DataFrame to be converted</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/pyspark_to_pandas.py</code> <pre><code>class PySparkToPandasTransformer(TransformerInterface):\n    \"\"\"\n    Converts a PySpark DataFrame to a Pandas DataFrame.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import PySparkToPandasTransformer\n\n    pyspark_to_pandas = PySparkToPandasTransformer(\n        df=df\n    )\n\n    result = pyspark_to_pandas.transform()\n    ```\n\n    Parameters:\n        df (DataFrame): PySpark DataFrame to be converted\n    \"\"\"\n\n    df: PySparkDataFrame\n\n    def __init__(self, df: PySparkDataFrame) -&gt; None:\n        self.df = df\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; PandasDataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A Pandas dataframe converted from a PySpark DataFrame.\n        \"\"\"\n        df = self.df.toPandas()\n        return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/pyspark_to_pandas/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.pyspark_to_pandas.PySparkToPandasTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/pyspark_to_pandas.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/pyspark_to_pandas/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.pyspark_to_pandas.PySparkToPandasTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A Pandas dataframe converted from a PySpark DataFrame.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/pyspark_to_pandas.py</code> <pre><code>def transform(self) -&gt; PandasDataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A Pandas dataframe converted from a PySpark DataFrame.\n    \"\"\"\n    df = self.df.toPandas()\n    return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/sem_json_to_pcdm/","title":"Convert SEM Json to Process Control Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/sem_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.sem_json_to_pcdm.SEMJsonToPCDMTransformer","title":"<code>SEMJsonToPCDMTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a Spark Dataframe column containing a json string created by SEM to the Process Control Data Model.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/sem_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.sem_json_to_pcdm.SEMJsonToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import SEMJsonToPCDMTransformer\n\nsem_json_to_pcdm_transformer = SEMJsonToPCDMTransformer(\n    data=df\n    source_column_name=\"body\",\n    version=10,\n    status_null_value=\"Good\",\n    change_type_value=\"insert\"\n)\n\nresult = sem_json_to_pcdm_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe containing the column with SEM data</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the Json SEM data</p> required <code>version</code> <code>int</code> <p>The version for the OBC field mappings. The latest version is 10.</p> required <code>status_null_value</code> <code>optional str</code> <p>If populated, will replace 'Good' in the Status column with the specified value.</p> <code>'Good'</code> <code>change_type_value</code> <code>optional str</code> <p>If populated, will replace 'insert' in the ChangeType column with the specified value.</p> <code>'insert'</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/sem_json_to_pcdm.py</code> <pre><code>class SEMJsonToPCDMTransformer(TransformerInterface):\n    \"\"\"\n    Converts a Spark Dataframe column containing a json string created by SEM to the Process Control Data Model.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import SEMJsonToPCDMTransformer\n\n    sem_json_to_pcdm_transformer = SEMJsonToPCDMTransformer(\n        data=df\n        source_column_name=\"body\",\n        version=10,\n        status_null_value=\"Good\",\n        change_type_value=\"insert\"\n    )\n\n    result = sem_json_to_pcdm_transformer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): Dataframe containing the column with SEM data\n        source_column_name (str): Spark Dataframe column containing the Json SEM data\n        version (int): The version for the OBC field mappings. The latest version is 10.\n        status_null_value (optional str): If populated, will replace 'Good' in the Status column with the specified value.\n        change_type_value (optional str): If populated, will replace 'insert' in the ChangeType column with the specified value.\n    \"\"\"\n\n    data: DataFrame\n    source_column_name: str\n    version: int\n    status_null_value: str\n    change_type_value: str\n\n    def __init__(\n        self,\n        data: DataFrame,\n        source_column_name: str,\n        version: int,\n        status_null_value: str = \"Good\",\n        change_type_value: str = \"insert\",\n    ) -&gt; None:\n        _package_version_meets_minimum(\"pyspark\", \"3.4.0\")\n        self.data = data\n        self.source_column_name = source_column_name\n        self.version = version\n        self.status_null_value = status_null_value\n        self.change_type_value = change_type_value\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the specified column converted to PCDM\n        \"\"\"\n        if self.version == 10:\n            mapping = obc_field_mappings.OBC_FIELD_MAPPINGS_V10\n            df = (\n                self.data.withColumn(\n                    self.source_column_name,\n                    from_json(self.source_column_name, SEM_SCHEMA),\n                )\n                .select(self.source_column_name + \".readings\")\n                .melt(\n                    ids=[\"readings.resourceName\"],\n                    values=[\"readings.value\"],\n                    variableColumnName=\"var\",\n                    valueColumnName=\"value\",\n                )\n                .drop(\"var\")\n                .select(map_from_arrays(\"resourceName\", \"value\").alias(\"resourceName\"))\n                .select(\"resourceName.dID\", \"resourceName.d\", \"resourceName.t\")\n                .select(\n                    regexp_replace(col(\"t\").cast(\"string\"), \"(\\d{10})(\\d+)\", \"$1.$2\")\n                    .cast(\"double\")\n                    .alias(\"timestamp\"),\n                    \"dID\",\n                    posexplode(split(expr(\"substring(d, 2, length(d)-2)\"), \",\")),\n                )\n                .select(\n                    to_timestamp(\"timestamp\").alias(\"EventTime\"),\n                    col(\"dID\"),\n                    col(\"pos\").cast(\"string\"),\n                    col(\"col\").alias(\"Value\"),\n                )\n                .withColumn(\n                    \"TagName\",\n                    concat(\n                        col(\"dID\"),\n                        lit(\":\"),\n                        udf(lambda row: mapping[row][\"TagName\"])(col(\"pos\")),\n                    ),\n                )\n                .withColumn(\n                    \"ValueType\", udf(lambda row: mapping[row][\"ValueType\"])(col(\"pos\"))\n                )\n                .withColumn(\"Status\", lit(self.status_null_value))\n                .withColumn(\"ChangeType\", lit(self.change_type_value))\n            )\n            return df.select(\n                \"EventTime\", \"TagName\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n            )\n        else:\n            return logging.exception(\n                \"The wrong version was specified. Please use the latest version\"\n            )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/sem_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.sem_json_to_pcdm.SEMJsonToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/sem_json_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/sem_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.sem_json_to_pcdm.SEMJsonToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the specified column converted to PCDM</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/sem_json_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the specified column converted to PCDM\n    \"\"\"\n    if self.version == 10:\n        mapping = obc_field_mappings.OBC_FIELD_MAPPINGS_V10\n        df = (\n            self.data.withColumn(\n                self.source_column_name,\n                from_json(self.source_column_name, SEM_SCHEMA),\n            )\n            .select(self.source_column_name + \".readings\")\n            .melt(\n                ids=[\"readings.resourceName\"],\n                values=[\"readings.value\"],\n                variableColumnName=\"var\",\n                valueColumnName=\"value\",\n            )\n            .drop(\"var\")\n            .select(map_from_arrays(\"resourceName\", \"value\").alias(\"resourceName\"))\n            .select(\"resourceName.dID\", \"resourceName.d\", \"resourceName.t\")\n            .select(\n                regexp_replace(col(\"t\").cast(\"string\"), \"(\\d{10})(\\d+)\", \"$1.$2\")\n                .cast(\"double\")\n                .alias(\"timestamp\"),\n                \"dID\",\n                posexplode(split(expr(\"substring(d, 2, length(d)-2)\"), \",\")),\n            )\n            .select(\n                to_timestamp(\"timestamp\").alias(\"EventTime\"),\n                col(\"dID\"),\n                col(\"pos\").cast(\"string\"),\n                col(\"col\").alias(\"Value\"),\n            )\n            .withColumn(\n                \"TagName\",\n                concat(\n                    col(\"dID\"),\n                    lit(\":\"),\n                    udf(lambda row: mapping[row][\"TagName\"])(col(\"pos\")),\n                ),\n            )\n            .withColumn(\n                \"ValueType\", udf(lambda row: mapping[row][\"ValueType\"])(col(\"pos\"))\n            )\n            .withColumn(\"Status\", lit(self.status_null_value))\n            .withColumn(\"ChangeType\", lit(self.change_type_value))\n        )\n        return df.select(\n            \"EventTime\", \"TagName\", \"Status\", \"Value\", \"ValueType\", \"ChangeType\"\n        )\n    else:\n        return logging.exception(\n            \"The wrong version was specified. Please use the latest version\"\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/ssip_pi_binary_file_to_pcdm/","title":"Convert SSIP PI Binary File data to the Process Control Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/ssip_pi_binary_file_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.ssip_pi_binary_file_to_pcdm.SSIPPIBinaryFileToPCDMTransformer","title":"<code>SSIPPIBinaryFileToPCDMTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a Spark DataFrame column containing binaryFile parquet data to the Process Control Data Model.</p> <p>This DataFrame should contain a path and the binary data. Typically this can be done using the Autoloader source component and specify \"binaryFile\" as the format.</p> <p>For more information about the SSIP PI Batch Connector, please see here.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/ssip_pi_binary_file_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.ssip_pi_binary_file_to_pcdm.SSIPPIBinaryFileToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import SSIPPIBinaryFileToPCDMTransformer\n\nssip_pi_binary_file_to_pcdm_transformer = SSIPPIBinaryFileToPCDMTransformer(\n    data=df\n)\n\nresult = ssip_pi_binary_file_to_pcdm_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame containing the path and binaryFile data</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ssip_pi_binary_file_to_pcdm.py</code> <pre><code>class SSIPPIBinaryFileToPCDMTransformer(TransformerInterface):\n    \"\"\"\n    Converts a Spark DataFrame column containing binaryFile parquet data to the Process Control Data Model.\n\n    This DataFrame should contain a path and the binary data. Typically this can be done using the Autoloader source component and specify \"binaryFile\" as the format.\n\n    For more information about the SSIP PI Batch Connector, please see [here.](https://bakerhughesc3.ai/oai-solution/shell-sensor-intelligence-platform/)\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import SSIPPIBinaryFileToPCDMTransformer\n\n    ssip_pi_binary_file_to_pcdm_transformer = SSIPPIBinaryFileToPCDMTransformer(\n        data=df\n    )\n\n    result = ssip_pi_binary_file_to_pcdm_transformer.transform()\n    ```\n\n    Parameters:\n        data (DataFrame): DataFrame containing the path and binaryFile data\n    \"\"\"\n\n    data: DataFrame\n\n    def __init__(self, data: DataFrame) -&gt; None:\n        self.data = data\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_pypi_library(get_default_package(\"pyarrow\"))\n        libraries.add_pypi_library(get_default_package(\"pandas\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    @staticmethod\n    def _convert_binary_to_pandas(pdf):\n        try:\n            binary_list = pdf.values.tolist()\n            binary_data = binary_list[0][3]\n            buf = pa.py_buffer(binary_data)\n            table = pq.read_table(buf)\n        except Exception as e:\n            print(str(e))\n            return pd.DataFrame(\n                {\n                    \"EventDate\": pd.Series([], dtype=\"datetime64[ns]\"),\n                    \"TagName\": pd.Series([], dtype=\"str\"),\n                    \"EventTime\": pd.Series([], dtype=\"datetime64[ns]\"),\n                    \"Status\": pd.Series([], dtype=\"str\"),\n                    \"Value\": pd.Series([], dtype=\"str\"),\n                    \"ValueType\": pd.Series([], dtype=\"str\"),\n                    \"ChangeType\": pd.Series([], dtype=\"str\"),\n                }\n            )\n\n        output_pdf = table.to_pandas()\n\n        if \"ValueType\" not in output_pdf.columns:\n            value_type = str(table.schema.field(\"Value\").type)\n            if value_type == \"int16\" or value_type == \"int32\":\n                value_type = \"integer\"\n            output_pdf[\"ValueType\"] = value_type\n\n        if \"ChangeType\" not in output_pdf.columns:\n            output_pdf[\"ChangeType\"] = \"insert\"\n\n        output_pdf[\"EventDate\"] = output_pdf[\"EventTime\"].dt.date\n        output_pdf[\"Value\"] = output_pdf[\"Value\"].astype(str)\n        output_pdf = output_pdf[\n            [\n                \"EventDate\",\n                \"TagName\",\n                \"EventTime\",\n                \"Status\",\n                \"Value\",\n                \"ValueType\",\n                \"ChangeType\",\n            ]\n        ]\n        return output_pdf\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the provided Binary data convert to PCDM\n        \"\"\"\n        return self.data.groupBy(\"path\").applyInPandas(\n            SSIPPIBinaryFileToPCDMTransformer._convert_binary_to_pandas,\n            schema=\"EventDate DATE, TagName STRING, EventTime TIMESTAMP, Status STRING, Value STRING, ValueType STRING, ChangeType STRING\",\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/ssip_pi_binary_file_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.ssip_pi_binary_file_to_pcdm.SSIPPIBinaryFileToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ssip_pi_binary_file_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/ssip_pi_binary_file_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.ssip_pi_binary_file_to_pcdm.SSIPPIBinaryFileToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the provided Binary data convert to PCDM</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ssip_pi_binary_file_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the provided Binary data convert to PCDM\n    \"\"\"\n    return self.data.groupBy(\"path\").applyInPandas(\n        SSIPPIBinaryFileToPCDMTransformer._convert_binary_to_pandas,\n        schema=\"EventDate DATE, TagName STRING, EventTime TIMESTAMP, Status STRING, Value STRING, ValueType STRING, ChangeType STRING\",\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/ssip_pi_binary_json_to_pcdm/","title":"Convert SSIP PI Binary JSON data to the Process Control Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/ssip_pi_binary_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.ssip_pi_binary_json_to_pcdm.SSIPPIJsonStreamToPCDMTransformer","title":"<code>SSIPPIJsonStreamToPCDMTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a Spark DataFrame containing Binary JSON data and related Properties to the Process Control Data Model</p> <p>For more information about the SSIP PI Streaming Connector, please see here.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/ssip_pi_binary_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.ssip_pi_binary_json_to_pcdm.SSIPPIJsonStreamToPCDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import SSIPPIJsonStreamToPCDMTransformer\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nssip_pi_json_stream_to_pcdm_transformer = SSIPPIJsonStreamToPCDMTransformer(\n    spark=spark,\n    data=df,\n    source_column_name=\"body\",\n    properties_column_name=\"\",\n    metadata_delta_table=None\n)\n\nresult = ssip_pi_json_stream_to_pcdm_transformer.transform()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session</p> required <code>data</code> <code>DataFrame</code> <p>DataFrame containing the path and binaryFile data</p> required <code>source_column_name</code> <code>str</code> <p>Spark Dataframe column containing the Binary json data</p> required <code>properties_column_name</code> <code>str</code> <p>Spark Dataframe struct typed column containing an element with the PointType</p> required <code>metadata_delta_table</code> <code>(optional, str)</code> <p>Name of a metadata table that can be used for PointType mappings</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ssip_pi_binary_json_to_pcdm.py</code> <pre><code>class SSIPPIJsonStreamToPCDMTransformer(TransformerInterface):\n    \"\"\"\n    Converts a Spark DataFrame containing Binary JSON data and related Properties to the Process Control Data Model\n\n    For more information about the SSIP PI Streaming Connector, please see [here.](https://bakerhughesc3.ai/oai-solution/shell-sensor-intelligence-platform/)\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import SSIPPIJsonStreamToPCDMTransformer\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    ssip_pi_json_stream_to_pcdm_transformer = SSIPPIJsonStreamToPCDMTransformer(\n        spark=spark,\n        data=df,\n        source_column_name=\"body\",\n        properties_column_name=\"\",\n        metadata_delta_table=None\n    )\n\n    result = ssip_pi_json_stream_to_pcdm_transformer.transform()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session\n        data (DataFrame): DataFrame containing the path and binaryFile data\n        source_column_name (str): Spark Dataframe column containing the Binary json data\n        properties_column_name (str): Spark Dataframe struct typed column containing an element with the PointType\n        metadata_delta_table (optional, str): Name of a metadata table that can be used for PointType mappings\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    source_column_name: str\n    properties_column_name: str\n    metadata_delta_table: str\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n        source_column_name: str,\n        properties_column_name: str,\n        metadata_delta_table: str = None,\n    ) -&gt; None:\n        self.spark = spark\n        self.data = data\n        self.source_column_name = source_column_name\n        self.properties_column_name = properties_column_name\n        self.metadata_delta_table = metadata_delta_table\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the provided Binary data converted to PCDM\n        \"\"\"\n        df = (\n            self.data.withColumn(\n                self.source_column_name, col(self.source_column_name).cast(\"string\")\n            )\n            .withColumn(\n                \"EventDate\",\n                get_json_object(col(self.source_column_name), \"$.EventTime\").cast(\n                    \"date\"\n                ),\n            )\n            .withColumn(\n                \"TagName\",\n                get_json_object(col(self.source_column_name), \"$.TagName\").cast(\n                    \"string\"\n                ),\n            )\n            .withColumn(\n                \"EventTime\",\n                get_json_object(col(self.source_column_name), \"$.EventTime\").cast(\n                    \"timestamp\"\n                ),\n            )\n            .withColumn(\n                \"Status\",\n                get_json_object(col(self.source_column_name), \"$.Quality\").cast(\n                    \"string\"\n                ),\n            )\n            .withColumn(\n                \"Value\",\n                get_json_object(col(self.source_column_name), \"$.Value\").cast(\"string\"),\n            )\n            .withColumn(\n                \"PointType\", element_at(col(self.properties_column_name), \"PointType\")\n            )\n            .withColumn(\n                \"Action\",\n                element_at(col(self.properties_column_name), \"Action\").cast(\"string\"),\n            )\n        )\n\n        if self.metadata_delta_table != None:\n            metadata_df = SparkDeltaSource(\n                self.spark, {}, self.metadata_delta_table\n            ).read_batch()\n            metadata_df = metadata_df.select(\n                \"TagName\", col(\"PointType\").alias(\"MetadataPointType\")\n            )\n            df = df.join(metadata_df, (df.TagName == metadata_df.TagName), \"left\")\n            df = df.withColumn(\n                \"PointType\",\n                (when(col(\"PointType\").isNull(), col(\"MetadataPointType\"))).otherwise(\n                    col(\"PointType\")\n                ),\n            )\n\n        return (\n            df.withColumn(\n                \"ValueType\",\n                (\n                    when(col(\"PointType\") == \"Digital\", \"string\")\n                    .when(col(\"PointType\") == \"String\", \"string\")\n                    .when(col(\"PointType\") == \"Float16\", \"float\")\n                    .when(col(\"PointType\") == \"Float32\", \"float\")\n                    .when(col(\"PointType\") == \"Float64\", \"float\")\n                    .when(col(\"PointType\") == \"Int16\", \"integer\")\n                    .when(col(\"PointType\") == \"Int32\", \"integer\")\n                    .otherwise(\"string\")\n                ),\n            )\n            .selectExpr(\n                \"*\",\n                \"CASE WHEN ValueType = 'integer' THEN try_cast(Value as integer) END as Value_Integer\",\n                \"CASE WHEN ValueType = 'float' THEN try_cast(Value as float) END as Value_Float\",\n            )\n            .withColumn(\n                \"ValueType\",\n                when(\n                    (col(\"Value_Integer\").isNull()) &amp; (col(\"ValueType\") == \"integer\"),\n                    \"string\",\n                )\n                .when(\n                    (col(\"Value_Float\").isNull()) &amp; (col(\"ValueType\") == \"float\"),\n                    \"string\",\n                )\n                .otherwise(col(\"ValueType\")),\n            )\n            .withColumn(\n                \"ChangeType\",\n                (\n                    when(col(\"Action\") == \"Insert\", \"insert\")\n                    .when(col(\"Action\") == \"Add\", \"insert\")\n                    .when(col(\"Action\") == \"Delete\", \"delete\")\n                    .when(col(\"Action\") == \"Update\", \"update\")\n                    .when(col(\"Action\") == \"Refresh\", \"update\")\n                ),\n            )\n            .select(\n                col(\"EventDate\"),\n                col(\"TagName\"),\n                col(\"EventTime\"),\n                col(\"Status\"),\n                col(\"Value\"),\n                col(\"ValueType\"),\n                col(\"ChangeType\"),\n            )\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/ssip_pi_binary_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.ssip_pi_binary_json_to_pcdm.SSIPPIJsonStreamToPCDMTransformer.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ssip_pi_binary_json_to_pcdm.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/ssip_pi_binary_json_to_pcdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.ssip_pi_binary_json_to_pcdm.SSIPPIJsonStreamToPCDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the provided Binary data converted to PCDM</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ssip_pi_binary_json_to_pcdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the provided Binary data converted to PCDM\n    \"\"\"\n    df = (\n        self.data.withColumn(\n            self.source_column_name, col(self.source_column_name).cast(\"string\")\n        )\n        .withColumn(\n            \"EventDate\",\n            get_json_object(col(self.source_column_name), \"$.EventTime\").cast(\n                \"date\"\n            ),\n        )\n        .withColumn(\n            \"TagName\",\n            get_json_object(col(self.source_column_name), \"$.TagName\").cast(\n                \"string\"\n            ),\n        )\n        .withColumn(\n            \"EventTime\",\n            get_json_object(col(self.source_column_name), \"$.EventTime\").cast(\n                \"timestamp\"\n            ),\n        )\n        .withColumn(\n            \"Status\",\n            get_json_object(col(self.source_column_name), \"$.Quality\").cast(\n                \"string\"\n            ),\n        )\n        .withColumn(\n            \"Value\",\n            get_json_object(col(self.source_column_name), \"$.Value\").cast(\"string\"),\n        )\n        .withColumn(\n            \"PointType\", element_at(col(self.properties_column_name), \"PointType\")\n        )\n        .withColumn(\n            \"Action\",\n            element_at(col(self.properties_column_name), \"Action\").cast(\"string\"),\n        )\n    )\n\n    if self.metadata_delta_table != None:\n        metadata_df = SparkDeltaSource(\n            self.spark, {}, self.metadata_delta_table\n        ).read_batch()\n        metadata_df = metadata_df.select(\n            \"TagName\", col(\"PointType\").alias(\"MetadataPointType\")\n        )\n        df = df.join(metadata_df, (df.TagName == metadata_df.TagName), \"left\")\n        df = df.withColumn(\n            \"PointType\",\n            (when(col(\"PointType\").isNull(), col(\"MetadataPointType\"))).otherwise(\n                col(\"PointType\")\n            ),\n        )\n\n    return (\n        df.withColumn(\n            \"ValueType\",\n            (\n                when(col(\"PointType\") == \"Digital\", \"string\")\n                .when(col(\"PointType\") == \"String\", \"string\")\n                .when(col(\"PointType\") == \"Float16\", \"float\")\n                .when(col(\"PointType\") == \"Float32\", \"float\")\n                .when(col(\"PointType\") == \"Float64\", \"float\")\n                .when(col(\"PointType\") == \"Int16\", \"integer\")\n                .when(col(\"PointType\") == \"Int32\", \"integer\")\n                .otherwise(\"string\")\n            ),\n        )\n        .selectExpr(\n            \"*\",\n            \"CASE WHEN ValueType = 'integer' THEN try_cast(Value as integer) END as Value_Integer\",\n            \"CASE WHEN ValueType = 'float' THEN try_cast(Value as float) END as Value_Float\",\n        )\n        .withColumn(\n            \"ValueType\",\n            when(\n                (col(\"Value_Integer\").isNull()) &amp; (col(\"ValueType\") == \"integer\"),\n                \"string\",\n            )\n            .when(\n                (col(\"Value_Float\").isNull()) &amp; (col(\"ValueType\") == \"float\"),\n                \"string\",\n            )\n            .otherwise(col(\"ValueType\")),\n        )\n        .withColumn(\n            \"ChangeType\",\n            (\n                when(col(\"Action\") == \"Insert\", \"insert\")\n                .when(col(\"Action\") == \"Add\", \"insert\")\n                .when(col(\"Action\") == \"Delete\", \"delete\")\n                .when(col(\"Action\") == \"Update\", \"update\")\n                .when(col(\"Action\") == \"Refresh\", \"update\")\n            ),\n        )\n        .select(\n            col(\"EventDate\"),\n            col(\"TagName\"),\n            col(\"EventTime\"),\n            col(\"Status\"),\n            col(\"Value\"),\n            col(\"ValueType\"),\n            col(\"ChangeType\"),\n        )\n    )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/ecmwf/nc_extractbase_to_weather_data_model/","title":"NC Forecast Extract Base To Weather Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/ecmwf/nc_extractbase_to_weather_data_model/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.ecmwf.nc_extractbase_to_weather_data_model.ECMWFExtractBaseToWeatherDataModel","title":"<code>ECMWFExtractBaseToWeatherDataModel</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Base class for extracting forecast data downloaded in .nc format from ECMWF MARS Server.</p> <p>Parameters:</p> Name Type Description Default <code>load_path</code> <code>str</code> <p>Path to local directory where the nc files will be stored, in format \"yyyy-mm-dd_HH.nc\"</p> required <code>date_start</code> <code>str</code> <p>Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format</p> required <code>date_end</code> <code>str</code> <p>End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format</p> required <code>run_frequency</code> <code>str</code> <p>Frequency format of runs to download, e.g. \"H\"</p> required <code>run_interval</code> <code>str</code> <p>Interval of runs, e.g. a run_frequency of \"H\" and run_interval of \"12\" will extract the data of the 00 and 12 run for each day.</p> required <code>lat</code> <code>DataArray</code> <p>Latitude values to extract from nc files</p> required <code>lon</code> <code>DataArray</code> <p>Longitude values to extract from nc files</p> required <code>utc</code> <code>bool = True</code> <p>Whether to convert the time to UTC or not</p> <code>True</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ecmwf/nc_extractbase_to_weather_data_model.py</code> <pre><code>class ECMWFExtractBaseToWeatherDataModel(TransformerInterface):\n    \"\"\"\n    Base class for extracting forecast data downloaded in .nc format from ECMWF MARS Server.\n\n    Args:\n        load_path (str): Path to local directory where the nc files will be stored, in format \"yyyy-mm-dd_HH.nc\"\n        date_start (str): Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format\n        date_end (str): End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format\n        run_frequency (str):Frequency format of runs to download, e.g. \"H\"\n        run_interval (str): Interval of runs, e.g. a run_frequency of \"H\" and run_interval of \"12\" will extract the data of the 00 and 12 run for each day.\n        lat (DataArray): Latitude values to extract from nc files\n        lon (DataArray): Longitude values to extract from nc files\n        utc (bool = True): Whether to convert the time to UTC or not\n    \"\"\"\n\n    def __init__(\n        self,\n        load_path: str,\n        date_start: str,\n        date_end: str,\n        run_interval: str,\n        run_frequency: str,\n        lat: xr.DataArray,\n        lon: xr.DataArray,\n        utc: bool = True,\n    ):\n        self.load_path = load_path\n        self.lat = lat\n        self.lon = lon\n        self.date_start = date_start\n        self.date_end = date_end\n        self.run_frequency = run_frequency\n        self.run_interval = run_interval\n        self.utc = utc\n        self.dates = pd.date_range(\n            start=self.date_start,\n            end=self.date_end,\n            freq=self.run_interval + self.run_frequency,\n        )\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self):\n        return True\n\n    @staticmethod\n    def _convert_ws_tag_names(x: list):\n        \"\"\"\n        Converts the tag names of wind speed from the format used in the nc files to the format used in the weather data model.\n\n        Args:\n            x (list): List of variable names of raw tags to be extracted from the nc files\n\n        Returns:\n            new_tags(list): List of variable names of raw tags to be extracted from the nc files, converted to the format used in the weather data model.\n        \"\"\"\n        convert_dict = {\n            \"10u\": \"u10\",\n            \"100u\": \"u100\",\n            \"200u\": \"u200\",\n            \"10v\": \"v10\",\n            \"100v\": \"v100\",\n            \"200v\": \"v200\",\n        }\n        new_tags = [convert_dict[i] if i in convert_dict.keys() else i for i in x]\n        return new_tags\n\n    def transform(\n        self, tag_prefix: str, variables: list, method: str = \"nearest\"\n    ) -&gt; pd.DataFrame:\n        \"\"\"Extract raw data from stored nc filed downloaded via ECMWF MARS.\n\n        Args:\n            tag_prefix (str): Prefix of the tag names of raw tags to be added to the dataframe\n            variables (list): List of variable names of raw tags to be extracted from the nc files\n            method (str, optional): The method used to match latitude/longitude in xarray using .sel(), by default \"nearest\"\n\n        Returns:\n            df (pd.DataFrame): Raw data extracted with lat, lon, run_time, target_time as a pd.multiindex and variables as columns.\n        \"\"\"\n        df = []\n        # e.g. 10u variable is saved as u10 in the file...\n        vars_processed = self._convert_ws_tag_names(variables)\n\n        for i in self.dates:\n            filename = f\"{str(i.date())}_{i.hour:02}.nc\"\n            fullpath = os.path.join(self.load_path, filename)\n            ds = xr.open_dataset(fullpath)\n            tmp = (\n                ds[vars_processed]\n                .sel(latitude=self.lat, longitude=self.lon, method=method)\n                .to_dataframe()\n            )\n            tmp[\"run_time\"] = i\n            df.append(tmp)\n            ds.close()\n\n        df = pd.concat(df, axis=0)\n\n        df = df.rename_axis(\n            index={\n                \"time\": \"target_time\",\n                \"latitude\": \"lat\",\n                \"longitude\": \"lon\",\n            }\n        )\n\n        df = df.reset_index([\"lat\", \"lon\"])\n        df[[\"lat\", \"lon\"]] = df[[\"lat\", \"lon\"]].apply(\n            lambda x: np.round(x.astype(float), 5)\n        )\n\n        if \"level\" in df.index.names:\n            index_names = [\"lat\", \"lon\", \"level\", \"run_time\", \"target_time\"]\n        else:\n            index_names = [\"lat\", \"lon\", \"run_time\", \"target_time\"]\n        df = df.reset_index().set_index(index_names)\n\n        if self.utc:\n            df = df.tz_localize(\"UTC\", level=\"target_time\")\n            df = df.tz_localize(\"UTC\", level=\"run_time\")\n\n        df = df[~(df.index.duplicated(keep=\"first\"))]\n        df = df.sort_index(axis=0)\n        df = df.sort_index(axis=1)\n\n        df_new = df.reset_index()\n\n        df_new = df_new.rename(\n            columns={\n                \"lat\": \"Latitude\",\n                \"lon\": \"Longitude\",\n                \"run_time\": \"EnqueuedTime\",\n                \"target_time\": \"EventTime\",\n            }\n        )\n\n        df_new = (\n            df_new.set_index([\"Latitude\", \"Longitude\", \"EnqueuedTime\", \"EventTime\"])[\n                vars_processed\n            ]\n            .rename_axis(\"Measure\", axis=1)\n            .stack()\n            .reset_index(name=\"Value\")\n        )\n\n        df_new[\"Source\"] = \"ECMWF_MARS\"\n        df_new[\"Status\"] = \"Good\"\n        df_new[\"Latest\"] = True\n        df_new[\"EventDate\"] = pd.to_datetime(df_new[\"EventTime\"]).dt.date\n        df_new[\"TagName\"] = (\n            tag_prefix\n            + df_new[\"Latitude\"].astype(str)\n            + \"_\"\n            + df_new[\"Longitude\"].astype(str)\n            + \"_\"\n            + df_new[\"Source\"]\n            + \"_\"\n            + df_new[\"Measure\"]\n        )\n        df_final = df_new.drop(\"Measure\", axis=1)\n\n        return df_final\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/ecmwf/nc_extractbase_to_weather_data_model/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.ecmwf.nc_extractbase_to_weather_data_model.ECMWFExtractBaseToWeatherDataModel.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ecmwf/nc_extractbase_to_weather_data_model.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/ecmwf/nc_extractbase_to_weather_data_model/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.ecmwf.nc_extractbase_to_weather_data_model.ECMWFExtractBaseToWeatherDataModel.transform","title":"<code>transform(tag_prefix, variables, method='nearest')</code>","text":"<p>Extract raw data from stored nc filed downloaded via ECMWF MARS.</p> <p>Parameters:</p> Name Type Description Default <code>tag_prefix</code> <code>str</code> <p>Prefix of the tag names of raw tags to be added to the dataframe</p> required <code>variables</code> <code>list</code> <p>List of variable names of raw tags to be extracted from the nc files</p> required <code>method</code> <code>str</code> <p>The method used to match latitude/longitude in xarray using .sel(), by default \"nearest\"</p> <code>'nearest'</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Raw data extracted with lat, lon, run_time, target_time as a pd.multiindex and variables as columns.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ecmwf/nc_extractbase_to_weather_data_model.py</code> <pre><code>def transform(\n    self, tag_prefix: str, variables: list, method: str = \"nearest\"\n) -&gt; pd.DataFrame:\n    \"\"\"Extract raw data from stored nc filed downloaded via ECMWF MARS.\n\n    Args:\n        tag_prefix (str): Prefix of the tag names of raw tags to be added to the dataframe\n        variables (list): List of variable names of raw tags to be extracted from the nc files\n        method (str, optional): The method used to match latitude/longitude in xarray using .sel(), by default \"nearest\"\n\n    Returns:\n        df (pd.DataFrame): Raw data extracted with lat, lon, run_time, target_time as a pd.multiindex and variables as columns.\n    \"\"\"\n    df = []\n    # e.g. 10u variable is saved as u10 in the file...\n    vars_processed = self._convert_ws_tag_names(variables)\n\n    for i in self.dates:\n        filename = f\"{str(i.date())}_{i.hour:02}.nc\"\n        fullpath = os.path.join(self.load_path, filename)\n        ds = xr.open_dataset(fullpath)\n        tmp = (\n            ds[vars_processed]\n            .sel(latitude=self.lat, longitude=self.lon, method=method)\n            .to_dataframe()\n        )\n        tmp[\"run_time\"] = i\n        df.append(tmp)\n        ds.close()\n\n    df = pd.concat(df, axis=0)\n\n    df = df.rename_axis(\n        index={\n            \"time\": \"target_time\",\n            \"latitude\": \"lat\",\n            \"longitude\": \"lon\",\n        }\n    )\n\n    df = df.reset_index([\"lat\", \"lon\"])\n    df[[\"lat\", \"lon\"]] = df[[\"lat\", \"lon\"]].apply(\n        lambda x: np.round(x.astype(float), 5)\n    )\n\n    if \"level\" in df.index.names:\n        index_names = [\"lat\", \"lon\", \"level\", \"run_time\", \"target_time\"]\n    else:\n        index_names = [\"lat\", \"lon\", \"run_time\", \"target_time\"]\n    df = df.reset_index().set_index(index_names)\n\n    if self.utc:\n        df = df.tz_localize(\"UTC\", level=\"target_time\")\n        df = df.tz_localize(\"UTC\", level=\"run_time\")\n\n    df = df[~(df.index.duplicated(keep=\"first\"))]\n    df = df.sort_index(axis=0)\n    df = df.sort_index(axis=1)\n\n    df_new = df.reset_index()\n\n    df_new = df_new.rename(\n        columns={\n            \"lat\": \"Latitude\",\n            \"lon\": \"Longitude\",\n            \"run_time\": \"EnqueuedTime\",\n            \"target_time\": \"EventTime\",\n        }\n    )\n\n    df_new = (\n        df_new.set_index([\"Latitude\", \"Longitude\", \"EnqueuedTime\", \"EventTime\"])[\n            vars_processed\n        ]\n        .rename_axis(\"Measure\", axis=1)\n        .stack()\n        .reset_index(name=\"Value\")\n    )\n\n    df_new[\"Source\"] = \"ECMWF_MARS\"\n    df_new[\"Status\"] = \"Good\"\n    df_new[\"Latest\"] = True\n    df_new[\"EventDate\"] = pd.to_datetime(df_new[\"EventTime\"]).dt.date\n    df_new[\"TagName\"] = (\n        tag_prefix\n        + df_new[\"Latitude\"].astype(str)\n        + \"_\"\n        + df_new[\"Longitude\"].astype(str)\n        + \"_\"\n        + df_new[\"Source\"]\n        + \"_\"\n        + df_new[\"Measure\"]\n    )\n    df_final = df_new.drop(\"Measure\", axis=1)\n\n    return df_final\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/ecmwf/nc_extractgrid_to_weather_data_model/","title":"NC Forecast Extract Grid To Weather Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/ecmwf/nc_extractgrid_to_weather_data_model/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.ecmwf.nc_extractgrid_to_weather_data_model.ECMWFExtractGridToWeatherDataModel","title":"<code>ECMWFExtractGridToWeatherDataModel</code>","text":"<p>               Bases: <code>ECMWFExtractBaseToWeatherDataModel</code></p> <p>Extract a grid from a local .nc file downloaded from ECMWF via MARS</p> <p>Parameters:</p> Name Type Description Default <code>lat_min</code> <code>float</code> <p>Minimum latitude of grid to extract</p> required <code>lat_max</code> <code>float</code> <p>Maximum latitude of grid to extract</p> required <code>lon_min</code> <code>float</code> <p>Minimum longitude of grid to extract</p> required <code>lon_max</code> <code>float</code> <p>Maximum longitude of grid to extract</p> required <code>grid_step</code> <code>float</code> <p>The grid length to use to define the grid, e.g. 0.1.</p> required <code>load_path</code> <code>str</code> <p>Path to local directory with nc files downloaded in format \"yyyy-mm-dd_HH.nc\"</p> required <code>date_start</code> <code>str</code> <p>Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format</p> required <code>date_end</code> <code>str</code> <p>End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format</p> required <code>run_frequency</code> <code>str</code> <p>Frequency format of runs to download, e.g. \"H\"</p> required <code>run_interval</code> <code>str</code> <p>Interval of runs, e.g. a run_frequency of \"H\" and run_interval of \"12\" will extract the data of the 00 and 12 run for each day.</p> required <code>utc</code> <code>bool</code> <p>Add utc to the datetime indexes? Defaults to True.</p> <code>True</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ecmwf/nc_extractgrid_to_weather_data_model.py</code> <pre><code>class ECMWFExtractGridToWeatherDataModel(ECMWFExtractBaseToWeatherDataModel):\n    \"\"\"Extract a grid from a local .nc file downloaded from ECMWF via MARS\n\n    Args:\n        lat_min (float): Minimum latitude of grid to extract\n        lat_max (float): Maximum latitude of grid to extract\n        lon_min (float): Minimum longitude of grid to extract\n        lon_max (float): Maximum longitude of grid to extract\n        grid_step (float): The grid length to use to define the grid, e.g. 0.1.\n        load_path (str): Path to local directory with nc files downloaded in format \"yyyy-mm-dd_HH.nc\"\n        date_start (str): Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format\n        date_end (str): End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format\n        run_frequency (str): Frequency format of runs to download, e.g. \"H\"\n        run_interval (str): Interval of runs, e.g. a run_frequency of \"H\" and run_interval of \"12\" will extract the data of the 00 and 12 run for each day.\n        utc (bool, optional): Add utc to the datetime indexes? Defaults to True.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        lat_min: float,\n        lat_max: float,\n        lon_min: float,\n        lon_max: float,\n        grid_step: float,\n        load_path: str,\n        date_start: str,\n        date_end: str,\n        run_interval: str,\n        run_frequency: str,\n        utc: bool = True,\n    ):\n        # hmm careful with floating points, this seems to work ok...\n        lat_xr = xr.DataArray(\n            np.linspace(\n                lat_min, lat_max, int(np.round((lat_max - lat_min) / grid_step)) + 1\n            ),\n            dims=[\"latitude\"],\n        )\n        lon_xr = xr.DataArray(\n            np.linspace(\n                lon_min, lon_max, int(np.round((lon_max - lon_min) / grid_step)) + 1\n            ),\n            dims=[\"longitude\"],\n        )\n\n        self.load_path = load_path\n        self.lat_min = lat_min\n        self.lat_max = lat_max\n        self.lon_min = lon_min\n        self.lon_max = lon_max\n        self.grid_step = grid_step\n        self.lat = lat_xr\n        self.lon = lon_xr\n        self.date_start = date_start\n        self.date_end = date_end\n        self.run_frequency = run_frequency\n        self.run_interval = run_interval\n        self.utc = utc\n\n        super(ECMWFExtractGridToWeatherDataModel, self).__init__(\n            lat=lat_xr,\n            lon=lon_xr,\n            load_path=load_path,\n            date_start=date_start,\n            date_end=date_end,\n            run_interval=run_interval,\n            run_frequency=run_frequency,\n            utc=utc,\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/ecmwf/nc_extractpoint_to_weather_data_model/","title":"NC Forecast Extract Point To Weather Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/ecmwf/nc_extractpoint_to_weather_data_model/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.ecmwf.nc_extractpoint_to_weather_data_model.ECMWFExtractPointToWeatherDataModel","title":"<code>ECMWFExtractPointToWeatherDataModel</code>","text":"<p>               Bases: <code>ECMWFExtractBaseToWeatherDataModel</code></p> <p>Extract a single point from a local .nc file downloaded from ECMWF via MARS</p> <p>Parameters:</p> Name Type Description Default <code>lat</code> <code>float</code> <p>Latitude of point to extract</p> required <code>lon</code> <code>float</code> <p>Longitude of point to extract</p> required <code>load_path</code> <code>str</code> <p>Path to local directory with nc files downloaded in format \"yyyy-mm-dd_HH.nc\"</p> required <code>date_start</code> <code>str</code> <p>Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format</p> required <code>date_end</code> <code>str</code> <p>End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format</p> required <code>run_frequency</code> <code>str</code> <p>Frequency format of runs to download, e.g. \"H\"</p> required <code>run_interval</code> <code>str</code> <p>Interval of runs, e.g. a run_frequency of \"H\" and run_interval of \"12\" will extract the data of the 00 and 12 run for each day.</p> required <code>utc</code> <code>bool</code> <p>Add utc to the datetime indexes? Defaults to True.</p> <code>True</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/ecmwf/nc_extractpoint_to_weather_data_model.py</code> <pre><code>class ECMWFExtractPointToWeatherDataModel(ECMWFExtractBaseToWeatherDataModel):\n    \"\"\"\n    Extract a single point from a local .nc file downloaded from ECMWF via MARS\n\n    Args:\n        lat (float): Latitude of point to extract\n        lon (float): Longitude of point to extract\n        load_path (str): Path to local directory with nc files downloaded in format \"yyyy-mm-dd_HH.nc\"\n        date_start (str): Start date of extraction in \"YYYY-MM-DD HH:MM:SS\" format\n        date_end (str): End date of extraction in \"YYYY-MM-DD HH:MM:SS\" format\n        run_frequency (str): Frequency format of runs to download, e.g. \"H\"\n        run_interval (str): Interval of runs, e.g. a run_frequency of \"H\" and run_interval of \"12\" will extract the data of the 00 and 12 run for each day.\n        utc (bool, optional): Add utc to the datetime indexes? Defaults to True.\n    \"\"\"\n\n    def __init__(\n        self,\n        lat: float,\n        lon: float,\n        load_path: str,\n        date_start: str,\n        date_end: str,\n        run_interval: str,\n        run_frequency: str,\n        utc: bool = True,\n    ):\n        lat_xr = xr.DataArray([lat], dims=[\"latitude\"])\n        lon_xr = xr.DataArray([lon], dims=[\"longitude\"])\n\n        self.lat = lat_xr\n        self.lon = lon_xr\n        self.load_path = load_path\n        self.date_start = date_start\n        self.date_end = date_end\n        self.run_frequency = run_frequency\n        self.run_interval = run_interval\n        self.utc = utc\n\n        super(ECMWFExtractPointToWeatherDataModel, self).__init__(\n            lat=lat_xr,\n            lon=lon_xr,\n            load_path=load_path,\n            date_start=date_start,\n            date_end=date_end,\n            run_interval=run_interval,\n            run_frequency=run_frequency,\n            utc=utc,\n        )\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/caiso_to_mdm/","title":"CAISO To Meters Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/caiso_to_mdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.iso.caiso_to_mdm.CAISOToMDMTransformer","title":"<code>CAISOToMDMTransformer</code>","text":"<p>               Bases: <code>BaseRawToMDMTransformer</code></p> <p>Converts CAISO Raw data into Meters Data Model.</p> <p>Please check the BaseRawToMDMTransformer for the required arguments and methods.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/caiso_to_mdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.iso.caiso_to_mdm.CAISOToMDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import CAISOToMDMTransformer\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\ncaiso_to_mdm_transformer = CAISOToMDMTransformer(\n    spark=spark,\n    data=df,\n    output_type=\"usage\",\n    name=None,\n    description=None,\n    value_type=None,\n    version=None,\n    series_id=None,\n    series_parent_id=None\n)\n\nresult = caiso_to_mdm_transformer.transform()\n</code></pre> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/iso/caiso_to_mdm.py</code> <pre><code>class CAISOToMDMTransformer(BaseRawToMDMTransformer):\n    \"\"\"\n    Converts CAISO Raw data into Meters Data Model.\n\n    Please check the BaseRawToMDMTransformer for the required arguments and methods.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import CAISOToMDMTransformer\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    caiso_to_mdm_transformer = CAISOToMDMTransformer(\n        spark=spark,\n        data=df,\n        output_type=\"usage\",\n        name=None,\n        description=None,\n        value_type=None,\n        version=None,\n        series_id=None,\n        series_parent_id=None\n    )\n\n    result = caiso_to_mdm_transformer.transform()\n    ```\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    input_schema = CAISO_SCHEMA\n    uid_col = \"TacAreaName\"\n    series_id_col = \"'series_std_001'\"\n    timestamp_col = \"to_timestamp(StartTime)\"\n    interval_timestamp_col = \"Timestamp + INTERVAL 1 HOURS\"\n    value_col = \"Load\"\n    series_parent_id_col = \"'series_parent_std_001'\"\n    name_col = \"'CAISO API'\"\n    uom_col = \"'mwh'\"\n    description_col = \"'CAISO data pulled from CAISO ISO API'\"\n    timestamp_start_col = \"StartTime\"\n    timestamp_end_col = \"StartTime + INTERVAL 1 HOURS\"\n    time_zone_col = \"'PST'\"\n    version_col = \"'1'\"\n    series_type = SeriesType.Hour\n    model_type = ModelType.Default\n    value_type = ValueType.Usage\n    properties_col = \"null\"\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/ercot_to_mdm/","title":"ERCOT To Meters Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/ercot_to_mdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.iso.ercot_to_mdm.ERCOTToMDMTransformer","title":"<code>ERCOTToMDMTransformer</code>","text":"<p>               Bases: <code>BaseRawToMDMTransformer</code></p> <p>Converts ERCOT Raw data into Meters Data Model.</p> <p>Please check the BaseRawToMDMTransformer for the required arguments and methods.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/ercot_to_mdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.iso.ercot_to_mdm.ERCOTToMDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import ERCOTToMDMTransformer\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nercot_to_mdm_transformer = ERCOTToMDMTransformer(\n    spark=spark,\n    data=df,\n    output_type=\"usage\",\n    name=None,\n    description=None,\n    value_type=None,\n    version=None,\n    series_id=None,\n    series_parent_id=None\n)\n\nresult = ercot_to_mdm_transformer.transform()\n</code></pre> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/iso/ercot_to_mdm.py</code> <pre><code>class ERCOTToMDMTransformer(BaseRawToMDMTransformer):\n    \"\"\"\n    Converts ERCOT Raw data into Meters Data Model.\n\n    Please check the BaseRawToMDMTransformer for the required arguments and methods.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import ERCOTToMDMTransformer\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    ercot_to_mdm_transformer = ERCOTToMDMTransformer(\n        spark=spark,\n        data=df,\n        output_type=\"usage\",\n        name=None,\n        description=None,\n        value_type=None,\n        version=None,\n        series_id=None,\n        series_parent_id=None\n    )\n\n    result = ercot_to_mdm_transformer.transform()\n    ```\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    input_schema = ERCOT_SCHEMA\n    uid_col = \"variable\"\n    series_id_col = \"'series_std_001'\"\n    timestamp_col = \"to_utc_timestamp(StartTime, 'America/Chicago')\"\n    interval_timestamp_col = \"Timestamp + INTERVAL 1 HOURS\"\n    value_col = \"value\"\n    series_parent_id_col = \"'series_parent_std_001'\"\n    name_col = \"'ERCOT API'\"\n    uom_col = \"'mwh'\"\n    description_col = \"'ERCOT data pulled from ERCOT ISO API'\"\n    timestamp_start_col = \"StartTime\"\n    timestamp_end_col = \"StartTime + INTERVAL 1 HOURS\"\n    time_zone_col = \"'America/Chicago'\"\n    version_col = \"'1'\"\n    series_type = SeriesType.Hour\n    model_type = ModelType.Default\n    value_type = ValueType.Usage\n    properties_col = \"null\"\n\n    def _pre_process(self) -&gt; DataFrame:\n        df: DataFrame = super(ERCOTToMDMTransformer, self)._pre_process()\n        df = melt(\n            df,\n            id_vars=[\"Date\", \"HourEnding\", \"DstFlag\"],\n            value_vars=[\n                \"Coast\",\n                \"East\",\n                \"FarWest\",\n                \"North\",\n                \"NorthCentral\",\n                \"SouthCentral\",\n                \"Southern\",\n                \"West\",\n                \"SystemTotal\",\n            ],\n        )\n        df = df.withColumn(\n            \"StartTime\",\n            F.expr(\n                \"Date + MAKE_INTERVAL(0,0,0,0,cast(split(HourEnding,':')[0] as integer),0,0)\"\n            ),\n        )\n        return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/miso_to_mdm/","title":"MISO To Meters Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/miso_to_mdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.iso.miso_to_mdm.MISOToMDMTransformer","title":"<code>MISOToMDMTransformer</code>","text":"<p>               Bases: <code>BaseRawToMDMTransformer</code></p> <p>Converts MISO Raw data into Meters Data Model.</p> <p>Please check the BaseRawToMDMTransformer for the required arguments and methods.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/miso_to_mdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.iso.miso_to_mdm.MISOToMDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import MISOToMDMTransformer\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nmiso_to_mdm_transformer = MISOToMDMTransformer(\n    spark=spark,\n    data=df,\n    output_type=\"usage\",\n    name=None,\n    description=None,\n    value_type=None,\n    version=None,\n    series_id=None,\n    series_parent_id=None\n)\n\nresult = miso_to_mdm_transformer.transform()\n</code></pre> BaseRawToMDMTransformer Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/iso/miso_to_mdm.py</code> <pre><code>class MISOToMDMTransformer(BaseRawToMDMTransformer):\n    \"\"\"\n    Converts MISO Raw data into Meters Data Model.\n\n    Please check the BaseRawToMDMTransformer for the required arguments and methods.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import MISOToMDMTransformer\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    miso_to_mdm_transformer = MISOToMDMTransformer(\n        spark=spark,\n        data=df,\n        output_type=\"usage\",\n        name=None,\n        description=None,\n        value_type=None,\n        version=None,\n        series_id=None,\n        series_parent_id=None\n    )\n\n    result = miso_to_mdm_transformer.transform()\n    ```\n\n    BaseRawToMDMTransformer:\n        ::: src.sdk.python.rtdip_sdk.pipelines.transformers.spark.base_raw_to_mdm\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    input_schema = MISO_SCHEMA\n    uid_col = \"variable\"\n    series_id_col = \"'series_std_001'\"\n    timestamp_col = \"to_utc_timestamp(Datetime, 'US/Central')\"\n    interval_timestamp_col = \"Timestamp + INTERVAL 1 HOURS\"\n    value_col = \"bround(value, 2)\"\n    series_parent_id_col = \"'series_parent_std_001'\"\n    name_col = \"'Miso API'\"\n    uom_col = \"'mwh'\"\n    description_col = \"'Miso data pulled from Miso ISO API'\"\n    timestamp_start_col = \"Datetime\"\n    timestamp_end_col = \"Datetime + INTERVAL 1 HOURS\"\n    time_zone_col = \"'US/Central'\"\n    version_col = \"'1'\"\n    series_type = SeriesType.Hour\n    model_type = ModelType.Default\n    value_type = ValueType.Usage\n    properties_col = \"null\"\n\n    def _pre_process(self) -&gt; DataFrame:\n        df: DataFrame = super(MISOToMDMTransformer, self)._pre_process()\n        df = melt(\n            df,\n            id_vars=[\"Datetime\"],\n            value_vars=[\n                \"Lrz1\",\n                \"Lrz2_7\",\n                \"Lrz3_5\",\n                \"Lrz4\",\n                \"Lrz6\",\n                \"Lrz8_9_10\",\n                \"Miso\",\n            ],\n        )\n        return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/miso_to_mdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.base_raw_to_mdm.BaseRawToMDMTransformer","title":"<code>BaseRawToMDMTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Base class for all the Raw to Meters Data Model Transformers.</p> Meters Data Model requires two outputs <ul> <li><code>UsageData</code> : To store measurement(value) as timeseries data.</li> <li><code>MetaData</code> : To store meters related meta information.</li> </ul> <p>It supports the generation of both the outputs as they share some common properties.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance.</p> required <code>data</code> <code>DataFrame</code> <p>Dataframe containing the raw MISO data.</p> required <code>output_type</code> <code>str</code> <p>Must be one of <code>usage</code> or <code>meta</code>.</p> required <code>name</code> <code>str</code> <p>Set this to override default <code>name</code> column.</p> <code>None</code> <code>description</code> <code>str</code> <p>Set this to override default <code>description</code> column.</p> <code>None</code> <code>value_type</code> <code>ValueType</code> <p>Set this to override default <code>value_type</code> column.</p> <code>None</code> <code>version</code> <code>str</code> <p>Set this to override default <code>version</code> column.</p> <code>None</code> <code>series_id</code> <code>str</code> <p>Set this to override default <code>series_id</code> column.</p> <code>None</code> <code>series_parent_id</code> <code>str</code> <p>Set this to override default <code>series_parent_id</code> column.</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/base_raw_to_mdm.py</code> <pre><code>class BaseRawToMDMTransformer(TransformerInterface):\n    \"\"\"\n    Base class for all the Raw to Meters Data Model Transformers.\n\n    Meters Data Model requires two outputs:\n        - `UsageData` : To store measurement(value) as timeseries data.\n        - `MetaData` : To store meters related meta information.\n\n    It supports the generation of both the outputs as they share some common properties.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance.\n        data (DataFrame): Dataframe containing the raw MISO data.\n        output_type (str): Must be one of `usage` or `meta`.\n        name (str): Set this to override default `name` column.\n        description (str): Set this to override default `description` column.\n        value_type (ValueType): Set this to override default `value_type` column.\n        version (str): Set this to override default `version` column.\n        series_id (str): Set this to override default `series_id` column.\n        series_parent_id (str): Set this to override default `series_parent_id` column.\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    output_type: str\n    input_schema: StructType\n    target_schema: StructType\n    uid_col: str\n    series_id_col: str\n    timestamp_col: str\n    interval_timestamp_col: str\n    value_col: str\n    series_parent_id_col: str\n    name_col: str\n    uom_col: str\n    description_col: str\n    timestamp_start_col: str\n    timestamp_end_col: str\n    time_zone_col: str\n    version_col: str\n    series_type: SeriesType\n    model_type: ModelType\n    value_type: ValueType\n    properties_col: str\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n        output_type: str,\n        name: str = None,\n        description: str = None,\n        value_type: ValueType = None,\n        version: str = None,\n        series_id: str = None,\n        series_parent_id: str = None,\n    ):\n        self.spark = spark\n        self.data = data\n        self.output_type = output_type\n        self.name = name if name is not None else self.name_col\n        self.description = (\n            description if description is not None else self.description_col\n        )\n        self.value_type = value_type if value_type is not None else self.value_type\n        self.version = version if version is not None else self.version_col\n        self.series_id = series_id if series_id is not None else self.series_id_col\n        self.series_parent_id = (\n            series_parent_id\n            if series_parent_id is not None\n            else self.series_parent_id_col\n        )\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self) -&gt; bool:\n        valid_output_types = [\"usage\", \"meta\"]\n        if self.output_type not in valid_output_types:\n            raise ValueError(\n                f\"Invalid output_type `{self.output_type}` given. Must be one of {valid_output_types}\"\n            )\n\n        assert str(self.data.schema) == str(self.input_schema)\n        assert type(self.series_type).__name__ == SeriesType.__name__\n        assert type(self.model_type).__name__ == ModelType.__name__\n        assert type(self.value_type).__name__ == ValueType.__name__\n        return True\n\n    def post_transform_validation(self) -&gt; bool:\n        assert str(self.data.schema) == str(self.target_schema)\n        return True\n\n    def _get_transformed_df(self) -&gt; DataFrame:\n        if self.output_type == \"usage\":\n            self.target_schema = MDM_USAGE_SCHEMA\n            return self._get_usage_transformed_df()\n        else:\n            self.target_schema = MDM_META_SCHEMA\n            return self._get_meta_transformed_df()\n\n    def _convert_into_target_schema(self) -&gt; None:\n        \"\"\"\n        Converts a Spark DataFrame structure into new structure based on the Target Schema.\n\n        Returns: Nothing.\n\n        \"\"\"\n\n        df: DataFrame = self.data\n        df = df.select(self.target_schema.names)\n\n        for field in self.target_schema.fields:\n            df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n\n        self.data = self.spark.createDataFrame(df.rdd, self.target_schema)\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the raw data converted into MDM.\n        \"\"\"\n\n        self.pre_transform_validation()\n        self.data = self._get_transformed_df()\n        self._convert_into_target_schema()\n        self.post_transform_validation()\n\n        return self.data\n\n    def _add_uid_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Uid\", expr(self.uid_col))\n\n    def _add_series_id_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"SeriesId\", expr(self.series_id))\n\n    def _add_timestamp_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Timestamp\", expr(self.timestamp_col))\n\n    def _add_interval_timestamp_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"IntervalTimestamp\", expr(self.interval_timestamp_col))\n\n    def _add_value_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Value\", expr(self.value_col))\n\n    def _add_series_parent_id_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"SeriesParentId\", expr(self.series_parent_id))\n\n    def _add_name_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Name\", expr(self.name))\n\n    def _add_uom_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Uom\", expr(self.uom_col))\n\n    def _add_description_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Description\", expr(self.description))\n\n    def _add_timestamp_start_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"TimestampStart\", expr(self.timestamp_start_col))\n\n    def _add_timestamp_end_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"TimestampEnd\", expr(self.timestamp_end_col))\n\n    def _add_time_zone_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Timezone\", expr(self.time_zone_col))\n\n    def _add_version_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Version\", expr(self.version))\n\n    def _add_series_type_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"SeriesType\", lit(self.series_type.value))\n\n    def _add_model_type_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"ModelType\", lit(self.model_type.value))\n\n    def _add_value_type_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"ValueType\", lit(self.value_type.value))\n\n    def _add_properties_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Properties\", expr(self.properties_col))\n\n    def _pre_process(self) -&gt; DataFrame:\n        return self.data\n\n    @staticmethod\n    def _post_process(df: DataFrame) -&gt; DataFrame:\n        return df\n\n    def _get_usage_transformed_df(self) -&gt; DataFrame:\n        df = self._pre_process()\n\n        df = self._add_uid_column(df)\n        df = self._add_series_id_column(df)\n        df = self._add_timestamp_column(df)\n        df = self._add_interval_timestamp_column(df)\n        df = self._add_value_column(df)\n\n        df = self._post_process(df)\n\n        return df\n\n    def _get_meta_transformed_df(self) -&gt; DataFrame:\n        df = self._pre_process()\n\n        df = self._add_uid_column(df)\n        df = self._add_series_id_column(df)\n        df = self._add_series_parent_id_column(df)\n        df = self._add_name_column(df)\n        df = self._add_uom_column(df)\n        df = self._add_description_column(df)\n        df = self._add_timestamp_start_column(df)\n        df = self._add_timestamp_end_column(df)\n        df = self._add_time_zone_column(df)\n        df = self._add_version_column(df)\n        df = self._add_series_type_column(df)\n        df = self._add_model_type_column(df)\n        df = self._add_value_type_column(df)\n        df = self._add_properties_column(df)\n\n        df = self._post_process(df)\n\n        return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/miso_to_mdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.base_raw_to_mdm.BaseRawToMDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the raw data converted into MDM.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/base_raw_to_mdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the raw data converted into MDM.\n    \"\"\"\n\n    self.pre_transform_validation()\n    self.data = self._get_transformed_df()\n    self._convert_into_target_schema()\n    self.post_transform_validation()\n\n    return self.data\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/pjm_to_mdm/","title":"PJM To Meters Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/pjm_to_mdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.iso.pjm_to_mdm.PJMToMDMTransformer","title":"<code>PJMToMDMTransformer</code>","text":"<p>               Bases: <code>BaseRawToMDMTransformer</code></p> <p>Converts PJM Raw data into Meters Data Model.</p> <p>Please check the BaseRawToMDMTransformer for the required arguments and methods.</p>"},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/pjm_to_mdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.iso.pjm_to_mdm.PJMToMDMTransformer--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.transformers import PJMToMDMTransformer\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\npjm_to_mdm_transformer = PJMToMDMTransformer(\n    spark=spark,\n    data=df,\n    output_type=\"usage\",\n    name=None,\n    description=None,\n    value_type=None,\n    version=None,\n    series_id=None,\n    series_parent_id=None\n)\n\nresult = pjm_to_mdm_transformer.transform()\n</code></pre> BaseRawToMDMTransformer Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/iso/pjm_to_mdm.py</code> <pre><code>class PJMToMDMTransformer(BaseRawToMDMTransformer):\n    \"\"\"\n    Converts PJM Raw data into Meters Data Model.\n\n    Please check the BaseRawToMDMTransformer for the required arguments and methods.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.transformers import PJMToMDMTransformer\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    pjm_to_mdm_transformer = PJMToMDMTransformer(\n        spark=spark,\n        data=df,\n        output_type=\"usage\",\n        name=None,\n        description=None,\n        value_type=None,\n        version=None,\n        series_id=None,\n        series_parent_id=None\n    )\n\n    result = pjm_to_mdm_transformer.transform()\n    ```\n\n    BaseRawToMDMTransformer:\n        ::: src.sdk.python.rtdip_sdk.pipelines.transformers.spark.base_raw_to_mdm\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    input_schema = PJM_SCHEMA\n    uid_col = \"Zone\"\n    series_id_col = \"'series_std_001'\"\n    timestamp_col = \"to_utc_timestamp(StartTime, 'America/New_York')\"\n    interval_timestamp_col = \"Timestamp + INTERVAL 1 HOURS\"\n    value_col = \"bround(Load, 2)\"\n    series_parent_id_col = \"'series_parent_std_001'\"\n    name_col = \"'PJM API'\"\n    uom_col = \"'mwh'\"\n    description_col = \"'PJM data pulled from PJM ISO API'\"\n    timestamp_start_col = \"StartTime\"\n    timestamp_end_col = \"StartTime + INTERVAL 1 HOURS\"\n    time_zone_col = \"'America/New_York'\"\n    version_col = \"'1'\"\n    series_type = SeriesType.Hour\n    model_type = ModelType.Default\n    value_type = ValueType.Usage\n    properties_col = \"null\"\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/pjm_to_mdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.base_raw_to_mdm.BaseRawToMDMTransformer","title":"<code>BaseRawToMDMTransformer</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Base class for all the Raw to Meters Data Model Transformers.</p> Meters Data Model requires two outputs <ul> <li><code>UsageData</code> : To store measurement(value) as timeseries data.</li> <li><code>MetaData</code> : To store meters related meta information.</li> </ul> <p>It supports the generation of both the outputs as they share some common properties.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance.</p> required <code>data</code> <code>DataFrame</code> <p>Dataframe containing the raw MISO data.</p> required <code>output_type</code> <code>str</code> <p>Must be one of <code>usage</code> or <code>meta</code>.</p> required <code>name</code> <code>str</code> <p>Set this to override default <code>name</code> column.</p> <code>None</code> <code>description</code> <code>str</code> <p>Set this to override default <code>description</code> column.</p> <code>None</code> <code>value_type</code> <code>ValueType</code> <p>Set this to override default <code>value_type</code> column.</p> <code>None</code> <code>version</code> <code>str</code> <p>Set this to override default <code>version</code> column.</p> <code>None</code> <code>series_id</code> <code>str</code> <p>Set this to override default <code>series_id</code> column.</p> <code>None</code> <code>series_parent_id</code> <code>str</code> <p>Set this to override default <code>series_parent_id</code> column.</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/base_raw_to_mdm.py</code> <pre><code>class BaseRawToMDMTransformer(TransformerInterface):\n    \"\"\"\n    Base class for all the Raw to Meters Data Model Transformers.\n\n    Meters Data Model requires two outputs:\n        - `UsageData` : To store measurement(value) as timeseries data.\n        - `MetaData` : To store meters related meta information.\n\n    It supports the generation of both the outputs as they share some common properties.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance.\n        data (DataFrame): Dataframe containing the raw MISO data.\n        output_type (str): Must be one of `usage` or `meta`.\n        name (str): Set this to override default `name` column.\n        description (str): Set this to override default `description` column.\n        value_type (ValueType): Set this to override default `value_type` column.\n        version (str): Set this to override default `version` column.\n        series_id (str): Set this to override default `series_id` column.\n        series_parent_id (str): Set this to override default `series_parent_id` column.\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n    output_type: str\n    input_schema: StructType\n    target_schema: StructType\n    uid_col: str\n    series_id_col: str\n    timestamp_col: str\n    interval_timestamp_col: str\n    value_col: str\n    series_parent_id_col: str\n    name_col: str\n    uom_col: str\n    description_col: str\n    timestamp_start_col: str\n    timestamp_end_col: str\n    time_zone_col: str\n    version_col: str\n    series_type: SeriesType\n    model_type: ModelType\n    value_type: ValueType\n    properties_col: str\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n        output_type: str,\n        name: str = None,\n        description: str = None,\n        value_type: ValueType = None,\n        version: str = None,\n        series_id: str = None,\n        series_parent_id: str = None,\n    ):\n        self.spark = spark\n        self.data = data\n        self.output_type = output_type\n        self.name = name if name is not None else self.name_col\n        self.description = (\n            description if description is not None else self.description_col\n        )\n        self.value_type = value_type if value_type is not None else self.value_type\n        self.version = version if version is not None else self.version_col\n        self.series_id = series_id if series_id is not None else self.series_id_col\n        self.series_parent_id = (\n            series_parent_id\n            if series_parent_id is not None\n            else self.series_parent_id_col\n        )\n\n    @staticmethod\n    def system_type():\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self) -&gt; bool:\n        valid_output_types = [\"usage\", \"meta\"]\n        if self.output_type not in valid_output_types:\n            raise ValueError(\n                f\"Invalid output_type `{self.output_type}` given. Must be one of {valid_output_types}\"\n            )\n\n        assert str(self.data.schema) == str(self.input_schema)\n        assert type(self.series_type).__name__ == SeriesType.__name__\n        assert type(self.model_type).__name__ == ModelType.__name__\n        assert type(self.value_type).__name__ == ValueType.__name__\n        return True\n\n    def post_transform_validation(self) -&gt; bool:\n        assert str(self.data.schema) == str(self.target_schema)\n        return True\n\n    def _get_transformed_df(self) -&gt; DataFrame:\n        if self.output_type == \"usage\":\n            self.target_schema = MDM_USAGE_SCHEMA\n            return self._get_usage_transformed_df()\n        else:\n            self.target_schema = MDM_META_SCHEMA\n            return self._get_meta_transformed_df()\n\n    def _convert_into_target_schema(self) -&gt; None:\n        \"\"\"\n        Converts a Spark DataFrame structure into new structure based on the Target Schema.\n\n        Returns: Nothing.\n\n        \"\"\"\n\n        df: DataFrame = self.data\n        df = df.select(self.target_schema.names)\n\n        for field in self.target_schema.fields:\n            df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n\n        self.data = self.spark.createDataFrame(df.rdd, self.target_schema)\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A dataframe with the raw data converted into MDM.\n        \"\"\"\n\n        self.pre_transform_validation()\n        self.data = self._get_transformed_df()\n        self._convert_into_target_schema()\n        self.post_transform_validation()\n\n        return self.data\n\n    def _add_uid_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Uid\", expr(self.uid_col))\n\n    def _add_series_id_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"SeriesId\", expr(self.series_id))\n\n    def _add_timestamp_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Timestamp\", expr(self.timestamp_col))\n\n    def _add_interval_timestamp_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"IntervalTimestamp\", expr(self.interval_timestamp_col))\n\n    def _add_value_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Value\", expr(self.value_col))\n\n    def _add_series_parent_id_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"SeriesParentId\", expr(self.series_parent_id))\n\n    def _add_name_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Name\", expr(self.name))\n\n    def _add_uom_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Uom\", expr(self.uom_col))\n\n    def _add_description_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Description\", expr(self.description))\n\n    def _add_timestamp_start_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"TimestampStart\", expr(self.timestamp_start_col))\n\n    def _add_timestamp_end_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"TimestampEnd\", expr(self.timestamp_end_col))\n\n    def _add_time_zone_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Timezone\", expr(self.time_zone_col))\n\n    def _add_version_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Version\", expr(self.version))\n\n    def _add_series_type_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"SeriesType\", lit(self.series_type.value))\n\n    def _add_model_type_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"ModelType\", lit(self.model_type.value))\n\n    def _add_value_type_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"ValueType\", lit(self.value_type.value))\n\n    def _add_properties_column(self, df: DataFrame) -&gt; DataFrame:\n        return df.withColumn(\"Properties\", expr(self.properties_col))\n\n    def _pre_process(self) -&gt; DataFrame:\n        return self.data\n\n    @staticmethod\n    def _post_process(df: DataFrame) -&gt; DataFrame:\n        return df\n\n    def _get_usage_transformed_df(self) -&gt; DataFrame:\n        df = self._pre_process()\n\n        df = self._add_uid_column(df)\n        df = self._add_series_id_column(df)\n        df = self._add_timestamp_column(df)\n        df = self._add_interval_timestamp_column(df)\n        df = self._add_value_column(df)\n\n        df = self._post_process(df)\n\n        return df\n\n    def _get_meta_transformed_df(self) -&gt; DataFrame:\n        df = self._pre_process()\n\n        df = self._add_uid_column(df)\n        df = self._add_series_id_column(df)\n        df = self._add_series_parent_id_column(df)\n        df = self._add_name_column(df)\n        df = self._add_uom_column(df)\n        df = self._add_description_column(df)\n        df = self._add_timestamp_start_column(df)\n        df = self._add_timestamp_end_column(df)\n        df = self._add_time_zone_column(df)\n        df = self._add_version_column(df)\n        df = self._add_series_type_column(df)\n        df = self._add_model_type_column(df)\n        df = self._add_value_type_column(df)\n        df = self._add_properties_column(df)\n\n        df = self._post_process(df)\n\n        return df\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/iso/pjm_to_mdm/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.base_raw_to_mdm.BaseRawToMDMTransformer.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe with the raw data converted into MDM.</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/base_raw_to_mdm.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A dataframe with the raw data converted into MDM.\n    \"\"\"\n\n    self.pre_transform_validation()\n    self.data = self._get_transformed_df()\n    self._convert_into_target_schema()\n    self.post_transform_validation()\n\n    return self.data\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/the_weather_company/raw_forecast_to_weather_data_model/","title":"Convert Forecast Raw JSON data to the Weather Data Model","text":""},{"location":"sdk/code-reference/pipelines/transformers/spark/the_weather_company/raw_forecast_to_weather_data_model/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.the_weather_company.raw_forecast_to_weather_data_model.RawForecastToWeatherDataModel","title":"<code>RawForecastToWeatherDataModel</code>","text":"<p>               Bases: <code>TransformerInterface</code></p> <p>Converts a raw forecast into weather data model.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session instance.</p> required <code>data</code> <code>DataFrame</code> <p>Dataframe to be transformed</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/the_weather_company/raw_forecast_to_weather_data_model.py</code> <pre><code>class RawForecastToWeatherDataModel(TransformerInterface):\n    \"\"\"\n    Converts a raw forecast into weather data model.\n\n    Parameters:\n        spark (SparkSession): Spark Session instance.\n        data (DataFrame): Dataframe to be transformed\n    \"\"\"\n\n    spark: SparkSession\n    data: DataFrame\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        data: DataFrame,\n    ) -&gt; None:\n        self.spark = spark\n        self.data = data\n        self.target_schema = WEATHER_DATA_MODEL\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def pre_transform_validation(self):\n        return True\n\n    def post_transform_validation(self) -&gt; bool:\n        assert str(self.data.schema) == str(self.target_schema)\n        return True\n\n    def _convert_into_target_schema(self) -&gt; None:\n        \"\"\"\n        Converts a Spark DataFrame structure into new structure based on the Target Schema.\n\n        Returns: Nothing.\n\n        \"\"\"\n\n        df: DataFrame = self.data\n        df = df.select(self.target_schema.names)\n\n        for field in self.target_schema.fields:\n            df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n\n        self.data = self.spark.createDataFrame(df.rdd, self.target_schema)\n\n    def transform(self) -&gt; DataFrame:\n        \"\"\"\n        Returns:\n            DataFrame: A Forecast dataframe converted into Weather Data Model\n        \"\"\"\n\n        self.pre_transform_validation()\n\n        processed_date = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        df = (\n            self.data.withColumn(\"WeatherDay\", substring(\"FcstValidLocal\", 0, 10))\n            .withColumn(\n                \"WeatherHour\",\n                (substring(\"FcstValidLocal\", 12, 2).cast(IntegerType()) + 1),\n            )\n            .withColumn(\"WeatherTimezoneOffset\", substring(\"FcstValidLocal\", 20, 5))\n            .withColumn(\"WeatherType\", lit(\"F\"))\n            .withColumn(\"ProcessedDate\", lit(processed_date))\n            .withColumnRenamed(\"Temp\", \"Temperature\")\n            .withColumnRenamed(\"Dewpt\", \"DewPoint\")\n            .withColumnRenamed(\"Rh\", \"Humidity\")\n            .withColumnRenamed(\"Hi\", \"HeatIndex\")\n            .withColumnRenamed(\"Wc\", \"WindChill\")\n            .withColumnRenamed(\"Wdir\", \"WindDirection\")\n            .withColumnRenamed(\"Wspd\", \"WindSpeed\")\n            .withColumnRenamed(\"Clds\", \"CloudCover\")\n            .withColumn(\"WetBulbTemp\", lit(\"\"))\n            .withColumn(\"SolarIrradiance\", lit(\"\"))\n            .withColumnRenamed(\"Qpf\", \"Precipitation\")\n            .withColumnRenamed(\"DayInd\", \"DayOrNight\")\n            .withColumnRenamed(\"Dow\", \"DayOfWeek\")\n            .withColumnRenamed(\"Gust\", \"WindGust\")\n            .withColumnRenamed(\"Mslp\", \"MslPressure\")\n            .withColumnRenamed(\"Num\", \"ForecastDayNum\")\n            .withColumnRenamed(\"Pop\", \"PropOfPrecip\")\n            .withColumnRenamed(\"PrecipType\", \"PrecipType\")\n            .withColumnRenamed(\"SnowQpf\", \"SnowAccumulation\")\n            .withColumnRenamed(\"UvIndex\", \"UvIndex\")\n            .withColumnRenamed(\"Vis\", \"Visibility\")\n        )\n\n        columns = df.columns\n        for column in columns:\n            df = df.withColumn(\n                column, when(col(column) == \"\", lit(None)).otherwise(col(column))\n            )\n\n        self.data = df\n        self._convert_into_target_schema()\n        self.post_transform_validation()\n\n        return self.data\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/the_weather_company/raw_forecast_to_weather_data_model/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.the_weather_company.raw_forecast_to_weather_data_model.RawForecastToWeatherDataModel.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/the_weather_company/raw_forecast_to_weather_data_model.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/transformers/spark/the_weather_company/raw_forecast_to_weather_data_model/#src.sdk.python.rtdip_sdk.pipelines.transformers.spark.the_weather_company.raw_forecast_to_weather_data_model.RawForecastToWeatherDataModel.transform","title":"<code>transform()</code>","text":"<p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A Forecast dataframe converted into Weather Data Model</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/transformers/spark/the_weather_company/raw_forecast_to_weather_data_model.py</code> <pre><code>def transform(self) -&gt; DataFrame:\n    \"\"\"\n    Returns:\n        DataFrame: A Forecast dataframe converted into Weather Data Model\n    \"\"\"\n\n    self.pre_transform_validation()\n\n    processed_date = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    df = (\n        self.data.withColumn(\"WeatherDay\", substring(\"FcstValidLocal\", 0, 10))\n        .withColumn(\n            \"WeatherHour\",\n            (substring(\"FcstValidLocal\", 12, 2).cast(IntegerType()) + 1),\n        )\n        .withColumn(\"WeatherTimezoneOffset\", substring(\"FcstValidLocal\", 20, 5))\n        .withColumn(\"WeatherType\", lit(\"F\"))\n        .withColumn(\"ProcessedDate\", lit(processed_date))\n        .withColumnRenamed(\"Temp\", \"Temperature\")\n        .withColumnRenamed(\"Dewpt\", \"DewPoint\")\n        .withColumnRenamed(\"Rh\", \"Humidity\")\n        .withColumnRenamed(\"Hi\", \"HeatIndex\")\n        .withColumnRenamed(\"Wc\", \"WindChill\")\n        .withColumnRenamed(\"Wdir\", \"WindDirection\")\n        .withColumnRenamed(\"Wspd\", \"WindSpeed\")\n        .withColumnRenamed(\"Clds\", \"CloudCover\")\n        .withColumn(\"WetBulbTemp\", lit(\"\"))\n        .withColumn(\"SolarIrradiance\", lit(\"\"))\n        .withColumnRenamed(\"Qpf\", \"Precipitation\")\n        .withColumnRenamed(\"DayInd\", \"DayOrNight\")\n        .withColumnRenamed(\"Dow\", \"DayOfWeek\")\n        .withColumnRenamed(\"Gust\", \"WindGust\")\n        .withColumnRenamed(\"Mslp\", \"MslPressure\")\n        .withColumnRenamed(\"Num\", \"ForecastDayNum\")\n        .withColumnRenamed(\"Pop\", \"PropOfPrecip\")\n        .withColumnRenamed(\"PrecipType\", \"PrecipType\")\n        .withColumnRenamed(\"SnowQpf\", \"SnowAccumulation\")\n        .withColumnRenamed(\"UvIndex\", \"UvIndex\")\n        .withColumnRenamed(\"Vis\", \"Visibility\")\n    )\n\n    columns = df.columns\n    for column in columns:\n        df = df.withColumn(\n            column, when(col(column) == \"\", lit(None)).otherwise(col(column))\n        )\n\n    self.data = df\n    self._convert_into_target_schema()\n    self.post_transform_validation()\n\n    return self.data\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/aws/s3_bucket_policy/","title":"S3 Bucket Policy","text":""},{"location":"sdk/code-reference/pipelines/utilities/aws/s3_bucket_policy/#src.sdk.python.rtdip_sdk.pipelines.utilities.aws.s3_bucket_policy.S3BucketPolicyUtility","title":"<code>S3BucketPolicyUtility</code>","text":"<p>               Bases: <code>UtilitiesInterface</code></p> <p>Assigns an IAM Bucket Policy to an S3 Bucket.</p>"},{"location":"sdk/code-reference/pipelines/utilities/aws/s3_bucket_policy/#src.sdk.python.rtdip_sdk.pipelines.utilities.aws.s3_bucket_policy.S3BucketPolicyUtility--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.utilities import S3BucketPolicyUtility\n\ns3_bucket_policy_utility = S3BucketPolicyUtility(\n    bucket_name=\"YOUR-BUCKET-NAME\",\n    aws_access_key_id=\"YOUR-AWS-ACCESS-KEY\",\n    aws_secret_access_key=\"YOUR-AWS-SECRET-ACCESS-KEY\",\n    aws_session_token=\"YOUR-AWS-SESSION-TOKEN\",\n    sid=\"YOUD-SID\",\n    effect=\"EFFECT\",\n    principal=\"PRINCIPAL\",\n    action=[\"ACTIONS\"],\n    resource=[\"RESOURCES\"]\n)\n\nresult = s3_bucket_policy_utility.execute()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>S3 Bucket Name</p> required <code>aws_access_key_id</code> <code>str</code> <p>AWS Access Key</p> required <code>aws_secret_access_key</code> <code>str</code> <p>AWS Secret Key</p> required <code>aws_session_token</code> <code>str</code> <p>AWS Session Token</p> required <code>sid</code> <code>str</code> <p>S3 Bucket Policy Sid to be updated</p> required <code>effect</code> <code>str</code> <p>Effect to be applied to the policy</p> required <code>principal</code> <code>str</code> <p>Principal to be applied to Policy</p> required <code>action</code> <code>list[str]</code> <p>List of actions to be applied to the policy</p> required <code>resource</code> <code>list[str]</code> <p>List of resources to be applied to the policy</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/aws/s3_bucket_policy.py</code> <pre><code>class S3BucketPolicyUtility(UtilitiesInterface):\n    \"\"\"\n    Assigns an IAM Bucket Policy to an S3 Bucket.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.utilities import S3BucketPolicyUtility\n\n    s3_bucket_policy_utility = S3BucketPolicyUtility(\n        bucket_name=\"YOUR-BUCKET-NAME\",\n        aws_access_key_id=\"YOUR-AWS-ACCESS-KEY\",\n        aws_secret_access_key=\"YOUR-AWS-SECRET-ACCESS-KEY\",\n        aws_session_token=\"YOUR-AWS-SESSION-TOKEN\",\n        sid=\"YOUD-SID\",\n        effect=\"EFFECT\",\n        principal=\"PRINCIPAL\",\n        action=[\"ACTIONS\"],\n        resource=[\"RESOURCES\"]\n    )\n\n    result = s3_bucket_policy_utility.execute()\n    ```\n\n    Parameters:\n        bucket_name (str): S3 Bucket Name\n        aws_access_key_id (str): AWS Access Key\n        aws_secret_access_key (str): AWS Secret Key\n        aws_session_token (str): AWS Session Token\n        sid (str): S3 Bucket Policy Sid to be updated\n        effect (str): Effect to be applied to the policy\n        principal (str): Principal to be applied to Policy\n        action (list[str]): List of actions to be applied to the policy\n        resource (list[str]): List of resources to be applied to the policy\n    \"\"\"\n\n    bucket_name: str\n    aws_access_key_id: str\n    aws_secret_access_key: str\n    aws_session_token: str\n    sid: str\n    effect: str\n    principal: str\n    action: List[str]\n    resource: List[str]\n\n    def __init__(\n        self,\n        bucket_name: str,\n        aws_access_key_id: str,\n        aws_secret_access_key: str,\n        aws_session_token: str,\n        sid: str,\n        principal: str,\n        effect: str,\n        action: List[str],\n        resource: List[str],\n    ) -&gt; None:\n        self.bucket_name = bucket_name\n        self.aws_access_key_id = aws_access_key_id\n        self.aws_secret_access_key = aws_secret_access_key\n        self.aws_session_token = aws_session_token\n        self.sid = sid\n        self.effect = effect\n        self.principal = principal\n        self.action = action\n        self.resource = resource\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYTHON\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_pypi_library(get_default_package(\"aws_boto3\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def execute(self) -&gt; bool:\n        try:\n            s3_client = boto3.client(\n                \"s3\",\n                aws_access_key_id=self.aws_access_key_id,\n                aws_secret_access_key=self.aws_secret_access_key,\n                aws_session_token=self.aws_session_token,\n            )\n\n            bucket_policy = s3_client.get_bucket_policy(Bucket=self.bucket_name)\n\n            policy_statement = None\n            if \"Policy\" in bucket_policy and bucket_policy[\"Policy\"] != None:\n                policy_statement = json.loads(bucket_policy[\"Policy\"])\n\n            if policy_statement is None:\n                policy_statement = {\"Version\": \"2012-10-17\", \"Statement\": []}\n\n            sid_found = False\n            for statement in policy_statement[\"Statement\"]:\n                if statement[\"Sid\"] == self.sid:\n                    sid_found = True\n                    statement[\"Effect\"] = self.effect\n                    statement[\"Principal\"] = self.principal\n                    statement[\"Action\"] = self.action\n                    if isinstance(statement[\"Resource\"], list):\n                        statement[\"Resource\"] + self.resource\n                    else:\n                        self.resource.append(statement[\"Resource\"])\n                        statement[\"Resource\"] = self.resource\n                    statement[\"Resource\"] = list(set(statement[\"Resource\"]))\n\n            if not sid_found:\n                policy_statement[\"Statement\"].append(\n                    {\n                        \"Sid\": self.sid,\n                        \"Effect\": self.effect,\n                        \"Principal\": self.principal,\n                        \"Action\": self.action,\n                        \"Resource\": self.resource,\n                    }\n                )\n\n            policy = json.dumps(policy_statement)\n            s3_client.put_bucket_policy(Bucket=self.bucket_name, Policy=policy)\n\n            return True\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/aws/s3_bucket_policy/#src.sdk.python.rtdip_sdk.pipelines.utilities.aws.s3_bucket_policy.S3BucketPolicyUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYTHON</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/aws/s3_bucket_policy.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYTHON\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/aws/s3_copy_utility/","title":"S3 Copy Utility","text":""},{"location":"sdk/code-reference/pipelines/utilities/aws/s3_copy_utility/#src.sdk.python.rtdip_sdk.pipelines.utilities.aws.s3_copy_utility.S3CopyUtility","title":"<code>S3CopyUtility</code>","text":"<p>               Bases: <code>UtilitiesInterface</code></p> <p>Copies an object from S3 to S3, from Local to S3 and S3 to local depending on the source and destination uri.</p>"},{"location":"sdk/code-reference/pipelines/utilities/aws/s3_copy_utility/#src.sdk.python.rtdip_sdk.pipelines.utilities.aws.s3_copy_utility.S3CopyUtility--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.utilities import S3CopyUtility\n\ns3_copy_utility = S3CopyUtility(\n    source_uri=\"YOUR-SOURCE-URI\",\n    destination_uri=\"YOUR-DESTINATION-URI\",\n    source_version_id=\"YOUR-VERSION-ID\",\n    extra_args={},\n    callback=\"YOUD-SID\",\n    source_client=\"PRINCIPAL\",\n    transfer_config=[\"ACTIONS\"]\n)\n\nresult = s3_bucket_policy_utility.execute()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>source_uri</code> <code>str</code> <p>URI of the source object</p> required <code>destination_uri</code> <code>str</code> <p>URI of the destination object</p> required <code>source_version_id</code> <code>optional str</code> <p>Version ID of the source bucket</p> <code>None</code> <code>extra_args</code> <code>optional dict</code> <p>Extra arguments that can be passed to the client operation. See here for a list of download arguments</p> <code>None</code> <code>callback</code> <code>optional function</code> <p>Takes a UDF used for tracking the progress of the copy operation</p> <code>None</code> <code>source_client</code> <code>optional botocore or boto3 client</code> <p>A different S3 client to use for the source bucket during the copy operation</p> <code>None</code> <code>transfer_config</code> <code>optional class</code> <p>The transfer configuration used during the copy. See here for all parameters</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/aws/s3_copy_utility.py</code> <pre><code>class S3CopyUtility(UtilitiesInterface):\n    \"\"\"\n    Copies an object from S3 to S3, from Local to S3 and S3 to local depending on the source and destination uri.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.utilities import S3CopyUtility\n\n    s3_copy_utility = S3CopyUtility(\n        source_uri=\"YOUR-SOURCE-URI\",\n        destination_uri=\"YOUR-DESTINATION-URI\",\n        source_version_id=\"YOUR-VERSION-ID\",\n        extra_args={},\n        callback=\"YOUD-SID\",\n        source_client=\"PRINCIPAL\",\n        transfer_config=[\"ACTIONS\"]\n    )\n\n    result = s3_bucket_policy_utility.execute()\n    ```\n\n    Parameters:\n        source_uri (str): URI of the source object\n        destination_uri (str): URI of the destination object\n        source_version_id (optional str): Version ID of the source bucket\n        extra_args (optional dict): Extra arguments that can be passed to the client operation. See [here](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/customizations/s3.html#boto3.s3.transfer.S3Transfer.ALLOWED_DOWNLOAD_ARGS){ target=\"_blank\" } for a list of download arguments\n        callback (optional function): Takes a UDF used for tracking the progress of the copy operation\n        source_client (optional botocore or boto3 client): A different S3 client to use for the source bucket during the copy operation\n        transfer_config (optional class): The transfer configuration used during the copy. See [here](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/customizations/s3.html#boto3.s3.transfer.TransferConfig){ target=\"_blank\" } for all parameters\n\n    \"\"\"\n\n    source_uri: str\n    destination_uri: str\n    destination_key: str\n    extra_args: dict\n    callback: str\n    source_client: S3Transfer\n    transfer_config: Config\n\n    def __init__(\n        self,\n        source_uri: str,\n        destination_uri: str,\n        source_version_id: str = None,\n        extra_args: dict = None,\n        callback=None,\n        source_client: S3Transfer = None,\n        transfer_config: Config = None,\n    ):\n        self.source_uri = source_uri\n        self.destination_uri = destination_uri\n        self.source_version_id = source_version_id\n        self.extra_args = extra_args\n        self.callback = callback\n        self.source_client = source_client\n        self.transfer_config = transfer_config\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYTHON\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_pypi_library(get_default_package(\"aws_boto3\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def execute(self) -&gt; bool:\n        # S3 to S3 Copy\n        if self.source_uri.startswith(\n            storage_objects_utils.S3_SCHEME\n        ) and self.destination_uri.startswith(storage_objects_utils.S3_SCHEME):\n            schema, source_domain, source_key = storage_objects_utils.validate_uri(\n                self.source_uri\n            )\n            (\n                schema,\n                destination_domain,\n                destination_key,\n            ) = storage_objects_utils.validate_uri(self.destination_uri)\n\n            s3 = boto3.resource(schema)\n            copy_source = {\"Bucket\": source_domain, \"Key\": source_key}\n            if self.source_version_id is not None:\n                copy_source[\"VersionId\"] = self.source_version_id\n\n            try:\n                s3.meta.client.copy(\n                    copy_source,\n                    destination_domain,\n                    destination_key,\n                    self.extra_args,\n                    self.callback,\n                    self.source_client,\n                    self.transfer_config,\n                )\n\n            except Exception as ex:\n                logging.error(ex)\n                return False\n        # Local File to S3 Copy (Upload)\n        elif (os.path.isfile(self.source_uri)) and self.destination_uri.startswith(\n            storage_objects_utils.S3_SCHEME\n        ):\n            (\n                schema,\n                destination_domain,\n                destination_key,\n            ) = storage_objects_utils.validate_uri(self.destination_uri)\n\n            s3_client = boto3.client(schema)\n\n            try:\n                s3_client.upload_file(\n                    self.source_uri, destination_domain, destination_key\n                )\n            except Exception as ex:\n                logging.error(ex)\n                return False\n        # S3 to Local File Copy (Download)\n        elif self.source_uri.startswith(\n            storage_objects_utils.S3_SCHEME\n        ) and not self.destination_uri.startswith(storage_objects_utils.S3_SCHEME):\n            try:\n                schema, source_domain, source_key = storage_objects_utils.validate_uri(\n                    self.source_uri\n                )\n                s3 = boto3.client(schema)\n                s3.download_file(source_domain, source_key, self.destination_uri)\n            except Exception as ex:\n                logging.error(ex)\n                return False\n        else:\n            logging.error(\n                \"Not Implemented. From: %s \\n\\t to: %s\",\n                self.source_uri,\n                self.destination_uri,\n            )\n        return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/aws/s3_copy_utility/#src.sdk.python.rtdip_sdk.pipelines.utilities.aws.s3_copy_utility.S3CopyUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYTHON</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/aws/s3_copy_utility.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYTHON\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/azure/adls_gen2_acl/","title":"ADLS Gen 2 ACLs","text":""},{"location":"sdk/code-reference/pipelines/utilities/azure/adls_gen2_acl/#src.sdk.python.rtdip_sdk.pipelines.utilities.azure.adls_gen2_acl.ADLSGen2DirectoryACLUtility","title":"<code>ADLSGen2DirectoryACLUtility</code>","text":"<p>               Bases: <code>UtilitiesInterface</code></p> <p>Assigns Azure AD Groups to ACLs on directories in an Azure Data Lake Store Gen 2 storage account.</p>"},{"location":"sdk/code-reference/pipelines/utilities/azure/adls_gen2_acl/#src.sdk.python.rtdip_sdk.pipelines.utilities.azure.adls_gen2_acl.ADLSGen2DirectoryACLUtility--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.utilities import ADLSGen2DirectoryACLUtility\n\nadls_gen2_directory_acl_utility = ADLSGen2DirectoryACLUtility(\n    storage_account=\"YOUR-STORAGAE-ACCOUNT-NAME\",\n    container=\"YOUR-ADLS_CONTAINER_NAME\",\n    credential=\"YOUR-TOKEN-CREDENTIAL\",\n    directory=\"DIRECTORY\",\n    group_object_id=\"GROUP-OBJECT\",\n    folder_permissions=\"r-x\",\n    parent_folder_permissions=\"r-x\",\n    root_folder_permissions=\"r-x\",\n    set_as_default_acl=True,\n    create_directory_if_not_exists=True\n)\n\nresult = adls_gen2_directory_acl_utility.execute()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>storage_account</code> <code>str</code> <p>ADLS Gen 2 Storage Account Name</p> required <code>container</code> <code>str</code> <p>ADLS Gen 2 Container Name</p> required <code>credential</code> <code>TokenCredential</code> <p>Credentials to authenticate with ADLS Gen 2 Storage Account</p> required <code>directory</code> <code>str</code> <p>Directory to be assign ACLS to in an ADLS Gen 2</p> required <code>group_object_id</code> <code>str</code> <p>Azure AD Group Object ID to be assigned to Directory</p> required <code>folder_permissions</code> <code>(optional, str)</code> <p>Folder Permissions to Assign to directory</p> <code>'r-x'</code> <code>parent_folder_permissions</code> <code>(optional, str)</code> <p>Folder Permissions to Assign to parent directories. Parent Folder ACLs not set if None</p> <code>'r-x'</code> <code>root_folder_permissions</code> <code>(optional, str)</code> <p>Folder Permissions to Assign to root directory. Root Folder ACL not set if None</p> <code>'r-x'</code> <code>set_as_default_acl</code> <code>bool</code> <p>Sets the ACL as the default ACL on the folder</p> <code>True</code> <code>create_directory_if_not_exists</code> <code>bool</code> <p>Creates the directory(and Parent Directories) if it does not exist</p> <code>True</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/azure/adls_gen2_acl.py</code> <pre><code>class ADLSGen2DirectoryACLUtility(UtilitiesInterface):\n    \"\"\"\n    Assigns Azure AD Groups to ACLs on directories in an Azure Data Lake Store Gen 2 storage account.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.utilities import ADLSGen2DirectoryACLUtility\n\n    adls_gen2_directory_acl_utility = ADLSGen2DirectoryACLUtility(\n        storage_account=\"YOUR-STORAGAE-ACCOUNT-NAME\",\n        container=\"YOUR-ADLS_CONTAINER_NAME\",\n        credential=\"YOUR-TOKEN-CREDENTIAL\",\n        directory=\"DIRECTORY\",\n        group_object_id=\"GROUP-OBJECT\",\n        folder_permissions=\"r-x\",\n        parent_folder_permissions=\"r-x\",\n        root_folder_permissions=\"r-x\",\n        set_as_default_acl=True,\n        create_directory_if_not_exists=True\n    )\n\n    result = adls_gen2_directory_acl_utility.execute()\n    ```\n\n    Parameters:\n        storage_account (str): ADLS Gen 2 Storage Account Name\n        container (str): ADLS Gen 2 Container Name\n        credential (TokenCredential): Credentials to authenticate with ADLS Gen 2 Storage Account\n        directory (str): Directory to be assign ACLS to in an ADLS Gen 2\n        group_object_id (str): Azure AD Group Object ID to be assigned to Directory\n        folder_permissions (optional, str): Folder Permissions to Assign to directory\n        parent_folder_permissions (optional, str): Folder Permissions to Assign to parent directories. Parent Folder ACLs not set if None\n        root_folder_permissions (optional, str): Folder Permissions to Assign to root directory. Root Folder ACL not set if None\n        set_as_default_acl (bool, optional): Sets the ACL as the default ACL on the folder\n        create_directory_if_not_exists (bool, optional): Creates the directory(and Parent Directories) if it does not exist\n    \"\"\"\n\n    storage_account: str\n    container: str\n    credential: Union[\n        str,\n        Dict[str, str],\n        AzureNamedKeyCredential,\n        AzureSasCredential,\n        TokenCredential,\n        None,\n    ]\n    directory: str\n    group_object_id: str\n    folder_permissions: str\n    parent_folder_permissions: str\n    root_folder_permissions: str\n    set_as_default_acl: bool\n    create_directory_if_not_exists: bool\n\n    def __init__(\n        self,\n        storage_account: str,\n        container: str,\n        credential: Union[\n            str,\n            Dict[str, str],\n            AzureNamedKeyCredential,\n            AzureSasCredential,\n            TokenCredential,\n            None,\n        ],\n        directory: str,\n        group_object_id: str,\n        folder_permissions: str = \"r-x\",\n        parent_folder_permissions: Union[str, None] = \"r-x\",\n        root_folder_permissions: Union[str, None] = \"r-x\",\n        set_as_default_acl: bool = True,\n        create_directory_if_not_exists: bool = True,\n    ) -&gt; None:\n        self.storage_account = storage_account\n        self.container = container\n        self.credential = credential\n        self.directory = directory\n        self.group_object_id = group_object_id\n        self.folder_permissions = folder_permissions\n        self.parent_folder_permissions = parent_folder_permissions\n        self.root_folder_permissions = root_folder_permissions\n        self.set_as_default_acl = set_as_default_acl\n        self.create_directory_if_not_exists = create_directory_if_not_exists\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYTHON\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_pypi_library(get_default_package(\"azure_adls_gen_2\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def _set_acl(\n        self,\n        file_system_client: FileSystemClient,\n        path: str,\n        group_object_id: str,\n        folder_permissions: str,\n        set_as_default_acl: bool,\n    ):\n        acl_directory_client = file_system_client.get_directory_client(path)\n\n        group_id_acl = \"group:{}:{}\".format(group_object_id, folder_permissions)\n        acl_props = acl_directory_client.get_access_control().get(\"acl\")\n        acl_props_list = acl_props.split(\",\")\n\n        for acl in acl_props_list:\n            if group_object_id in acl:\n                acl_props_list.remove(acl)\n\n        acl_props_list.append(group_id_acl)\n        if set_as_default_acl == True:\n            acl_props_list.append(\"default:{}\".format(group_id_acl))\n\n        new_acl_props = \",\".join(acl_props_list)\n        acl_directory_client.set_access_control(acl=new_acl_props)\n\n    def execute(self) -&gt; bool:\n        try:\n            # Setup file system client\n            service_client = DataLakeServiceClient(\n                account_url=\"{}://{}.dfs.core.windows.net\".format(\n                    \"https\", self.storage_account\n                ),\n                credential=self.credential,\n            )\n            file_system_client = service_client.get_file_system_client(\n                file_system=self.container\n            )\n\n            # Create directory if it doesn't already exist\n            if self.create_directory_if_not_exists:\n                directory_client = file_system_client.get_directory_client(\n                    self.directory\n                )\n                if not directory_client.exists():\n                    file_system_client.create_directory(self.directory)\n\n            group_object_id = str(self.group_object_id)\n            acl_path = \"\"\n            directory_list = self.directory.split(\"/\")\n\n            # Set Root Folder ACLs if specified\n            if self.root_folder_permissions != None:\n                self._set_acl(\n                    file_system_client,\n                    \"/\",\n                    group_object_id,\n                    self.root_folder_permissions,\n                    False,\n                )\n\n            # Set Parent Folders ACLs if specified\n            if self.parent_folder_permissions != None:\n                for directory in directory_list[:-1]:\n                    if directory == \"\":\n                        acl_path = \"/\"\n                        continue\n                    elif acl_path == \"/\":\n                        acl_path += directory\n                    else:\n                        acl_path += \"/\" + directory\n\n                    self._set_acl(\n                        file_system_client,\n                        acl_path,\n                        group_object_id,\n                        self.parent_folder_permissions,\n                        False,\n                    )\n\n            # Set Folder ACLs\n            self._set_acl(\n                file_system_client,\n                self.directory,\n                group_object_id,\n                self.folder_permissions,\n                self.set_as_default_acl,\n            )\n\n            return True\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/azure/adls_gen2_acl/#src.sdk.python.rtdip_sdk.pipelines.utilities.azure.adls_gen2_acl.ADLSGen2DirectoryACLUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYTHON</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/azure/adls_gen2_acl.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYTHON\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/azure/autoloader_resources/","title":"Autoloader Resources","text":""},{"location":"sdk/code-reference/pipelines/utilities/azure/autoloader_resources/#src.sdk.python.rtdip_sdk.pipelines.utilities.azure.autoloader_resources.AzureAutoloaderResourcesUtility","title":"<code>AzureAutoloaderResourcesUtility</code>","text":"<p>               Bases: <code>UtilitiesInterface</code></p> <p>Creates the required Azure Resources for the Databricks Autoloader Notification Mode.</p>"},{"location":"sdk/code-reference/pipelines/utilities/azure/autoloader_resources/#src.sdk.python.rtdip_sdk.pipelines.utilities.azure.autoloader_resources.AzureAutoloaderResourcesUtility--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.utilities import AzureAutoloaderResourcesUtility\n\nazure_autoloader_resources_utility = AzureAutoloaderResourcesUtility(\n    subscription_id=\"YOUR-SUBSCRIPTION-ID\",\n    resource_group_name=\"YOUR-RESOURCE-GROUP\",\n    storage_account=\"YOUR-STORAGE-ACCOUNT-NAME\",\n    container=\"YOUR-CONTAINER-NAME\",\n    directory=\"DIRECTORY\",\n    credential=\"YOUR-CLIENT-ID\",\n    event_subscription_name=\"YOUR-EVENT-SUBSCRIPTION\",\n    queue_name=\"YOUR-QUEUE-NAME\",\n    system_topic_name=None\n)\n\nresult = azure_autoloader_resources_utility.execute()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>subscription_id</code> <code>str</code> <p>Azure Subscription ID</p> required <code>resource_group_name</code> <code>str</code> <p>Resource Group Name of Subscription</p> required <code>storage_account</code> <code>str</code> <p>Storage Account Name</p> required <code>container</code> <code>str</code> <p>Container Name</p> required <code>directory</code> <code>str</code> <p>Directory to be used for filtering messages in the Event Subscription. This will be equivalent to the Databricks Autoloader Path</p> required <code>credential</code> <code>TokenCredential</code> <p>Credentials to authenticate with Storage Account</p> required <code>event_subscription_name</code> <code>str</code> <p>Name of the Event Subscription</p> required <code>queue_name</code> <code>str</code> <p>Name of the queue that will be used for the Endpoint of the Messages</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/azure/autoloader_resources.py</code> <pre><code>class AzureAutoloaderResourcesUtility(UtilitiesInterface):\n    \"\"\"\n    Creates the required Azure Resources for the Databricks Autoloader Notification Mode.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.utilities import AzureAutoloaderResourcesUtility\n\n    azure_autoloader_resources_utility = AzureAutoloaderResourcesUtility(\n        subscription_id=\"YOUR-SUBSCRIPTION-ID\",\n        resource_group_name=\"YOUR-RESOURCE-GROUP\",\n        storage_account=\"YOUR-STORAGE-ACCOUNT-NAME\",\n        container=\"YOUR-CONTAINER-NAME\",\n        directory=\"DIRECTORY\",\n        credential=\"YOUR-CLIENT-ID\",\n        event_subscription_name=\"YOUR-EVENT-SUBSCRIPTION\",\n        queue_name=\"YOUR-QUEUE-NAME\",\n        system_topic_name=None\n    )\n\n    result = azure_autoloader_resources_utility.execute()\n    ```\n\n    Parameters:\n        subscription_id (str): Azure Subscription ID\n        resource_group_name (str): Resource Group Name of Subscription\n        storage_account (str): Storage Account Name\n        container (str): Container Name\n        directory (str): Directory to be used for filtering messages in the Event Subscription. This will be equivalent to the Databricks Autoloader Path\n        credential (TokenCredential): Credentials to authenticate with Storage Account\n        event_subscription_name (str): Name of the Event Subscription\n        queue_name (str): Name of the queue that will be used for the Endpoint of the Messages\n    \"\"\"\n\n    subscription_id: str\n    resource_group_name: str\n    storage_account: str\n    container: str\n    directory: str\n    credential: TokenCredential\n    event_subscription_name: str\n    queue_name: str\n\n    def __init__(\n        self,\n        subscription_id: str,\n        resource_group_name: str,\n        storage_account: str,\n        container: str,\n        directory: str,\n        credential: TokenCredential,\n        event_subscription_name: str,\n        queue_name: str,\n    ) -&gt; None:\n        self.subscription_id = subscription_id\n        self.resource_group_name = resource_group_name\n        self.storage_account = storage_account\n        self.container = container\n        self.directory = directory\n        self.credential = credential\n        self.event_subscription_name = event_subscription_name\n        self.queue_name = queue_name\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYTHON\n        \"\"\"\n        return SystemType.PYTHON\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_pypi_library(get_default_package(\"azure_eventgrid_mgmt\"))\n        libraries.add_pypi_library(get_default_package(\"azure_storage_mgmt\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def execute(self) -&gt; bool:\n        storage_mgmt_client = StorageManagementClient(\n            credential=self.credential, subscription_id=self.subscription_id\n        )\n\n        try:\n            queue_response = storage_mgmt_client.queue.get(\n                resource_group_name=self.resource_group_name,\n                account_name=self.storage_account,\n                queue_name=self.queue_name,\n            )\n        except ResourceNotFoundError:\n            queue_response = None\n\n        if queue_response == None:\n            storage_mgmt_client.queue.create(\n                resource_group_name=self.resource_group_name,\n                account_name=self.storage_account,\n                queue_name=self.queue_name,\n                queue=StorageQueue(),\n            )\n\n        eventgrid_client = EventGridManagementClient(\n            credential=self.credential, subscription_id=self.subscription_id\n        )\n\n        source = \"/subscriptions/{}/resourceGroups/{}/providers/Microsoft.Storage/StorageAccounts/{}\".format(\n            self.subscription_id, self.resource_group_name, self.storage_account\n        )\n\n        try:\n            event_subscription_response = eventgrid_client.event_subscriptions.get(\n                scope=source, event_subscription_name=self.event_subscription_name\n            )\n        except ResourceNotFoundError:\n            event_subscription_response = None\n\n        if event_subscription_response == None:\n            event_subscription_destination = StorageQueueEventSubscriptionDestination(\n                resource_id=source,\n                queue_name=self.queue_name,\n                queue_message_time_to_live_in_seconds=None,\n            )\n\n            event_subscription_filter = EventSubscriptionFilter(\n                subject_begins_with=\"/blobServices/default/containers/{}/blobs/{}\".format(\n                    self.container, self.directory\n                ),\n                included_event_types=[\n                    \"Microsoft.Storage.BlobCreated\",\n                    \"Microsoft.Storage.BlobRenamed\",\n                    \"Microsoft.Storage.DirectoryRenamed\",\n                ],\n                advanced_filters=[\n                    StringContainsAdvancedFilter(\n                        key=\"data.api\",\n                        values=[\n                            \"CopyBlob\",\n                            \"PutBlob\",\n                            \"PutBlockList\",\n                            \"FlushWithClose\",\n                            \"RenameFile\",\n                            \"RenameDirectory\",\n                        ],\n                    )\n                ],\n            )\n\n            retry_policy = RetryPolicy()\n\n            event_subscription_info = EventSubscription(\n                destination=event_subscription_destination,\n                filter=event_subscription_filter,\n                event_delivery_schema=EventDeliverySchema.EVENT_GRID_SCHEMA,\n                retry_policy=retry_policy,\n            )\n\n            eventgrid_client.event_subscriptions.begin_create_or_update(\n                scope=source,\n                event_subscription_name=self.event_subscription_name,\n                event_subscription_info=event_subscription_info,\n            ).result()\n\n            return True\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/azure/autoloader_resources/#src.sdk.python.rtdip_sdk.pipelines.utilities.azure.autoloader_resources.AzureAutoloaderResourcesUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYTHON</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/azure/autoloader_resources.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYTHON\n    \"\"\"\n    return SystemType.PYTHON\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/adls_gen2_spn_connect/","title":"ADLS Gen 2 Service Principal Connect","text":""},{"location":"sdk/code-reference/pipelines/utilities/spark/adls_gen2_spn_connect/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.adls_gen2_spn_connect.SparkADLSGen2SPNConnectUtility","title":"<code>SparkADLSGen2SPNConnectUtility</code>","text":"<p>               Bases: <code>UtilitiesInterface</code></p> <p>Configures Spark to Connect to an ADLS Gen 2 Storage Account using a Service Principal.</p>"},{"location":"sdk/code-reference/pipelines/utilities/spark/adls_gen2_spn_connect/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.adls_gen2_spn_connect.SparkADLSGen2SPNConnectUtility--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.utilities import SparkADLSGen2SPNConnectUtility\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nadls_gen2_connect_utility = SparkADLSGen2SPNConnectUtility(\n    spark=spark,\n    storage_account=\"YOUR-STORAGAE-ACCOUNT-NAME\",\n    tenant_id=\"YOUR-TENANT-ID\",\n    client_id=\"YOUR-CLIENT-ID\",\n    client_secret=\"YOUR-CLIENT-SECRET\"\n)\n\nresult = adls_gen2_connect_utility.execute()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from cloud storage</p> required <code>storage_account</code> <code>str</code> <p>Name of the ADLS Gen 2 Storage Account</p> required <code>tenant_id</code> <code>str</code> <p>Tenant ID of the Service Principal</p> required <code>client_id</code> <code>str</code> <p>Service Principal Client ID</p> required <code>client_secret</code> <code>str</code> <p>Service Principal Client Secret</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/adls_gen2_spn_connect.py</code> <pre><code>class SparkADLSGen2SPNConnectUtility(UtilitiesInterface):\n    \"\"\"\n    Configures Spark to Connect to an ADLS Gen 2 Storage Account using a Service Principal.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.utilities import SparkADLSGen2SPNConnectUtility\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    adls_gen2_connect_utility = SparkADLSGen2SPNConnectUtility(\n        spark=spark,\n        storage_account=\"YOUR-STORAGAE-ACCOUNT-NAME\",\n        tenant_id=\"YOUR-TENANT-ID\",\n        client_id=\"YOUR-CLIENT-ID\",\n        client_secret=\"YOUR-CLIENT-SECRET\"\n    )\n\n    result = adls_gen2_connect_utility.execute()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to read data from cloud storage\n        storage_account (str): Name of the ADLS Gen 2 Storage Account\n        tenant_id (str): Tenant ID of the Service Principal\n        client_id (str): Service Principal Client ID\n        client_secret (str): Service Principal Client Secret\n    \"\"\"\n\n    spark: SparkSession\n    storage_account: str\n    tenant_id: str\n    client_id: str\n    client_secret: str\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        storage_account: str,\n        tenant_id: str,\n        client_id: str,\n        client_secret: str,\n    ) -&gt; None:\n        self.spark = spark\n        self.storage_account = storage_account\n        self.tenant_id = tenant_id\n        self.client_id = client_id\n        self.client_secret = client_secret\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def execute(self) -&gt; bool:\n        \"\"\"Executes spark configuration to connect to an ADLS Gen 2 Storage Account using a service principal\"\"\"\n        try:\n            adls_gen2_config = SparkConfigurationUtility(\n                spark=self.spark,\n                config={\n                    \"fs.azure.account.auth.type.{}.dfs.core.windows.net\".format(\n                        self.storage_account\n                    ): \"OAuth\",\n                    \"fs.azure.account.oauth.provider.type.{}.dfs.core.windows.net\".format(\n                        self.storage_account\n                    ): \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n                    \"fs.azure.account.oauth2.client.id.{}.dfs.core.windows.net\".format(\n                        self.storage_account\n                    ): self.client_id,\n                    \"fs.azure.account.oauth2.client.secret.{}.dfs.core.windows.net\".format(\n                        self.storage_account\n                    ): self.client_secret,\n                    \"fs.azure.account.oauth2.client.endpoint.{}.dfs.core.windows.net\".format(\n                        self.storage_account\n                    ): \"https://login.microsoftonline.com/{}/oauth2/token\".format(\n                        self.tenant_id\n                    ),\n                },\n            )\n            adls_gen2_config.execute()\n            return True\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/adls_gen2_spn_connect/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.adls_gen2_spn_connect.SparkADLSGen2SPNConnectUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/adls_gen2_spn_connect.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/adls_gen2_spn_connect/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.adls_gen2_spn_connect.SparkADLSGen2SPNConnectUtility.execute","title":"<code>execute()</code>","text":"<p>Executes spark configuration to connect to an ADLS Gen 2 Storage Account using a service principal</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/adls_gen2_spn_connect.py</code> <pre><code>def execute(self) -&gt; bool:\n    \"\"\"Executes spark configuration to connect to an ADLS Gen 2 Storage Account using a service principal\"\"\"\n    try:\n        adls_gen2_config = SparkConfigurationUtility(\n            spark=self.spark,\n            config={\n                \"fs.azure.account.auth.type.{}.dfs.core.windows.net\".format(\n                    self.storage_account\n                ): \"OAuth\",\n                \"fs.azure.account.oauth.provider.type.{}.dfs.core.windows.net\".format(\n                    self.storage_account\n                ): \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n                \"fs.azure.account.oauth2.client.id.{}.dfs.core.windows.net\".format(\n                    self.storage_account\n                ): self.client_id,\n                \"fs.azure.account.oauth2.client.secret.{}.dfs.core.windows.net\".format(\n                    self.storage_account\n                ): self.client_secret,\n                \"fs.azure.account.oauth2.client.endpoint.{}.dfs.core.windows.net\".format(\n                    self.storage_account\n                ): \"https://login.microsoftonline.com/{}/oauth2/token\".format(\n                    self.tenant_id\n                ),\n            },\n        )\n        adls_gen2_config.execute()\n        return True\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/configuration/","title":"Configuration","text":""},{"location":"sdk/code-reference/pipelines/utilities/spark/configuration/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.configuration.SparkConfigurationUtility","title":"<code>SparkConfigurationUtility</code>","text":"<p>               Bases: <code>UtilitiesInterface</code></p> <p>Sets configuration key value pairs to a Spark Session</p>"},{"location":"sdk/code-reference/pipelines/utilities/spark/configuration/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.configuration.SparkConfigurationUtility--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.sources import SparkConfigurationUtility\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n# Not required if using Databricks\nspark = SparkSessionUtility(config={}).execute()\n\nconfiguration_utility = SparkConfigurationUtility(\n    spark=spark,\n    config={}\n)\n\nresult = configuration_utility.execute()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from cloud storage</p> required <code>config</code> <code>dict</code> <p>Dictionary of spark configuration to be applied to the spark session</p> required Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/configuration.py</code> <pre><code>class SparkConfigurationUtility(UtilitiesInterface):\n    \"\"\"\n    Sets configuration key value pairs to a Spark Session\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.sources import SparkConfigurationUtility\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    # Not required if using Databricks\n    spark = SparkSessionUtility(config={}).execute()\n\n    configuration_utility = SparkConfigurationUtility(\n        spark=spark,\n        config={}\n    )\n\n    result = configuration_utility.execute()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to read data from cloud storage\n        config (dict): Dictionary of spark configuration to be applied to the spark session\n    \"\"\"\n\n    spark: SparkSession\n    config: dict\n    columns: List[StructField]\n    partitioned_by: List[str]\n    location: str\n    properties: dict\n    comment: str\n\n    def __init__(self, spark: SparkSession, config: dict) -&gt; None:\n        self.spark = spark\n        self.config = config\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def execute(self) -&gt; bool:\n        \"\"\"Executes configuration key value pairs to a Spark Session\"\"\"\n        try:\n            for configuration in self.config.items():\n                self.spark.conf.set(configuration[0], configuration[1])\n            return True\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/configuration/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.configuration.SparkConfigurationUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/configuration.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/configuration/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.configuration.SparkConfigurationUtility.execute","title":"<code>execute()</code>","text":"<p>Executes configuration key value pairs to a Spark Session</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/configuration.py</code> <pre><code>def execute(self) -&gt; bool:\n    \"\"\"Executes configuration key value pairs to a Spark Session\"\"\"\n    try:\n        for configuration in self.config.items():\n            self.spark.conf.set(configuration[0], configuration[1])\n        return True\n\n    except Py4JJavaError as e:\n        logging.exception(e.errmsg)\n        raise e\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/delta_table_create/","title":"Delta Table Create","text":""},{"location":"sdk/code-reference/pipelines/utilities/spark/delta_table_create/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.delta_table_create.DeltaTableCreateUtility","title":"<code>DeltaTableCreateUtility</code>","text":"<p>               Bases: <code>UtilitiesInterface</code></p> <p>Creates a Delta Table in a Hive Metastore or in Databricks Unity Catalog.</p>"},{"location":"sdk/code-reference/pipelines/utilities/spark/delta_table_create/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.delta_table_create.DeltaTableCreateUtility--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.utilities.spark.delta_table_create import DeltaTableCreateUtility, DeltaTableColumn\n\ntable_create_utility = DeltaTableCreateUtility(\n    spark=spark_session,\n    table_name=\"delta_table\",\n    columns=[\n        DeltaTableColumn(name=\"EventDate\", type=\"date\", nullable=False, metadata={\"delta.generationExpression\": \"CAST(EventTime AS DATE)\"}),\n        DeltaTableColumn(name=\"TagName\", type=\"string\", nullable=False),\n        DeltaTableColumn(name=\"EventTime\", type=\"timestamp\", nullable=False),\n        DeltaTableColumn(name=\"Status\", type=\"string\", nullable=True),\n        DeltaTableColumn(name=\"Value\", type=\"float\", nullable=True)\n    ],\n    partitioned_by=[\"EventDate\"],\n    properties={\"delta.logRetentionDuration\": \"7 days\", \"delta.enableChangeDataFeed\": \"true\"},\n    comment=\"Creation of Delta Table\"\n)\n\nresult = table_create_utility.execute()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from cloud storage</p> required <code>table_name</code> <code>str</code> <p>Name of the table, including catalog and schema if table is to be created in Unity Catalog</p> required <code>columns</code> <code>list[DeltaTableColumn]</code> <p>List of columns and their related column properties</p> required <code>partitioned_by</code> <code>list[str]</code> <p>List of column names to partition the table by</p> <code>None</code> <code>location</code> <code>str</code> <p>Path to storage location</p> <code>None</code> <code>properties</code> <code>dict</code> <p>Propoerties that can be specified for a Delta Table. Further information on the options available are here</p> <code>None</code> <code>comment</code> <code>str</code> <p>Provides a comment on the table metadata</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/delta_table_create.py</code> <pre><code>class DeltaTableCreateUtility(UtilitiesInterface):\n    \"\"\"\n    Creates a Delta Table in a Hive Metastore or in Databricks Unity Catalog.\n\n    Example\n    -------\n    ```python\n    from rtdip_sdk.pipelines.utilities.spark.delta_table_create import DeltaTableCreateUtility, DeltaTableColumn\n\n    table_create_utility = DeltaTableCreateUtility(\n        spark=spark_session,\n        table_name=\"delta_table\",\n        columns=[\n            DeltaTableColumn(name=\"EventDate\", type=\"date\", nullable=False, metadata={\"delta.generationExpression\": \"CAST(EventTime AS DATE)\"}),\n            DeltaTableColumn(name=\"TagName\", type=\"string\", nullable=False),\n            DeltaTableColumn(name=\"EventTime\", type=\"timestamp\", nullable=False),\n            DeltaTableColumn(name=\"Status\", type=\"string\", nullable=True),\n            DeltaTableColumn(name=\"Value\", type=\"float\", nullable=True)\n        ],\n        partitioned_by=[\"EventDate\"],\n        properties={\"delta.logRetentionDuration\": \"7 days\", \"delta.enableChangeDataFeed\": \"true\"},\n        comment=\"Creation of Delta Table\"\n    )\n\n    result = table_create_utility.execute()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to read data from cloud storage\n        table_name (str): Name of the table, including catalog and schema if table is to be created in Unity Catalog\n        columns (list[DeltaTableColumn]): List of columns and their related column properties\n        partitioned_by (list[str], optional): List of column names to partition the table by\n        location (str, optional): Path to storage location\n        properties (dict, optional): Propoerties that can be specified for a Delta Table. Further information on the options available are [here](https://docs.databricks.com/delta/table-properties.html#delta-table-properties)\n        comment (str, optional): Provides a comment on the table metadata\n\n\n    \"\"\"\n\n    spark: SparkSession\n    table_name: str\n    columns: List[DeltaTableColumn]\n    partitioned_by: List[str]\n    location: str\n    properties: dict\n    comment: str\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        table_name: str,\n        columns: List[StructField],\n        partitioned_by: List[str] = None,\n        location: str = None,\n        properties: dict = None,\n        comment: str = None,\n    ) -&gt; None:\n        self.spark = spark\n        self.table_name = table_name\n        self.columns = columns\n        self.partitioned_by = partitioned_by\n        self.location = location\n        self.properties = properties\n        self.comment = comment\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_core\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def execute(self) -&gt; bool:\n        try:\n            columns = [StructField.fromJson(column.dict()) for column in self.columns]\n\n            delta_table = (\n                DeltaTable.createIfNotExists(self.spark)\n                .tableName(self.table_name)\n                .addColumns(columns)\n            )\n\n            if self.partitioned_by is not None:\n                delta_table = delta_table.partitionedBy(self.partitioned_by)\n\n            if self.location is not None:\n                delta_table = delta_table.location(self.location)\n\n            if self.properties is not None:\n                for key, value in self.properties.items():\n                    delta_table = delta_table.property(key, value)\n\n            if self.comment is not None:\n                delta_table = delta_table.comment(self.comment)\n\n            delta_table.execute()\n            return True\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/delta_table_create/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.delta_table_create.DeltaTableCreateUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/delta_table_create.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/delta_table_optimize/","title":"Delta Table Optimize","text":""},{"location":"sdk/code-reference/pipelines/utilities/spark/delta_table_optimize/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.delta_table_optimize.DeltaTableOptimizeUtility","title":"<code>DeltaTableOptimizeUtility</code>","text":"<p>               Bases: <code>UtilitiesInterface</code></p> <p>Optimizes a Delta Table.</p>"},{"location":"sdk/code-reference/pipelines/utilities/spark/delta_table_optimize/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.delta_table_optimize.DeltaTableOptimizeUtility--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.utilities.spark.delta_table_optimize import DeltaTableOptimizeUtility\n\ntable_optimize_utility = DeltaTableOptimizeUtility(\n    spark=spark_session,\n    table_name=\"delta_table\",\n    where=\"EventDate&lt;=current_date()\",\n    zorder_by=[\"EventDate\"]\n)\n\nresult = table_optimize_utility.execute()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from cloud storage</p> required <code>table_name</code> <code>str</code> <p>Name of the table, including catalog and schema if table is to be created in Unity Catalog</p> required <code>where</code> <code>str</code> <p>Apply a partition filter to limit optimize to specific partitions. Example, \"date='2021-11-18'\" or \"EventDate&lt;=current_date()\"</p> <code>None</code> <code>zorder_by</code> <code>list[str]</code> <p>List of column names to zorder the table by. For more information, see here.</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/delta_table_optimize.py</code> <pre><code>class DeltaTableOptimizeUtility(UtilitiesInterface):\n    \"\"\"\n    [Optimizes](https://docs.delta.io/latest/optimizations-oss.html) a Delta Table.\n\n    Example\n    -------\n    ```python\n    from rtdip_sdk.pipelines.utilities.spark.delta_table_optimize import DeltaTableOptimizeUtility\n\n    table_optimize_utility = DeltaTableOptimizeUtility(\n        spark=spark_session,\n        table_name=\"delta_table\",\n        where=\"EventDate&lt;=current_date()\",\n        zorder_by=[\"EventDate\"]\n    )\n\n    result = table_optimize_utility.execute()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to read data from cloud storage\n        table_name (str): Name of the table, including catalog and schema if table is to be created in Unity Catalog\n        where (str, optional): Apply a partition filter to limit optimize to specific partitions. Example, \"date='2021-11-18'\" or \"EventDate&lt;=current_date()\"\n        zorder_by (list[str], optional): List of column names to zorder the table by. For more information, see [here.](https://docs.delta.io/latest/optimizations-oss.html#optimize-performance-with-file-management&amp;language-python)\n    \"\"\"\n\n    spark: SparkSession\n    table_name: str\n    where: Optional[str]\n    zorder_by: Optional[List[str]]\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        table_name: str,\n        where: str = None,\n        zorder_by: List[str] = None,\n    ) -&gt; None:\n        self.spark = spark\n        self.table_name = table_name\n        self.where = where\n        self.zorder_by = zorder_by\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_core\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def execute(self) -&gt; bool:\n        try:\n            delta_table = DeltaTable.forName(self.spark, self.table_name).optimize()\n\n            if self.where is not None:\n                delta_table = delta_table.where(self.where)\n\n            if self.zorder_by is not None:\n                delta_table = delta_table.executeZOrderBy(self.zorder_by)\n            else:\n                delta_table.executeCompaction()\n\n            return True\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/delta_table_optimize/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.delta_table_optimize.DeltaTableOptimizeUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/delta_table_optimize.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/delta_table_vacuum/","title":"Delta Table Vacuum","text":""},{"location":"sdk/code-reference/pipelines/utilities/spark/delta_table_vacuum/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.delta_table_vacuum.DeltaTableVacuumUtility","title":"<code>DeltaTableVacuumUtility</code>","text":"<p>               Bases: <code>UtilitiesInterface</code></p> <p>Vacuums a Delta Table.</p>"},{"location":"sdk/code-reference/pipelines/utilities/spark/delta_table_vacuum/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.delta_table_vacuum.DeltaTableVacuumUtility--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.utilities.spark.delta_table_vacuum import DeltaTableVacuumUtility\n\ntable_vacuum_utility =  DeltaTableVacuumUtility(\n    spark=spark_session,\n    table_name=\"delta_table\",\n    retention_hours=\"168\"\n)\n\nresult = table_vacuum_utility.execute()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session required to read data from cloud storage</p> required <code>table_name</code> <code>str</code> <p>Name of the table, including catalog and schema if table is to be created in Unity Catalog</p> required <code>retention_hours</code> <code>int</code> <p>Sets the retention threshold in hours.</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/delta_table_vacuum.py</code> <pre><code>class DeltaTableVacuumUtility(UtilitiesInterface):\n    \"\"\"\n    [Vacuums](https://docs.delta.io/latest/delta-utility.html#-delta-vacuum) a Delta Table.\n\n    Example\n    -------\n    ```python\n    from rtdip_sdk.pipelines.utilities.spark.delta_table_vacuum import DeltaTableVacuumUtility\n\n    table_vacuum_utility =  DeltaTableVacuumUtility(\n        spark=spark_session,\n        table_name=\"delta_table\",\n        retention_hours=\"168\"\n    )\n\n    result = table_vacuum_utility.execute()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session required to read data from cloud storage\n        table_name (str): Name of the table, including catalog and schema if table is to be created in Unity Catalog\n        retention_hours (int, optional): Sets the retention threshold in hours.\n    \"\"\"\n\n    spark: SparkSession\n    table_name: str\n    retention_hours: Optional[int]\n\n    def __init__(\n        self, spark: SparkSession, table_name: str, retention_hours: int = None\n    ) -&gt; None:\n        self.spark = spark\n        self.table_name = table_name\n        self.retention_hours = retention_hours\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        libraries.add_maven_library(get_default_package(\"spark_delta_core\"))\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def execute(self) -&gt; bool:\n        try:\n            delta_table = DeltaTable.forName(self.spark, self.table_name)\n\n            delta_table.vacuum(retentionHours=self.retention_hours)\n\n            return True\n\n        except Py4JJavaError as e:\n            logging.exception(e.errmsg)\n            raise e\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/delta_table_vacuum/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.delta_table_vacuum.DeltaTableVacuumUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/delta_table_vacuum.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/session/","title":"Session","text":""},{"location":"sdk/code-reference/pipelines/utilities/spark/session/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.session.SparkSessionUtility","title":"<code>SparkSessionUtility</code>","text":"<p>               Bases: <code>UtilitiesInterface</code></p> <p>Creates or Gets a Spark Session and uses settings and libraries of the imported RTDIP components to populate the spark configuration and jars in the spark session.</p> <p>Call this component after all imports of the RTDIP components to ensure that the spark session is configured correctly.</p>"},{"location":"sdk/code-reference/pipelines/utilities/spark/session/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.session.SparkSessionUtility--example","title":"Example","text":"<pre><code>from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\nspark_session_utility = SparkSessionUtility(\n    config={},\n    module=None,\n    remote=None\n)\n\nresult = spark_session_utility.execute()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>optional dict</code> <p>Dictionary of spark configuration to be applied to the spark session</p> <code>None</code> <code>module</code> <code>optional str</code> <p>Provide the module to use for imports of rtdip-sdk components. If not populated, it will use the calling module to check for imports</p> <code>None</code> <code>remote</code> <code>optional str</code> <p>Specify the remote parameters if intending to use Spark Connect</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/session.py</code> <pre><code>class SparkSessionUtility(UtilitiesInterface):\n    \"\"\"\n    Creates or Gets a Spark Session and uses settings and libraries of the imported RTDIP components to populate the spark configuration and jars in the spark session.\n\n    Call this component after all imports of the RTDIP components to ensure that the spark session is configured correctly.\n\n    Example\n    --------\n    ```python\n    from rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n    spark_session_utility = SparkSessionUtility(\n        config={},\n        module=None,\n        remote=None\n    )\n\n    result = spark_session_utility.execute()\n    ```\n\n    Parameters:\n        config (optional dict): Dictionary of spark configuration to be applied to the spark session\n        module (optional str): Provide the module to use for imports of rtdip-sdk components. If not populated, it will use the calling module to check for imports\n        remote (optional str): Specify the remote parameters if intending to use Spark Connect\n    \"\"\"\n\n    spark: SparkSession\n    config: dict\n    module: str\n\n    def __init__(\n        self, config: dict = None, module: str = None, remote: str = None\n    ) -&gt; None:\n        self.config = config\n        if module == None:\n            frm = inspect.stack()[1]\n            mod = inspect.getmodule(frm[0])\n            self.module = mod.__name__\n        else:\n            self.module = module\n        self.remote = remote\n\n    @staticmethod\n    def system_type():\n        \"\"\"\n        Attributes:\n            SystemType (Environment): Requires PYSPARK\n        \"\"\"\n        return SystemType.PYSPARK\n\n    @staticmethod\n    def libraries():\n        libraries = Libraries()\n        return libraries\n\n    @staticmethod\n    def settings() -&gt; dict:\n        return {}\n\n    def execute(self) -&gt; SparkSession:\n        \"\"\"To execute\"\"\"\n        try:\n            (task_libraries, spark_configuration) = PipelineComponentsGetUtility(\n                self.module, self.config\n            ).execute()\n            self.spark = SparkClient(\n                spark_configuration=spark_configuration,\n                spark_libraries=task_libraries,\n                spark_remote=self.remote,\n            ).spark_session\n            return self.spark\n\n        except Exception as e:\n            logging.exception(str(e))\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/session/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.session.SparkSessionUtility.system_type","title":"<code>system_type()</code>  <code>staticmethod</code>","text":"<p>Attributes:</p> Name Type Description <code>SystemType</code> <code>Environment</code> <p>Requires PYSPARK</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/session.py</code> <pre><code>@staticmethod\ndef system_type():\n    \"\"\"\n    Attributes:\n        SystemType (Environment): Requires PYSPARK\n    \"\"\"\n    return SystemType.PYSPARK\n</code></pre>"},{"location":"sdk/code-reference/pipelines/utilities/spark/session/#src.sdk.python.rtdip_sdk.pipelines.utilities.spark.session.SparkSessionUtility.execute","title":"<code>execute()</code>","text":"<p>To execute</p> Source code in <code>src/sdk/python/rtdip_sdk/pipelines/utilities/spark/session.py</code> <pre><code>def execute(self) -&gt; SparkSession:\n    \"\"\"To execute\"\"\"\n    try:\n        (task_libraries, spark_configuration) = PipelineComponentsGetUtility(\n            self.module, self.config\n        ).execute()\n        self.spark = SparkClient(\n            spark_configuration=spark_configuration,\n            spark_libraries=task_libraries,\n            spark_remote=self.remote,\n        ).spark_session\n        return self.spark\n\n    except Exception as e:\n        logging.exception(str(e))\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/db-sql-connector/","title":"Databricks SQL Connector","text":""},{"location":"sdk/code-reference/query/connectors/db-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.db_sql_connector.DatabricksSQLConnection","title":"<code>DatabricksSQLConnection</code>","text":"<p>               Bases: <code>ConnectionInterface</code></p> <p>The Databricks SQL Connector for Python is a Python library that allows you to use Python code to run SQL commands on Databricks clusters and Databricks SQL warehouses.</p> <p>The connection class represents a connection to a database and uses the Databricks SQL Connector API's for Python to interact with cluster/jobs. To find details for SQL warehouses server_hostname and http_path location to the SQL Warehouse tab in the documentation.</p> <p>Parameters:</p> Name Type Description Default <code>server_hostname</code> <code>str</code> <p>Server hostname for the cluster or SQL Warehouse</p> required <code>http_path</code> <code>str</code> <p>Http path for the cluster or SQL Warehouse</p> required <code>access_token</code> <code>str</code> <p>Azure AD or Databricks PAT token</p> required Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/db_sql_connector.py</code> <pre><code>class DatabricksSQLConnection(ConnectionInterface):\n    \"\"\"\n    The Databricks SQL Connector for Python is a Python library that allows you to use Python code to run SQL commands on Databricks clusters and Databricks SQL warehouses.\n\n    The connection class represents a connection to a database and uses the Databricks SQL Connector API's for Python to interact with cluster/jobs.\n    To find details for SQL warehouses server_hostname and http_path location to the SQL Warehouse tab in the documentation.\n\n    Args:\n        server_hostname: Server hostname for the cluster or SQL Warehouse\n        http_path: Http path for the cluster or SQL Warehouse\n        access_token: Azure AD or Databricks PAT token\n    \"\"\"\n\n    def __init__(\n        self,\n        server_hostname: str,\n        http_path: str,\n        access_token: str,\n        return_type=ConnectionReturnType.Pandas,\n    ) -&gt; None:\n        self.server_hostname = server_hostname\n        self.http_path = http_path\n        self.access_token = access_token\n        self.return_type = return_type\n        # call auth method\n        self.connection = self._connect()\n\n    def _connect(self):\n        \"\"\"Connects to the endpoint\"\"\"\n        try:\n            return sql.connect(\n                server_hostname=self.server_hostname,\n                http_path=self.http_path,\n                access_token=self.access_token,\n                _user_agent_entry=\"RTDIP\",\n            )\n        except Exception as e:\n            logging.exception(\"error while connecting to the endpoint\")\n            raise e\n\n    def close(self) -&gt; None:\n        \"\"\"Closes connection to database.\"\"\"\n        try:\n            self.connection.close()\n        except Exception as e:\n            logging.exception(\"error while closing connection\")\n            raise e\n\n    def cursor(self) -&gt; object:\n        \"\"\"\n        Initiates the cursor and returns it.\n\n        Returns:\n          DatabricksSQLCursor: Object to represent a databricks workspace with methods to interact with clusters/jobs.\n        \"\"\"\n        try:\n            if self.connection.open == False:\n                self.connection = self._connect()\n            return DatabricksSQLCursor(self.connection.cursor(), self.return_type)\n        except Exception as e:\n            logging.exception(\"error with cursor object\")\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/db-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.db_sql_connector.DatabricksSQLConnection.close","title":"<code>close()</code>","text":"<p>Closes connection to database.</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/db_sql_connector.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Closes connection to database.\"\"\"\n    try:\n        self.connection.close()\n    except Exception as e:\n        logging.exception(\"error while closing connection\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/db-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.db_sql_connector.DatabricksSQLConnection.cursor","title":"<code>cursor()</code>","text":"<p>Initiates the cursor and returns it.</p> <p>Returns:</p> Name Type Description <code>DatabricksSQLCursor</code> <code>object</code> <p>Object to represent a databricks workspace with methods to interact with clusters/jobs.</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/db_sql_connector.py</code> <pre><code>def cursor(self) -&gt; object:\n    \"\"\"\n    Initiates the cursor and returns it.\n\n    Returns:\n      DatabricksSQLCursor: Object to represent a databricks workspace with methods to interact with clusters/jobs.\n    \"\"\"\n    try:\n        if self.connection.open == False:\n            self.connection = self._connect()\n        return DatabricksSQLCursor(self.connection.cursor(), self.return_type)\n    except Exception as e:\n        logging.exception(\"error with cursor object\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/db-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.db_sql_connector.DatabricksSQLCursor","title":"<code>DatabricksSQLCursor</code>","text":"<p>               Bases: <code>CursorInterface</code></p> <p>Object to represent a databricks workspace with methods to interact with clusters/jobs.</p> <p>Parameters:</p> Name Type Description Default <code>cursor</code> <code>object</code> <p>controls execution of commands on cluster or SQL Warehouse</p> required Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/db_sql_connector.py</code> <pre><code>class DatabricksSQLCursor(CursorInterface):\n    \"\"\"\n    Object to represent a databricks workspace with methods to interact with clusters/jobs.\n\n    Args:\n        cursor: controls execution of commands on cluster or SQL Warehouse\n    \"\"\"\n\n    def __init__(self, cursor: object, return_type=ConnectionReturnType.Pandas) -&gt; None:\n        self.cursor = cursor\n        self.return_type = return_type\n\n    def execute(self, query: str) -&gt; None:\n        \"\"\"\n        Prepares and runs a database query.\n\n        Args:\n            query: sql query to execute on the cluster or SQL Warehouse\n        \"\"\"\n        try:\n            self.cursor.execute(query)\n        except Exception as e:\n            logging.exception(\"error while executing the query\")\n            raise e\n\n    def fetch_all(self, fetch_size=5_000_000) -&gt; Union[list, dict]:\n        \"\"\"\n        Gets all rows of a query.\n\n        Returns:\n            list: list of results\n        \"\"\"\n        try:\n            get_next_result = True\n            results = None if self.return_type == ConnectionReturnType.String else []\n            count = 0\n            sample_row = None\n            while get_next_result:\n                result = self.cursor.fetchmany_arrow(fetch_size)\n                count += result.num_rows\n                if self.return_type == ConnectionReturnType.List:\n                    column_list = []\n                    for column in result.columns:\n                        column_list.append(column.to_pylist())\n                    results.extend(zip(*column_list))\n                elif self.return_type == ConnectionReturnType.String:\n                    column_list = []\n                    for column in result.columns:\n                        column_list.append(column.to_pylist())\n                    rows = [str(item[0]) for item in zip(*column_list)]\n                    if len(rows) &gt; 0:\n                        sample_row = rows[0]\n                    strings = \",\".join(rows)\n                    if results is None:\n                        results = strings\n                    else:\n                        results = \",\".join([results, strings])\n                else:\n                    results.append(result)\n                if result.num_rows &lt; fetch_size:\n                    get_next_result = False\n\n            if self.return_type == ConnectionReturnType.Pandas:\n                pyarrow_table = pa.concat_tables(results)\n                return pyarrow_table.to_pandas()\n            elif self.return_type == ConnectionReturnType.Pyarrow:\n                pyarrow_table = pa.concat_tables(results)\n                return pyarrow_table\n            elif self.return_type == ConnectionReturnType.List:\n                return results\n            elif self.return_type == ConnectionReturnType.String:\n                return {\n                    \"data\": results,\n                    \"sample_row\": sample_row,\n                    \"count\": count,\n                }\n        except Exception as e:\n            logging.exception(\"error while fetching the rows of a query\")\n            raise e\n\n    def close(self) -&gt; None:\n        \"\"\"Closes the cursor.\"\"\"\n        try:\n            self.cursor.close()\n        except Exception as e:\n            logging.exception(\"error while closing the cursor\")\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/db-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.db_sql_connector.DatabricksSQLCursor.execute","title":"<code>execute(query)</code>","text":"<p>Prepares and runs a database query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>sql query to execute on the cluster or SQL Warehouse</p> required Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/db_sql_connector.py</code> <pre><code>def execute(self, query: str) -&gt; None:\n    \"\"\"\n    Prepares and runs a database query.\n\n    Args:\n        query: sql query to execute on the cluster or SQL Warehouse\n    \"\"\"\n    try:\n        self.cursor.execute(query)\n    except Exception as e:\n        logging.exception(\"error while executing the query\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/db-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.db_sql_connector.DatabricksSQLCursor.fetch_all","title":"<code>fetch_all(fetch_size=5000000)</code>","text":"<p>Gets all rows of a query.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>Union[list, dict]</code> <p>list of results</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/db_sql_connector.py</code> <pre><code>def fetch_all(self, fetch_size=5_000_000) -&gt; Union[list, dict]:\n    \"\"\"\n    Gets all rows of a query.\n\n    Returns:\n        list: list of results\n    \"\"\"\n    try:\n        get_next_result = True\n        results = None if self.return_type == ConnectionReturnType.String else []\n        count = 0\n        sample_row = None\n        while get_next_result:\n            result = self.cursor.fetchmany_arrow(fetch_size)\n            count += result.num_rows\n            if self.return_type == ConnectionReturnType.List:\n                column_list = []\n                for column in result.columns:\n                    column_list.append(column.to_pylist())\n                results.extend(zip(*column_list))\n            elif self.return_type == ConnectionReturnType.String:\n                column_list = []\n                for column in result.columns:\n                    column_list.append(column.to_pylist())\n                rows = [str(item[0]) for item in zip(*column_list)]\n                if len(rows) &gt; 0:\n                    sample_row = rows[0]\n                strings = \",\".join(rows)\n                if results is None:\n                    results = strings\n                else:\n                    results = \",\".join([results, strings])\n            else:\n                results.append(result)\n            if result.num_rows &lt; fetch_size:\n                get_next_result = False\n\n        if self.return_type == ConnectionReturnType.Pandas:\n            pyarrow_table = pa.concat_tables(results)\n            return pyarrow_table.to_pandas()\n        elif self.return_type == ConnectionReturnType.Pyarrow:\n            pyarrow_table = pa.concat_tables(results)\n            return pyarrow_table\n        elif self.return_type == ConnectionReturnType.List:\n            return results\n        elif self.return_type == ConnectionReturnType.String:\n            return {\n                \"data\": results,\n                \"sample_row\": sample_row,\n                \"count\": count,\n            }\n    except Exception as e:\n        logging.exception(\"error while fetching the rows of a query\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/db-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.db_sql_connector.DatabricksSQLCursor.close","title":"<code>close()</code>","text":"<p>Closes the cursor.</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/db_sql_connector.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Closes the cursor.\"\"\"\n    try:\n        self.cursor.close()\n    except Exception as e:\n        logging.exception(\"error while closing the cursor\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/pyodbc-sql-connector/","title":"PYODBC SQL Connector","text":""},{"location":"sdk/code-reference/query/connectors/pyodbc-sql-connector/#pyodbc-driver-paths","title":"PYODBC Driver Paths","text":"<p>To use PYODBC SQL Connect you will require the driver path specified below per operating system.</p> Operating Systems Driver Paths Windows <code>C:\\Program Files\\Simba Spark ODBC Driver</code> MacOS <code>/Library/simba/spark/lib/libsparkodbc_sbu.dylib</code> Linux 64-bit <code>/opt/simba/spark/lib/64/libsparkodbc_sb64.so</code> Linux 32-bit <code>/opt/simba/spark/lib/32/libsparkodbc_sb32.so</code>"},{"location":"sdk/code-reference/query/connectors/pyodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.pyodbc_sql_connector.PYODBCSQLConnection","title":"<code>PYODBCSQLConnection</code>","text":"<p>               Bases: <code>ConnectionInterface</code></p> <p>PYODBC is an open source python module which allows access to ODBC databases. This allows the user to connect through ODBC to data in azure databricks clusters or sql warehouses.</p> <p>Uses the databricks API's (2.0) to connect to the sql server.</p> <p>Parameters:</p> Name Type Description Default <code>driver_path</code> <code>str</code> <p>Driver installed to work with PYODBC</p> required <code>server_hostname</code> <code>str</code> <p>Server hostname for the cluster or SQL Warehouse</p> required <code>http_path</code> <code>str</code> <p>Http path for the cluster or SQL Warehouse</p> required <code>access_token</code> <code>str</code> <p>Azure AD Token</p> required Note 1 <p>More fields can be configured here in the connection ie PORT, Schema, etc.</p> Note 2 <p>When using Unix, Linux or Mac OS brew installation of PYODBC is required for connection.</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/pyodbc_sql_connector.py</code> <pre><code>class PYODBCSQLConnection(ConnectionInterface):\n    \"\"\"\n    PYODBC is an open source python module which allows access to ODBC databases.\n    This allows the user to connect through ODBC to data in azure databricks clusters or sql warehouses.\n\n    Uses the databricks API's (2.0) to connect to the sql server.\n\n    Args:\n        driver_path: Driver installed to work with PYODBC\n        server_hostname: Server hostname for the cluster or SQL Warehouse\n        http_path: Http path for the cluster or SQL Warehouse\n        access_token: Azure AD Token\n\n    Note 1:\n        More fields can be configured here in the connection ie PORT, Schema, etc.\n\n    Note 2:\n        When using Unix, Linux or Mac OS brew installation of PYODBC is required for connection.\n    \"\"\"\n\n    def __init__(\n        self, driver_path: str, server_hostname: str, http_path: str, access_token: str\n    ) -&gt; None:\n        self.driver_path = driver_path\n        self.server_hostname = server_hostname\n        self.http_path = http_path\n        self.access_token = access_token\n        # call auth method\n        self.connection = self._connect()\n        self.open = True\n\n    def _connect(self):\n        \"\"\"Connects to the endpoint\"\"\"\n        try:\n            return pyodbc.connect(\n                \"Driver=\"\n                + self.driver_path\n                + \";\"\n                + \"HOST=\"\n                + self.server_hostname\n                + \";\"\n                + \"PORT=443;\"\n                + \"Schema=default;\"\n                + \"SparkServerType=3;\"\n                + \"AuthMech=11;\"\n                + \"UID=token;\"\n                + \"UserAgentEntry=RTDIP;\"\n                +\n                #'PWD=' + access_token+ \";\" +\n                \"Auth_AccessToken=\" + self.access_token + \";\"\n                \"ThriftTransport=2;\" + \"SSL=1;\" + \"HTTPPath=\" + self.http_path,\n                autocommit=True,\n            )\n        except Exception as e:\n            logging.exception(\"error while connecting to the endpoint\")\n            raise e\n\n    def close(self) -&gt; None:\n        \"\"\"Closes connection to database.\"\"\"\n        try:\n            self.connection.close()\n            self.open = False\n        except Exception as e:\n            logging.exception(\"error while closing the connection\")\n            raise e\n\n    def cursor(self) -&gt; object:\n        \"\"\"\n        Intiates the cursor and returns it.\n\n        Returns:\n          PYODBCSQLCursor: Object to represent a databricks workspace with methods to interact with clusters/jobs.\n        \"\"\"\n        try:\n            if self.open == False:\n                self.connection = self._connect()\n            return PYODBCSQLCursor(self.connection.cursor())\n        except Exception as e:\n            logging.exception(\"error with cursor object\")\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/pyodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.pyodbc_sql_connector.PYODBCSQLConnection.close","title":"<code>close()</code>","text":"<p>Closes connection to database.</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/pyodbc_sql_connector.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Closes connection to database.\"\"\"\n    try:\n        self.connection.close()\n        self.open = False\n    except Exception as e:\n        logging.exception(\"error while closing the connection\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/pyodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.pyodbc_sql_connector.PYODBCSQLConnection.cursor","title":"<code>cursor()</code>","text":"<p>Intiates the cursor and returns it.</p> <p>Returns:</p> Name Type Description <code>PYODBCSQLCursor</code> <code>object</code> <p>Object to represent a databricks workspace with methods to interact with clusters/jobs.</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/pyodbc_sql_connector.py</code> <pre><code>def cursor(self) -&gt; object:\n    \"\"\"\n    Intiates the cursor and returns it.\n\n    Returns:\n      PYODBCSQLCursor: Object to represent a databricks workspace with methods to interact with clusters/jobs.\n    \"\"\"\n    try:\n        if self.open == False:\n            self.connection = self._connect()\n        return PYODBCSQLCursor(self.connection.cursor())\n    except Exception as e:\n        logging.exception(\"error with cursor object\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/pyodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.pyodbc_sql_connector.PYODBCSQLCursor","title":"<code>PYODBCSQLCursor</code>","text":"<p>               Bases: <code>CursorInterface</code></p> <p>Object to represent a databricks workspace with methods to interact with clusters/jobs.</p> <p>Parameters:</p> Name Type Description Default <code>cursor</code> <code>object</code> <p>controls execution of commands on cluster or SQL Warehouse</p> required Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/pyodbc_sql_connector.py</code> <pre><code>class PYODBCSQLCursor(CursorInterface):\n    \"\"\"\n    Object to represent a databricks workspace with methods to interact with clusters/jobs.\n\n    Args:\n        cursor: controls execution of commands on cluster or SQL Warehouse\n    \"\"\"\n\n    def __init__(self, cursor: object) -&gt; None:\n        self.cursor = cursor\n\n    def execute(self, query: str) -&gt; None:\n        \"\"\"\n        Prepares and runs a database query.\n\n        Args:\n            query: sql query to execute on the cluster or SQL Warehouse\n        \"\"\"\n        try:\n            self.cursor.execute(query)\n\n        except Exception as e:\n            logging.exception(\"error while executing the query\")\n            raise e\n\n    def fetch_all(self) -&gt; list:\n        \"\"\"\n        Gets all rows of a query.\n\n        Returns:\n            list: list of results\n        \"\"\"\n        try:\n            result = self.cursor.fetchall()\n            cols = [column[0] for column in self.cursor.description]\n            result = [list(x) for x in result]\n            df = pd.DataFrame(result)\n            df.columns = cols\n            return df\n        except Exception as e:\n            logging.exception(\"error while fetching rows from the query\")\n            raise e\n\n    def close(self) -&gt; None:\n        \"\"\"Closes the cursor.\"\"\"\n        try:\n            self.cursor.close()\n        except Exception as e:\n            logging.exception(\"error while closing the cursor\")\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/pyodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.pyodbc_sql_connector.PYODBCSQLCursor.execute","title":"<code>execute(query)</code>","text":"<p>Prepares and runs a database query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>sql query to execute on the cluster or SQL Warehouse</p> required Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/pyodbc_sql_connector.py</code> <pre><code>def execute(self, query: str) -&gt; None:\n    \"\"\"\n    Prepares and runs a database query.\n\n    Args:\n        query: sql query to execute on the cluster or SQL Warehouse\n    \"\"\"\n    try:\n        self.cursor.execute(query)\n\n    except Exception as e:\n        logging.exception(\"error while executing the query\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/pyodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.pyodbc_sql_connector.PYODBCSQLCursor.fetch_all","title":"<code>fetch_all()</code>","text":"<p>Gets all rows of a query.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>list of results</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/pyodbc_sql_connector.py</code> <pre><code>def fetch_all(self) -&gt; list:\n    \"\"\"\n    Gets all rows of a query.\n\n    Returns:\n        list: list of results\n    \"\"\"\n    try:\n        result = self.cursor.fetchall()\n        cols = [column[0] for column in self.cursor.description]\n        result = [list(x) for x in result]\n        df = pd.DataFrame(result)\n        df.columns = cols\n        return df\n    except Exception as e:\n        logging.exception(\"error while fetching rows from the query\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/pyodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.pyodbc_sql_connector.PYODBCSQLCursor.close","title":"<code>close()</code>","text":"<p>Closes the cursor.</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/pyodbc_sql_connector.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Closes the cursor.\"\"\"\n    try:\n        self.cursor.close()\n    except Exception as e:\n        logging.exception(\"error while closing the cursor\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/spark-connector/","title":"Spark Connector","text":""},{"location":"sdk/code-reference/query/connectors/spark-connector/#src.sdk.python.rtdip_sdk.connectors.grpc.spark_connector.SparkConnection","title":"<code>SparkConnection</code>","text":"<p>               Bases: <code>ConnectionInterface</code></p> <p>The Spark Connector enables running Spark Sql queries via a Spark Session.</p> <p>Additionally, this connector supports Spark Connect which was introduced in Pyspark 3.4.0 and allows Spark Sessions to connect to remote Spark Clusters. This enables Spark SQL to be constructed locally, but executed remotely. To find out more about Spark Connect and the connection string to be provided to the <code>spark_remote</code> parameter of the Spark Session, please see here.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>(optional, SparkSession)</code> <p>Provide an existing spark session if one exists. A new Spark Session will be created if not populated</p> <code>None</code> <code>spark_configuration</code> <code>(optional, dict)</code> <p>Spark configuration to be provided to the spark session</p> <code>None</code> <code>spark_libraries</code> <code>(optional, Libraries)</code> <p>Additional JARs to be included in the Spark Session.</p> <code>None</code> <code>spark_remote</code> <code>(optional, str)</code> <p>Remote connection string of Spark Server and any authentication details. The Spark Connect introduced in Pyspark 3.4.0 allows Spark Sessions to connect to remote Spark Clusters. This enables Spark SQL to be constructed locally, but executed remotely.</p> <code>None</code> Source code in <code>src/sdk/python/rtdip_sdk/connectors/grpc/spark_connector.py</code> <pre><code>class SparkConnection(ConnectionInterface):\n    \"\"\"\n    The Spark Connector enables running Spark Sql queries via a Spark Session.\n\n    Additionally, this connector supports Spark Connect which was introduced in Pyspark 3.4.0 and allows Spark Sessions to connect to remote Spark Clusters. This enables Spark SQL to be constructed locally, but executed remotely.\n    To find out more about Spark Connect and the connection string to be provided to the `spark_remote` parameter of the Spark Session, please see [here.](https://spark.apache.org/docs/latest/spark-connect-overview.html#specify-spark-connect-when-creating-spark-session)\n\n    Args:\n        spark (optional, SparkSession): Provide an existing spark session if one exists. A new Spark Session will be created if not populated\n        spark_configuration (optional, dict): Spark configuration to be provided to the spark session\n        spark_libraries (optional, Libraries): Additional JARs to be included in the Spark Session.\n        spark_remote (optional, str): Remote connection string of Spark Server and any authentication details. The Spark Connect introduced in Pyspark 3.4.0 allows Spark Sessions to connect to remote Spark Clusters. This enables Spark SQL to be constructed locally, but executed remotely.\n    \"\"\"\n\n    def __init__(\n        self,\n        spark: SparkSession = None,\n        spark_configuration: dict = None,\n        spark_libraries: Libraries = None,\n        spark_remote: str = None,\n    ) -&gt; None:\n        if spark_remote != None:\n            _package_version_meets_minimum(\"pyspark\", \"3.4.0\")\n\n        if spark is None:\n            self.connection = SparkClient(\n                spark_configuration=(\n                    {} if spark_configuration is None else spark_configuration\n                ),\n                spark_libraries=(\n                    Libraries() if spark_libraries is None else spark_libraries\n                ),\n                spark_remote=spark_remote,\n            ).spark_session\n        else:\n            self.connection = spark\n\n    def close(self) -&gt; None:\n        \"\"\"Not relevant for spark sessions\"\"\"\n        pass\n\n    def cursor(self) -&gt; object:\n        \"\"\"\n        Intiates the cursor and returns it.\n\n        Returns:\n          DatabricksSQLCursor: Object to represent a databricks workspace with methods to interact with clusters/jobs.\n        \"\"\"\n        try:\n            return SparkCursor(self.connection)\n        except Exception as e:\n            logging.exception(\"error with cursor object\")\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/spark-connector/#src.sdk.python.rtdip_sdk.connectors.grpc.spark_connector.SparkConnection.close","title":"<code>close()</code>","text":"<p>Not relevant for spark sessions</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/grpc/spark_connector.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Not relevant for spark sessions\"\"\"\n    pass\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/spark-connector/#src.sdk.python.rtdip_sdk.connectors.grpc.spark_connector.SparkConnection.cursor","title":"<code>cursor()</code>","text":"<p>Intiates the cursor and returns it.</p> <p>Returns:</p> Name Type Description <code>DatabricksSQLCursor</code> <code>object</code> <p>Object to represent a databricks workspace with methods to interact with clusters/jobs.</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/grpc/spark_connector.py</code> <pre><code>def cursor(self) -&gt; object:\n    \"\"\"\n    Intiates the cursor and returns it.\n\n    Returns:\n      DatabricksSQLCursor: Object to represent a databricks workspace with methods to interact with clusters/jobs.\n    \"\"\"\n    try:\n        return SparkCursor(self.connection)\n    except Exception as e:\n        logging.exception(\"error with cursor object\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/spark-connector/#src.sdk.python.rtdip_sdk.connectors.grpc.spark_connector.SparkCursor","title":"<code>SparkCursor</code>","text":"<p>               Bases: <code>CursorInterface</code></p> <p>Object to represent a spark session with methods to interact with clusters/jobs using the remote connection information.</p> <p>Parameters:</p> Name Type Description Default <code>cursor</code> <code>object</code> <p>controls execution of commands on Spark Cluster</p> required Source code in <code>src/sdk/python/rtdip_sdk/connectors/grpc/spark_connector.py</code> <pre><code>class SparkCursor(CursorInterface):\n    \"\"\"\n    Object to represent a spark session with methods to interact with clusters/jobs using the remote connection information.\n\n    Args:\n        cursor: controls execution of commands on Spark Cluster\n    \"\"\"\n\n    execute_result: DataFrame\n\n    def __init__(self, cursor: object) -&gt; None:\n        self.cursor = cursor\n\n    def execute(self, query: str) -&gt; None:\n        \"\"\"\n        Prepares and runs a database query.\n\n        Args:\n            query: sql query to execute on the cluster or SQL Warehouse\n        \"\"\"\n        try:\n            self.execute_result = self.cursor.sql(query)\n        except Exception as e:\n            logging.exception(\"error while executing the query\")\n            raise e\n\n    def fetch_all(self) -&gt; DataFrame:\n        \"\"\"\n        Gets all rows of a query.\n\n        Returns:\n          DataFrame: Spark DataFrame of results\n        \"\"\"\n        try:\n            df = self.execute_result\n            return df\n        except Exception as e:\n            logging.exception(\"error while fetching the rows of a query\")\n            raise e\n\n    def close(self) -&gt; None:\n        \"\"\"Not relevant for dataframes\"\"\"\n        pass\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/spark-connector/#src.sdk.python.rtdip_sdk.connectors.grpc.spark_connector.SparkCursor.execute","title":"<code>execute(query)</code>","text":"<p>Prepares and runs a database query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>sql query to execute on the cluster or SQL Warehouse</p> required Source code in <code>src/sdk/python/rtdip_sdk/connectors/grpc/spark_connector.py</code> <pre><code>def execute(self, query: str) -&gt; None:\n    \"\"\"\n    Prepares and runs a database query.\n\n    Args:\n        query: sql query to execute on the cluster or SQL Warehouse\n    \"\"\"\n    try:\n        self.execute_result = self.cursor.sql(query)\n    except Exception as e:\n        logging.exception(\"error while executing the query\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/spark-connector/#src.sdk.python.rtdip_sdk.connectors.grpc.spark_connector.SparkCursor.fetch_all","title":"<code>fetch_all()</code>","text":"<p>Gets all rows of a query.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Spark DataFrame of results</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/grpc/spark_connector.py</code> <pre><code>def fetch_all(self) -&gt; DataFrame:\n    \"\"\"\n    Gets all rows of a query.\n\n    Returns:\n      DataFrame: Spark DataFrame of results\n    \"\"\"\n    try:\n        df = self.execute_result\n        return df\n    except Exception as e:\n        logging.exception(\"error while fetching the rows of a query\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/spark-connector/#src.sdk.python.rtdip_sdk.connectors.grpc.spark_connector.SparkCursor.close","title":"<code>close()</code>","text":"<p>Not relevant for dataframes</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/grpc/spark_connector.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Not relevant for dataframes\"\"\"\n    pass\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/turbodbc-sql-connector/","title":"TURBODBC SQL Connector","text":""},{"location":"sdk/code-reference/query/connectors/turbodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.turbodbc_sql_connector.TURBODBCSQLConnection","title":"<code>TURBODBCSQLConnection</code>","text":"<p>               Bases: <code>ConnectionInterface</code></p> <p>Turbodbc is a python module used to access relational databases through an ODBC interface. It will allow a user to connect to databricks clusters or sql warehouses.</p> <p>Turbodbc offers built-in NumPy support allowing it to be much faster for processing compared to other connectors. To find details for SQL warehouses server_hostname and http_path location to the SQL Warehouse tab in the documentation.</p> <p>Parameters:</p> Name Type Description Default <code>server_hostname</code> <code>str</code> <p>hostname for the cluster or SQL Warehouse</p> required <code>http_path</code> <code>str</code> <p>Http path for the cluster or SQL Warehouse</p> required <code>access_token</code> <code>str</code> <p>Azure AD Token</p> required Note <p>More fields such as driver can be configured upon extension.</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/turbodbc_sql_connector.py</code> <pre><code>class TURBODBCSQLConnection(ConnectionInterface):\n    \"\"\"\n    Turbodbc is a python module used to access relational databases through an ODBC interface. It will allow a user to connect to databricks clusters or sql warehouses.\n\n    Turbodbc offers built-in NumPy support allowing it to be much faster for processing compared to other connectors.\n    To find details for SQL warehouses server_hostname and http_path location to the SQL Warehouse tab in the documentation.\n\n    Args:\n        server_hostname: hostname for the cluster or SQL Warehouse\n        http_path: Http path for the cluster or SQL Warehouse\n        access_token: Azure AD Token\n\n    Note:\n        More fields such as driver can be configured upon extension.\n    \"\"\"\n\n    def __init__(\n        self,\n        server_hostname: str,\n        http_path: str,\n        access_token: str,\n        return_type=ConnectionReturnType.Pandas,\n    ) -&gt; None:\n        _package_version_meets_minimum(\"turbodbc\", \"4.0.0\")\n        self.server_hostname = server_hostname\n        self.http_path = http_path\n        self.access_token = access_token\n        self.return_type = return_type\n        # call auth method\n        self.connection = self._connect()\n        self.open = True\n\n    def _connect(self):\n        \"\"\"Connects to the endpoint\"\"\"\n        try:\n            options = make_options(\n                autocommit=True, read_buffer_size=Megabytes(100), use_async_io=True\n            )\n\n            return connect(\n                Driver=\"Simba Spark ODBC Driver\",\n                Host=self.server_hostname,\n                Port=443,\n                SparkServerType=3,\n                ThriftTransport=2,\n                SSL=1,\n                AuthMech=11,\n                Auth_AccessToken=self.access_token,\n                Auth_Flow=0,\n                HTTPPath=self.http_path,\n                UseNativeQuery=1,\n                FastSQLPrepare=1,\n                ApplyFastSQLPrepareToAllQueries=1,\n                DisableLimitZero=1,\n                EnableAsyncExec=1,\n                RowsFetchedPerBlock=os.getenv(\"RTDIP_ODBC_ROW_BLOCK_SIZE\", 500000),\n                UserAgentEntry=\"RTDIP\",\n                turbodbc_options=options,\n            )\n\n        except Exception as e:\n            logging.exception(\"error while connecting to the endpoint\")\n            raise e\n\n    def close(self) -&gt; None:\n        \"\"\"Closes connection to database.\"\"\"\n        try:\n            self.connection.close()\n            self.open = False\n        except Exception as e:\n            logging.exception(\"error while closing the connection\")\n            raise e\n\n    def cursor(self) -&gt; object:\n        \"\"\"\n        Intiates the cursor and returns it.\n\n        Returns:\n          TURBODBCSQLCursor: Object to represent a databricks workspace with methods to interact with clusters/jobs.\n        \"\"\"\n        try:\n            if self.open == False:\n                self.connection = self._connect()\n            return TURBODBCSQLCursor(\n                self.connection.cursor(), return_type=self.return_type\n            )\n        except Exception as e:\n            logging.exception(\"error with cursor object\")\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/turbodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.turbodbc_sql_connector.TURBODBCSQLConnection.close","title":"<code>close()</code>","text":"<p>Closes connection to database.</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/turbodbc_sql_connector.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Closes connection to database.\"\"\"\n    try:\n        self.connection.close()\n        self.open = False\n    except Exception as e:\n        logging.exception(\"error while closing the connection\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/turbodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.turbodbc_sql_connector.TURBODBCSQLConnection.cursor","title":"<code>cursor()</code>","text":"<p>Intiates the cursor and returns it.</p> <p>Returns:</p> Name Type Description <code>TURBODBCSQLCursor</code> <code>object</code> <p>Object to represent a databricks workspace with methods to interact with clusters/jobs.</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/turbodbc_sql_connector.py</code> <pre><code>def cursor(self) -&gt; object:\n    \"\"\"\n    Intiates the cursor and returns it.\n\n    Returns:\n      TURBODBCSQLCursor: Object to represent a databricks workspace with methods to interact with clusters/jobs.\n    \"\"\"\n    try:\n        if self.open == False:\n            self.connection = self._connect()\n        return TURBODBCSQLCursor(\n            self.connection.cursor(), return_type=self.return_type\n        )\n    except Exception as e:\n        logging.exception(\"error with cursor object\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/turbodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.turbodbc_sql_connector.TURBODBCSQLCursor","title":"<code>TURBODBCSQLCursor</code>","text":"<p>               Bases: <code>CursorInterface</code></p> <p>Object to represent a databricks workspace with methods to interact with clusters/jobs.</p> <p>Parameters:</p> Name Type Description Default <code>cursor</code> <code>object</code> <p>controls execution of commands on cluster or SQL Warehouse</p> required Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/turbodbc_sql_connector.py</code> <pre><code>class TURBODBCSQLCursor(CursorInterface):\n    \"\"\"\n    Object to represent a databricks workspace with methods to interact with clusters/jobs.\n\n    Args:\n        cursor: controls execution of commands on cluster or SQL Warehouse\n    \"\"\"\n\n    def __init__(self, cursor: object, return_type=ConnectionReturnType.Pandas) -&gt; None:\n        self.cursor = cursor\n        self.return_type = return_type\n\n    def execute(self, query: str) -&gt; None:\n        \"\"\"\n        Prepares and runs a database query.\n\n        Args:\n            query: sql query to execute on the cluster or SQL Warehouse\n        \"\"\"\n        try:\n            self.cursor.execute(query)\n        except Exception as e:\n            logging.exception(\"error while executing the query\")\n            raise e\n\n    def fetch_all(self) -&gt; list:\n        \"\"\"\n        Gets all rows of a query.\n\n        Returns:\n            list: list of results\n        \"\"\"\n        try:\n            result = self.cursor.fetchallarrow()\n            if self.return_type == ConnectionReturnType.Pyarrow:\n                return result\n            elif self.return_type == ConnectionReturnType.Pandas:\n                return result.to_pandas()\n        except Exception as e:\n            logging.exception(\"error while fetching the rows from the query\")\n            raise e\n\n    def close(self) -&gt; None:\n        \"\"\"Closes the cursor.\"\"\"\n        try:\n            self.cursor.close()\n        except Exception as e:\n            logging.exception(\"error while closing the cursor\")\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/turbodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.turbodbc_sql_connector.TURBODBCSQLCursor.execute","title":"<code>execute(query)</code>","text":"<p>Prepares and runs a database query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>sql query to execute on the cluster or SQL Warehouse</p> required Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/turbodbc_sql_connector.py</code> <pre><code>def execute(self, query: str) -&gt; None:\n    \"\"\"\n    Prepares and runs a database query.\n\n    Args:\n        query: sql query to execute on the cluster or SQL Warehouse\n    \"\"\"\n    try:\n        self.cursor.execute(query)\n    except Exception as e:\n        logging.exception(\"error while executing the query\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/turbodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.turbodbc_sql_connector.TURBODBCSQLCursor.fetch_all","title":"<code>fetch_all()</code>","text":"<p>Gets all rows of a query.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>list of results</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/turbodbc_sql_connector.py</code> <pre><code>def fetch_all(self) -&gt; list:\n    \"\"\"\n    Gets all rows of a query.\n\n    Returns:\n        list: list of results\n    \"\"\"\n    try:\n        result = self.cursor.fetchallarrow()\n        if self.return_type == ConnectionReturnType.Pyarrow:\n            return result\n        elif self.return_type == ConnectionReturnType.Pandas:\n            return result.to_pandas()\n    except Exception as e:\n        logging.exception(\"error while fetching the rows from the query\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/connectors/turbodbc-sql-connector/#src.sdk.python.rtdip_sdk.connectors.odbc.turbodbc_sql_connector.TURBODBCSQLCursor.close","title":"<code>close()</code>","text":"<p>Closes the cursor.</p> Source code in <code>src/sdk/python/rtdip_sdk/connectors/odbc/turbodbc_sql_connector.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Closes the cursor.\"\"\"\n    try:\n        self.cursor.close()\n    except Exception as e:\n        logging.exception(\"error while closing the cursor\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/metadata/","title":"Metadata Function","text":""},{"location":"sdk/code-reference/query/functions/metadata/#src.sdk.python.rtdip_sdk.queries.metadata.get","title":"<code>get(connection, parameters_dict)</code>","text":"<p>A function to return back the metadata by querying databricks SQL Warehouse using a connection specified by the user.</p> <p>The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.</p> <p>The available authentcation methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.</p> <p>This function requires the user to input a dictionary of parameters. (See Attributes table below)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>business_unit</code> <code>str</code> <p>Business unit</p> <code>region</code> <code>str</code> <p>Region</p> <code>asset</code> <code>str</code> <p>Asset</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>tag_names</code> <code>(optional, list)</code> <p>Either pass a list of tagname/tagnames [\"tag_1\", \"tag_2\"] or leave the list blank [] or leave the parameter out completely</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>case_insensitivity_tag_search</code> <code>optional bool</code> <p>Search for tags using case insensitivity with True or case sensitivity with False</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of metadata.</p> <p>Warning</p> <p>Setting <code>case_insensitivity_tag_search</code> to True will result in a longer query time.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/metadata.py</code> <pre><code>def get(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    A function to return back the metadata by querying databricks SQL Warehouse using a connection specified by the user.\n\n    The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.\n\n    The available authentcation methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.\n\n    This function requires the user to input a dictionary of parameters. (See Attributes table below)\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict: A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        business_unit (str): Business unit\n        region (str): Region\n        asset (str): Asset\n        data_security_level (str): Level of data security\n        tag_names (optional, list): Either pass a list of tagname/tagnames [\"tag_1\", \"tag_2\"] or leave the list blank [] or leave the parameter out completely\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n        case_insensitivity_tag_search (optional bool): Search for tags using case insensitivity with True or case sensitivity with False\n\n    Returns:\n        DataFrame: A dataframe of metadata.\n\n    !!! warning\n        Setting `case_insensitivity_tag_search` to True will result in a longer query time.\n\n    \"\"\"\n    try:\n        query = _query_builder(parameters_dict, \"metadata\")\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error returning metadata function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/metadata/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .metadata(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n    )\n)\n\nprint(data)\n</code></pre> <p>This example is using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code>, <code>TURBODBCSQLConnection()</code> or <code>SparkConnection()</code>.</p> <p>Note</p> <p>See Samples Repository for full list of examples.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/functions/time_series/circular-average/","title":"Circular Average Function","text":""},{"location":"sdk/code-reference/query/functions/time_series/circular-average/#src.sdk.python.rtdip_sdk.queries.time_series.circular_average.get","title":"<code>get(connection, parameters_dict)</code>","text":"<p>A function that receives a dataframe of raw tag data and computes the circular mean for samples in a range, returning the results.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>business_unit</code> <code>str</code> <p>Business unit</p> <code>region</code> <code>str</code> <p>Region</p> <code>asset</code> <code>str</code> <p>Asset</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>tag_names</code> <code>list</code> <p>List of tagname or tagnames</p> <code>start_date</code> <code>str</code> <p>Start date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>end_date</code> <code>str</code> <p>End date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>time_interval_rate</code> <code>str</code> <p>The time interval rate (numeric input)</p> <code>time_interval_unit</code> <code>str</code> <p>The time interval unit (second, minute, day, hour)</p> <code>lower_bound</code> <code>int</code> <p>Lower boundary for the sample range</p> <code>upper_bound</code> <code>int</code> <p>Upper boundary for the sample range</p> <code>include_bad_data</code> <code>bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>pivot</code> <code>bool</code> <p>Pivot the data on timestamp column with True or do not pivot the data with False</p> <code>display_uom</code> <code>optional bool</code> <p>Display the unit of measure with True or False. Does not apply to pivoted tables. Defaults to False</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>case_insensitivity_tag_search</code> <code>optional bool</code> <p>Search for tags using case insensitivity with True or case sensitivity with False</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe containing the circular averages.</p> <p>Warning</p> <p>Setting <code>case_insensitivity_tag_search</code> to True will result in a longer query time.</p> <p>Note</p> <p><code>display_uom</code> True will not work in conjunction with <code>pivot</code> set to True.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/circular_average.py</code> <pre><code>def get(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    A function that receives a dataframe of raw tag data and computes the circular mean for samples in a range, returning the results.\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict (dict): A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        business_unit (str): Business unit\n        region (str): Region\n        asset (str): Asset\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        tag_names (list): List of tagname or tagnames\n        start_date (str): Start date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        time_interval_rate (str): The time interval rate (numeric input)\n        time_interval_unit (str): The time interval unit (second, minute, day, hour)\n        lower_bound (int): Lower boundary for the sample range\n        upper_bound (int): Upper boundary for the sample range\n        include_bad_data (bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        pivot (bool): Pivot the data on timestamp column with True or do not pivot the data with False\n        display_uom (optional bool): Display the unit of measure with True or False. Does not apply to pivoted tables. Defaults to False\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n        case_insensitivity_tag_search (optional bool): Search for tags using case insensitivity with True or case sensitivity with False\n\n    Returns:\n        DataFrame: A dataframe containing the circular averages.\n\n    !!! warning\n        Setting `case_insensitivity_tag_search` to True will result in a longer query time.\n\n    !!! Note\n        `display_uom` True will not work in conjunction with `pivot` set to True.\n    \"\"\"\n    if isinstance(parameters_dict[\"tag_names\"], list) is False:\n        raise ValueError(\"tag_names must be a list\")\n\n    if \"pivot\" in parameters_dict and \"display_uom\" in parameters_dict:\n        if parameters_dict[\"pivot\"] is True and parameters_dict[\"display_uom\"] is True:\n            raise ValueError(\"pivot True and display_uom True cannot be used together\")\n\n    try:\n        query = _query_builder(parameters_dict, \"circular_average\")\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error with circular average function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/circular-average/#example","title":"Example","text":"<pre><code> from rtdip_sdk.authentication.azure import DefaultAuth\n from rtdip_sdk.connectors import DatabricksSQLConnection\n from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n auth = DefaultAuth().authenticate()\n token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n data = (\n     TimeSeriesQueryBuilder()\n     .connect(connection)\n     .source(\"{tablename_or_path}\")\n     .circular_average(\n         tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n         start_date=\"2023-01-01\",\n         end_date=\"2023-01-31\",\n         time_interval_rate=\"15\",\n         time_interval_unit=\"minute\",\n         lower_bound=\"0\",\n         upper_bound=\"360\",\n     )\n )\n\n print(data)\n</code></pre> <p>This example is using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code>, <code>TURBODBCSQLConnection()</code> or <code>SparkConnection()</code>.</p> <p>Note</p> <p>See Samples Repository for full list of examples.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/functions/time_series/circular-standard-deviation/","title":"Circular Standard Deviation Function","text":""},{"location":"sdk/code-reference/query/functions/time_series/circular-standard-deviation/#src.sdk.python.rtdip_sdk.queries.time_series.circular_standard_deviation.get","title":"<code>get(connection, parameters_dict)</code>","text":"<p>A function that receives a dataframe of raw tag data and computes the circular standard deviation for samples assumed to be in the range, returning the results.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>business_unit</code> <code>str</code> <p>Business unit</p> <code>region</code> <code>str</code> <p>Region</p> <code>asset</code> <code>str</code> <p>Asset</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>tag_names</code> <code>list</code> <p>List of tagname or tagnames</p> <code>start_date</code> <code>str</code> <p>Start date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>end_date</code> <code>str</code> <p>End date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>time_interval_rate</code> <code>str</code> <p>The time interval rate (numeric input)</p> <code>time_interval_unit</code> <code>str</code> <p>The time interval unit (second, minute, day, hour)</p> <code>lower_bound</code> <code>int</code> <p>Lower boundary for the sample range</p> <code>upper_bound</code> <code>int</code> <p>Upper boundary for the sample range</p> <code>include_bad_data</code> <code>bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>pivot</code> <code>bool</code> <p>Pivot the data on timestamp column with True or do not pivot the data with False</p> <code>display_uom</code> <code>optional bool</code> <p>Display the unit of measure with True or False. Defaults to False</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>case_insensitivity_tag_search</code> <code>optional bool</code> <p>Search for tags using case insensitivity with True or case sensitivity with False</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe containing the circular standard deviations.</p> <p>Warning</p> <p>Setting <code>case_insensitivity_tag_search</code> to True will result in a longer query time.</p> <p>Note</p> <p><code>display_uom</code> True will not work in conjunction with <code>pivot</code> set to True.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/circular_standard_deviation.py</code> <pre><code>def get(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    A function that receives a dataframe of raw tag data and computes the circular standard deviation for samples assumed to be in the range, returning the results.\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict (dict): A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        business_unit (str): Business unit\n        region (str): Region\n        asset (str): Asset\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        tag_names (list): List of tagname or tagnames\n        start_date (str): Start date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        time_interval_rate (str): The time interval rate (numeric input)\n        time_interval_unit (str): The time interval unit (second, minute, day, hour)\n        lower_bound (int): Lower boundary for the sample range\n        upper_bound (int): Upper boundary for the sample range\n        include_bad_data (bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        pivot (bool): Pivot the data on timestamp column with True or do not pivot the data with False\n        display_uom (optional bool): Display the unit of measure with True or False. Defaults to False\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n        case_insensitivity_tag_search (optional bool): Search for tags using case insensitivity with True or case sensitivity with False\n\n    Returns:\n        DataFrame: A dataframe containing the circular standard deviations.\n\n    !!! warning\n        Setting `case_insensitivity_tag_search` to True will result in a longer query time.\n\n    !!! Note\n        `display_uom` True will not work in conjunction with `pivot` set to True.\n    \"\"\"\n    if isinstance(parameters_dict[\"tag_names\"], list) is False:\n        raise ValueError(\"tag_names must be a list\")\n\n    if \"pivot\" in parameters_dict and \"display_uom\" in parameters_dict:\n        if parameters_dict[\"pivot\"] is True and parameters_dict[\"display_uom\"] is True:\n            raise ValueError(\"pivot True and display_uom True cannot be used together\")\n\n    try:\n        query = _query_builder(parameters_dict, \"circular_standard_deviation\")\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error with circular standard_deviation function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/circular-standard-deviation/#example","title":"Example","text":"<pre><code> from rtdip_sdk.authentication.azure import DefaultAuth\n from rtdip_sdk.connectors import DatabricksSQLConnection\n from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n auth = DefaultAuth().authenticate()\n token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n data = (\n     TimeSeriesQueryBuilder()\n     .connect(connection)\n     .source(\"{tablename_or_path}\")\n     .circular_standard_deviation(\n         tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n         start_date=\"2023-01-01\",\n         end_date=\"2023-01-31\",\n         time_interval_rate=\"15\",\n         time_interval_unit=\"minute\",\n         lower_bound=\"0\",\n         upper_bound=\"360\",\n     )\n )\n\n print(data)\n</code></pre> <p>This example is using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code>, <code>TURBODBCSQLConnection()</code> or <code>SparkConnection()</code>.</p> <p>Note</p> <p>See Samples Repository for full list of examples.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/functions/time_series/interpolate/","title":"Interpolate Function","text":""},{"location":"sdk/code-reference/query/functions/time_series/interpolate/#src.sdk.python.rtdip_sdk.queries.time_series.interpolate.get","title":"<code>get(connection, parameters_dict)</code>","text":"<p>An RTDIP interpolation function that is intertwined with the RTDIP Resampling function.</p> <p>The Interpolation function will forward fill, backward fill or linearly interpolate the resampled data depending users specified interpolation method.</p> <p>This function requires the user to input a dictionary of parameters. (See Attributes table below.)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>business_unit</code> <code>str</code> <p>Business unit of the data</p> <code>region</code> <code>str</code> <p>Region</p> <code>asset</code> <code>str</code> <p>Asset</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>tag_names</code> <code>list</code> <p>List of tagname or tagnames [\"tag_1\", \"tag_2\"]</p> <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>time_interval_rate</code> <code>str</code> <p>The time interval rate (numeric input)</p> <code>time_interval_unit</code> <code>str</code> <p>The time interval unit (second, minute, day, hour)</p> <code>include_bad_data</code> <code>bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>pivot</code> <code>bool</code> <p>Pivot the data on timestamp column with True or do not pivot the data with False</p> <code>display_uom</code> <code>optional bool</code> <p>Display the unit of measure with True or False. Does not apply to pivoted tables. Defaults to False</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>sort</code> <code>optional bool</code> <p>Sort the data in ascending order by the TagName and Timestamp columns or, if pivot is True, by the Timestamp column</p> <code>case_insensitivity_tag_search</code> <code>optional bool</code> <p>Search for tags using case insensitivity with True or case sensitivity with False</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A resampled and interpolated dataframe.</p> <p>Warning</p> <p>Setting <code>case_insensitivity_tag_search</code> to True will result in a longer query time.</p> <p>Note</p> <p><code>display_uom</code> True will not work in conjunction with <code>pivot</code> set to True.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/interpolate.py</code> <pre><code>def get(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    An RTDIP interpolation function that is intertwined with the RTDIP Resampling function.\n\n    The Interpolation function will forward fill, backward fill or linearly interpolate the resampled data depending users specified interpolation method.\n\n    This function requires the user to input a dictionary of parameters. (See Attributes table below.)\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict: A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        business_unit (str): Business unit of the data\n        region (str): Region\n        asset (str):  Asset\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        tag_names (list): List of tagname or tagnames [\"tag_1\", \"tag_2\"]\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        time_interval_rate (str): The time interval rate (numeric input)\n        time_interval_unit (str): The time interval unit (second, minute, day, hour)\n        include_bad_data (bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        pivot (bool): Pivot the data on timestamp column with True or do not pivot the data with False\n        display_uom (optional bool): Display the unit of measure with True or False. Does not apply to pivoted tables. Defaults to False\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n        sort (optional bool): Sort the data in ascending order by the TagName and Timestamp columns or, if pivot is True, by the Timestamp column\n        case_insensitivity_tag_search (optional bool): Search for tags using case insensitivity with True or case sensitivity with False\n\n    Returns:\n        DataFrame: A resampled and interpolated dataframe.\n\n    !!! warning\n        Setting `case_insensitivity_tag_search` to True will result in a longer query time.\n\n    !!! Note\n        `display_uom` True will not work in conjunction with `pivot` set to True.\n\n    \"\"\"\n    if isinstance(parameters_dict[\"tag_names\"], list) is False:\n        raise ValueError(\"tag_names must be a list\")\n\n    if \"pivot\" in parameters_dict and \"display_uom\" in parameters_dict:\n        if parameters_dict[\"pivot\"] is True and parameters_dict[\"display_uom\"] is True:\n            raise ValueError(\"pivot True and display_uom True cannot be used together\")\n\n    try:\n        query = _query_builder(parameters_dict, \"interpolate\")\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error with interpolate function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/interpolate/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .interpolate(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n        time_interval_rate=\"15\",\n        time_interval_unit=\"minute\",\n    )\n)\n\nprint(data)\n</code></pre> <p>This example is using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code>, <code>TURBODBCSQLConnection()</code> or <code>SparkConnection()</code>.</p> <p>Note</p> <p>See Samples Repository for full list of examples.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/functions/time_series/interpolation-at-time/","title":"Interpolation at Time Function","text":""},{"location":"sdk/code-reference/query/functions/time_series/interpolation-at-time/#src.sdk.python.rtdip_sdk.queries.time_series.interpolation_at_time.get","title":"<code>get(connection, parameters_dict)</code>","text":"<p>An RTDIP interpolation at time function which works out the linear interpolation at a specific time based on the points before and after.</p> <p>This function requires the user to input a dictionary of parameters. (See Attributes table below.)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>business_unit</code> <code>str</code> <p>Business unit of the data</p> <code>region</code> <code>str</code> <p>Region</p> <code>asset</code> <code>str</code> <p>Asset</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>tag_names</code> <code>str</code> <p>Name of the tag</p> <code>timestamps</code> <code>list</code> <p>List of timestamp or timestamps in the format YYY-MM-DDTHH:MM:SS or YYY-MM-DDTHH:MM:SS+zz:zz where %z is the timezone. (Example +00:00 is the UTC timezone)</p> <code>window_length</code> <code>int</code> <p>Add longer window time in days for the start or end of specified date to cater for edge cases.</p> <code>include_bad_data</code> <code>bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>pivot</code> <code>bool</code> <p>Pivot the data on timestamp column with True or do not pivot the data with False</p> <code>display_uom</code> <code>optional bool</code> <p>Display the unit of measure with True or False. Does not apply to pivoted tables. Defaults to False</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>case_insensitivity_tag_search</code> <code>optional bool</code> <p>Search for tags using case insensitivity with True or case sensitivity with False</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A interpolated at time dataframe.</p> <p>Warning</p> <p>Setting <code>case_insensitivity_tag_search</code> to True will result in a longer query time.</p> <p>Note</p> <p><code>display_uom</code> True will not work in conjunction with <code>pivot</code> set to True.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/interpolation_at_time.py</code> <pre><code>def get(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    An RTDIP interpolation at time function which works out the linear interpolation at a specific time based on the points before and after.\n\n    This function requires the user to input a dictionary of parameters. (See Attributes table below.)\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict: A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        business_unit (str): Business unit of the data\n        region (str): Region\n        asset (str):  Asset\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        tag_names (str): Name of the tag\n        timestamps (list): List of timestamp or timestamps in the format YYY-MM-DDTHH:MM:SS or YYY-MM-DDTHH:MM:SS+zz:zz where %z is the timezone. (Example +00:00 is the UTC timezone)\n        window_length (int): Add longer window time in days for the start or end of specified date to cater for edge cases.\n        include_bad_data (bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        pivot (bool): Pivot the data on timestamp column with True or do not pivot the data with False\n        display_uom (optional bool): Display the unit of measure with True or False. Does not apply to pivoted tables. Defaults to False\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n        case_insensitivity_tag_search (optional bool): Search for tags using case insensitivity with True or case sensitivity with False\n\n    Returns:\n        DataFrame: A interpolated at time dataframe.\n\n    !!! warning\n        Setting `case_insensitivity_tag_search` to True will result in a longer query time.\n\n    !!! Note\n        `display_uom` True will not work in conjunction with `pivot` set to True.\n    \"\"\"\n    if isinstance(parameters_dict[\"tag_names\"], list) is False:\n        raise ValueError(\"tag_names must be a list\")\n\n    if isinstance(parameters_dict[\"timestamps\"], list) is False:\n        raise ValueError(\"timestamps must be a list\")\n\n    if \"pivot\" in parameters_dict and \"display_uom\" in parameters_dict:\n        if parameters_dict[\"pivot\"] is True and parameters_dict[\"display_uom\"] is True:\n            raise ValueError(\"pivot True and display_uom True cannot be used together\")\n\n    try:\n        query = _query_builder(parameters_dict, \"interpolation_at_time\")\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error with interpolation at time function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/interpolation-at-time/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .interpolation_at_time(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        timestamp_filter=[\"2023-01-01T09:30:00\", \"2023-01-02T12:00:00\"],\n    )\n)\n\nprint(data)\n</code></pre> <p>This example is using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code>, <code>TURBODBCSQLConnection()</code> or <code>SparkConnection()</code>.</p> <p>Note</p> <p>See Samples Repository for full list of examples.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/functions/time_series/latest/","title":"Latest Function","text":""},{"location":"sdk/code-reference/query/functions/time_series/latest/#src.sdk.python.rtdip_sdk.queries.time_series.latest.get","title":"<code>get(connection, parameters_dict)</code>","text":"<p>A function to return the latest event values by querying databricks SQL Warehouse using a connection specified by the user.</p> <p>The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.</p> <p>The available authentcation methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.</p> <p>This function requires the user to input a dictionary of parameters. (See Attributes table below)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>business_unit</code> <code>str</code> <p>Business unit</p> <code>region</code> <code>str</code> <p>Region</p> <code>asset</code> <code>str</code> <p>Asset</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>tag_names</code> <code>(optional, list)</code> <p>Either pass a list of tagname/tagnames [\"tag_1\", \"tag_2\"] or leave the list blank [] or leave the parameter out completely</p> <code>display_uom</code> <code>optional bool</code> <p>Display the unit of measure with True or False. Does not apply to pivoted tables. Defaults to False</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>case_insensitivity_tag_search</code> <code>optional bool</code> <p>Search for tags using case insensitivity with True or case sensitivity with False</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of event latest values.</p> <p>Warning</p> <p>Setting <code>case_insensitivity_tag_search</code> to True will result in a longer query time.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/latest.py</code> <pre><code>def get(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    A function to return the latest event values by querying databricks SQL Warehouse using a connection specified by the user.\n\n    The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.\n\n    The available authentcation methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.\n\n    This function requires the user to input a dictionary of parameters. (See Attributes table below)\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict: A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        business_unit (str): Business unit\n        region (str): Region\n        asset (str): Asset\n        data_security_level (str): Level of data security\n        tag_names (optional, list): Either pass a list of tagname/tagnames [\"tag_1\", \"tag_2\"] or leave the list blank [] or leave the parameter out completely\n        display_uom (optional bool): Display the unit of measure with True or False. Does not apply to pivoted tables. Defaults to False\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n        case_insensitivity_tag_search (optional bool): Search for tags using case insensitivity with True or case sensitivity with False\n\n    Returns:\n        DataFrame: A dataframe of event latest values.\n\n    !!! warning\n        Setting `case_insensitivity_tag_search` to True will result in a longer query time.\n    \"\"\"\n    try:\n        query = _query_builder(parameters_dict, \"latest\")\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error returning latest function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/latest/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .latest(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n    )\n)\n\nprint(data)\n</code></pre> <p>This example is using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code>, <code>TURBODBCSQLConnection()</code> or <code>SparkConnection()</code>.</p> <p>Note</p> <p>See Samples Repository for full list of examples.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/functions/time_series/plot/","title":"Plot Function","text":""},{"location":"sdk/code-reference/query/functions/time_series/plot/#src.sdk.python.rtdip_sdk.queries.time_series.plot.get","title":"<code>get(connection, parameters_dict)</code>","text":"<p>An RTDIP Resampling plot function in spark to resample data to find the First, Last, Min, Max and Exception Value for the time interval by querying databricks SQL warehouses using a connection and authentication method specified by the user. This spark resample function will return a resampled dataframe.</p> <p>The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.</p> <p>The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.</p> <p>This function requires the user to input a dictionary of parameters. (See Attributes table below)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>business_unit</code> <code>str</code> <p>Business unit of the data</p> <code>region</code> <code>str</code> <p>Region</p> <code>asset</code> <code>str</code> <p>Asset</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>tag_names</code> <code>list</code> <p>List of tagname or tagnames [\"tag_1\", \"tag_2\"]</p> <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>time_interval_rate</code> <code>str</code> <p>The time interval rate (numeric input)</p> <code>time_interval_unit</code> <code>str</code> <p>The time interval unit (second, minute, day, hour)</p> <code>include_bad_data</code> <code>bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>pivot</code> <code>optional bool</code> <p>Pivot the data on timestamp column with True or do not pivot the data with False</p> <code>display_uom</code> <code>optional bool</code> <p>Display the unit of measure with True or False. Does not apply to pivoted tables. Defaults to False</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>case_insensitivity_tag_search</code> <code>optional bool</code> <p>Search for tags using case insensitivity with True or case sensitivity with False</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A resampled dataframe.</p> <p>Warning</p> <p>Setting <code>case_insensitivity_tag_search</code> to True will result in a longer query time.</p> <p>Note</p> <p><code>display_uom</code> True will not work in conjunction with <code>pivot</code> set to True.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/plot.py</code> <pre><code>def get(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    An RTDIP Resampling plot function in spark to resample data to find the First, Last, Min, Max and Exception Value for the time interval by querying databricks SQL warehouses using a connection and authentication method specified by the user. This spark resample function will return a resampled dataframe.\n\n    The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.\n\n    The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.\n\n    This function requires the user to input a dictionary of parameters. (See Attributes table below)\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict: A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        business_unit (str): Business unit of the data\n        region (str): Region\n        asset (str):  Asset\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        tag_names (list): List of tagname or tagnames [\"tag_1\", \"tag_2\"]\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        time_interval_rate (str): The time interval rate (numeric input)\n        time_interval_unit (str): The time interval unit (second, minute, day, hour)\n        include_bad_data (bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        pivot (optional bool): Pivot the data on timestamp column with True or do not pivot the data with False\n        display_uom (optional bool): Display the unit of measure with True or False. Does not apply to pivoted tables. Defaults to False\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n        case_insensitivity_tag_search (optional bool): Search for tags using case insensitivity with True or case sensitivity with False\n\n    Returns:\n        DataFrame: A resampled dataframe.\n\n    !!! warning\n        Setting `case_insensitivity_tag_search` to True will result in a longer query time.\n\n    !!! Note\n        `display_uom` True will not work in conjunction with `pivot` set to True.\n    \"\"\"\n    if isinstance(parameters_dict[\"tag_names\"], list) is False:\n        raise ValueError(\"tag_names must be a list\")\n\n    if \"pivot\" in parameters_dict and \"display_uom\" in parameters_dict:\n        if parameters_dict[\"pivot\"] is True and parameters_dict[\"display_uom\"] is True:\n            raise ValueError(\"pivot True and display_uom True cannot be used together\")\n\n    try:\n        query = _query_builder(parameters_dict, \"plot\")\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error with plot resampling function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/plot/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .plot(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n        time_interval_rate=\"15\",\n        time_interval_unit=\"minute\",\n    )\n)\n\nprint(data)\n</code></pre> <p>This example is using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code>, <code>TURBODBCSQLConnection()</code> or <code>SparkConnection()</code>.</p> <p>Note</p> <p>See Samples Repository for full list of examples.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/functions/time_series/raw/","title":"Raw Function","text":""},{"location":"sdk/code-reference/query/functions/time_series/raw/#src.sdk.python.rtdip_sdk.queries.time_series.raw.get","title":"<code>get(connection, parameters_dict)</code>","text":"<p>A function to return back raw data by querying databricks SQL Warehouse using a connection specified by the user.</p> <p>The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.</p> <p>The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.</p> <p>This function requires the user to input a dictionary of parameters. (See Attributes table below)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>business_unit</code> <code>str</code> <p>Business unit</p> <code>region</code> <code>str</code> <p>Region</p> <code>asset</code> <code>str</code> <p>Asset</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>tag_names</code> <code>list</code> <p>List of tagname or tagnames [\"tag_1\", \"tag_2\"]</p> <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>include_bad_data</code> <code>bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>display_uom</code> <code>optional bool</code> <p>Display the unit of measure with True or False. Defaults to False</p> <code>sort</code> <code>optional bool</code> <p>Sort the data in ascending order by the TagName and Timestamp columns</p> <code>case_insensitivity_tag_search</code> <code>optional bool</code> <p>Search for tags using case insensitivity with True or case sensitivity with False</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of raw timeseries data.</p> <p>Warning</p> <p>Setting <code>case_insensitivity_tag_search</code> to True will result in a longer query time.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/raw.py</code> <pre><code>def get(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    A function to return back raw data by querying databricks SQL Warehouse using a connection specified by the user.\n\n    The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.\n\n    The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.\n\n    This function requires the user to input a dictionary of parameters. (See Attributes table below)\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict: A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        business_unit (str): Business unit\n        region (str): Region\n        asset (str): Asset\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        tag_names (list): List of tagname or tagnames [\"tag_1\", \"tag_2\"]\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        include_bad_data (bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n        display_uom (optional bool): Display the unit of measure with True or False. Defaults to False\n        sort (optional bool): Sort the data in ascending order by the TagName and Timestamp columns\n        case_insensitivity_tag_search (optional bool): Search for tags using case insensitivity with True or case sensitivity with False\n\n    Returns:\n        DataFrame: A dataframe of raw timeseries data.\n\n    !!! warning\n        Setting `case_insensitivity_tag_search` to True will result in a longer query time.\n    \"\"\"\n    try:\n        query = _query_builder(parameters_dict, \"raw\")\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error with raw function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/raw/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .raw(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n        include_bad_data=True,\n    )\n)\n\nprint(data)\n</code></pre> <p>This example is using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code>, <code>TURBODBCSQLConnection()</code> or <code>SparkConnection()</code>.</p> <p>Note</p> <p>See Samples Repository for full list of examples.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/functions/time_series/resample/","title":"Resample Function","text":""},{"location":"sdk/code-reference/query/functions/time_series/resample/#src.sdk.python.rtdip_sdk.queries.time_series.resample.get","title":"<code>get(connection, parameters_dict)</code>","text":"<p>An RTDIP Resampling function in spark to resample data by querying databricks SQL warehouses using a connection and authentication method specified by the user. This spark resample function will return a resampled dataframe.</p> <p>The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.</p> <p>The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.</p> <p>This function requires the user to input a dictionary of parameters. (See Attributes table below)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>business_unit</code> <code>str</code> <p>Business unit of the data</p> <code>region</code> <code>str</code> <p>Region</p> <code>asset</code> <code>str</code> <p>Asset</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>tag_names</code> <code>list</code> <p>List of tagname or tagnames [\"tag_1\", \"tag_2\"]</p> <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>time_interval_rate</code> <code>str</code> <p>The time interval rate (numeric input)</p> <code>time_interval_unit</code> <code>str</code> <p>The time interval unit (second, minute, day, hour)</p> <code>agg_method</code> <code>str</code> <p>Aggregation Method (first, last, avg, min, max)</p> <code>include_bad_data</code> <code>bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>fill</code> <code>optional bool</code> <p>Fill the data with intervals where no data exists. The Value column will be filled with Null</p> <code>pivot</code> <code>optional bool</code> <p>Pivot the data on timestamp column with True or do not pivot the data with False</p> <code>display_uom</code> <code>optional bool</code> <p>Display the unit of measure with True or False. Does not apply to pivoted tables. Defaults to False</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>sort</code> <code>optional bool</code> <p>Sort the data in ascending order by the TagName and Timestamp columns or, if pivot is True, by the Timestamp column</p> <code>case_insensitivity_tag_search</code> <code>optional bool</code> <p>Search for tags using case insensitivity with True or case sensitivity with False</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A resampled dataframe.</p> <p>Warning</p> <p>Setting <code>case_insensitivity_tag_search</code> to True will result in a longer query time.</p> <p>Note</p> <p><code>display_uom</code> True will not work in conjunction with <code>pivot</code> set to True.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/resample.py</code> <pre><code>def get(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    An RTDIP Resampling function in spark to resample data by querying databricks SQL warehouses using a connection and authentication method specified by the user. This spark resample function will return a resampled dataframe.\n\n    The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.\n\n    The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.\n\n    This function requires the user to input a dictionary of parameters. (See Attributes table below)\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict: A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        business_unit (str): Business unit of the data\n        region (str): Region\n        asset (str):  Asset\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        tag_names (list): List of tagname or tagnames [\"tag_1\", \"tag_2\"]\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        time_interval_rate (str): The time interval rate (numeric input)\n        time_interval_unit (str): The time interval unit (second, minute, day, hour)\n        agg_method (str): Aggregation Method (first, last, avg, min, max)\n        include_bad_data (bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        fill (optional bool): Fill the data with intervals where no data exists. The Value column will be filled with Null\n        pivot (optional bool): Pivot the data on timestamp column with True or do not pivot the data with False\n        display_uom (optional bool): Display the unit of measure with True or False. Does not apply to pivoted tables. Defaults to False\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n        sort (optional bool): Sort the data in ascending order by the TagName and Timestamp columns or, if pivot is True, by the Timestamp column\n        case_insensitivity_tag_search (optional bool): Search for tags using case insensitivity with True or case sensitivity with False\n\n\n    Returns:\n        DataFrame: A resampled dataframe.\n\n    !!! warning\n        Setting `case_insensitivity_tag_search` to True will result in a longer query time.\n\n    !!! Note\n        `display_uom` True will not work in conjunction with `pivot` set to True.\n    \"\"\"\n\n    if isinstance(parameters_dict[\"tag_names\"], list) is False:\n        raise ValueError(\"tag_names must be a list\")\n\n    if \"pivot\" in parameters_dict and \"display_uom\" in parameters_dict:\n        if parameters_dict[\"pivot\"] is True and parameters_dict[\"display_uom\"] is True:\n            raise ValueError(\"pivot True and display_uom True cannot be used together\")\n\n    try:\n        query = _query_builder(parameters_dict, \"resample\")\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error with resampling function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/resample/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .resample(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n        time_interval_rate=\"15\",\n        time_interval_unit=\"minute\",\n        agg_method=\"first\",\n    )\n)\n\nprint(data)\n</code></pre> <p>This example is using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code>, <code>TURBODBCSQLConnection()</code> or <code>SparkConnection()</code>.</p> <p>Note</p> <p>See Samples Repository for full list of examples.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/functions/time_series/summary/","title":"Summary Function","text":""},{"location":"sdk/code-reference/query/functions/time_series/summary/#src.sdk.python.rtdip_sdk.queries.time_series.summary.get","title":"<code>get(connection, parameters_dict)</code>","text":"<p>A function to return back a summary of statistics (Avg, Min, Max, Count, StDev, Sum, Variance) by querying databricks SQL Warehouse using a connection specified by the user.</p> <p>The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.</p> <p>The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.</p> <p>This function requires the user to input a dictionary of parameters. (See Attributes table below)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>business_unit</code> <code>str</code> <p>Business unit</p> <code>region</code> <code>str</code> <p>Region</p> <code>asset</code> <code>str</code> <p>Asset</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>tag_names</code> <code>list</code> <p>List of tagname or tagnames [\"tag_1\", \"tag_2\"]</p> <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>include_bad_data</code> <code>bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>display_uom</code> <code>optional bool</code> <p>Display the unit of measure with True or False. Does not apply to pivoted tables. Defaults to False</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>case_insensitivity_tag_search</code> <code>optional bool</code> <p>Search for tags using case insensitivity with True or case sensitivity with False</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of summary statistics.</p> <p>Warning</p> <p>Setting <code>case_insensitivity_tag_search</code> to True will result in a longer query time.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/summary.py</code> <pre><code>def get(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    A function to return back a summary of statistics (Avg, Min, Max, Count, StDev, Sum, Variance) by querying databricks SQL Warehouse using a connection specified by the user.\n\n    The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.\n\n    The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.\n\n    This function requires the user to input a dictionary of parameters. (See Attributes table below)\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict: A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        business_unit (str): Business unit\n        region (str): Region\n        asset (str): Asset\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        tag_names (list): List of tagname or tagnames [\"tag_1\", \"tag_2\"]\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        include_bad_data (bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        display_uom (optional bool): Display the unit of measure with True or False. Does not apply to pivoted tables. Defaults to False\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n        case_insensitivity_tag_search (optional bool): Search for tags using case insensitivity with True or case sensitivity with False\n\n    Returns:\n        DataFrame: A dataframe of summary statistics.\n\n    !!! warning\n        Setting `case_insensitivity_tag_search` to True will result in a longer query time.\n    \"\"\"\n    try:\n        query = _query_builder(parameters_dict, \"summary\")\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning summary dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error with summary function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/summary/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .summary(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n    )\n)\n\nprint(data)\n</code></pre> <p>This example is using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code>, <code>TURBODBCSQLConnection()</code> or <code>SparkConnection()</code>.</p> <p>Note</p> <p>See Samples Repository for full list of examples.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/functions/time_series/time-weighted-average/","title":"Time Weighted Average","text":""},{"location":"sdk/code-reference/query/functions/time_series/time-weighted-average/#src.sdk.python.rtdip_sdk.queries.time_series.time_weighted_average.get","title":"<code>get(connection, parameters_dict)</code>","text":"<p>A function that receives a dataframe of raw tag data and performs a time weighted averages, returning the results.</p> <p>This function requires the input of a pandas dataframe acquired via the rtdip.functions.raw() method and the user to input a dictionary of parameters. (See Attributes table below)</p> <p>Pi data points will either have step enabled (True) or step disabled (False). You can specify whether you want step to be fetched by \"Pi\" or you can set the step parameter to True/False in the dictionary below.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>business_unit</code> <code>str</code> <p>Business unit</p> <code>region</code> <code>str</code> <p>Region</p> <code>asset</code> <code>str</code> <p>Asset</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>tag_names</code> <code>list</code> <p>List of tagname or tagnames</p> <code>start_date</code> <code>str</code> <p>Start date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>end_date</code> <code>str</code> <p>End date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>time_interval_rate</code> <code>str</code> <p>The time interval rate (numeric input)</p> <code>time_interval_unit</code> <code>str</code> <p>The time interval unit (second, minute, day, hour)</p> <code>window_length</code> <code>int</code> <p>Add longer window time in days for the start or end of specified date to cater for edge cases.</p> <code>include_bad_data</code> <code>bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>step</code> <code>str</code> <p>data points with step \"enabled\" or \"disabled\". The options for step are \"true\", \"false\" or \"metadata\". \"metadata\" will retrieve the step value from the metadata table.</p> <code>display_uom</code> <code>optional bool</code> <p>Display the unit of measure with True or False. Does not apply to pivoted tables. Defaults to False</p> <code>pivot</code> <code>bool</code> <p>Pivot the data on timestamp column with True or do not pivot the data with False</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>case_insensitivity_tag_search</code> <code>optional bool</code> <p>Search for tags using case insensitivity with True or case sensitivity with False</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe containing the time weighted averages.</p> <p>Warning</p> <p>Setting <code>case_insensitivity_tag_search</code> to True will result in a longer query time.</p> <p>Note</p> <p><code>display_uom</code> True will not work in conjunction with <code>pivot</code> set to True.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_weighted_average.py</code> <pre><code>def get(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    A function that receives a dataframe of raw tag data and performs a time weighted averages, returning the results.\n\n    This function requires the input of a pandas dataframe acquired via the rtdip.functions.raw() method and the user to input a dictionary of parameters. (See Attributes table below)\n\n    Pi data points will either have step enabled (True) or step disabled (False). You can specify whether you want step to be fetched by \"Pi\" or you can set the step parameter to True/False in the dictionary below.\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict (dict): A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        business_unit (str): Business unit\n        region (str): Region\n        asset (str): Asset\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        tag_names (list): List of tagname or tagnames\n        start_date (str): Start date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        time_interval_rate (str): The time interval rate (numeric input)\n        time_interval_unit (str): The time interval unit (second, minute, day, hour)\n        window_length (int): Add longer window time in days for the start or end of specified date to cater for edge cases.\n        include_bad_data (bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        step (str): data points with step \"enabled\" or \"disabled\". The options for step are \"true\", \"false\" or \"metadata\". \"metadata\" will retrieve the step value from the metadata table.\n        display_uom (optional bool): Display the unit of measure with True or False. Does not apply to pivoted tables. Defaults to False\n        pivot (bool): Pivot the data on timestamp column with True or do not pivot the data with False\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n        case_insensitivity_tag_search (optional bool): Search for tags using case insensitivity with True or case sensitivity with False\n\n    Returns:\n        DataFrame: A dataframe containing the time weighted averages.\n\n    !!! warning\n        Setting `case_insensitivity_tag_search` to True will result in a longer query time.\n\n    !!! Note\n        `display_uom` True will not work in conjunction with `pivot` set to True.\n    \"\"\"\n    if isinstance(parameters_dict[\"tag_names\"], list) is False:\n        raise ValueError(\"tag_names must be a list\")\n\n    if \"pivot\" in parameters_dict and \"display_uom\" in parameters_dict:\n        if parameters_dict[\"pivot\"] is True and parameters_dict[\"display_uom\"] is True:\n            raise ValueError(\"pivot True and display_uom True cannot be used together\")\n\n    if \"step\" not in parameters_dict:  # default step to metadata if not provided\n        parameters_dict[\"step\"] = \"metadata\"\n\n    try:\n        query = _query_builder(parameters_dict, \"time_weighted_average\")\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error with time weighted average function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time-weighted-average/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .time_weighted_average(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n        time_interval_rate=\"15\",\n        time_interval_unit=\"minute\",\n        step=\"true\",\n    )\n)\n\nprint(data)\n</code></pre> <p>This example is using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code>, <code>TURBODBCSQLConnection()</code> or <code>SparkConnection()</code>.</p> <p>Note</p> <p>See Samples Repository for full list of examples.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/","title":"Query Builder","text":""},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder","title":"<code>TimeSeriesQueryBuilder</code>","text":"<p>A builder for developing RTDIP queries using any delta table.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>class TimeSeriesQueryBuilder:\n    \"\"\"\n    A builder for developing RTDIP queries using any delta table.\n    \"\"\"\n\n    parameters: dict\n    connection: ConnectionInterface\n    close_connection: bool\n    data_source: str\n    tagname_column: str\n    timestamp_column: str\n    status_column: str\n    value_column: str\n    metadata_source: str\n    metadata_tagname_column: str\n    metadata_uom_column: str\n\n    def __init__(self):\n        self.metadata_source = None\n        self.metadata_tagname_column = None\n        self.metadata_uom_column = None\n\n    def connect(self, connection: ConnectionInterface):\n        \"\"\"\n        Specifies the connection to be used for the query.\n\n        **Example:**\n        ```python\n        from rtdip_sdk.authentication.azure import DefaultAuth\n        from rtdip_sdk.connectors import DatabricksSQLConnection\n        from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n        auth = DefaultAuth().authenticate()\n        token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n        connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n        connect = (\n            TimeSeriesQueryBuilder()\n            .connect(connection)\n        )\n\n        ```\n\n        Args:\n            connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        \"\"\"\n        self.connection = connection\n        return self\n\n    def source(\n        self,\n        source: str,\n        tagname_column: str = \"TagName\",\n        timestamp_column: str = \"EventTime\",\n        status_column: Union[str, None] = \"Status\",\n        value_column: str = \"Value\",\n    ):\n        \"\"\"\n        Specifies the source of the query.\n\n        **Example:**\n        ```python\n        from rtdip_sdk.authentication.azure import DefaultAuth\n        from rtdip_sdk.connectors import DatabricksSQLConnection\n        from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n        auth = DefaultAuth().authenticate()\n        token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n        connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n        source = (\n            TimeSeriesQueryBuilder()\n            .connect(connection)\n            .source(\n                source=\"{tablename_or_path}\"\n            )\n        )\n\n        ```\n\n        Args:\n            source (str): Source of the query can be a Unity Catalog table, Hive metastore table or path\n            tagname_column (optional str): The column name in the source that contains the tagnames or series\n            timestamp_column (optional str): The timestamp column name in the source\n            status_column (optional str): The status column name in the source indicating `Good` or `Bad`. If this is not available, specify `None`\n            value_column (optional str): The value column name in the source which is normally a float or string value for the time series event\n        \"\"\"\n        self.data_source = \"`.`\".join(source.split(\".\"))\n        self.tagname_column = tagname_column\n        self.timestamp_column = timestamp_column\n        self.status_column = status_column\n        self.value_column = value_column\n        return self\n\n    def m_source(\n        self,\n        metadata_source: str,\n        metadata_tagname_column: str = \"TagName\",\n        metadata_uom_column: str = \"UoM\",\n    ):\n        \"\"\"\n        Specifies the Metadata source of the query. This is only required if display_uom is set to True or Step is set to \"metadata\". Otherwise, it is optional.\n\n        **Example:**\n        ```python\n        from rtdip_sdk.authentication.azure import DefaultAuth\n        from rtdip_sdk.connectors import DatabricksSQLConnection\n        from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n        auth = DefaultAuth().authenticate()\n        token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n        connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n        source = (\n            TimeSeriesQueryBuilder()\n            .connect(connection)\n            .source(\n                source=\"{tablename_or_path}\"\n            )\n            .m_source(\n                metadata_source=\"{metadata_table_or_path}\"\n                metadata_tagname_column=\"TagName\",\n                metadata_uom_column=\"UoM\")\n        )\n\n        ```\n\n        Args:\n            metadata_source (str): Source of the query can be a Unity Catalog table, Hive metastore table or path\n            metadata_tagname_column (optional str): The column name in the source that contains the tagnames or series\n            metadata_uom_column (optional str): The column name in the source that contains the unit of measure\n        \"\"\"\n        self.metadata_source = f\"`{'`.`'.join(metadata_source.split('.'))}`\"\n        self.metadata_tagname_column = metadata_tagname_column\n        self.metadata_uom_column = metadata_uom_column\n        return self\n\n    def raw(\n        self,\n        tagname_filter: [str],\n        start_date: str,\n        end_date: str,\n        include_bad_data: bool = False,\n        display_uom: bool = False,\n        sort: bool = True,\n        limit: int = None,\n        offset: int = None,\n    ) -&gt; DataFrame:\n        \"\"\"\n        A function to return back raw data.\n\n        **Example:**\n        ```python\n        from rtdip_sdk.authentication.azure import DefaultAuth\n        from rtdip_sdk.connectors import DatabricksSQLConnection\n        from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n        auth = DefaultAuth().authenticate()\n        token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n        connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n        data = (\n            TimeSeriesQueryBuilder()\n            .connect(connection)\n            .source(\"{tablename_or_path}\")\n            .raw(\n                tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n                start_date=\"2023-01-01\",\n                end_date=\"2023-01-31\",\n            )\n        )\n\n        print(data)\n\n        ```\n\n        Args:\n            tagname_filter (list str): List of tagnames to filter on the source\n            start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n            display_uom (optional bool): Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated\n            sort (optional bool): Sort the data in ascending order by the TagName and Timestamp columns\n            limit (optional int): The number of rows to be returned\n            offset (optional int): The number of rows to skip before returning rows\n\n        Returns:\n            DataFrame: A dataframe of raw timeseries data.\n        \"\"\"\n        raw_parameters = {\n            \"source\": self.data_source,\n            \"metadata_source\": self.metadata_source,\n            \"tag_names\": tagname_filter,\n            \"start_date\": start_date,\n            \"end_date\": end_date,\n            \"include_bad_data\": include_bad_data,\n            \"display_uom\": display_uom,\n            \"sort\": sort,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"tagname_column\": self.tagname_column,\n            \"timestamp_column\": self.timestamp_column,\n            \"status_column\": self.status_column,\n            \"value_column\": self.value_column,\n            \"metadata_tagname_column\": self.metadata_tagname_column,\n            \"metadata_uom_column\": self.metadata_uom_column,\n            \"supress_warning\": True,\n        }\n\n        if \"display_uom\" in raw_parameters and raw_parameters[\"display_uom\"] is True:\n            if raw_parameters[\"metadata_source\"] is None:\n                raise ValueError(\n                    \"display_uom True requires metadata_source to be populated\"\n                )\n\n        return raw.get(self.connection, raw_parameters)\n\n    def resample(\n        self,\n        tagname_filter: [str],\n        start_date: str,\n        end_date: str,\n        time_interval_rate: str,\n        time_interval_unit: str,\n        agg_method: str,\n        include_bad_data: bool = False,\n        fill: bool = False,\n        pivot: bool = False,\n        display_uom: bool = False,\n        sort: bool = True,\n        limit: int = None,\n        offset: int = None,\n    ) -&gt; DataFrame:\n        \"\"\"\n        A query to resample the source data.\n\n        **Example:**\n        ```python\n        from rtdip_sdk.authentication.azure import DefaultAuth\n        from rtdip_sdk.connectors import DatabricksSQLConnection\n        from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n        auth = DefaultAuth().authenticate()\n        token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n        connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n        data = (\n            TimeSeriesQueryBuilder()\n            .connect(connection)\n            .source(\"{tablename_or_path}\")\n            .resample(\n                tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n                start_date=\"2023-01-01\",\n                end_date=\"2023-01-31\",\n                time_interval_rate=\"15\",\n                time_interval_unit=\"minute\",\n                agg_method=\"first\",\n            )\n        )\n\n        print(data)\n\n        ```\n\n        Args:\n            tagname_filter (list str): List of tagnames to filter on the source\n            start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            time_interval_rate (str): The time interval rate (numeric input)\n            time_interval_unit (str): The time interval unit (second, minute, day, hour)\n            agg_method (str): Aggregation Method (first, last, avg, min, max)\n            include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n            fill (bool): Fill the data with intervals where no data exists. The Value column will be filled with Null\n            pivot (optional bool): Pivot the data on the timestamp column with True or do not pivot the data with False\n            display_uom (optional bool): Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated\n            sort (optional bool): Sort the data in ascending order by the TagName and Timestamp columns or, if pivot is True, by the Timestamp column\n            limit (optional int): The number of rows to be returned\n            offset (optional int): The number of rows to skip before returning rows\n\n        Returns:\n            DataFrame: A dataframe of resampled timeseries data.\n        \"\"\"\n\n        resample_parameters = {\n            \"source\": self.data_source,\n            \"metadata_source\": self.metadata_source,\n            \"tag_names\": tagname_filter,\n            \"start_date\": start_date,\n            \"end_date\": end_date,\n            \"include_bad_data\": include_bad_data,\n            \"time_interval_rate\": time_interval_rate,\n            \"time_interval_unit\": time_interval_unit,\n            \"agg_method\": agg_method,\n            \"fill\": fill,\n            \"pivot\": pivot,\n            \"display_uom\": display_uom,\n            \"sort\": sort,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"tagname_column\": self.tagname_column,\n            \"timestamp_column\": self.timestamp_column,\n            \"status_column\": self.status_column,\n            \"value_column\": self.value_column,\n            \"metadata_tagname_column\": self.metadata_tagname_column,\n            \"metadata_uom_column\": self.metadata_uom_column,\n            \"supress_warning\": True,\n        }\n\n        if (\n            \"display_uom\" in resample_parameters\n            and resample_parameters[\"display_uom\"] is True\n        ):\n            if resample_parameters[\"metadata_source\"] is None:\n                raise ValueError(\n                    \"display_uom True requires metadata_source to be populated\"\n                )\n\n        return resample.get(self.connection, resample_parameters)\n\n    def plot(\n        self,\n        tagname_filter: [str],\n        start_date: str,\n        end_date: str,\n        time_interval_rate: str,\n        time_interval_unit: str,\n        include_bad_data: bool = False,\n        pivot: bool = False,\n        display_uom: bool = False,\n        sort: bool = True,\n        limit: int = None,\n        offset: int = None,\n    ) -&gt; DataFrame:\n        \"\"\"\n        A query to plot the source data for a time interval for Min, Max, First, Last and an Exception Value(Status = Bad), if it exists.\n\n        **Example:**\n        ```python\n        from rtdip_sdk.authentication.azure import DefaultAuth\n        from rtdip_sdk.connectors import DatabricksSQLConnection\n        from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n        auth = DefaultAuth().authenticate()\n        token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n        connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n        data = (\n            TimeSeriesQueryBuilder()\n            .connect(connection)\n            .source(\"{tablename_or_path}\")\n            .plot(\n                tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n                start_date=\"2023-01-01\",\n                end_date=\"2023-01-31\",\n                time_interval_rate=\"15\",\n                time_interval_unit=\"minute\",\n            )\n        )\n\n        print(data)\n\n        ```\n\n        Args:\n            tagname_filter (list str): List of tagnames to filter on the source\n            start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            time_interval_rate (str): The time interval rate (numeric input)\n            time_interval_unit (str): The time interval unit (second, minute, day, hour)\n            include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n            pivot (optional bool): Pivot the data on the timestamp column with True or do not pivot the data with False\n            display_uom (optional bool): Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated\n            sort (optional bool): Sort the data in ascending order by the TagName and Timestamp columns\n            limit (optional int): The number of rows to be returned\n            offset (optional int): The number of rows to skip before returning rows\n\n        Returns:\n            DataFrame: A dataframe of resampled timeseries data.\n        \"\"\"\n\n        plot_parameters = {\n            \"source\": self.data_source,\n            \"metadata_source\": self.metadata_source,\n            \"tag_names\": tagname_filter,\n            \"start_date\": start_date,\n            \"end_date\": end_date,\n            \"time_interval_rate\": time_interval_rate,\n            \"time_interval_unit\": time_interval_unit,\n            \"include_bad_data\": include_bad_data,\n            \"pivot\": pivot,\n            \"display_uom\": display_uom,\n            \"sort\": sort,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"tagname_column\": self.tagname_column,\n            \"timestamp_column\": self.timestamp_column,\n            \"status_column\": self.status_column,\n            \"value_column\": self.value_column,\n            \"metadata_tagname_column\": self.metadata_tagname_column,\n            \"metadata_uom_column\": self.metadata_uom_column,\n            \"supress_warning\": True,\n        }\n\n        if \"display_uom\" in plot_parameters and plot_parameters[\"display_uom\"] is True:\n            if plot_parameters[\"metadata_source\"] is None:\n                raise ValueError(\n                    \"display_uom True requires metadata_source to be populated\"\n                )\n\n        return plot.get(self.connection, plot_parameters)\n\n    def interpolate(\n        self,\n        tagname_filter: [str],\n        start_date: str,\n        end_date: str,\n        time_interval_rate: str,\n        time_interval_unit: str,\n        include_bad_data: bool = False,\n        pivot: bool = False,\n        display_uom: bool = False,\n        sort: bool = True,\n        limit: int = None,\n        offset: int = None,\n    ) -&gt; DataFrame:\n        \"\"\"\n        The Interpolate function will forward fill, backward fill or linearly interpolate the resampled data depending on the parameters specified.\n\n        **Example:**\n        ```python\n        from rtdip_sdk.authentication.azure import DefaultAuth\n        from rtdip_sdk.connectors import DatabricksSQLConnection\n        from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n        auth = DefaultAuth().authenticate()\n        token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n        connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n        data = (\n            TimeSeriesQueryBuilder()\n            .connect(connection)\n            .source(\"{tablename_or_path}\")\n            .interpolate(\n                tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n                start_date=\"2023-01-01\",\n                end_date=\"2023-01-31\",\n                time_interval_rate=\"15\",\n                time_interval_unit=\"minute\",\n            )\n        )\n\n        print(data)\n\n        ```\n\n        Args:\n            tagname_filter (list str): List of tagnames to filter on the source\n            start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            time_interval_rate (str): The time interval rate (numeric input)\n            time_interval_unit (str): The time interval unit (second, minute, day, hour)\n            include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n            pivot (optional bool): Pivot the data on the timestamp column with True or do not pivot the data with False\n            display_uom (optional bool): Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated\n            sort (optional bool): Sort the data in ascending order by the TagName and Timestamp columns or, if pivot is True, by the Timestamp column\n            limit (optional int): The number of rows to be returned\n            offset (optional int): The number of rows to skip before returning rows\n\n        Returns:\n            DataFrame: A dataframe of interpolated timeseries data.\n        \"\"\"\n        interpolation_parameters = {\n            \"source\": self.data_source,\n            \"metadata_source\": self.metadata_source,\n            \"tag_names\": tagname_filter,\n            \"start_date\": start_date,\n            \"end_date\": end_date,\n            \"include_bad_data\": include_bad_data,\n            \"time_interval_rate\": time_interval_rate,\n            \"time_interval_unit\": time_interval_unit,\n            \"pivot\": pivot,\n            \"display_uom\": display_uom,\n            \"sort\": sort,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"tagname_column\": self.tagname_column,\n            \"timestamp_column\": self.timestamp_column,\n            \"status_column\": self.status_column,\n            \"value_column\": self.value_column,\n            \"metadata_tagname_column\": self.metadata_tagname_column,\n            \"metadata_uom_column\": self.metadata_uom_column,\n            \"supress_warning\": True,\n        }\n\n        if (\n            \"display_uom\" in interpolation_parameters\n            and interpolation_parameters[\"display_uom\"] is True\n        ):\n            if interpolation_parameters[\"metadata_source\"] is None:\n                raise ValueError(\n                    \"display_uom True requires metadata_source to be populated\"\n                )\n\n        return interpolate.get(self.connection, interpolation_parameters)\n\n    def interpolation_at_time(\n        self,\n        tagname_filter: [str],\n        timestamp_filter: [str],\n        include_bad_data: bool = False,\n        window_length: int = 1,\n        pivot: bool = False,\n        display_uom: bool = False,\n        limit: int = None,\n        offset: int = None,\n    ) -&gt; DataFrame:\n        \"\"\"\n        A interpolation at time function which works out the linear interpolation at a specific time based on the points before and after.\n\n        **Example:**\n        ```python\n        from rtdip_sdk.authentication.azure import DefaultAuth\n        from rtdip_sdk.connectors import DatabricksSQLConnection\n        from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n        auth = DefaultAuth().authenticate()\n        token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n        connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n        data = (\n            TimeSeriesQueryBuilder()\n            .connect(connection)\n            .source(\"{tablename_or_path}\")\n            .interpolation_at_time(\n                tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n                timestamp_filter=[\"2023-01-01T09:30:00\", \"2023-01-02T12:00:00\"],\n            )\n        )\n\n        print(data)\n\n        ```\n\n        Args:\n            tagname_filter (list str): List of tagnames to filter on the source\n            timestamp_filter (list): List of timestamp or timestamps in the format YYY-MM-DDTHH:MM:SS or YYY-MM-DDTHH:MM:SS+zz:zz where %z is the timezone. (Example +00:00 is the UTC timezone)\n            include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n            window_length (optional int): Add longer window time in days for the start or end of specified date to cater for edge cases\n            pivot (optional bool): Pivot the data on the timestamp column with True or do not pivot the data with False\n            display_uom (optional bool): Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated\n            limit (optional int): The number of rows to be returned\n            offset (optional int): The number of rows to skip before returning rows\n\n        Returns:\n            DataFrame: A dataframe of interpolation at time timeseries data\n        \"\"\"\n        interpolation_at_time_parameters = {\n            \"source\": self.data_source,\n            \"metadata_source\": self.metadata_source,\n            \"tag_names\": tagname_filter,\n            \"timestamps\": timestamp_filter,\n            \"include_bad_data\": include_bad_data,\n            \"window_length\": window_length,\n            \"pivot\": pivot,\n            \"display_uom\": display_uom,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"tagname_column\": self.tagname_column,\n            \"timestamp_column\": self.timestamp_column,\n            \"status_column\": self.status_column,\n            \"value_column\": self.value_column,\n            \"metadata_tagname_column\": self.metadata_tagname_column,\n            \"metadata_uom_column\": self.metadata_uom_column,\n            \"supress_warning\": True,\n        }\n\n        if (\n            \"display_uom\" in interpolation_at_time_parameters\n            and interpolation_at_time_parameters[\"display_uom\"] is True\n        ):\n            if interpolation_at_time_parameters[\"metadata_source\"] is None:\n                raise ValueError(\n                    \"display_uom True requires metadata_source to be populated\"\n                )\n\n        return interpolation_at_time.get(\n            self.connection, interpolation_at_time_parameters\n        )\n\n    def time_weighted_average(\n        self,\n        tagname_filter: [str],\n        start_date: str,\n        end_date: str,\n        time_interval_rate: str,\n        time_interval_unit: str,\n        step: str,\n        source_metadata: str = None,\n        include_bad_data: bool = False,\n        window_length: int = 1,\n        pivot: bool = False,\n        display_uom: bool = False,\n        limit: int = None,\n        offset: int = None,\n    ) -&gt; DataFrame:\n        \"\"\"\n        A function that receives a dataframe of raw tag data and performs a time weighted averages.\n\n        **Example:**\n        ```python\n        from rtdip_sdk.authentication.azure import DefaultAuth\n        from rtdip_sdk.connectors import DatabricksSQLConnection\n        from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n        auth = DefaultAuth().authenticate()\n        token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n        connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n        data = (\n            TimeSeriesQueryBuilder()\n            .connect(connection)\n            .source(\"{tablename_or_path}\")\n            .time_weighted_average(\n                tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n                start_date=\"2023-01-01\",\n                end_date=\"2023-01-31\",\n                time_interval_rate=\"15\",\n                time_interval_unit=\"minute\",\n                step=\"true\",\n            )\n        )\n\n        print(data)\n\n        ```\n\n        Args:\n            tagname_filter (list str): List of tagnames to filter on the source\n            start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            time_interval_rate (str): The time interval rate (numeric input)\n            time_interval_unit (str): The time interval unit (second, minute, day, hour)\n            step (str): data points with step \"enabled\" or \"disabled\". The options for step are \"true\", \"false\" or \"metadata\". \"metadata\" will retrieve the step value from the metadata table\n            source_metadata (optional str): if step is set to \"metadata\", then this parameter must be populated with the source containing the tagname metadata with a column called \"Step\"\n            include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n            window_length (optional int): Add longer window time in days for the start or end of specified date to cater for edge cases\n            pivot (optional bool): Pivot the data on the timestamp column with True or do not pivot the data with False\n            display_uom (optional bool): Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated\n            limit (optional int): The number of rows to be returned\n            offset (optional int): The number of rows to skip before returning rows\n\n        Returns:\n            DataFrame: A dataframe of time weighted averages timeseries data\n        \"\"\"\n        time_weighted_average_parameters = {\n            \"source\": self.data_source,\n            \"metadata_source\": self.metadata_source,\n            \"tag_names\": tagname_filter,\n            \"start_date\": start_date,\n            \"end_date\": end_date,\n            \"include_bad_data\": include_bad_data,\n            \"time_interval_rate\": time_interval_rate,\n            \"time_interval_unit\": time_interval_unit,\n            \"step\": step,\n            \"source_metadata\": (\n                None\n                if source_metadata is None\n                else \"`.`\".join(source_metadata.split(\".\"))\n            ),\n            \"window_length\": window_length,\n            \"pivot\": pivot,\n            \"display_uom\": display_uom,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"tagname_column\": self.tagname_column,\n            \"timestamp_column\": self.timestamp_column,\n            \"status_column\": self.status_column,\n            \"value_column\": self.value_column,\n            \"metadata_tagname_column\": self.metadata_tagname_column,\n            \"metadata_uom_column\": self.metadata_uom_column,\n            \"supress_warning\": True,\n        }\n\n        if (\n            \"display_uom\" in time_weighted_average_parameters\n            and time_weighted_average_parameters[\"display_uom\"] is True\n        ):\n            if time_weighted_average_parameters[\"metadata_source\"] is None:\n                raise ValueError(\n                    \"display_uom True requires metadata_source to be populated\"\n                )\n\n        return time_weighted_average.get(\n            self.connection, time_weighted_average_parameters\n        )\n\n    def metadata(\n        self,\n        tagname_filter: [str] = None,\n        limit: int = None,\n        offset: int = None,\n    ) -&gt; DataFrame:\n        \"\"\"\n        A query to retrieve metadata.\n\n        **Example:**\n        ```python\n        from rtdip_sdk.authentication.azure import DefaultAuth\n        from rtdip_sdk.connectors import DatabricksSQLConnection\n        from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n        auth = DefaultAuth().authenticate()\n        token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n        connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n        data = (\n            TimeSeriesQueryBuilder()\n            .connect(connection)\n            .source(\"{tablename_or_path}\")\n            .metadata(\n                tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n            )\n        )\n\n        print(data)\n\n        ```\n\n        Args:\n            tagname_filter (list str): List of tagnames to filter on the source\n            limit (optional int): The number of rows to be returned\n            offset (optional int): The number of rows to skip before returning rows\n\n        Returns:\n            DataFrame: A dataframe of metadata\n        \"\"\"\n        metadata_parameters = {\n            \"source\": self.data_source,\n            \"tag_names\": [] if tagname_filter is None else tagname_filter,\n            \"tagname_column\": self.tagname_column,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"supress_warning\": True,\n        }\n\n        return metadata.get(self.connection, metadata_parameters)\n\n    def latest(\n        self,\n        tagname_filter: [str] = None,\n        display_uom: bool = False,\n        limit: int = None,\n        offset: int = None,\n    ) -&gt; DataFrame:\n        \"\"\"\n        A query to retrieve latest event_values.\n\n        **Example:**\n        ```python\n        from rtdip_sdk.authentication.azure import DefaultAuth\n        from rtdip_sdk.connectors import DatabricksSQLConnection\n        from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n        auth = DefaultAuth().authenticate()\n        token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n        connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n        data = (\n            TimeSeriesQueryBuilder()\n            .connect(connection)\n            .source(\"{tablename_or_path}\")\n            .latest(\n                tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n            )\n        )\n\n        print(data)\n\n        ```\n\n        Args:\n            tagname_filter (list str): List of tagnames to filter on the source\n            display_uom (optional bool): Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated\n            limit (optional int): The number of rows to be returned\n            offset (optional int): The number of rows to skip before returning rows\n\n        Returns:\n            DataFrame: A dataframe of events latest_values\n        \"\"\"\n        latest_parameters = {\n            \"source\": self.data_source,\n            \"metadata_source\": self.metadata_source,\n            \"tag_names\": [] if tagname_filter is None else tagname_filter,\n            \"tagname_column\": self.tagname_column,\n            \"display_uom\": display_uom,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"metadata_tagname_column\": self.metadata_tagname_column,\n            \"metadata_uom_column\": self.metadata_uom_column,\n            \"supress_warning\": True,\n        }\n\n        if (\n            \"display_uom\" in latest_parameters\n            and latest_parameters[\"display_uom\"] is True\n        ):\n            if latest_parameters[\"metadata_source\"] is None:\n                raise ValueError(\n                    \"display_uom True requires metadata_source to be populated\"\n                )\n\n        return latest.get(self.connection, latest_parameters)\n\n    def circular_average(\n        self,\n        tagname_filter: [str],\n        start_date: str,\n        end_date: str,\n        time_interval_rate: str,\n        time_interval_unit: str,\n        lower_bound: int,\n        upper_bound: int,\n        include_bad_data: bool = False,\n        pivot: bool = False,\n        display_uom: bool = False,\n        limit: int = None,\n        offset: int = None,\n    ) -&gt; DataFrame:\n        \"\"\"\n        A function that receives a dataframe of raw tag data and computes the circular mean for samples in a range.\n\n        **Example:**\n        ```python\n        from rtdip_sdk.authentication.azure import DefaultAuth\n        from rtdip_sdk.connectors import DatabricksSQLConnection\n        from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n        auth = DefaultAuth().authenticate()\n        token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n        connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n        data = (\n            TimeSeriesQueryBuilder()\n            .connect(connection)\n            .source(\"{tablename_or_path}\")\n            .circular_average(\n                tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n                start_date=\"2023-01-01\",\n                end_date=\"2023-01-31\",\n                time_interval_rate=\"15\",\n                time_interval_unit=\"minute\",\n                lower_bound=\"0\",\n                upper_bound=\"360\",\n            )\n        )\n\n        print(data)\n\n        ```\n\n        Args:\n            tagname_filter (list str): List of tagnames to filter on the source\n            start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            time_interval_rate (str): The time interval rate (numeric input)\n            time_interval_unit (str): The time interval unit (second, minute, day, hour)\n            lower_bound (int): Lower boundary for the sample range\n            upper_bound (int): Upper boundary for the sample range\n            include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n            pivot (optional bool): Pivot the data on the timestamp column with True or do not pivot the data with False\n            display_uom (optional bool): Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated\n            limit (optional int): The number of rows to be returned\n            offset (optional int): The number of rows to skip before returning rows\n\n        Returns:\n            DataFrame: A dataframe containing the circular averages\n        \"\"\"\n        circular_average_parameters = {\n            \"source\": self.data_source,\n            \"metadata_source\": self.metadata_source,\n            \"tag_names\": tagname_filter,\n            \"start_date\": start_date,\n            \"end_date\": end_date,\n            \"include_bad_data\": include_bad_data,\n            \"time_interval_rate\": time_interval_rate,\n            \"time_interval_unit\": time_interval_unit,\n            \"lower_bound\": lower_bound,\n            \"upper_bound\": upper_bound,\n            \"pivot\": pivot,\n            \"display_uom\": display_uom,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"tagname_column\": self.tagname_column,\n            \"timestamp_column\": self.timestamp_column,\n            \"status_column\": self.status_column,\n            \"value_column\": self.value_column,\n            \"metadata_tagname_column\": self.metadata_tagname_column,\n            \"metadata_uom_column\": self.metadata_uom_column,\n            \"supress_warning\": True,\n        }\n\n        if (\n            \"display_uom\" in circular_average_parameters\n            and circular_average_parameters[\"display_uom\"] is True\n        ):\n            if circular_average_parameters[\"metadata_source\"] is None:\n                raise ValueError(\n                    \"display_uom True requires metadata_source to be populated\"\n                )\n\n        return circular_average.get(self.connection, circular_average_parameters)\n\n    def circular_standard_deviation(\n        self,\n        tagname_filter: [str],\n        start_date: str,\n        end_date: str,\n        time_interval_rate: str,\n        time_interval_unit: str,\n        lower_bound: int,\n        upper_bound: int,\n        include_bad_data: bool = False,\n        pivot: bool = False,\n        display_uom: bool = False,\n        limit: int = None,\n        offset: int = None,\n    ) -&gt; DataFrame:\n        \"\"\"\n        A function that receives a dataframe of raw tag data and computes the circular standard deviation for samples assumed to be in the range.\n\n        **Example:**\n        ```python\n        from rtdip_sdk.authentication.azure import DefaultAuth\n        from rtdip_sdk.connectors import DatabricksSQLConnection\n        from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n        auth = DefaultAuth().authenticate()\n        token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n        connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n        data = (\n            TimeSeriesQueryBuilder()\n            .connect(connection)\n            .source(\"{tablename_or_path}\")\n            .circular_standard_deviation(\n                tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n                start_date=\"2023-01-01\",\n                end_date=\"2023-01-31\",\n                time_interval_rate=\"15\",\n                time_interval_unit=\"minute\",\n                lower_bound=\"0\",\n                upper_bound=\"360\",\n            )\n        )\n\n        print(data)\n\n        ```\n\n        Args:\n            tagname_filter (list str): List of tagnames to filter on the source\n            start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            time_interval_rate (str): The time interval rate (numeric input)\n            time_interval_unit (str): The time interval unit (second, minute, day, hour)\n            lower_bound (int): Lower boundary for the sample range\n            upper_bound (int): Upper boundary for the sample range\n            include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n            pivot (optional bool): Pivot the data on the timestamp column with True or do not pivot the data with False\n            display_uom (optional bool): Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated\n            limit (optional int): The number of rows to be returned\n            offset (optional int): The number of rows to skip before returning rows\n\n        Returns:\n            DataFrame: A dataframe containing the circular standard deviations\n        \"\"\"\n        circular_stdev_parameters = {\n            \"source\": self.data_source,\n            \"metadata_source\": self.metadata_source,\n            \"tag_names\": tagname_filter,\n            \"start_date\": start_date,\n            \"end_date\": end_date,\n            \"include_bad_data\": include_bad_data,\n            \"time_interval_rate\": time_interval_rate,\n            \"time_interval_unit\": time_interval_unit,\n            \"lower_bound\": lower_bound,\n            \"upper_bound\": upper_bound,\n            \"pivot\": pivot,\n            \"display_uom\": display_uom,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"tagname_column\": self.tagname_column,\n            \"timestamp_column\": self.timestamp_column,\n            \"status_column\": self.status_column,\n            \"value_column\": self.value_column,\n            \"metadata_tagname_column\": self.metadata_tagname_column,\n            \"metadata_uom_column\": self.metadata_uom_column,\n            \"supress_warning\": True,\n        }\n\n        if (\n            \"display_uom\" in circular_stdev_parameters\n            and circular_stdev_parameters[\"display_uom\"] is True\n        ):\n            if circular_stdev_parameters[\"metadata_source\"] is None:\n                raise ValueError(\n                    \"display_uom True requires metadata_source to be populated\"\n                )\n\n        return circular_standard_deviation.get(\n            self.connection, circular_stdev_parameters\n        )\n\n    def summary(\n        self,\n        tagname_filter: [str],\n        start_date: str,\n        end_date: str,\n        include_bad_data: bool = False,\n        display_uom: bool = False,\n        limit: int = None,\n        offset: int = None,\n    ) -&gt; DataFrame:\n        \"\"\"\n        A function to return back a summary of statistics.\n\n        **Example:**\n        ```python\n        from rtdip_sdk.authentication.azure import DefaultAuth\n        from rtdip_sdk.connectors import DatabricksSQLConnection\n        from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n        auth = DefaultAuth().authenticate()\n        token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n        connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n        data = (\n            TimeSeriesQueryBuilder()\n            .connect(connection)\n            .source(\"{tablename_or_path}\")\n            .summary(\n                tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n                start_date=\"2023-01-01\",\n                end_date=\"2023-01-31\",\n            )\n        )\n\n        print(data)\n\n        ```\n\n        Args:\n            tagname_filter (list str): List of tagnames to filter on the source\n            start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n            display_uom (optional bool): Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated\n            limit (optional int): The number of rows to be returned\n            offset (optional int): The number of rows to skip before returning rows\n\n        Returns:\n            DataFrame: A dataframe of raw timeseries data.\n        \"\"\"\n        summary_parameters = {\n            \"source\": self.data_source,\n            \"metadata_source\": self.metadata_source,\n            \"tag_names\": tagname_filter,\n            \"start_date\": start_date,\n            \"end_date\": end_date,\n            \"include_bad_data\": include_bad_data,\n            \"display_uom\": display_uom,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"tagname_column\": self.tagname_column,\n            \"timestamp_column\": self.timestamp_column,\n            \"status_column\": self.status_column,\n            \"value_column\": self.value_column,\n            \"metadata_tagname_column\": self.metadata_tagname_column,\n            \"metadata_uom_column\": self.metadata_uom_column,\n            \"supress_warning\": True,\n        }\n\n        if (\n            \"display_uom\" in summary_parameters\n            and summary_parameters[\"display_uom\"] is True\n        ):\n            if summary_parameters[\"metadata_source\"] is None:\n                raise ValueError(\n                    \"display_uom True requires metadata_source to be populated\"\n                )\n\n        return summary.get(self.connection, summary_parameters)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder.connect","title":"<code>connect(connection)</code>","text":"<p>Specifies the connection to be used for the query.</p> <p>Example: <pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\nconnect = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>ConnectionInterface</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>def connect(self, connection: ConnectionInterface):\n    \"\"\"\n    Specifies the connection to be used for the query.\n\n    **Example:**\n    ```python\n    from rtdip_sdk.authentication.azure import DefaultAuth\n    from rtdip_sdk.connectors import DatabricksSQLConnection\n    from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n    auth = DefaultAuth().authenticate()\n    token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n    connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n    connect = (\n        TimeSeriesQueryBuilder()\n        .connect(connection)\n    )\n\n    ```\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n    \"\"\"\n    self.connection = connection\n    return self\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder.source","title":"<code>source(source, tagname_column='TagName', timestamp_column='EventTime', status_column='Status', value_column='Value')</code>","text":"<p>Specifies the source of the query.</p> <p>Example: <pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\nsource = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\n        source=\"{tablename_or_path}\"\n    )\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Source of the query can be a Unity Catalog table, Hive metastore table or path</p> required <code>tagname_column</code> <code>optional str</code> <p>The column name in the source that contains the tagnames or series</p> <code>'TagName'</code> <code>timestamp_column</code> <code>optional str</code> <p>The timestamp column name in the source</p> <code>'EventTime'</code> <code>status_column</code> <code>optional str</code> <p>The status column name in the source indicating <code>Good</code> or <code>Bad</code>. If this is not available, specify <code>None</code></p> <code>'Status'</code> <code>value_column</code> <code>optional str</code> <p>The value column name in the source which is normally a float or string value for the time series event</p> <code>'Value'</code> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>def source(\n    self,\n    source: str,\n    tagname_column: str = \"TagName\",\n    timestamp_column: str = \"EventTime\",\n    status_column: Union[str, None] = \"Status\",\n    value_column: str = \"Value\",\n):\n    \"\"\"\n    Specifies the source of the query.\n\n    **Example:**\n    ```python\n    from rtdip_sdk.authentication.azure import DefaultAuth\n    from rtdip_sdk.connectors import DatabricksSQLConnection\n    from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n    auth = DefaultAuth().authenticate()\n    token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n    connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n    source = (\n        TimeSeriesQueryBuilder()\n        .connect(connection)\n        .source(\n            source=\"{tablename_or_path}\"\n        )\n    )\n\n    ```\n\n    Args:\n        source (str): Source of the query can be a Unity Catalog table, Hive metastore table or path\n        tagname_column (optional str): The column name in the source that contains the tagnames or series\n        timestamp_column (optional str): The timestamp column name in the source\n        status_column (optional str): The status column name in the source indicating `Good` or `Bad`. If this is not available, specify `None`\n        value_column (optional str): The value column name in the source which is normally a float or string value for the time series event\n    \"\"\"\n    self.data_source = \"`.`\".join(source.split(\".\"))\n    self.tagname_column = tagname_column\n    self.timestamp_column = timestamp_column\n    self.status_column = status_column\n    self.value_column = value_column\n    return self\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder.m_source","title":"<code>m_source(metadata_source, metadata_tagname_column='TagName', metadata_uom_column='UoM')</code>","text":"<p>Specifies the Metadata source of the query. This is only required if display_uom is set to True or Step is set to \"metadata\". Otherwise, it is optional.</p> <p>Example: <pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\nsource = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\n        source=\"{tablename_or_path}\"\n    )\n    .m_source(\n        metadata_source=\"{metadata_table_or_path}\"\n        metadata_tagname_column=\"TagName\",\n        metadata_uom_column=\"UoM\")\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>metadata_source</code> <code>str</code> <p>Source of the query can be a Unity Catalog table, Hive metastore table or path</p> required <code>metadata_tagname_column</code> <code>optional str</code> <p>The column name in the source that contains the tagnames or series</p> <code>'TagName'</code> <code>metadata_uom_column</code> <code>optional str</code> <p>The column name in the source that contains the unit of measure</p> <code>'UoM'</code> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>def m_source(\n    self,\n    metadata_source: str,\n    metadata_tagname_column: str = \"TagName\",\n    metadata_uom_column: str = \"UoM\",\n):\n    \"\"\"\n    Specifies the Metadata source of the query. This is only required if display_uom is set to True or Step is set to \"metadata\". Otherwise, it is optional.\n\n    **Example:**\n    ```python\n    from rtdip_sdk.authentication.azure import DefaultAuth\n    from rtdip_sdk.connectors import DatabricksSQLConnection\n    from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n    auth = DefaultAuth().authenticate()\n    token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n    connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n    source = (\n        TimeSeriesQueryBuilder()\n        .connect(connection)\n        .source(\n            source=\"{tablename_or_path}\"\n        )\n        .m_source(\n            metadata_source=\"{metadata_table_or_path}\"\n            metadata_tagname_column=\"TagName\",\n            metadata_uom_column=\"UoM\")\n    )\n\n    ```\n\n    Args:\n        metadata_source (str): Source of the query can be a Unity Catalog table, Hive metastore table or path\n        metadata_tagname_column (optional str): The column name in the source that contains the tagnames or series\n        metadata_uom_column (optional str): The column name in the source that contains the unit of measure\n    \"\"\"\n    self.metadata_source = f\"`{'`.`'.join(metadata_source.split('.'))}`\"\n    self.metadata_tagname_column = metadata_tagname_column\n    self.metadata_uom_column = metadata_uom_column\n    return self\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder.raw","title":"<code>raw(tagname_filter, start_date, end_date, include_bad_data=False, display_uom=False, sort=True, limit=None, offset=None)</code>","text":"<p>A function to return back raw data.</p> <p>Example: <pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .raw(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n    )\n)\n\nprint(data)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tagname_filter</code> <code>list str</code> <p>List of tagnames to filter on the source</p> required <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>include_bad_data</code> <code>optional bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>False</code> <code>display_uom</code> <code>optional bool</code> <p>Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated</p> <code>False</code> <code>sort</code> <code>optional bool</code> <p>Sort the data in ascending order by the TagName and Timestamp columns</p> <code>True</code> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of raw timeseries data.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>def raw(\n    self,\n    tagname_filter: [str],\n    start_date: str,\n    end_date: str,\n    include_bad_data: bool = False,\n    display_uom: bool = False,\n    sort: bool = True,\n    limit: int = None,\n    offset: int = None,\n) -&gt; DataFrame:\n    \"\"\"\n    A function to return back raw data.\n\n    **Example:**\n    ```python\n    from rtdip_sdk.authentication.azure import DefaultAuth\n    from rtdip_sdk.connectors import DatabricksSQLConnection\n    from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n    auth = DefaultAuth().authenticate()\n    token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n    connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n    data = (\n        TimeSeriesQueryBuilder()\n        .connect(connection)\n        .source(\"{tablename_or_path}\")\n        .raw(\n            tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n            start_date=\"2023-01-01\",\n            end_date=\"2023-01-31\",\n        )\n    )\n\n    print(data)\n\n    ```\n\n    Args:\n        tagname_filter (list str): List of tagnames to filter on the source\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        display_uom (optional bool): Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated\n        sort (optional bool): Sort the data in ascending order by the TagName and Timestamp columns\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n\n    Returns:\n        DataFrame: A dataframe of raw timeseries data.\n    \"\"\"\n    raw_parameters = {\n        \"source\": self.data_source,\n        \"metadata_source\": self.metadata_source,\n        \"tag_names\": tagname_filter,\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"include_bad_data\": include_bad_data,\n        \"display_uom\": display_uom,\n        \"sort\": sort,\n        \"limit\": limit,\n        \"offset\": offset,\n        \"tagname_column\": self.tagname_column,\n        \"timestamp_column\": self.timestamp_column,\n        \"status_column\": self.status_column,\n        \"value_column\": self.value_column,\n        \"metadata_tagname_column\": self.metadata_tagname_column,\n        \"metadata_uom_column\": self.metadata_uom_column,\n        \"supress_warning\": True,\n    }\n\n    if \"display_uom\" in raw_parameters and raw_parameters[\"display_uom\"] is True:\n        if raw_parameters[\"metadata_source\"] is None:\n            raise ValueError(\n                \"display_uom True requires metadata_source to be populated\"\n            )\n\n    return raw.get(self.connection, raw_parameters)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder.resample","title":"<code>resample(tagname_filter, start_date, end_date, time_interval_rate, time_interval_unit, agg_method, include_bad_data=False, fill=False, pivot=False, display_uom=False, sort=True, limit=None, offset=None)</code>","text":"<p>A query to resample the source data.</p> <p>Example: <pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .resample(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n        time_interval_rate=\"15\",\n        time_interval_unit=\"minute\",\n        agg_method=\"first\",\n    )\n)\n\nprint(data)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tagname_filter</code> <code>list str</code> <p>List of tagnames to filter on the source</p> required <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>time_interval_rate</code> <code>str</code> <p>The time interval rate (numeric input)</p> required <code>time_interval_unit</code> <code>str</code> <p>The time interval unit (second, minute, day, hour)</p> required <code>agg_method</code> <code>str</code> <p>Aggregation Method (first, last, avg, min, max)</p> required <code>include_bad_data</code> <code>optional bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>False</code> <code>fill</code> <code>bool</code> <p>Fill the data with intervals where no data exists. The Value column will be filled with Null</p> <code>False</code> <code>pivot</code> <code>optional bool</code> <p>Pivot the data on the timestamp column with True or do not pivot the data with False</p> <code>False</code> <code>display_uom</code> <code>optional bool</code> <p>Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated</p> <code>False</code> <code>sort</code> <code>optional bool</code> <p>Sort the data in ascending order by the TagName and Timestamp columns or, if pivot is True, by the Timestamp column</p> <code>True</code> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of resampled timeseries data.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>def resample(\n    self,\n    tagname_filter: [str],\n    start_date: str,\n    end_date: str,\n    time_interval_rate: str,\n    time_interval_unit: str,\n    agg_method: str,\n    include_bad_data: bool = False,\n    fill: bool = False,\n    pivot: bool = False,\n    display_uom: bool = False,\n    sort: bool = True,\n    limit: int = None,\n    offset: int = None,\n) -&gt; DataFrame:\n    \"\"\"\n    A query to resample the source data.\n\n    **Example:**\n    ```python\n    from rtdip_sdk.authentication.azure import DefaultAuth\n    from rtdip_sdk.connectors import DatabricksSQLConnection\n    from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n    auth = DefaultAuth().authenticate()\n    token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n    connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n    data = (\n        TimeSeriesQueryBuilder()\n        .connect(connection)\n        .source(\"{tablename_or_path}\")\n        .resample(\n            tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n            start_date=\"2023-01-01\",\n            end_date=\"2023-01-31\",\n            time_interval_rate=\"15\",\n            time_interval_unit=\"minute\",\n            agg_method=\"first\",\n        )\n    )\n\n    print(data)\n\n    ```\n\n    Args:\n        tagname_filter (list str): List of tagnames to filter on the source\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        time_interval_rate (str): The time interval rate (numeric input)\n        time_interval_unit (str): The time interval unit (second, minute, day, hour)\n        agg_method (str): Aggregation Method (first, last, avg, min, max)\n        include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        fill (bool): Fill the data with intervals where no data exists. The Value column will be filled with Null\n        pivot (optional bool): Pivot the data on the timestamp column with True or do not pivot the data with False\n        display_uom (optional bool): Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated\n        sort (optional bool): Sort the data in ascending order by the TagName and Timestamp columns or, if pivot is True, by the Timestamp column\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n\n    Returns:\n        DataFrame: A dataframe of resampled timeseries data.\n    \"\"\"\n\n    resample_parameters = {\n        \"source\": self.data_source,\n        \"metadata_source\": self.metadata_source,\n        \"tag_names\": tagname_filter,\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"include_bad_data\": include_bad_data,\n        \"time_interval_rate\": time_interval_rate,\n        \"time_interval_unit\": time_interval_unit,\n        \"agg_method\": agg_method,\n        \"fill\": fill,\n        \"pivot\": pivot,\n        \"display_uom\": display_uom,\n        \"sort\": sort,\n        \"limit\": limit,\n        \"offset\": offset,\n        \"tagname_column\": self.tagname_column,\n        \"timestamp_column\": self.timestamp_column,\n        \"status_column\": self.status_column,\n        \"value_column\": self.value_column,\n        \"metadata_tagname_column\": self.metadata_tagname_column,\n        \"metadata_uom_column\": self.metadata_uom_column,\n        \"supress_warning\": True,\n    }\n\n    if (\n        \"display_uom\" in resample_parameters\n        and resample_parameters[\"display_uom\"] is True\n    ):\n        if resample_parameters[\"metadata_source\"] is None:\n            raise ValueError(\n                \"display_uom True requires metadata_source to be populated\"\n            )\n\n    return resample.get(self.connection, resample_parameters)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder.plot","title":"<code>plot(tagname_filter, start_date, end_date, time_interval_rate, time_interval_unit, include_bad_data=False, pivot=False, display_uom=False, sort=True, limit=None, offset=None)</code>","text":"<p>A query to plot the source data for a time interval for Min, Max, First, Last and an Exception Value(Status = Bad), if it exists.</p> <p>Example: <pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .plot(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n        time_interval_rate=\"15\",\n        time_interval_unit=\"minute\",\n    )\n)\n\nprint(data)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tagname_filter</code> <code>list str</code> <p>List of tagnames to filter on the source</p> required <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>time_interval_rate</code> <code>str</code> <p>The time interval rate (numeric input)</p> required <code>time_interval_unit</code> <code>str</code> <p>The time interval unit (second, minute, day, hour)</p> required <code>include_bad_data</code> <code>optional bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>False</code> <code>pivot</code> <code>optional bool</code> <p>Pivot the data on the timestamp column with True or do not pivot the data with False</p> <code>False</code> <code>display_uom</code> <code>optional bool</code> <p>Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated</p> <code>False</code> <code>sort</code> <code>optional bool</code> <p>Sort the data in ascending order by the TagName and Timestamp columns</p> <code>True</code> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of resampled timeseries data.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>def plot(\n    self,\n    tagname_filter: [str],\n    start_date: str,\n    end_date: str,\n    time_interval_rate: str,\n    time_interval_unit: str,\n    include_bad_data: bool = False,\n    pivot: bool = False,\n    display_uom: bool = False,\n    sort: bool = True,\n    limit: int = None,\n    offset: int = None,\n) -&gt; DataFrame:\n    \"\"\"\n    A query to plot the source data for a time interval for Min, Max, First, Last and an Exception Value(Status = Bad), if it exists.\n\n    **Example:**\n    ```python\n    from rtdip_sdk.authentication.azure import DefaultAuth\n    from rtdip_sdk.connectors import DatabricksSQLConnection\n    from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n    auth = DefaultAuth().authenticate()\n    token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n    connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n    data = (\n        TimeSeriesQueryBuilder()\n        .connect(connection)\n        .source(\"{tablename_or_path}\")\n        .plot(\n            tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n            start_date=\"2023-01-01\",\n            end_date=\"2023-01-31\",\n            time_interval_rate=\"15\",\n            time_interval_unit=\"minute\",\n        )\n    )\n\n    print(data)\n\n    ```\n\n    Args:\n        tagname_filter (list str): List of tagnames to filter on the source\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        time_interval_rate (str): The time interval rate (numeric input)\n        time_interval_unit (str): The time interval unit (second, minute, day, hour)\n        include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        pivot (optional bool): Pivot the data on the timestamp column with True or do not pivot the data with False\n        display_uom (optional bool): Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated\n        sort (optional bool): Sort the data in ascending order by the TagName and Timestamp columns\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n\n    Returns:\n        DataFrame: A dataframe of resampled timeseries data.\n    \"\"\"\n\n    plot_parameters = {\n        \"source\": self.data_source,\n        \"metadata_source\": self.metadata_source,\n        \"tag_names\": tagname_filter,\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"time_interval_rate\": time_interval_rate,\n        \"time_interval_unit\": time_interval_unit,\n        \"include_bad_data\": include_bad_data,\n        \"pivot\": pivot,\n        \"display_uom\": display_uom,\n        \"sort\": sort,\n        \"limit\": limit,\n        \"offset\": offset,\n        \"tagname_column\": self.tagname_column,\n        \"timestamp_column\": self.timestamp_column,\n        \"status_column\": self.status_column,\n        \"value_column\": self.value_column,\n        \"metadata_tagname_column\": self.metadata_tagname_column,\n        \"metadata_uom_column\": self.metadata_uom_column,\n        \"supress_warning\": True,\n    }\n\n    if \"display_uom\" in plot_parameters and plot_parameters[\"display_uom\"] is True:\n        if plot_parameters[\"metadata_source\"] is None:\n            raise ValueError(\n                \"display_uom True requires metadata_source to be populated\"\n            )\n\n    return plot.get(self.connection, plot_parameters)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder.interpolate","title":"<code>interpolate(tagname_filter, start_date, end_date, time_interval_rate, time_interval_unit, include_bad_data=False, pivot=False, display_uom=False, sort=True, limit=None, offset=None)</code>","text":"<p>The Interpolate function will forward fill, backward fill or linearly interpolate the resampled data depending on the parameters specified.</p> <p>Example: <pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .interpolate(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n        time_interval_rate=\"15\",\n        time_interval_unit=\"minute\",\n    )\n)\n\nprint(data)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tagname_filter</code> <code>list str</code> <p>List of tagnames to filter on the source</p> required <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>time_interval_rate</code> <code>str</code> <p>The time interval rate (numeric input)</p> required <code>time_interval_unit</code> <code>str</code> <p>The time interval unit (second, minute, day, hour)</p> required <code>include_bad_data</code> <code>optional bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>False</code> <code>pivot</code> <code>optional bool</code> <p>Pivot the data on the timestamp column with True or do not pivot the data with False</p> <code>False</code> <code>display_uom</code> <code>optional bool</code> <p>Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated</p> <code>False</code> <code>sort</code> <code>optional bool</code> <p>Sort the data in ascending order by the TagName and Timestamp columns or, if pivot is True, by the Timestamp column</p> <code>True</code> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of interpolated timeseries data.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>def interpolate(\n    self,\n    tagname_filter: [str],\n    start_date: str,\n    end_date: str,\n    time_interval_rate: str,\n    time_interval_unit: str,\n    include_bad_data: bool = False,\n    pivot: bool = False,\n    display_uom: bool = False,\n    sort: bool = True,\n    limit: int = None,\n    offset: int = None,\n) -&gt; DataFrame:\n    \"\"\"\n    The Interpolate function will forward fill, backward fill or linearly interpolate the resampled data depending on the parameters specified.\n\n    **Example:**\n    ```python\n    from rtdip_sdk.authentication.azure import DefaultAuth\n    from rtdip_sdk.connectors import DatabricksSQLConnection\n    from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n    auth = DefaultAuth().authenticate()\n    token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n    connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n    data = (\n        TimeSeriesQueryBuilder()\n        .connect(connection)\n        .source(\"{tablename_or_path}\")\n        .interpolate(\n            tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n            start_date=\"2023-01-01\",\n            end_date=\"2023-01-31\",\n            time_interval_rate=\"15\",\n            time_interval_unit=\"minute\",\n        )\n    )\n\n    print(data)\n\n    ```\n\n    Args:\n        tagname_filter (list str): List of tagnames to filter on the source\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        time_interval_rate (str): The time interval rate (numeric input)\n        time_interval_unit (str): The time interval unit (second, minute, day, hour)\n        include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        pivot (optional bool): Pivot the data on the timestamp column with True or do not pivot the data with False\n        display_uom (optional bool): Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated\n        sort (optional bool): Sort the data in ascending order by the TagName and Timestamp columns or, if pivot is True, by the Timestamp column\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n\n    Returns:\n        DataFrame: A dataframe of interpolated timeseries data.\n    \"\"\"\n    interpolation_parameters = {\n        \"source\": self.data_source,\n        \"metadata_source\": self.metadata_source,\n        \"tag_names\": tagname_filter,\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"include_bad_data\": include_bad_data,\n        \"time_interval_rate\": time_interval_rate,\n        \"time_interval_unit\": time_interval_unit,\n        \"pivot\": pivot,\n        \"display_uom\": display_uom,\n        \"sort\": sort,\n        \"limit\": limit,\n        \"offset\": offset,\n        \"tagname_column\": self.tagname_column,\n        \"timestamp_column\": self.timestamp_column,\n        \"status_column\": self.status_column,\n        \"value_column\": self.value_column,\n        \"metadata_tagname_column\": self.metadata_tagname_column,\n        \"metadata_uom_column\": self.metadata_uom_column,\n        \"supress_warning\": True,\n    }\n\n    if (\n        \"display_uom\" in interpolation_parameters\n        and interpolation_parameters[\"display_uom\"] is True\n    ):\n        if interpolation_parameters[\"metadata_source\"] is None:\n            raise ValueError(\n                \"display_uom True requires metadata_source to be populated\"\n            )\n\n    return interpolate.get(self.connection, interpolation_parameters)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder.interpolation_at_time","title":"<code>interpolation_at_time(tagname_filter, timestamp_filter, include_bad_data=False, window_length=1, pivot=False, display_uom=False, limit=None, offset=None)</code>","text":"<p>A interpolation at time function which works out the linear interpolation at a specific time based on the points before and after.</p> <p>Example: <pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .interpolation_at_time(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        timestamp_filter=[\"2023-01-01T09:30:00\", \"2023-01-02T12:00:00\"],\n    )\n)\n\nprint(data)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tagname_filter</code> <code>list str</code> <p>List of tagnames to filter on the source</p> required <code>timestamp_filter</code> <code>list</code> <p>List of timestamp or timestamps in the format YYY-MM-DDTHH:MM:SS or YYY-MM-DDTHH:MM:SS+zz:zz where %z is the timezone. (Example +00:00 is the UTC timezone)</p> required <code>include_bad_data</code> <code>optional bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>False</code> <code>window_length</code> <code>optional int</code> <p>Add longer window time in days for the start or end of specified date to cater for edge cases</p> <code>1</code> <code>pivot</code> <code>optional bool</code> <p>Pivot the data on the timestamp column with True or do not pivot the data with False</p> <code>False</code> <code>display_uom</code> <code>optional bool</code> <p>Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated</p> <code>False</code> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of interpolation at time timeseries data</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>def interpolation_at_time(\n    self,\n    tagname_filter: [str],\n    timestamp_filter: [str],\n    include_bad_data: bool = False,\n    window_length: int = 1,\n    pivot: bool = False,\n    display_uom: bool = False,\n    limit: int = None,\n    offset: int = None,\n) -&gt; DataFrame:\n    \"\"\"\n    A interpolation at time function which works out the linear interpolation at a specific time based on the points before and after.\n\n    **Example:**\n    ```python\n    from rtdip_sdk.authentication.azure import DefaultAuth\n    from rtdip_sdk.connectors import DatabricksSQLConnection\n    from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n    auth = DefaultAuth().authenticate()\n    token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n    connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n    data = (\n        TimeSeriesQueryBuilder()\n        .connect(connection)\n        .source(\"{tablename_or_path}\")\n        .interpolation_at_time(\n            tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n            timestamp_filter=[\"2023-01-01T09:30:00\", \"2023-01-02T12:00:00\"],\n        )\n    )\n\n    print(data)\n\n    ```\n\n    Args:\n        tagname_filter (list str): List of tagnames to filter on the source\n        timestamp_filter (list): List of timestamp or timestamps in the format YYY-MM-DDTHH:MM:SS or YYY-MM-DDTHH:MM:SS+zz:zz where %z is the timezone. (Example +00:00 is the UTC timezone)\n        include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        window_length (optional int): Add longer window time in days for the start or end of specified date to cater for edge cases\n        pivot (optional bool): Pivot the data on the timestamp column with True or do not pivot the data with False\n        display_uom (optional bool): Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n\n    Returns:\n        DataFrame: A dataframe of interpolation at time timeseries data\n    \"\"\"\n    interpolation_at_time_parameters = {\n        \"source\": self.data_source,\n        \"metadata_source\": self.metadata_source,\n        \"tag_names\": tagname_filter,\n        \"timestamps\": timestamp_filter,\n        \"include_bad_data\": include_bad_data,\n        \"window_length\": window_length,\n        \"pivot\": pivot,\n        \"display_uom\": display_uom,\n        \"limit\": limit,\n        \"offset\": offset,\n        \"tagname_column\": self.tagname_column,\n        \"timestamp_column\": self.timestamp_column,\n        \"status_column\": self.status_column,\n        \"value_column\": self.value_column,\n        \"metadata_tagname_column\": self.metadata_tagname_column,\n        \"metadata_uom_column\": self.metadata_uom_column,\n        \"supress_warning\": True,\n    }\n\n    if (\n        \"display_uom\" in interpolation_at_time_parameters\n        and interpolation_at_time_parameters[\"display_uom\"] is True\n    ):\n        if interpolation_at_time_parameters[\"metadata_source\"] is None:\n            raise ValueError(\n                \"display_uom True requires metadata_source to be populated\"\n            )\n\n    return interpolation_at_time.get(\n        self.connection, interpolation_at_time_parameters\n    )\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder.time_weighted_average","title":"<code>time_weighted_average(tagname_filter, start_date, end_date, time_interval_rate, time_interval_unit, step, source_metadata=None, include_bad_data=False, window_length=1, pivot=False, display_uom=False, limit=None, offset=None)</code>","text":"<p>A function that receives a dataframe of raw tag data and performs a time weighted averages.</p> <p>Example: <pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .time_weighted_average(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n        time_interval_rate=\"15\",\n        time_interval_unit=\"minute\",\n        step=\"true\",\n    )\n)\n\nprint(data)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tagname_filter</code> <code>list str</code> <p>List of tagnames to filter on the source</p> required <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>time_interval_rate</code> <code>str</code> <p>The time interval rate (numeric input)</p> required <code>time_interval_unit</code> <code>str</code> <p>The time interval unit (second, minute, day, hour)</p> required <code>step</code> <code>str</code> <p>data points with step \"enabled\" or \"disabled\". The options for step are \"true\", \"false\" or \"metadata\". \"metadata\" will retrieve the step value from the metadata table</p> required <code>source_metadata</code> <code>optional str</code> <p>if step is set to \"metadata\", then this parameter must be populated with the source containing the tagname metadata with a column called \"Step\"</p> <code>None</code> <code>include_bad_data</code> <code>optional bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>False</code> <code>window_length</code> <code>optional int</code> <p>Add longer window time in days for the start or end of specified date to cater for edge cases</p> <code>1</code> <code>pivot</code> <code>optional bool</code> <p>Pivot the data on the timestamp column with True or do not pivot the data with False</p> <code>False</code> <code>display_uom</code> <code>optional bool</code> <p>Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated</p> <code>False</code> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of time weighted averages timeseries data</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>def time_weighted_average(\n    self,\n    tagname_filter: [str],\n    start_date: str,\n    end_date: str,\n    time_interval_rate: str,\n    time_interval_unit: str,\n    step: str,\n    source_metadata: str = None,\n    include_bad_data: bool = False,\n    window_length: int = 1,\n    pivot: bool = False,\n    display_uom: bool = False,\n    limit: int = None,\n    offset: int = None,\n) -&gt; DataFrame:\n    \"\"\"\n    A function that receives a dataframe of raw tag data and performs a time weighted averages.\n\n    **Example:**\n    ```python\n    from rtdip_sdk.authentication.azure import DefaultAuth\n    from rtdip_sdk.connectors import DatabricksSQLConnection\n    from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n    auth = DefaultAuth().authenticate()\n    token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n    connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n    data = (\n        TimeSeriesQueryBuilder()\n        .connect(connection)\n        .source(\"{tablename_or_path}\")\n        .time_weighted_average(\n            tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n            start_date=\"2023-01-01\",\n            end_date=\"2023-01-31\",\n            time_interval_rate=\"15\",\n            time_interval_unit=\"minute\",\n            step=\"true\",\n        )\n    )\n\n    print(data)\n\n    ```\n\n    Args:\n        tagname_filter (list str): List of tagnames to filter on the source\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        time_interval_rate (str): The time interval rate (numeric input)\n        time_interval_unit (str): The time interval unit (second, minute, day, hour)\n        step (str): data points with step \"enabled\" or \"disabled\". The options for step are \"true\", \"false\" or \"metadata\". \"metadata\" will retrieve the step value from the metadata table\n        source_metadata (optional str): if step is set to \"metadata\", then this parameter must be populated with the source containing the tagname metadata with a column called \"Step\"\n        include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        window_length (optional int): Add longer window time in days for the start or end of specified date to cater for edge cases\n        pivot (optional bool): Pivot the data on the timestamp column with True or do not pivot the data with False\n        display_uom (optional bool): Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n\n    Returns:\n        DataFrame: A dataframe of time weighted averages timeseries data\n    \"\"\"\n    time_weighted_average_parameters = {\n        \"source\": self.data_source,\n        \"metadata_source\": self.metadata_source,\n        \"tag_names\": tagname_filter,\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"include_bad_data\": include_bad_data,\n        \"time_interval_rate\": time_interval_rate,\n        \"time_interval_unit\": time_interval_unit,\n        \"step\": step,\n        \"source_metadata\": (\n            None\n            if source_metadata is None\n            else \"`.`\".join(source_metadata.split(\".\"))\n        ),\n        \"window_length\": window_length,\n        \"pivot\": pivot,\n        \"display_uom\": display_uom,\n        \"limit\": limit,\n        \"offset\": offset,\n        \"tagname_column\": self.tagname_column,\n        \"timestamp_column\": self.timestamp_column,\n        \"status_column\": self.status_column,\n        \"value_column\": self.value_column,\n        \"metadata_tagname_column\": self.metadata_tagname_column,\n        \"metadata_uom_column\": self.metadata_uom_column,\n        \"supress_warning\": True,\n    }\n\n    if (\n        \"display_uom\" in time_weighted_average_parameters\n        and time_weighted_average_parameters[\"display_uom\"] is True\n    ):\n        if time_weighted_average_parameters[\"metadata_source\"] is None:\n            raise ValueError(\n                \"display_uom True requires metadata_source to be populated\"\n            )\n\n    return time_weighted_average.get(\n        self.connection, time_weighted_average_parameters\n    )\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder.metadata","title":"<code>metadata(tagname_filter=None, limit=None, offset=None)</code>","text":"<p>A query to retrieve metadata.</p> <p>Example: <pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .metadata(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n    )\n)\n\nprint(data)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tagname_filter</code> <code>list str</code> <p>List of tagnames to filter on the source</p> <code>None</code> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of metadata</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>def metadata(\n    self,\n    tagname_filter: [str] = None,\n    limit: int = None,\n    offset: int = None,\n) -&gt; DataFrame:\n    \"\"\"\n    A query to retrieve metadata.\n\n    **Example:**\n    ```python\n    from rtdip_sdk.authentication.azure import DefaultAuth\n    from rtdip_sdk.connectors import DatabricksSQLConnection\n    from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n    auth = DefaultAuth().authenticate()\n    token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n    connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n    data = (\n        TimeSeriesQueryBuilder()\n        .connect(connection)\n        .source(\"{tablename_or_path}\")\n        .metadata(\n            tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        )\n    )\n\n    print(data)\n\n    ```\n\n    Args:\n        tagname_filter (list str): List of tagnames to filter on the source\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n\n    Returns:\n        DataFrame: A dataframe of metadata\n    \"\"\"\n    metadata_parameters = {\n        \"source\": self.data_source,\n        \"tag_names\": [] if tagname_filter is None else tagname_filter,\n        \"tagname_column\": self.tagname_column,\n        \"limit\": limit,\n        \"offset\": offset,\n        \"supress_warning\": True,\n    }\n\n    return metadata.get(self.connection, metadata_parameters)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder.latest","title":"<code>latest(tagname_filter=None, display_uom=False, limit=None, offset=None)</code>","text":"<p>A query to retrieve latest event_values.</p> <p>Example: <pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .latest(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n    )\n)\n\nprint(data)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tagname_filter</code> <code>list str</code> <p>List of tagnames to filter on the source</p> <code>None</code> <code>display_uom</code> <code>optional bool</code> <p>Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated</p> <code>False</code> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of events latest_values</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>def latest(\n    self,\n    tagname_filter: [str] = None,\n    display_uom: bool = False,\n    limit: int = None,\n    offset: int = None,\n) -&gt; DataFrame:\n    \"\"\"\n    A query to retrieve latest event_values.\n\n    **Example:**\n    ```python\n    from rtdip_sdk.authentication.azure import DefaultAuth\n    from rtdip_sdk.connectors import DatabricksSQLConnection\n    from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n    auth = DefaultAuth().authenticate()\n    token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n    connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n    data = (\n        TimeSeriesQueryBuilder()\n        .connect(connection)\n        .source(\"{tablename_or_path}\")\n        .latest(\n            tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        )\n    )\n\n    print(data)\n\n    ```\n\n    Args:\n        tagname_filter (list str): List of tagnames to filter on the source\n        display_uom (optional bool): Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n\n    Returns:\n        DataFrame: A dataframe of events latest_values\n    \"\"\"\n    latest_parameters = {\n        \"source\": self.data_source,\n        \"metadata_source\": self.metadata_source,\n        \"tag_names\": [] if tagname_filter is None else tagname_filter,\n        \"tagname_column\": self.tagname_column,\n        \"display_uom\": display_uom,\n        \"limit\": limit,\n        \"offset\": offset,\n        \"metadata_tagname_column\": self.metadata_tagname_column,\n        \"metadata_uom_column\": self.metadata_uom_column,\n        \"supress_warning\": True,\n    }\n\n    if (\n        \"display_uom\" in latest_parameters\n        and latest_parameters[\"display_uom\"] is True\n    ):\n        if latest_parameters[\"metadata_source\"] is None:\n            raise ValueError(\n                \"display_uom True requires metadata_source to be populated\"\n            )\n\n    return latest.get(self.connection, latest_parameters)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder.circular_average","title":"<code>circular_average(tagname_filter, start_date, end_date, time_interval_rate, time_interval_unit, lower_bound, upper_bound, include_bad_data=False, pivot=False, display_uom=False, limit=None, offset=None)</code>","text":"<p>A function that receives a dataframe of raw tag data and computes the circular mean for samples in a range.</p> <p>Example: <pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .circular_average(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n        time_interval_rate=\"15\",\n        time_interval_unit=\"minute\",\n        lower_bound=\"0\",\n        upper_bound=\"360\",\n    )\n)\n\nprint(data)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tagname_filter</code> <code>list str</code> <p>List of tagnames to filter on the source</p> required <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>time_interval_rate</code> <code>str</code> <p>The time interval rate (numeric input)</p> required <code>time_interval_unit</code> <code>str</code> <p>The time interval unit (second, minute, day, hour)</p> required <code>lower_bound</code> <code>int</code> <p>Lower boundary for the sample range</p> required <code>upper_bound</code> <code>int</code> <p>Upper boundary for the sample range</p> required <code>include_bad_data</code> <code>optional bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>False</code> <code>pivot</code> <code>optional bool</code> <p>Pivot the data on the timestamp column with True or do not pivot the data with False</p> <code>False</code> <code>display_uom</code> <code>optional bool</code> <p>Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated</p> <code>False</code> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe containing the circular averages</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>def circular_average(\n    self,\n    tagname_filter: [str],\n    start_date: str,\n    end_date: str,\n    time_interval_rate: str,\n    time_interval_unit: str,\n    lower_bound: int,\n    upper_bound: int,\n    include_bad_data: bool = False,\n    pivot: bool = False,\n    display_uom: bool = False,\n    limit: int = None,\n    offset: int = None,\n) -&gt; DataFrame:\n    \"\"\"\n    A function that receives a dataframe of raw tag data and computes the circular mean for samples in a range.\n\n    **Example:**\n    ```python\n    from rtdip_sdk.authentication.azure import DefaultAuth\n    from rtdip_sdk.connectors import DatabricksSQLConnection\n    from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n    auth = DefaultAuth().authenticate()\n    token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n    connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n    data = (\n        TimeSeriesQueryBuilder()\n        .connect(connection)\n        .source(\"{tablename_or_path}\")\n        .circular_average(\n            tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n            start_date=\"2023-01-01\",\n            end_date=\"2023-01-31\",\n            time_interval_rate=\"15\",\n            time_interval_unit=\"minute\",\n            lower_bound=\"0\",\n            upper_bound=\"360\",\n        )\n    )\n\n    print(data)\n\n    ```\n\n    Args:\n        tagname_filter (list str): List of tagnames to filter on the source\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        time_interval_rate (str): The time interval rate (numeric input)\n        time_interval_unit (str): The time interval unit (second, minute, day, hour)\n        lower_bound (int): Lower boundary for the sample range\n        upper_bound (int): Upper boundary for the sample range\n        include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        pivot (optional bool): Pivot the data on the timestamp column with True or do not pivot the data with False\n        display_uom (optional bool): Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n\n    Returns:\n        DataFrame: A dataframe containing the circular averages\n    \"\"\"\n    circular_average_parameters = {\n        \"source\": self.data_source,\n        \"metadata_source\": self.metadata_source,\n        \"tag_names\": tagname_filter,\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"include_bad_data\": include_bad_data,\n        \"time_interval_rate\": time_interval_rate,\n        \"time_interval_unit\": time_interval_unit,\n        \"lower_bound\": lower_bound,\n        \"upper_bound\": upper_bound,\n        \"pivot\": pivot,\n        \"display_uom\": display_uom,\n        \"limit\": limit,\n        \"offset\": offset,\n        \"tagname_column\": self.tagname_column,\n        \"timestamp_column\": self.timestamp_column,\n        \"status_column\": self.status_column,\n        \"value_column\": self.value_column,\n        \"metadata_tagname_column\": self.metadata_tagname_column,\n        \"metadata_uom_column\": self.metadata_uom_column,\n        \"supress_warning\": True,\n    }\n\n    if (\n        \"display_uom\" in circular_average_parameters\n        and circular_average_parameters[\"display_uom\"] is True\n    ):\n        if circular_average_parameters[\"metadata_source\"] is None:\n            raise ValueError(\n                \"display_uom True requires metadata_source to be populated\"\n            )\n\n    return circular_average.get(self.connection, circular_average_parameters)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder.circular_standard_deviation","title":"<code>circular_standard_deviation(tagname_filter, start_date, end_date, time_interval_rate, time_interval_unit, lower_bound, upper_bound, include_bad_data=False, pivot=False, display_uom=False, limit=None, offset=None)</code>","text":"<p>A function that receives a dataframe of raw tag data and computes the circular standard deviation for samples assumed to be in the range.</p> <p>Example: <pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .circular_standard_deviation(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n        time_interval_rate=\"15\",\n        time_interval_unit=\"minute\",\n        lower_bound=\"0\",\n        upper_bound=\"360\",\n    )\n)\n\nprint(data)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tagname_filter</code> <code>list str</code> <p>List of tagnames to filter on the source</p> required <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>time_interval_rate</code> <code>str</code> <p>The time interval rate (numeric input)</p> required <code>time_interval_unit</code> <code>str</code> <p>The time interval unit (second, minute, day, hour)</p> required <code>lower_bound</code> <code>int</code> <p>Lower boundary for the sample range</p> required <code>upper_bound</code> <code>int</code> <p>Upper boundary for the sample range</p> required <code>include_bad_data</code> <code>optional bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>False</code> <code>pivot</code> <code>optional bool</code> <p>Pivot the data on the timestamp column with True or do not pivot the data with False</p> <code>False</code> <code>display_uom</code> <code>optional bool</code> <p>Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated</p> <code>False</code> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe containing the circular standard deviations</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>def circular_standard_deviation(\n    self,\n    tagname_filter: [str],\n    start_date: str,\n    end_date: str,\n    time_interval_rate: str,\n    time_interval_unit: str,\n    lower_bound: int,\n    upper_bound: int,\n    include_bad_data: bool = False,\n    pivot: bool = False,\n    display_uom: bool = False,\n    limit: int = None,\n    offset: int = None,\n) -&gt; DataFrame:\n    \"\"\"\n    A function that receives a dataframe of raw tag data and computes the circular standard deviation for samples assumed to be in the range.\n\n    **Example:**\n    ```python\n    from rtdip_sdk.authentication.azure import DefaultAuth\n    from rtdip_sdk.connectors import DatabricksSQLConnection\n    from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n    auth = DefaultAuth().authenticate()\n    token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n    connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n    data = (\n        TimeSeriesQueryBuilder()\n        .connect(connection)\n        .source(\"{tablename_or_path}\")\n        .circular_standard_deviation(\n            tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n            start_date=\"2023-01-01\",\n            end_date=\"2023-01-31\",\n            time_interval_rate=\"15\",\n            time_interval_unit=\"minute\",\n            lower_bound=\"0\",\n            upper_bound=\"360\",\n        )\n    )\n\n    print(data)\n\n    ```\n\n    Args:\n        tagname_filter (list str): List of tagnames to filter on the source\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        time_interval_rate (str): The time interval rate (numeric input)\n        time_interval_unit (str): The time interval unit (second, minute, day, hour)\n        lower_bound (int): Lower boundary for the sample range\n        upper_bound (int): Upper boundary for the sample range\n        include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        pivot (optional bool): Pivot the data on the timestamp column with True or do not pivot the data with False\n        display_uom (optional bool): Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n\n    Returns:\n        DataFrame: A dataframe containing the circular standard deviations\n    \"\"\"\n    circular_stdev_parameters = {\n        \"source\": self.data_source,\n        \"metadata_source\": self.metadata_source,\n        \"tag_names\": tagname_filter,\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"include_bad_data\": include_bad_data,\n        \"time_interval_rate\": time_interval_rate,\n        \"time_interval_unit\": time_interval_unit,\n        \"lower_bound\": lower_bound,\n        \"upper_bound\": upper_bound,\n        \"pivot\": pivot,\n        \"display_uom\": display_uom,\n        \"limit\": limit,\n        \"offset\": offset,\n        \"tagname_column\": self.tagname_column,\n        \"timestamp_column\": self.timestamp_column,\n        \"status_column\": self.status_column,\n        \"value_column\": self.value_column,\n        \"metadata_tagname_column\": self.metadata_tagname_column,\n        \"metadata_uom_column\": self.metadata_uom_column,\n        \"supress_warning\": True,\n    }\n\n    if (\n        \"display_uom\" in circular_stdev_parameters\n        and circular_stdev_parameters[\"display_uom\"] is True\n    ):\n        if circular_stdev_parameters[\"metadata_source\"] is None:\n            raise ValueError(\n                \"display_uom True requires metadata_source to be populated\"\n            )\n\n    return circular_standard_deviation.get(\n        self.connection, circular_stdev_parameters\n    )\n</code></pre>"},{"location":"sdk/code-reference/query/functions/time_series/time_series_query_builder/#src.sdk.python.rtdip_sdk.queries.time_series.time_series_query_builder.TimeSeriesQueryBuilder.summary","title":"<code>summary(tagname_filter, start_date, end_date, include_bad_data=False, display_uom=False, limit=None, offset=None)</code>","text":"<p>A function to return back a summary of statistics.</p> <p>Example: <pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .summary(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n    )\n)\n\nprint(data)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tagname_filter</code> <code>list str</code> <p>List of tagnames to filter on the source</p> required <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>include_bad_data</code> <code>optional bool</code> <p>Include \"Bad\" data points with True or remove \"Bad\" data points with False</p> <code>False</code> <code>display_uom</code> <code>optional bool</code> <p>Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated</p> <code>False</code> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <code>offset</code> <code>optional int</code> <p>The number of rows to skip before returning rows</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of raw timeseries data.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/time_series/time_series_query_builder.py</code> <pre><code>def summary(\n    self,\n    tagname_filter: [str],\n    start_date: str,\n    end_date: str,\n    include_bad_data: bool = False,\n    display_uom: bool = False,\n    limit: int = None,\n    offset: int = None,\n) -&gt; DataFrame:\n    \"\"\"\n    A function to return back a summary of statistics.\n\n    **Example:**\n    ```python\n    from rtdip_sdk.authentication.azure import DefaultAuth\n    from rtdip_sdk.connectors import DatabricksSQLConnection\n    from rtdip_sdk.queries import TimeSeriesQueryBuilder\n\n    auth = DefaultAuth().authenticate()\n    token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n    connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n    data = (\n        TimeSeriesQueryBuilder()\n        .connect(connection)\n        .source(\"{tablename_or_path}\")\n        .summary(\n            tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n            start_date=\"2023-01-01\",\n            end_date=\"2023-01-31\",\n        )\n    )\n\n    print(data)\n\n    ```\n\n    Args:\n        tagname_filter (list str): List of tagnames to filter on the source\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        include_bad_data (optional bool): Include \"Bad\" data points with True or remove \"Bad\" data points with False\n        display_uom (optional bool): Display the unit of measure with True or False. Defaults to False. If True, metadata_source must be populated\n        limit (optional int): The number of rows to be returned\n        offset (optional int): The number of rows to skip before returning rows\n\n    Returns:\n        DataFrame: A dataframe of raw timeseries data.\n    \"\"\"\n    summary_parameters = {\n        \"source\": self.data_source,\n        \"metadata_source\": self.metadata_source,\n        \"tag_names\": tagname_filter,\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"include_bad_data\": include_bad_data,\n        \"display_uom\": display_uom,\n        \"limit\": limit,\n        \"offset\": offset,\n        \"tagname_column\": self.tagname_column,\n        \"timestamp_column\": self.timestamp_column,\n        \"status_column\": self.status_column,\n        \"value_column\": self.value_column,\n        \"metadata_tagname_column\": self.metadata_tagname_column,\n        \"metadata_uom_column\": self.metadata_uom_column,\n        \"supress_warning\": True,\n    }\n\n    if (\n        \"display_uom\" in summary_parameters\n        and summary_parameters[\"display_uom\"] is True\n    ):\n        if summary_parameters[\"metadata_source\"] is None:\n            raise ValueError(\n                \"display_uom True requires metadata_source to be populated\"\n            )\n\n    return summary.get(self.connection, summary_parameters)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/weather/latest/","title":"Weather Latest Function","text":""},{"location":"sdk/code-reference/query/functions/weather/latest/#src.sdk.python.rtdip_sdk.queries.weather.latest.get_grid","title":"<code>get_grid(connection, parameters_dict)</code>","text":"<p>A function to return the latest event values by querying databricks SQL Warehouse using a connection specified by the user.</p> <p>This will return the raw weather forecast data for a grid.</p> <p>The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.</p> <p>The available authentcation methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.</p> <p>This function requires the user to input a dictionary of parameters. (See Attributes table below)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>source</code> <code>optional str</code> <p>Source of the data the full table name</p> <code>forecast</code> <code>str</code> <p>Any specific identifier for forecast</p> <code>forecast_type(str)</code> <code>str</code> <p>Type of forecast ie weather, solar, power, etc</p> <code>region</code> <code>str</code> <p>Region</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>max_lat</code> <code>float</code> <p>Maximum latitude</p> <code>max_lon</code> <code>float</code> <p>Maximum longitude</p> <code>min_lat</code> <code>float</code> <p>Minimum latitude</p> <code>min_lon</code> <code>float</code> <p>Minimum longitude</p> <code>measurement</code> <code>optional str</code> <p>Measurement type</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of event latest values.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/weather/latest.py</code> <pre><code>def get_grid(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    A function to return the latest event values by querying databricks SQL Warehouse using a connection specified by the user.\n\n    This will return the raw weather forecast data for a grid.\n\n    The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.\n\n    The available authentcation methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.\n\n    This function requires the user to input a dictionary of parameters. (See Attributes table below)\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict: A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        source (optional str): Source of the data the full table name\n        forecast (str): Any specific identifier for forecast\n        forecast_type(str): Type of forecast ie weather, solar, power, etc\n        region (str): Region\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        max_lat (float): Maximum latitude\n        max_lon (float): Maximum longitude\n        min_lat (float): Minimum latitude\n        min_lon (float): Minimum longitude\n        measurement (optional str): Measurement type\n        limit (optional int): The number of rows to be returned\n\n    Returns:\n        DataFrame: A dataframe of event latest values.\n    \"\"\"\n    try:\n        query = _query_builder(parameters_dict, \"latest_grid\")\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error returning latest function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/weather/latest/#src.sdk.python.rtdip_sdk.queries.weather.latest.get_point","title":"<code>get_point(connection, parameters_dict)</code>","text":"<p>A function to return the latest event values by querying databricks SQL Warehouse using a connection specified by the user.</p> <p>This will return the raw weather forecast data for a single point.</p> <p>The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.</p> <p>The available authentcation methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.</p> <p>This function requires the user to input a dictionary of parameters. (See Attributes table below)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>source</code> <code>optional str</code> <p>Source of the data the full table name</p> <code>forecast</code> <code>str</code> <p>Any specific identifier for forecast</p> <code>forecast_type(str)</code> <code>str</code> <p>Type of forecast ie weather, solar, power, etc</p> <code>region</code> <code>str</code> <p>Region</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>lat</code> <code>float</code> <p>latitude</p> <code>lon</code> <code>float</code> <p>longitude</p> <code>measurement</code> <code>optional str</code> <p>Measurement type</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of event latest values.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/weather/latest.py</code> <pre><code>def get_point(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    A function to return the latest event values by querying databricks SQL Warehouse using a connection specified by the user.\n\n    This will return the raw weather forecast data for a single point.\n\n    The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.\n\n    The available authentcation methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.\n\n    This function requires the user to input a dictionary of parameters. (See Attributes table below)\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict: A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        source (optional str): Source of the data the full table name\n        forecast (str): Any specific identifier for forecast\n        forecast_type(str): Type of forecast ie weather, solar, power, etc\n        region (str): Region\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        lat (float): latitude\n        lon (float): longitude\n        measurement (optional str): Measurement type\n        limit (optional int): The number of rows to be returned\n\n    Returns:\n        DataFrame: A dataframe of event latest values.\n    \"\"\"\n    try:\n        query = _query_builder(parameters_dict, \"latest_point\")\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error returning latest function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/weather/latest/#example-get_point","title":"Example get_point","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import WeatherQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    WeatherQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .latest_point(\n        lat=\"{latitude}\",\n        lon=\"{longitude}\",\n    )\n)\n\nprint(data)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/weather/latest/#example-get_grid","title":"Example get_grid","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import WeatherQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    WeatherQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .latest_grid(\n        min_lat=\"{minimum_latitude}\",\n        min_lon=\"{minimum_longitude}\",\n        max_lat=\"{maximum_latitude}\",\n        max_lon=\"{maximum_longitude}\",\n    )\n)\n\nprint(data)\n</code></pre> <p>These examples are using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code>, <code>TURBODBCSQLConnection()</code> or <code>SparkConnection()</code>.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/functions/weather/raw/","title":"Weather Raw Function","text":""},{"location":"sdk/code-reference/query/functions/weather/raw/#src.sdk.python.rtdip_sdk.queries.weather.raw.get_grid","title":"<code>get_grid(connection, parameters_dict)</code>","text":"<p>A function to return back raw data by querying databricks SQL Warehouse using a connection specified by the user.</p> <p>This will return the raw weather forecast data for a grid.</p> <p>The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.</p> <p>The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.</p> <p>This function requires the user to input a dictionary of parameters which are focused on Weather Data. (See Attributes table below)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>source</code> <code>optional str</code> <p>Source of the data the full table name</p> <code>forecast</code> <code>str</code> <p>Any specific identifier for forecast</p> <code>forecast_type(str)</code> <code>str</code> <p>Type of forecast ie weather, solar, power, etc</p> <code>region</code> <code>str</code> <p>Region</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>forecast_run_start_date</code> <code>str</code> <p>Start date of the forecast run (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>forecast_run_end_date</code> <code>str</code> <p>End date of the forecast run (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>timestamp_column</code> <code>str</code> <p>The column which contains the the forecast output time. Default \"EventTime\".</p> <code>forecast_run_timestamp_column</code> <code>str</code> <p>The column which contains whent the forecast was run. Default \"EnqueuedTime\".</p> <code>max_lat</code> <code>float</code> <p>Maximum latitude</p> <code>max_lon</code> <code>float</code> <p>Maximum longitude</p> <code>min_lat</code> <code>float</code> <p>Minimum latitude</p> <code>min_lon</code> <code>float</code> <p>Minimum longitude</p> <code>measurement</code> <code>optional str</code> <p>Measurement type</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <p>}</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of raw weather forecast data.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/weather/raw.py</code> <pre><code>def get_grid(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    A function to return back raw data by querying databricks SQL Warehouse using a connection specified by the user.\n\n    This will return the raw weather forecast data for a grid.\n\n    The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.\n\n    The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.\n\n    This function requires the user to input a dictionary of parameters which are focused on Weather Data. (See Attributes table below)\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict: A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        source (optional str): Source of the data the full table name\n        forecast (str): Any specific identifier for forecast\n        forecast_type(str): Type of forecast ie weather, solar, power, etc\n        region (str): Region\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        forecast_run_start_date (str): Start date of the forecast run (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        forecast_run_end_date (str): End date of the forecast run (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        timestamp_column (str): The column which contains the the forecast output time. Default \"EventTime\".\n        forecast_run_timestamp_column (str): The column which contains whent the forecast was run. Default \"EnqueuedTime\".\n        max_lat (float): Maximum latitude\n        max_lon (float): Maximum longitude\n        min_lat (float): Minimum latitude\n        min_lon (float): Minimum longitude\n        measurement (optional str): Measurement type\n        limit (optional int): The number of rows to be returned\n    }\n\n    Returns:\n        DataFrame: A dataframe of raw weather forecast data.\n    \"\"\"\n    try:\n        query = _query_builder(parameters_dict, \"raw_grid\")\n\n        print(query)\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error with raw function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/weather/raw/#src.sdk.python.rtdip_sdk.queries.weather.raw.get_point","title":"<code>get_point(connection, parameters_dict)</code>","text":"<p>A function to return back raw data by querying databricks SQL Warehouse using a connection specified by the user.</p> <p>This will return the raw weather forecast data for a single point.</p> <p>The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.</p> <p>The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.</p> <p>This function requires the user to input a dictionary of parameters which are focused on Weather Data. (See Attributes table below)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>object</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required <code>parameters_dict</code> <code>dict</code> <p>A dictionary of parameters (see Attributes table below)</p> required <p>Attributes:</p> Name Type Description <code>source</code> <code>optional str</code> <p>Source of the data the full table name</p> <code>forecast</code> <code>str</code> <p>Any specific identifier for forecast</p> <code>forecast_type(str)</code> <code>str</code> <p>Type of forecast ie weather, solar, power, etc</p> <code>region</code> <code>str</code> <p>Region</p> <code>data_security_level</code> <code>str</code> <p>Level of data security</p> <code>data_type</code> <code>str</code> <p>Type of the data (float, integer, double, string)</p> <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>forecast_run_start_date</code> <code>str</code> <p>Start date of the forecast run (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>forecast_run_end_date</code> <code>str</code> <p>End date of the forecast run (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> <code>timestamp_column</code> <code>str</code> <p>The column which contains the the forecast output time. Default \"EventTime\".</p> <code>forecast_run_timestamp_column</code> <code>str</code> <p>The column which contains whent the forecast was run. Default \"EnqueuedTime.</p> <code>lat</code> <code>float</code> <p>latitude</p> <code>lon</code> <code>float</code> <p>longitude</p> <code>measurement</code> <code>optional str</code> <p>Measurement type</p> <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <p>}</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of raw weather forecast data.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/weather/raw.py</code> <pre><code>def get_point(connection: object, parameters_dict: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    A function to return back raw data by querying databricks SQL Warehouse using a connection specified by the user.\n\n    This will return the raw weather forecast data for a single point.\n\n    The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.\n\n    The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.\n\n    This function requires the user to input a dictionary of parameters which are focused on Weather Data. (See Attributes table below)\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        parameters_dict: A dictionary of parameters (see Attributes table below)\n\n    Attributes:\n        source (optional str): Source of the data the full table name\n        forecast (str): Any specific identifier for forecast\n        forecast_type(str): Type of forecast ie weather, solar, power, etc\n        region (str): Region\n        data_security_level (str): Level of data security\n        data_type (str): Type of the data (float, integer, double, string)\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        forecast_run_start_date (str): Start date of the forecast run (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        forecast_run_end_date (str): End date of the forecast run (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        timestamp_column (str): The column which contains the the forecast output time. Default \"EventTime\".\n        forecast_run_timestamp_column (str): The column which contains whent the forecast was run. Default \"EnqueuedTime.\n        lat (float): latitude\n        lon (float): longitude\n        measurement (optional str): Measurement type\n        limit (optional int): The number of rows to be returned\n    }\n\n    Returns:\n        DataFrame: A dataframe of raw weather forecast data.\n    \"\"\"\n    try:\n        query = _query_builder(parameters_dict, \"raw_point\")\n\n        print(query)\n\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error with raw function\")\n        raise e\n</code></pre>"},{"location":"sdk/code-reference/query/functions/weather/raw/#example-get_point","title":"Example get_point","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import WeatherQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    WeatherQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .raw_point(\n        start_date=\"{start_date}\",\n        end_date=\"{end_date}\",\n        forecast_run_start_date=\"{forecast_run_start_date}\",\n        forecast_run_end_date=\"{forecast_run_end_date}\",\n        lat=\"{latitude}\",\n        lon=\"{longitude}\",\n    )\n)\n\nprint(data)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/weather/raw/#example-get_grid","title":"Example get_grid","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import WeatherQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    WeatherQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .raw_grid(\n        start_date=\"{start_date}\",\n        end_date=\"{end_date}\",\n        forecast_run_start_date=\"{forecast_run_start_date}\",\n        forecast_run_end_date=\"{forecast_run_end_date}\",\n        min_lat=\"{minimum_latitude}\",\n        min_lon=\"{minimum_longitude}\",\n        max_lat=\"{maximum_latitude}\",\n        max_lon=\"{maximum_longitude}\",\n    )\n)\n\nprint(data)\n</code></pre> <p>These examples are using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code>, <code>TURBODBCSQLConnection()</code> or <code>SparkConnection()</code>.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/functions/weather/weather_query_builder/","title":"Weather Query Builder","text":"<p>Note</p> <p>These examples are using <code>DefaultAuth()</code> and <code>DatabricksSQLConnection()</code> to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by <code>PYODBCSQLConnection()</code>, <code>TURBODBCSQLConnection()</code> or <code>SparkConnection()</code>.</p> <p>Note</p> <p><code>server_hostname</code> and <code>http_path</code> can be found on the SQL Warehouses Page. </p>"},{"location":"sdk/code-reference/query/functions/weather/weather_query_builder/#src.sdk.python.rtdip_sdk.queries.weather.weather_query_builder.WeatherQueryBuilder","title":"<code>WeatherQueryBuilder</code>","text":"<p>A builder for developing RTDIP forecast queries using any delta table</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/weather/weather_query_builder.py</code> <pre><code>class WeatherQueryBuilder:\n    \"\"\"\n    A builder for developing RTDIP forecast queries using any delta table\n\n    \"\"\"\n\n    parameters: dict\n    connection: ConnectionInterface\n    close_connection: bool\n    data_source: str\n    tagname_column: str\n    timestamp_column: str\n    status_column: str\n    value_column: str\n\n    def connect(self, connection: ConnectionInterface):\n        \"\"\"\n        Specifies the connection to be used for the query\n\n        Args:\n            connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        \"\"\"\n        self.connection = connection\n        return self\n\n    def source(\n        self,\n        source: str,\n        tagname_column: str = \"TagName\",\n        timestamp_column: str = \"EventTime\",\n        forecast_run_timestamp_column: str = \"EnqueuedTime\",\n        status_column: Union[str, None] = \"Status\",\n        value_column: str = \"Value\",\n    ):\n        \"\"\"\n        Specifies the source of the query\n\n        Args:\n            source (str): Source of the query can be a Unity Catalog table, Hive metastore table or path\n            tagname_column (optional str): The column name in the source that contains the tagnames or series\n            timestamp_column (optional str): The timestamp column name in the source\n            forecast_run_timestamp_column (optional str): The forecast run timestamp column name in the source\n            status_column (optional str): The status column name in the source indicating `Good` or `Bad`. If this is not available, specify `None`\n            value_column (optional str): The value column name in the source which is normally a float or string value for the time series event\n        \"\"\"\n        self.data_source = \"`.`\".join(source.split(\".\"))\n        self.tagname_column = tagname_column\n        self.timestamp_column = timestamp_column\n        self.forecast_run_timestamp_column = forecast_run_timestamp_column\n        self.status_column = status_column\n        self.value_column = value_column\n        return self\n\n    def raw_point(\n        self,\n        start_date: str,\n        end_date: str,\n        forecast_run_start_date: str,\n        forecast_run_end_date: str,\n        lat: float,\n        lon: float,\n        limit: int = None,\n        measurement: str = None,\n    ) -&gt; DataFrame:\n        \"\"\"\n        A function to return back raw data for a point.\n\n        **Example:**\n        ```python\n        from rtdip_sdk.queries.weather.weather_query_builder import (\n            WeatherQueryBuilder,\n        )\n        from rtdip_sdk.authentication.azure import DefaultAuth\n        from rtdip_sdk.connectors import DatabricksSQLConnection\n\n        auth = DefaultAuth().authenticate()\n        token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n        connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n        data = (\n                WeatherQueryBuilder()\n                .connect(connection)\n                .source(\"example.forecast.table\")\n                .raw_point(\n                    start_date=\"2021-01-01\",\n                    end_date=\"2021-01-02\",\n                    forecast_run_start_date=\"2021-01-01\",\n                    forecast_run_end_date=\"2021-01-02\",\n                    lat=0.1,\n                    lon=0.1,\n                )\n            )\n\n        print(data)\n        ```\n\n        Args:\n            start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            forecast_run_start_date (str): Start date of the forecast run (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            forecast_run_end_date (str): End date of the forecast run (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            lat (float): latitude\n            lon (float): longitude\n            limit (optional int): The number of rows to be returned\n            measurement (optional str): Measurement type\n\n        Returns:\n            DataFrame: A dataframe of raw timeseries data.\n        \"\"\"\n        raw_parameters = {\n            \"source\": self.data_source,\n            \"start_date\": start_date,\n            \"end_date\": end_date,\n            \"forecast_run_start_date\": forecast_run_start_date,\n            \"forecast_run_end_date\": forecast_run_end_date,\n            \"timestamp_column\": self.timestamp_column,\n            \"forecast_run_timestamp_column\": self.forecast_run_timestamp_column,\n            \"lat\": lat,\n            \"lon\": lon,\n            \"limit\": limit,\n            \"measurement\": measurement,\n            \"supress_warning\": True,\n        }\n\n        return raw.get_point(self.connection, raw_parameters)\n\n    def latest_point(\n        self,\n        lat: float,\n        lon: float,\n        limit: int = None,\n        measurement: str = None,\n    ) -&gt; DataFrame:\n        \"\"\"\n        A function to return back the latest data for a point.\n\n        **Example:**\n        ```python\n        from rtdip_sdk.queries.weather.weather_query_builder import (\n            WeatherQueryBuilder,\n        )\n        from rtdip_sdk.authentication.azure import DefaultAuth\n        from rtdip_sdk.connectors import DatabricksSQLConnection\n\n        auth = DefaultAuth().authenticate()\n        token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n        connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n        data = (\n                WeatherQueryBuilder()\n                .connect(connection)\n                .source(\"example.forecast.table\")\n                .latest_point(\n                    lat=0.1,\n                    lon=0.1,\n                    )\n                )\n\n        print(data)\n        ```\n\n        Args:\n            lat (float): latitude\n            lon (float): longitude\n            limit (optional int): The number of rows to be returned\n            measurement (optional str): Measurement type\n\n        Returns:\n            DataFrame: A dataframe of raw timeseries data.\n        \"\"\"\n        raw_parameters = {\n            \"source\": self.data_source,\n            \"lat\": lat,\n            \"lon\": lon,\n            \"limit\": limit,\n            \"measurement\": measurement,\n            \"supress_warning\": True,\n        }\n\n        return latest.get_point(self.connection, raw_parameters)\n\n    def raw_grid(  # NOSONAR\n        self,  # NOSONAR\n        start_date: str,\n        end_date: str,\n        forecast_run_start_date: str,\n        forecast_run_end_date: str,\n        min_lat: float,\n        min_lon: float,\n        max_lat: float,\n        max_lon: float,\n        limit: int = None,  # NOSONAR\n        measurement: str = None,\n    ) -&gt; DataFrame:\n        \"\"\"\n        A function to return back raw data for a grid.\n\n        **Example:**\n        ```python\n        from rtdip_sdk.queries.weather.weather_query_builder import (\n            WeatherQueryBuilder,\n        )\n        from rtdip_sdk.authentication.azure import DefaultAuth\n        from rtdip_sdk.connectors import DatabricksSQLConnection\n\n        auth = DefaultAuth().authenticate()\n        token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n        connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n        data = (\n                WeatherQueryBuilder()\n                .connect(connection)\n                .source(\"example.forecast.table\")\n                .raw_grid(\n                    start_date=\"2021-01-01\",\n                    end_date=\"2021-01-02\",\n                    forecast_run_start_date=\"2021-01-01\",\n                    forecast_run_end_date=\"2021-01-02\",\n                    min_lat=0.1,\n                    max_lat=0.1,\n                    min_lon=0.1,\n                    max_lon=0.1,\n                )\n            )\n\n        print(data)\n        ```\n\n        Args:\n            start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            forecast_run_start_date (str): Start date of the forecast run (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            forecast_run_end_date (str): End date of the forecast run (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n            min_lat (float): Min latitude\n            min_lon (float): Min longitude\n            max_lat (float): Max latitude\n            max_lon (float): Max longitude\n            limit (optional int): The number of rows to be returned\n            measurement (optional str): Measurement type\n\n        Returns:\n            DataFrame: A dataframe of raw timeseries data.\n        \"\"\"\n        raw_parameters = {\n            \"source\": self.data_source,\n            \"start_date\": start_date,\n            \"end_date\": end_date,\n            \"forecast_run_start_date\": forecast_run_start_date,\n            \"forecast_run_end_date\": forecast_run_end_date,\n            \"timestamp_column\": self.timestamp_column,\n            \"forecast_run_timestamp_column\": self.forecast_run_timestamp_column,\n            \"min_lat\": min_lat,\n            \"min_lon\": min_lon,\n            \"max_lat\": max_lat,\n            \"max_lon\": max_lon,\n            \"limit\": limit,\n            \"measurement\": measurement,\n            \"supress_warning\": True,\n        }\n\n        return raw.get_grid(self.connection, raw_parameters)\n\n    def latest_grid(\n        self,\n        min_lat: float,\n        min_lon: float,\n        max_lat: float,\n        max_lon: float,\n        limit: int = None,\n        measurement: str = None,\n    ) -&gt; DataFrame:\n        \"\"\"\n        A function to return back the latest data for a grid.\n\n        **Example:**\n        ```python\n        from rtdip_sdk.queries.weather.weather_query_builder import (\n            WeatherQueryBuilder,\n        )\n        from rtdip_sdk.authentication.azure import DefaultAuth\n        from rtdip_sdk.connectors import DatabricksSQLConnection\n\n        auth = DefaultAuth().authenticate()\n        token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n        connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n        data = (\n                WeatherQueryBuilder()\n                .connect(connection)\n                .source(\"example.forecast.table\")\n                .latest_grid(\n                    min_lat=0.1,\n                    max_lat=0.1,\n                    min_lon=0.1,\n                    max_lon=0.1,\n                )\n            )\n\n        print(data)\n        ```\n\n        Args:\n            min_lat (float): Min latitude\n            min_lon (float): Min longitude\n            max_lat (float): Max latitude\n            max_lon (float): Max longitude\n            limit (optional int): The number of rows to be returned\n            measurement (optional str): Measurement type\n\n        Returns:\n            DataFrame: A dataframe of raw timeseries data.\n        \"\"\"\n        raw_parameters = {\n            \"source\": self.data_source,\n            \"min_lat\": min_lat,\n            \"min_lon\": min_lon,\n            \"max_lat\": max_lat,\n            \"max_lon\": max_lon,\n            \"limit\": limit,\n            \"measurement\": measurement,\n            \"supress_warning\": True,\n        }\n\n        return latest.get_grid(self.connection, raw_parameters)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/weather/weather_query_builder/#src.sdk.python.rtdip_sdk.queries.weather.weather_query_builder.WeatherQueryBuilder.connect","title":"<code>connect(connection)</code>","text":"<p>Specifies the connection to be used for the query</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>ConnectionInterface</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> required Source code in <code>src/sdk/python/rtdip_sdk/queries/weather/weather_query_builder.py</code> <pre><code>def connect(self, connection: ConnectionInterface):\n    \"\"\"\n    Specifies the connection to be used for the query\n\n    Args:\n        connection: Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n    \"\"\"\n    self.connection = connection\n    return self\n</code></pre>"},{"location":"sdk/code-reference/query/functions/weather/weather_query_builder/#src.sdk.python.rtdip_sdk.queries.weather.weather_query_builder.WeatherQueryBuilder.source","title":"<code>source(source, tagname_column='TagName', timestamp_column='EventTime', forecast_run_timestamp_column='EnqueuedTime', status_column='Status', value_column='Value')</code>","text":"<p>Specifies the source of the query</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Source of the query can be a Unity Catalog table, Hive metastore table or path</p> required <code>tagname_column</code> <code>optional str</code> <p>The column name in the source that contains the tagnames or series</p> <code>'TagName'</code> <code>timestamp_column</code> <code>optional str</code> <p>The timestamp column name in the source</p> <code>'EventTime'</code> <code>forecast_run_timestamp_column</code> <code>optional str</code> <p>The forecast run timestamp column name in the source</p> <code>'EnqueuedTime'</code> <code>status_column</code> <code>optional str</code> <p>The status column name in the source indicating <code>Good</code> or <code>Bad</code>. If this is not available, specify <code>None</code></p> <code>'Status'</code> <code>value_column</code> <code>optional str</code> <p>The value column name in the source which is normally a float or string value for the time series event</p> <code>'Value'</code> Source code in <code>src/sdk/python/rtdip_sdk/queries/weather/weather_query_builder.py</code> <pre><code>def source(\n    self,\n    source: str,\n    tagname_column: str = \"TagName\",\n    timestamp_column: str = \"EventTime\",\n    forecast_run_timestamp_column: str = \"EnqueuedTime\",\n    status_column: Union[str, None] = \"Status\",\n    value_column: str = \"Value\",\n):\n    \"\"\"\n    Specifies the source of the query\n\n    Args:\n        source (str): Source of the query can be a Unity Catalog table, Hive metastore table or path\n        tagname_column (optional str): The column name in the source that contains the tagnames or series\n        timestamp_column (optional str): The timestamp column name in the source\n        forecast_run_timestamp_column (optional str): The forecast run timestamp column name in the source\n        status_column (optional str): The status column name in the source indicating `Good` or `Bad`. If this is not available, specify `None`\n        value_column (optional str): The value column name in the source which is normally a float or string value for the time series event\n    \"\"\"\n    self.data_source = \"`.`\".join(source.split(\".\"))\n    self.tagname_column = tagname_column\n    self.timestamp_column = timestamp_column\n    self.forecast_run_timestamp_column = forecast_run_timestamp_column\n    self.status_column = status_column\n    self.value_column = value_column\n    return self\n</code></pre>"},{"location":"sdk/code-reference/query/functions/weather/weather_query_builder/#src.sdk.python.rtdip_sdk.queries.weather.weather_query_builder.WeatherQueryBuilder.raw_point","title":"<code>raw_point(start_date, end_date, forecast_run_start_date, forecast_run_end_date, lat, lon, limit=None, measurement=None)</code>","text":"<p>A function to return back raw data for a point.</p> <p>Example: <pre><code>from rtdip_sdk.queries.weather.weather_query_builder import (\n    WeatherQueryBuilder,\n)\nfrom rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n        WeatherQueryBuilder()\n        .connect(connection)\n        .source(\"example.forecast.table\")\n        .raw_point(\n            start_date=\"2021-01-01\",\n            end_date=\"2021-01-02\",\n            forecast_run_start_date=\"2021-01-01\",\n            forecast_run_end_date=\"2021-01-02\",\n            lat=0.1,\n            lon=0.1,\n        )\n    )\n\nprint(data)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>forecast_run_start_date</code> <code>str</code> <p>Start date of the forecast run (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>forecast_run_end_date</code> <code>str</code> <p>End date of the forecast run (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>lat</code> <code>float</code> <p>latitude</p> required <code>lon</code> <code>float</code> <p>longitude</p> required <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <code>measurement</code> <code>optional str</code> <p>Measurement type</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of raw timeseries data.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/weather/weather_query_builder.py</code> <pre><code>def raw_point(\n    self,\n    start_date: str,\n    end_date: str,\n    forecast_run_start_date: str,\n    forecast_run_end_date: str,\n    lat: float,\n    lon: float,\n    limit: int = None,\n    measurement: str = None,\n) -&gt; DataFrame:\n    \"\"\"\n    A function to return back raw data for a point.\n\n    **Example:**\n    ```python\n    from rtdip_sdk.queries.weather.weather_query_builder import (\n        WeatherQueryBuilder,\n    )\n    from rtdip_sdk.authentication.azure import DefaultAuth\n    from rtdip_sdk.connectors import DatabricksSQLConnection\n\n    auth = DefaultAuth().authenticate()\n    token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n    connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n    data = (\n            WeatherQueryBuilder()\n            .connect(connection)\n            .source(\"example.forecast.table\")\n            .raw_point(\n                start_date=\"2021-01-01\",\n                end_date=\"2021-01-02\",\n                forecast_run_start_date=\"2021-01-01\",\n                forecast_run_end_date=\"2021-01-02\",\n                lat=0.1,\n                lon=0.1,\n            )\n        )\n\n    print(data)\n    ```\n\n    Args:\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        forecast_run_start_date (str): Start date of the forecast run (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        forecast_run_end_date (str): End date of the forecast run (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        lat (float): latitude\n        lon (float): longitude\n        limit (optional int): The number of rows to be returned\n        measurement (optional str): Measurement type\n\n    Returns:\n        DataFrame: A dataframe of raw timeseries data.\n    \"\"\"\n    raw_parameters = {\n        \"source\": self.data_source,\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"forecast_run_start_date\": forecast_run_start_date,\n        \"forecast_run_end_date\": forecast_run_end_date,\n        \"timestamp_column\": self.timestamp_column,\n        \"forecast_run_timestamp_column\": self.forecast_run_timestamp_column,\n        \"lat\": lat,\n        \"lon\": lon,\n        \"limit\": limit,\n        \"measurement\": measurement,\n        \"supress_warning\": True,\n    }\n\n    return raw.get_point(self.connection, raw_parameters)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/weather/weather_query_builder/#src.sdk.python.rtdip_sdk.queries.weather.weather_query_builder.WeatherQueryBuilder.latest_point","title":"<code>latest_point(lat, lon, limit=None, measurement=None)</code>","text":"<p>A function to return back the latest data for a point.</p> <p>Example: <pre><code>from rtdip_sdk.queries.weather.weather_query_builder import (\n    WeatherQueryBuilder,\n)\nfrom rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n        WeatherQueryBuilder()\n        .connect(connection)\n        .source(\"example.forecast.table\")\n        .latest_point(\n            lat=0.1,\n            lon=0.1,\n            )\n        )\n\nprint(data)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>lat</code> <code>float</code> <p>latitude</p> required <code>lon</code> <code>float</code> <p>longitude</p> required <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <code>measurement</code> <code>optional str</code> <p>Measurement type</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of raw timeseries data.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/weather/weather_query_builder.py</code> <pre><code>def latest_point(\n    self,\n    lat: float,\n    lon: float,\n    limit: int = None,\n    measurement: str = None,\n) -&gt; DataFrame:\n    \"\"\"\n    A function to return back the latest data for a point.\n\n    **Example:**\n    ```python\n    from rtdip_sdk.queries.weather.weather_query_builder import (\n        WeatherQueryBuilder,\n    )\n    from rtdip_sdk.authentication.azure import DefaultAuth\n    from rtdip_sdk.connectors import DatabricksSQLConnection\n\n    auth = DefaultAuth().authenticate()\n    token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n    connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n    data = (\n            WeatherQueryBuilder()\n            .connect(connection)\n            .source(\"example.forecast.table\")\n            .latest_point(\n                lat=0.1,\n                lon=0.1,\n                )\n            )\n\n    print(data)\n    ```\n\n    Args:\n        lat (float): latitude\n        lon (float): longitude\n        limit (optional int): The number of rows to be returned\n        measurement (optional str): Measurement type\n\n    Returns:\n        DataFrame: A dataframe of raw timeseries data.\n    \"\"\"\n    raw_parameters = {\n        \"source\": self.data_source,\n        \"lat\": lat,\n        \"lon\": lon,\n        \"limit\": limit,\n        \"measurement\": measurement,\n        \"supress_warning\": True,\n    }\n\n    return latest.get_point(self.connection, raw_parameters)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/weather/weather_query_builder/#src.sdk.python.rtdip_sdk.queries.weather.weather_query_builder.WeatherQueryBuilder.raw_grid","title":"<code>raw_grid(start_date, end_date, forecast_run_start_date, forecast_run_end_date, min_lat, min_lon, max_lat, max_lon, limit=None, measurement=None)</code>","text":"<p>A function to return back raw data for a grid.</p> <p>Example: <pre><code>from rtdip_sdk.queries.weather.weather_query_builder import (\n    WeatherQueryBuilder,\n)\nfrom rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n        WeatherQueryBuilder()\n        .connect(connection)\n        .source(\"example.forecast.table\")\n        .raw_grid(\n            start_date=\"2021-01-01\",\n            end_date=\"2021-01-02\",\n            forecast_run_start_date=\"2021-01-01\",\n            forecast_run_end_date=\"2021-01-02\",\n            min_lat=0.1,\n            max_lat=0.1,\n            min_lon=0.1,\n            max_lon=0.1,\n        )\n    )\n\nprint(data)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>end_date</code> <code>str</code> <p>End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>forecast_run_start_date</code> <code>str</code> <p>Start date of the forecast run (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>forecast_run_end_date</code> <code>str</code> <p>End date of the forecast run (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)</p> required <code>min_lat</code> <code>float</code> <p>Min latitude</p> required <code>min_lon</code> <code>float</code> <p>Min longitude</p> required <code>max_lat</code> <code>float</code> <p>Max latitude</p> required <code>max_lon</code> <code>float</code> <p>Max longitude</p> required <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <code>measurement</code> <code>optional str</code> <p>Measurement type</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of raw timeseries data.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/weather/weather_query_builder.py</code> <pre><code>def raw_grid(  # NOSONAR\n    self,  # NOSONAR\n    start_date: str,\n    end_date: str,\n    forecast_run_start_date: str,\n    forecast_run_end_date: str,\n    min_lat: float,\n    min_lon: float,\n    max_lat: float,\n    max_lon: float,\n    limit: int = None,  # NOSONAR\n    measurement: str = None,\n) -&gt; DataFrame:\n    \"\"\"\n    A function to return back raw data for a grid.\n\n    **Example:**\n    ```python\n    from rtdip_sdk.queries.weather.weather_query_builder import (\n        WeatherQueryBuilder,\n    )\n    from rtdip_sdk.authentication.azure import DefaultAuth\n    from rtdip_sdk.connectors import DatabricksSQLConnection\n\n    auth = DefaultAuth().authenticate()\n    token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n    connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n    data = (\n            WeatherQueryBuilder()\n            .connect(connection)\n            .source(\"example.forecast.table\")\n            .raw_grid(\n                start_date=\"2021-01-01\",\n                end_date=\"2021-01-02\",\n                forecast_run_start_date=\"2021-01-01\",\n                forecast_run_end_date=\"2021-01-02\",\n                min_lat=0.1,\n                max_lat=0.1,\n                min_lon=0.1,\n                max_lon=0.1,\n            )\n        )\n\n    print(data)\n    ```\n\n    Args:\n        start_date (str): Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        end_date (str): End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        forecast_run_start_date (str): Start date of the forecast run (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        forecast_run_end_date (str): End date of the forecast run (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz)\n        min_lat (float): Min latitude\n        min_lon (float): Min longitude\n        max_lat (float): Max latitude\n        max_lon (float): Max longitude\n        limit (optional int): The number of rows to be returned\n        measurement (optional str): Measurement type\n\n    Returns:\n        DataFrame: A dataframe of raw timeseries data.\n    \"\"\"\n    raw_parameters = {\n        \"source\": self.data_source,\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"forecast_run_start_date\": forecast_run_start_date,\n        \"forecast_run_end_date\": forecast_run_end_date,\n        \"timestamp_column\": self.timestamp_column,\n        \"forecast_run_timestamp_column\": self.forecast_run_timestamp_column,\n        \"min_lat\": min_lat,\n        \"min_lon\": min_lon,\n        \"max_lat\": max_lat,\n        \"max_lon\": max_lon,\n        \"limit\": limit,\n        \"measurement\": measurement,\n        \"supress_warning\": True,\n    }\n\n    return raw.get_grid(self.connection, raw_parameters)\n</code></pre>"},{"location":"sdk/code-reference/query/functions/weather/weather_query_builder/#src.sdk.python.rtdip_sdk.queries.weather.weather_query_builder.WeatherQueryBuilder.latest_grid","title":"<code>latest_grid(min_lat, min_lon, max_lat, max_lon, limit=None, measurement=None)</code>","text":"<p>A function to return back the latest data for a grid.</p> <p>Example: <pre><code>from rtdip_sdk.queries.weather.weather_query_builder import (\n    WeatherQueryBuilder,\n)\nfrom rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n        WeatherQueryBuilder()\n        .connect(connection)\n        .source(\"example.forecast.table\")\n        .latest_grid(\n            min_lat=0.1,\n            max_lat=0.1,\n            min_lon=0.1,\n            max_lon=0.1,\n        )\n    )\n\nprint(data)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>min_lat</code> <code>float</code> <p>Min latitude</p> required <code>min_lon</code> <code>float</code> <p>Min longitude</p> required <code>max_lat</code> <code>float</code> <p>Max latitude</p> required <code>max_lon</code> <code>float</code> <p>Max longitude</p> required <code>limit</code> <code>optional int</code> <p>The number of rows to be returned</p> <code>None</code> <code>measurement</code> <code>optional str</code> <p>Measurement type</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of raw timeseries data.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/weather/weather_query_builder.py</code> <pre><code>def latest_grid(\n    self,\n    min_lat: float,\n    min_lon: float,\n    max_lat: float,\n    max_lon: float,\n    limit: int = None,\n    measurement: str = None,\n) -&gt; DataFrame:\n    \"\"\"\n    A function to return back the latest data for a grid.\n\n    **Example:**\n    ```python\n    from rtdip_sdk.queries.weather.weather_query_builder import (\n        WeatherQueryBuilder,\n    )\n    from rtdip_sdk.authentication.azure import DefaultAuth\n    from rtdip_sdk.connectors import DatabricksSQLConnection\n\n    auth = DefaultAuth().authenticate()\n    token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n    connection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\n    data = (\n            WeatherQueryBuilder()\n            .connect(connection)\n            .source(\"example.forecast.table\")\n            .latest_grid(\n                min_lat=0.1,\n                max_lat=0.1,\n                min_lon=0.1,\n                max_lon=0.1,\n            )\n        )\n\n    print(data)\n    ```\n\n    Args:\n        min_lat (float): Min latitude\n        min_lon (float): Min longitude\n        max_lat (float): Max latitude\n        max_lon (float): Max longitude\n        limit (optional int): The number of rows to be returned\n        measurement (optional str): Measurement type\n\n    Returns:\n        DataFrame: A dataframe of raw timeseries data.\n    \"\"\"\n    raw_parameters = {\n        \"source\": self.data_source,\n        \"min_lat\": min_lat,\n        \"min_lon\": min_lon,\n        \"max_lat\": max_lat,\n        \"max_lon\": max_lon,\n        \"limit\": limit,\n        \"measurement\": measurement,\n        \"supress_warning\": True,\n    }\n\n    return latest.get_grid(self.connection, raw_parameters)\n</code></pre>"},{"location":"sdk/code-reference/query/sql/sql_query/","title":"SQL Query","text":""},{"location":"sdk/code-reference/query/sql/sql_query/#src.sdk.python.rtdip_sdk.queries.sql.sql_query.SQLQueryBuilder","title":"<code>SQLQueryBuilder</code>","text":"<p>A builder for developing RTDIP queries using any delta table</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/sql/sql_query.py</code> <pre><code>class SQLQueryBuilder:\n    \"\"\"\n    A builder for developing RTDIP queries using any delta table\n    \"\"\"\n\n    sql_query: dict\n    connection: ConnectionInterface\n\n    def get(\n        self, connection=object, sql_query=str, to_json=False, limit=None, offset=None\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        A function to return back raw data by querying databricks SQL Warehouse using a connection specified by the user.\n\n        The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.\n\n        The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.\n\n        This function requires the user to input a dictionary of parameters. (See Attributes table below)\n\n        Args:\n            connection (obj): Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n            sql_query (str): A string of the SQL query to be executed.\n            limit (optional int): Limit the number of rows to be returned\n            offset (optional int): Offset the start of the rows to be returned\n\n        Returns:\n            DataFrame: A dataframe of data.\n        \"\"\"\n        try:\n            parameters_dict = {\"sql_statement\": sql_query}\n            parameters_dict[\"to_json\"] = to_json\n            parameters_dict[\"supress_warning\"] = True\n            if limit:\n                parameters_dict[\"limit\"] = limit\n            if offset:\n                parameters_dict[\"offset\"] = offset\n\n            query = _query_builder(parameters_dict, \"sql\")\n            try:\n                cursor = connection.cursor()\n                cursor.execute(query)\n                df = cursor.fetch_all()\n                cursor.close()\n                connection.close()\n                return df\n            except Exception as e:\n                logging.exception(\"Error returning dataframe\")\n                raise e\n\n        except Exception as e:\n            logging.exception(\"error with sql query\")\n            raise e\n</code></pre>"},{"location":"sdk/code-reference/query/sql/sql_query/#src.sdk.python.rtdip_sdk.queries.sql.sql_query.SQLQueryBuilder.get","title":"<code>get(connection=object, sql_query=str, to_json=False, limit=None, offset=None)</code>","text":"<p>A function to return back raw data by querying databricks SQL Warehouse using a connection specified by the user.</p> <p>The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.</p> <p>The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.</p> <p>This function requires the user to input a dictionary of parameters. (See Attributes table below)</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>obj</code> <p>Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)</p> <code>object</code> <code>sql_query</code> <code>str</code> <p>A string of the SQL query to be executed.</p> <code>str</code> <code>limit</code> <code>optional int</code> <p>Limit the number of rows to be returned</p> <code>None</code> <code>offset</code> <code>optional int</code> <p>Offset the start of the rows to be returned</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A dataframe of data.</p> Source code in <code>src/sdk/python/rtdip_sdk/queries/sql/sql_query.py</code> <pre><code>def get(\n    self, connection=object, sql_query=str, to_json=False, limit=None, offset=None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    A function to return back raw data by querying databricks SQL Warehouse using a connection specified by the user.\n\n    The available connectors by RTDIP are Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect.\n\n    The available authentication methods are Certificate Authentication, Client Secret Authentication or Default Authentication. See documentation.\n\n    This function requires the user to input a dictionary of parameters. (See Attributes table below)\n\n    Args:\n        connection (obj): Connection chosen by the user (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect)\n        sql_query (str): A string of the SQL query to be executed.\n        limit (optional int): Limit the number of rows to be returned\n        offset (optional int): Offset the start of the rows to be returned\n\n    Returns:\n        DataFrame: A dataframe of data.\n    \"\"\"\n    try:\n        parameters_dict = {\"sql_statement\": sql_query}\n        parameters_dict[\"to_json\"] = to_json\n        parameters_dict[\"supress_warning\"] = True\n        if limit:\n            parameters_dict[\"limit\"] = limit\n        if offset:\n            parameters_dict[\"offset\"] = offset\n\n        query = _query_builder(parameters_dict, \"sql\")\n        try:\n            cursor = connection.cursor()\n            cursor.execute(query)\n            df = cursor.fetch_all()\n            cursor.close()\n            connection.close()\n            return df\n        except Exception as e:\n            logging.exception(\"Error returning dataframe\")\n            raise e\n\n    except Exception as e:\n        logging.exception(\"error with sql query\")\n        raise e\n</code></pre>"},{"location":"sdk/examples/pipelines/deploy/ECMWF-to-Delta/","title":"ECMWF to Delta Pipeline","text":"<p>This article provides a guide on how to execute a pipeline that makes an API request to pull the ECMWF MARS Data as a .nc file, transform the .nc file to a dataframe from a grid range and writes to a Delta Table locally using the RTDIP SDK. </p> <p>This pipeline was tested on an M2 Macbook Pro using VS Code in a Python (3.11) environment.</p>"},{"location":"sdk/examples/pipelines/deploy/ECMWF-to-Delta/#prerequisites","title":"Prerequisites","text":"<p>This pipeline job requires the packages:</p> <ul> <li>rtdip-sdk</li> </ul>"},{"location":"sdk/examples/pipelines/deploy/ECMWF-to-Delta/#components","title":"Components","text":"Name Description SparkECMWFWeatherForecastSource Pulls data from ECMWF MARS API and stores as a .nc file. ECMWFExtractGridToWeatherDataModel Transforms ECMWF .nc file to a dataframe ingesting Grid Data. SparkDeltaDestination Writes to Delta."},{"location":"sdk/examples/pipelines/deploy/ECMWF-to-Delta/#common-errors","title":"Common Errors","text":"Error Solution [com.google.common.util.concurrent.ExecutionError: java.lang.NoClassDefFoundError: org/apache/spark/ErrorClassesJsonReader] The Delta version in the Spark Session must be compatible with your local Pyspark version. See here for version compatibility"},{"location":"sdk/examples/pipelines/deploy/ECMWF-to-Delta/#example","title":"Example","text":"<p>Below is an example of how to read from and write to Delta Tables locally without the need for Spark</p> <pre><code>from rtdip_sdk.pipelines.sources.spark.ecmwf.weather_forecast import SparkECMWFWeatherForecastSource\nfrom rtdip_sdk.pipelines.transformers.spark.ecmwf.nc_extractgrid_to_weather_data_model import ECMWFExtractGridToWeatherDataModel\nfrom rtdip_sdk.pipelines.destinations.spark.delta import SparkDeltaDestination\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n\necmwf_api_key = \"xxxxx\"\necmwf_api_email = \"xxxxx\"\n\ndate_start = \"2020-10-01 00:00:00\"\ndate_end = \"2020-10-02 00:00:00\"\n\necmwf_class = \"od\"\nstream = \"oper\"\nexpver = \"1\"\nleveltype = \"sfc\"\nrun_interval = \"12\"\nrun_frequency = \"H\"\ngrid_step = 0.1\nec_vars = [\n    \"cbh\", \"dsrp\", \"sp\", \"tcwv\", \"tcc\"\n]\n\ntag_prefix = \"US:[55, -130, 20, -60]:\"\nmethod = \"nearest\"\npath = '/dbfs/forecast/nc/US/' # Path to save the data can be changed\nforecast_area = [55, -130, 20, -60]  # N/W/S/E\nlat_max = 50\nlat_min = 25\nlon_max = -65\nlon_min = -75\n\n\ndef pipeline():\n\n    spark = SparkSessionUtility(config={}).execute()\n\n\n    weather_source = SparkECMWFWeatherForecastSource(\n        spark=spark,\n        date_start=date_start,\n        date_end=date_end,\n        save_path=path,\n        ecmwf_class=ecmwf_class,\n        stream=stream,\n        expver=expver,\n        leveltype=leveltype,\n        ec_vars=ec_vars,\n        forecast_area=forecast_area,\n        ecmwf_api_key=ecmwf_api_key,\n        ecmwf_api_email=ecmwf_api_email,\n    )\n\n    weather_source.read_batch()\n\n    extract = ECMWFExtractGridToWeatherDataModel(\n            lat_min=lat_min,\n            lat_max=lat_max,\n            lon_min=lon_min,\n            lon_max=lon_max,\n            grid_step=grid_step,\n            load_path=path,\n            date_start=date_start,\n            date_end=date_end,\n            run_interval=run_interval,\n            run_frequency=run_frequency\n    )\n\n    df = extract.transform(tag_prefix, ec_vars, method)\n\n    sparkdf = spark.createDataFrame(df) \n\n    SparkDeltaDestination(\n        data=sparkdf, options={}, destination=\"{path/to/table}\"\n    ).write_batch()\n\n\nif __name__ == \"__main__\":\n    pipeline()\n</code></pre>"},{"location":"sdk/examples/pipelines/deploy/EdgeX-Eventhub-to-Delta/","title":"EdgeX Eventhub to Delta Pipeline","text":"<p>This article provides a guide on how to execute a pipeline that batch reads EdgeX data from an Eventhub and writes to a Delta Table locally using the RTDIP SDK. This pipeline was tested on an M2 Macbook Pro using VS Code in a Python (3.10) environment.</p>"},{"location":"sdk/examples/pipelines/deploy/EdgeX-Eventhub-to-Delta/#prerequisites","title":"Prerequisites","text":"<p>This pipeline job requires the packages:</p> <ul> <li>rtdip-sdk</li> </ul>"},{"location":"sdk/examples/pipelines/deploy/EdgeX-Eventhub-to-Delta/#components","title":"Components","text":"Name Description SparkEventhubSource Reads data from an Eventhub. BinaryToStringTransformer Transforms Spark DataFrame column to string. EdgeXOPCUAJsonToPCDMTransformer Transforms EdgeX to PCDM. SparkDeltaDestination Writes to Delta."},{"location":"sdk/examples/pipelines/deploy/EdgeX-Eventhub-to-Delta/#common-errors","title":"Common Errors","text":"Error Solution [com.google.common.util.concurrent.ExecutionError: java.lang.NoClassDefFoundError: org/apache/spark/ErrorClassesJsonReader] The Delta version in the Spark Session must be compatible with your local Pyspark version. See here for version compatibility"},{"location":"sdk/examples/pipelines/deploy/EdgeX-Eventhub-to-Delta/#example","title":"Example","text":"<p>Below is an example of how to read from and write to Delta Tables locally without the need for Spark</p> <pre><code>from rtdip_sdk.pipelines.sources.spark.eventhub import SparkEventhubSource\nfrom rtdip_sdk.pipelines.transformers.spark.binary_to_string import (\n    BinaryToStringTransformer,\n)\nfrom rtdip_sdk.pipelines.destinations.spark.delta import SparkDeltaDestination\nfrom rtdip_sdk.pipelines.transformers.spark.edgex_opcua_json_to_pcdm import (\n    EdgeXOPCUAJsonToPCDMTransformer,\n)\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\nimport json\n\n\ndef pipeline():\n\n    spark = SparkSessionUtility(config={}).execute()\n\n    ehConf = {\n        \"eventhubs.connectionString\": \"{EventhubConnectionString}\",\n        \"eventhubs.consumerGroup\": \"{EventhubConsumerGroup}\",\n        \"eventhubs.startingPosition\": json.dumps(\n            {\"offset\": \"0\", \"seqNo\": -1, \"enqueuedTime\": None, \"isInclusive\": True}\n        ),\n    }\n\n    source = SparkEventhubSource(spark, ehConf).read_batch()\n    string_data = BinaryToStringTransformer(source, \"body\", \"body\").transform()\n    PCDM_data = EdgeXOPCUAJsonToPCDMTransformer(string_data, \"body\").transform()\n    SparkDeltaDestination(\n        data=PCDM_data, options={}, destination=\"{path/to/table}\"\n    ).write_batch()\n\n\nif __name__ == \"__main__\":\n    pipeline()\n</code></pre>"},{"location":"sdk/examples/pipelines/deploy/Fledge-SparkConnect-PCDM/","title":"Fledge Eventhub to Delta Pipeline using Spark Connect","text":"<p>This article provides a guide on how to execute a pipeline that batch reads Fledge data from an Eventhub, transforms to the Process Control Data Model and writes it to Delta via Spark Connect/Databricks Connect v2. This pipeline was tested on an M2 Macbook Pro using VS Code in a Python (3.11) environment.</p>"},{"location":"sdk/examples/pipelines/deploy/Fledge-SparkConnect-PCDM/#prerequisites","title":"Prerequisites","text":"<p>This pipeline job requires the packages:</p> <ul> <li>rtdip-sdk</li> </ul>"},{"location":"sdk/examples/pipelines/deploy/Fledge-SparkConnect-PCDM/#components","title":"Components","text":"Name Description SparkEventhubSource Reads data from an Eventhub. BinaryToStringTransformer Transforms Spark DataFrame column to string. FledgeOPCUAJsonToPCDMTransformer Transforms Fledge to PCDM. SparkPCDMToDeltaDestination Writes to Delta in the Pross Control Data Model format"},{"location":"sdk/examples/pipelines/deploy/Fledge-SparkConnect-PCDM/#example","title":"Example","text":"<p>Below is an example of reading from an Eventhub, transforming and writing to Delta using Spark Connect/Databricks Connect v2.</p> <pre><code>from rtdip_sdk.pipelines.sources import SparkKafkaEventhubSource\nfrom rtdip_sdk.pipelines.transformers import (\n    FledgeOPCUAJsonToPCDMTransformer,\n    BinaryToStringTransformer,\n)\nfrom rtdip_sdk.pipelines.destinations import SparkPCDMToDeltaDestination\nfrom rtdip_sdk.pipelines.secrets import AzureKeyVaultSecrets\nfrom rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.pipelines.utilities import SparkSessionUtility\n\n\ndef pipeline():\n    auth = DefaultAuth().authenticate()\n    token = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n\n    DATABRICKS_WORKSPACE = \"adb-xxxxxxxxxx.x.azuredatabricks.net\"\n    DATABRICKS_CLUSTER_ID = \"xxx-yyyyyy-zzzzzzzz\"\n    DATABRICKS_USER_ID = (\n        \"your_user_id@your_domain.com\"  # required for Spark Connect on Windows\n    )\n\n    AZURE_KEYVAULT = \"{YOUR-KEYVAULT-NAME}\"\n    AZURE_KEYVAULT_SECRET = \"{YOUR-SECRET-NAME}\"\n\n    spark_remote = \"sc://{}:443/;token={};x-databricks-cluster-id={};user_id={}\".format(\n        DATABRICKS_WORKSPACE, token, DATABRICKS_CLUSTER_ID, DATABRICKS_USER_ID\n    )\n\n    EVENTHUB_CONNECTION_STRING = AzureKeyVaultSecrets(\n        vault=AZURE_KEYVAULT,\n        key=AZURE_KEYVAULT_SECRET,\n        credential=auth,\n    ).get()\n    EVENTHUB_CONSUMER_GROUP = \"{YOUR-CONSUMER-GROUP}\"\n\n    DESTINATION_FLOAT = \"{YOUR-FLOAT-DELTA-TABLE}\"\n    DESTINATION_STRING = \"{YOUR-STRING-DELTA-TABLE}\"\n    DESTINATION_INTEGER = \"{YOUR-INTEGER-DELTA-TABLE}\"\n\n    spark = SparkSessionUtility(config={}, remote=spark_remote).execute()\n\n    source_df = SparkKafkaEventhubSource(\n        spark=spark,\n        options={\n            \"startingOffsets\": \"earliest\",\n            \"maxOffsetsPerTrigger\": 500000,\n            \"failOnDataLoss\": \"false\",\n        },\n        connection_string=EVENTHUB_CONNECTION_STRING,\n        consumer_group=EVENTHUB_CONSUMER_GROUP,\n        decode_kafka_headers_to_amqp_properties=False,\n    ).read_stream()\n\n    transform_df = BinaryToStringTransformer(\n        data=source_df, source_column_name=\"body\", target_column_name=\"body\"\n    ).transform()\n\n    transform_df = FledgeOPCUAJsonToPCDMTransformer(\n        data=transform_df, source_column_name=\"body\"\n    ).transform()\n\n    transform_df = transform_df.withColumn(\n        \"EventDate\", transform_df[\"EventTime\"].cast(\"date\")\n    )\n\n    SparkPCDMToDeltaDestination(\n        spark=spark,\n        data=transform_df,\n        options={\n            \"checkpointLocation\": \"dbfs:/checkpoints/rtdip-fledge-pcdm-stream-pipeline\"\n        },\n        destination_float=DESTINATION_FLOAT,\n        destination_string=DESTINATION_STRING,\n        destination_integer=DESTINATION_INTEGER,\n        mode=\"append\",\n        trigger=\"30 seconds\",\n        merge=False,\n        try_broadcast_join=False,\n        remove_nanoseconds=True,\n        remove_duplicates=True,\n        query_wait_interval=30,\n    ).write_stream()\n\n\nif __name__ == \"__main__\":\n    pipeline()\n</code></pre>"},{"location":"sdk/examples/pipelines/deploy/MISODailyLoad-Batch-Pipeline-Local/","title":"MISO Pipeline using RTDIP","text":"<p>This article provides a guide on how to execute a MISO pipeline using RTDIP. This pipeline was tested on an M2 Macbook Pro using VS Code in a Conda (3.11) environment.</p>"},{"location":"sdk/examples/pipelines/deploy/MISODailyLoad-Batch-Pipeline-Local/#prerequisites","title":"Prerequisites","text":"<p>This pipeline assumes you have followed the installation instructions as specified in the Getting Started section. In particular ensure you have installed the following:</p> <ul> <li> <p>RTDIP SDK</p> </li> <li> <p>Java</p> </li> </ul> <p>RTDIP SDK Installation</p> <p>Ensure you have installed the RTDIP SDK as follows: <pre><code>pip install \"rtdip-sdk[pipelines,pyspark]\"\n</code></pre></p>"},{"location":"sdk/examples/pipelines/deploy/MISODailyLoad-Batch-Pipeline-Local/#components","title":"Components","text":"Name Description MISODailyLoadISOSource Read daily load data from MISO API. MISOToMDMTransformer Converts MISO Raw data into Meters Data Model. SparkDeltaDestination Writes to a Delta table."},{"location":"sdk/examples/pipelines/deploy/MISODailyLoad-Batch-Pipeline-Local/#example","title":"Example","text":"<p>Below is an example of how to set up a pipeline to read daily load data from the MISO API, transform it into the Meters Data Model and write it to a Delta table. <pre><code>from rtdip_sdk.pipelines.sources import MISODailyLoadISOSource\nfrom rtdip_sdk.pipelines.transformers import MISOToMDMTransformer\nfrom rtdip_sdk.pipelines.destinations import SparkDeltaDestination\nfrom pyspark.sql import SparkSession\n\ndef pipeline():\n    spark = SparkSession.builder.config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\")\\\n                                .config(\"spark.sql.extensions\",\"io.delta.sql.DeltaSparkSessionExtension\")\\\n                                .config(\"spark.sql.catalog.spark_catalog\",\"org.apache.spark.sql.delta.catalog.DeltaCatalog\").getOrCreate()\n\n    source_df = MISODailyLoadISOSource(\n        spark = spark,\n        options = {\n        \"load_type\": \"actual\",\n        \"date\": \"20230520\",\n        }\n    ).read_batch()\n\n    transform_value_df = MISOToMDMTransformer(\n        spark=spark,\n        data=source_df,\n        output_type= \"usage\"\n    ).transform()\n\n    transform_meta_df = MISOToMDMTransformer(\n        spark=spark,\n        data=source_df,\n        output_type= \"meta\"\n    ).transform()\n\n    SparkDeltaDestination(\n        data=transform_value_df,\n        options={\n            \"partitionBy\":\"timestamp\"\n        },   \n        destination=\"miso_usage_data\"\n    ).write_batch()    \n\n    SparkDeltaDestination(\n        data=transform_meta_df,\n        options={\n            \"partitionBy\":\"timestamp\"\n        },   \n        destination=\"miso_meta_data\"\n    ).write_batch() \n\nif __name__ == \"__main__\":\n    pipeline()\n</code></pre></p> <p>Using environments</p> <p>If using an environment, include the following lines at the top of your script to prevent a difference in Python versions in worker and driver: <pre><code>import sys, os\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n</code></pre></p>"},{"location":"sdk/examples/pipelines/deploy/PJMDailyLoad-Batch-Pipeline-Local/","title":"MISO Pipeline using RTDIP","text":"<p>This article provides a guide on how to execute a MISO pipeline using RTDIP. This pipeline was tested on an M2 Macbook Pro using VS Code in a Conda (3.11) environment.</p>"},{"location":"sdk/examples/pipelines/deploy/PJMDailyLoad-Batch-Pipeline-Local/#prerequisites","title":"Prerequisites","text":"<p>This pipeline assumes you have a valid API key from PJM and have followed the installation instructions as specified in the Getting Started section. In particular ensure you have installed the following:</p> <ul> <li> <p>RTDIP SDK</p> </li> <li> <p>Java</p> </li> </ul> <p>RTDIP SDK Installation</p> <p>Ensure you have installed the RTDIP SDK as follows: <pre><code>pip install \"rtdip-sdk[pipelines,pyspark]\"\n</code></pre></p>"},{"location":"sdk/examples/pipelines/deploy/PJMDailyLoad-Batch-Pipeline-Local/#components","title":"Components","text":"Name Description PJMDailyLoadISOSource Read daily load data from MISO API. PJMToMDMTransformer Converts PJM Raw data into Meters Data Model. SparkDeltaDestination Writes to a Delta table."},{"location":"sdk/examples/pipelines/deploy/PJMDailyLoad-Batch-Pipeline-Local/#example","title":"Example","text":"<p>Below is an example of how to set up a pipeline to read daily load data from the PJM API, transform it into the Meters Data Model and write it to a Delta table. <pre><code>from rtdip_sdk.pipelines.sources import PJMDailyLoadISOSource\nfrom rtdip_sdk.pipelines.transformers import PJMToMDMTransformer\nfrom rtdip_sdk.pipelines.destinations import SparkDeltaDestination\nfrom pyspark.sql import SparkSession\n\ndef pipeline():\n    spark = SparkSession.builder.config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\")\\\n                                .config(\"spark.sql.extensions\",\"io.delta.sql.DeltaSparkSessionExtension\")\\\n                                .config(\"spark.sql.catalog.spark_catalog\",\"org.apache.spark.sql.delta.catalog.DeltaCatalog\").getOrCreate()\n\n    source_df = PJMDailyLoadISOSource(\n        spark = spark,\n        options = {\n            \"api_key\": \"{api_key}\", \n            \"load_type\": \"actual\"\n        }\n    ).read_batch()\n\n    transform_value_df = PJMToMDMTransformer(\n        spark=spark,\n        data=source_df,\n        output_type= \"usage\"\n    ).transform()\n\n    transform_meta_df = PJMToMDMTransformer(\n        spark=spark,\n        data=source_df,\n        output_type= \"meta\"\n    ).transform()\n\n    SparkDeltaDestination(\n        data=transform_value_df,\n        options={\n            \"partitionBy\":\"timestamp\"\n        },   \n        destination=\"pjm_usage_data\"\n    ).write_batch()    \n\n    SparkDeltaDestination(\n        data=transform_meta_df,\n        options={\n            \"partitionBy\":\"timestamp\"\n        },   \n        destination=\"pjm_meta_data\"\n    ).write_batch() \n\nif __name__ == \"__main__\":\n    pipeline()\n</code></pre></p> <p>Using environments</p> <p>If using an environment, include the following lines at the top of your script to prevent a difference in Python versions in worker and driver: <pre><code>import sys, os\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n</code></pre></p>"},{"location":"sdk/examples/pipelines/deploy/Python-Delta-to-Delta/","title":"Python Delta Local Pipeline","text":"<p>This article provides a guide on how to execute a simple Delta Table copy locally without Spark using the RTDIP SDK. This pipeline was tested on an M2 Macbook Pro using VS Code in a Python (3.10) environment.</p>"},{"location":"sdk/examples/pipelines/deploy/Python-Delta-to-Delta/#prerequisites","title":"Prerequisites","text":"<p>This pipeline job requires the packages:</p> <ul> <li>rtdip-sdk</li> </ul>"},{"location":"sdk/examples/pipelines/deploy/Python-Delta-to-Delta/#components","title":"Components","text":"Name Description PythonDeltaSource Reads data from a Delta Table. PythonDeltaDestination Writes to a Delta table."},{"location":"sdk/examples/pipelines/deploy/Python-Delta-to-Delta/#example","title":"Example","text":"<p>Below is an example of how to read from and write to Delta Tables locally without the need for Spark</p> <pre><code>from rtdip_sdk.pipelines.sources.python.delta import PythonDeltaSource\nfrom rtdip_sdk.pipelines.destinations.python.delta import PythonDeltaDestination\n\nsource = PythonDeltaSource(\"{/path/to/source/table}\").read_batch()\n\ndestination = PythonDeltaDestination(source, \"{/path/to/destination/table}\", mode=\"append\").write_batch()\n</code></pre>"},{"location":"sdk/examples/pipelines/deploy/dagster/Fledge-Dagster-Pipeline-Databricks/","title":"Fledge Pipeline using Dagster and Databricks Connect","text":"<p>This article provides a guide on how to deploy a pipeline in dagster using the RTDIP SDK and Databricks Connect. This pipeline was tested on an M2 Macbook Pro using VS Code in a Python (3.10) environment.</p> <p>Note</p> <p>Reading from Eventhubs is currently not supported on Databricks Connect.</p>"},{"location":"sdk/examples/pipelines/deploy/dagster/Fledge-Dagster-Pipeline-Databricks/#prerequisites","title":"Prerequisites","text":"<p>Deployment using Databricks Connect requires:</p> <ul> <li> <p>a Databricks workspace</p> </li> <li> <p>a cluster in the same workspace</p> </li> <li> <p>a personal access token</p> </li> </ul> <p>Further information on Databricks requirements can be found here.</p> <p>This pipeline job requires the packages:</p> <ul> <li> <p>rtdip-sdk</p> </li> <li> <p>databricks-connect</p> </li> <li> <p>dagster</p> </li> </ul> <p>Dagster Installation</p> <p>For Mac users with an M1 or M2 chip, installation of dagster should be done as follows: <pre><code>pip install dagster dagster-webserver --find-links=https://github.com/dagster-io/build-grpcio/wiki/Wheels\n</code></pre></p>"},{"location":"sdk/examples/pipelines/deploy/dagster/Fledge-Dagster-Pipeline-Databricks/#components","title":"Components","text":"Name Description SparkDeltaSource Read data from a Delta table. BinaryToStringTransformer Converts a Spark DataFrame column from binary to string. FledgeOPCUAJsonToPCDMTransformer Converts a Spark DataFrame column containing a json string to the Process Control Data Model. SparkDeltaDestination Writes to a Delta table."},{"location":"sdk/examples/pipelines/deploy/dagster/Fledge-Dagster-Pipeline-Databricks/#authentication","title":"Authentication","text":"<p>For Databricks authentication, the following fields should be added to a configuration profile in your <code>.databrickscfg</code> file:</p> <pre><code>[PROFILE]\nhost = https://{workspace_instance}\ntoken = dapi...\ncluster_id = {cluster_id}\n</code></pre> <p>This profile should match the configurations in your <code>DatabricksSession</code> in the example below as it will be used by the Databricks extension in VS Code for authenticating your Databricks cluster.</p>"},{"location":"sdk/examples/pipelines/deploy/dagster/Fledge-Dagster-Pipeline-Databricks/#example","title":"Example","text":"<p>Below is an example of how to set up a pipeline to read Fledge data from a Delta table, transform it to RTDIP's PCDM model and write it to a Delta table.</p> <pre><code>from dagster import Definitions, ResourceDefinition, graph, op\nfrom databricks.connect import DatabricksSession\nfrom rtdip_sdk.pipelines.sources.spark.delta import SparkDeltaSource\nfrom rtdip_sdk.pipelines.transformers.spark.binary_to_string import BinaryToStringTransformer\nfrom rtdip_sdk.pipelines.transformers.spark.fledge_opcua_json_to_pcdm import FledgeOPCUAJsonToPCDMTransformer\nfrom rtdip_sdk.pipelines.destinations.spark.delta import SparkDeltaDestination\n\n# Databricks cluster configuration\ndatabricks_resource = ResourceDefinition.hardcoded_resource(\n                    DatabricksSession.builder.remote(\n                        host       = \"https://{workspace_instance_name}\",\n                        token      = \"{token}\",\n                        cluster_id = \"{cluster_id}\"\n                        ).getOrCreate()\n)\n\n# Pipeline\n@op(required_resource_keys={\"databricks\"})\ndef pipeline(context):\n    spark = context.resources.databricks\n    source = SparkDeltaSource(spark, {}, \"{path_to_table}\").read_batch()\n    transformer = BinaryToStringTransformer(source, \"{source_column_name}\", \"{target_column_name}\").transform()\n    transformer = FledgeOPCUAJsonToPCDMTransformer(transformer, \"{source_column_name}\").transform()\n    SparkDeltaDestination(transformer, {}, \"{path_to_table}\").write_batch()\n\n@graph\ndef fledge_pipeline():\n    pipeline()\n\nfledge_pipeline_job = fledge_pipeline.to_job(\n    resource_defs={ \n                    \"databricks\": databricks_resource\n                   }\n)\n\ndefs = Definitions(jobs=[fledge_pipeline_job])\n</code></pre>"},{"location":"sdk/examples/pipelines/deploy/dagster/Fledge-Dagster-Pipeline-Databricks/#deploy","title":"Deploy","text":"<p>The following command deploys the pipeline to dagster: <code>dagster dev -f &lt;path/to/file.py&gt;</code></p> <p>Using the link provided from the command above, click on Launchpad and hit run to run the pipeline.</p>"},{"location":"sdk/examples/pipelines/deploy/dagster/Fledge-Dagster-Pipeline-Local/","title":"Fledge Pipeline using Dagster","text":"<p>This article provides a guide on how to deploy a pipeline in dagster using the RTDIP SDK. This pipeline was tested on an M2 Macbook Pro using VS Code in a Python (3.10) environment.</p>"},{"location":"sdk/examples/pipelines/deploy/dagster/Fledge-Dagster-Pipeline-Local/#prerequisites","title":"Prerequisites","text":"<p>This pipeline job requires the packages:</p> <ul> <li> <p>rtdip-sdk</p> </li> <li> <p>dagster</p> </li> </ul> <p>Dagster Installation</p> <p>For Mac users with an M1 or M2 chip, installation of dagster should be done as follows: <pre><code>pip install dagster dagster-webserver --find-links=https://github.com/dagster-io/build-grpcio/wiki/Wheels\n</code></pre></p>"},{"location":"sdk/examples/pipelines/deploy/dagster/Fledge-Dagster-Pipeline-Local/#components","title":"Components","text":"Name Description SparkEventhubSource Read data from an Eventhub. BinaryToStringTransformer Converts a Spark DataFrame column from binary to string. FledgeOPCUAJsonToPCDMTransformer Converts a Spark DataFrame column containing a json string to the Process Control Data Model. SparkDeltaDestination Writes to a Delta table."},{"location":"sdk/examples/pipelines/deploy/dagster/Fledge-Dagster-Pipeline-Local/#example","title":"Example","text":"<p>Below is an example of how to set up a pipeline to read Fledge data from an Eventhub, transform it to RTDIP's PCDM model and write it to a Delta table on your machine.</p> <pre><code>import json\nfrom datetime import datetime as dt\nfrom dagster import Definitions, graph, op\nfrom dagster_pyspark.resources import pyspark_resource\nfrom rtdip_sdk.pipelines.sources.spark.eventhub import SparkEventhubSource\nfrom rtdip_sdk.pipelines.transformers.spark.binary_to_string import BinaryToStringTransformer\nfrom rtdip_sdk.pipelines.transformers.spark.fledge_opcua_json_to_pcdm import FledgeOPCUAJsonToPCDMTransformer\nfrom rtdip_sdk.pipelines.destinations.spark.delta import SparkDeltaDestination\n\n# PySpark cluster configuration\npackages = \"com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.22,io.delta:delta-core_2.12:2.4.0\"\nmy_pyspark_resource = pyspark_resource.configured(\n    {\"spark_conf\": {\"spark.default.parallelism\": 1,\n                    \"spark.jars.packages\": packages,\n                    \"spark.sql.extensions\": \"io.delta.sql.DeltaSparkSessionExtension\", \n                    \"spark.sql.catalog.spark_catalog\": \"org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n                    }\n    }\n)\n\n# EventHub configuration\neventhub_connection_string = \"{eventhub_connection_string}\"\neventhub_consumer_group = \"{eventhub_consumer_group}\"\n\nstartOffset = \"-1\"\nendTime = dt.now().strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\n\nstartingEventPosition = {\n  \"offset\": startOffset,  \n  \"seqNo\": -1,            \n  \"enqueuedTime\": None,   \n  \"isInclusive\": True\n}\n\nendingEventPosition = {\n  \"offset\": None,           \n  \"seqNo\": -1,              \n  \"enqueuedTime\": endTime,\n  \"isInclusive\": True\n}\n\nehConf = {\n'eventhubs.connectionString' : eventhub_connection_string,\n'eventhubs.consumerGroup': eventhub_consumer_group,\n'eventhubs.startingPosition' : json.dumps(startingEventPosition),\n'eventhubs.endingPosition' : json.dumps(endingEventPosition),\n'maxEventsPerTrigger': 1000\n}\n\n# Pipeline\n@op(required_resource_keys={\"spark\"})\ndef pipeline(context):\n    spark = context.resources.pyspark.spark_session\n    source = SparkEventhubSource(spark, ehConf).read_batch()\n    transformer = BinaryToStringTransformer(source, \"{source_column_name}\", \"{target_column_name}\").transform()\n    transformer = FledgeOPCUAJsonToPCDMTransformer(transformer, \"{source_column_name}\").transform()\n    SparkDeltaDestination(transformer, {}, \"{path_to_table}\").write_batch()\n\n@graph\ndef fledge_pipeline():\n    pipeline()\n\nfledge_pipeline_job = fledge_pipeline.to_job(\n    resource_defs={\n                   \"spark\": my_pyspark_resource\n                   }\n)\n\ndefs = Definitions(jobs=[fledge_pipeline_job])\n</code></pre>"},{"location":"sdk/examples/pipelines/deploy/dagster/Fledge-Dagster-Pipeline-Local/#deploy","title":"Deploy","text":"<p>The following command deploys the pipeline to dagster: <code>dagster dev -f &lt;path/to/file.py&gt;</code></p> <p>Using the link provided from the command above, click on Launchpad and hit run to run the pipeline.</p>"},{"location":"sdk/examples/pipelines/deploy/databricks/MISODailyLoad-Batch-Pipeline-Databricks/","title":"MISO Pipeline using RTDIP and Databricks","text":"<p>This article provides a guide on how to deploy a MISO pipeline from a local file to a Databricks workflow using the RTDIP SDK and was tested on an M2 Macbook Pro using VS Code in a Conda (3.11) environment. RTDIP Pipeline Components provide Databricks with all the required Python packages and JARs to execute each component, this will automatically be set up during workflow creation.</p>"},{"location":"sdk/examples/pipelines/deploy/databricks/MISODailyLoad-Batch-Pipeline-Databricks/#prerequisites","title":"Prerequisites","text":"<p>This pipeline assumes you have a Databricks workspace and have followed the installation instructions as specified in the Getting Started section. In particular ensure you have installed the following:</p> <ul> <li> <p>RTDIP SDK</p> </li> <li> <p>Java</p> </li> </ul> <p>RTDIP SDK Installation</p> <p>Ensure you have installed the RTDIP SDK as follows: <pre><code>pip install \"rtdip-sdk[pipelines]\"\n</code></pre></p>"},{"location":"sdk/examples/pipelines/deploy/databricks/MISODailyLoad-Batch-Pipeline-Databricks/#components","title":"Components","text":"Name Description MISODailyLoadISOSource Read daily load data from MISO API. MISOToMDMTransformer Converts MISO Raw data into Meters Data Model. SparkDeltaDestination Writes to a Delta table. DatabricksSDKDeploy Deploys an RTDIP Pipeline to Databricks Workflows leveraging the Databricks SDK. DeltaTableOptimizeUtility Optimizes a Delta Table DeltaTableVacuumUtility Vacuums a Delta Table"},{"location":"sdk/examples/pipelines/deploy/databricks/MISODailyLoad-Batch-Pipeline-Databricks/#example","title":"Example","text":"<p>Below is an example of how to set up a pipeline job to read daily load data from the MISO API, transform it into the Meters Data Model and write it to a Delta table. <pre><code>from rtdip_sdk.pipelines.sources import MISODailyLoadISOSource\nfrom rtdip_sdk.pipelines.transformers import MISOToMDMTransformer\nfrom rtdip_sdk.pipelines.destinations import SparkDeltaDestination\n\ndef pipeline():   \n    source_df = MISODailyLoadISOSource(\n        spark = spark,\n        options = {\n        \"load_type\": \"actual\",\n        \"date\": \"20230520\",\n        }\n    ).read_batch()\n\n    transform_value_df = MISOToMDMTransformer(\n        spark=spark,\n        data=source_df,\n        output_type= \"usage\"\n    ).transform()\n\n    transform_meta_df = MISOToMDMTransformer(\n        spark=spark,\n        data=source_df,\n        output_type= \"meta\"\n    ).transform()\n\n    SparkDeltaDestination(\n        data=transform_value_df,\n        options={\n            \"partitionBy\":\"timestamp\"\n        },   \n        destination=\"miso_usage_data\" \n    ).write_batch()    \n\n    SparkDeltaDestination(\n        data=transform_meta_df,\n        options={\n            \"partitionBy\":\"timestamp\"\n        },   \n        destination=\"miso_meta_data\",\n        mode=\"overwrite\"\n    ).write_batch() \n\nif __name__ == \"__main__\":\n    pipeline()\n</code></pre></p>"},{"location":"sdk/examples/pipelines/deploy/databricks/MISODailyLoad-Batch-Pipeline-Databricks/#maintenance","title":"Maintenance","text":"<p>The RTDIP SDK can be used to maintain Delta tables in Databricks, an example of how to set up a maintenance job to optimize and vacuum the MISO tables written from the previous example is provided below. <pre><code>from rtdip_sdk.pipelines.utilities import DeltaTableOptimizeUtility, DeltaTableVacuumUtility\n\ndef maintenance():\n    TABLE_NAMES = [\n        \"{path.to.table.miso_usage_data}\",\n        \"{path.to.table.miso_meta_data}\"\n    ]\n\n    for table in TABLE_NAMES:\n\n        DeltaTableOptimizeUtility(\n            spark=spark, \n            table_name=table\n        ).execute()\n\n        DeltaTableVacuumUtility(\n            spark=spark,\n            table_name=table\n        ).execute()\n\nif __name__ == \"__main__\":\n    maintenance()\n</code></pre></p>"},{"location":"sdk/examples/pipelines/deploy/databricks/MISODailyLoad-Batch-Pipeline-Databricks/#deploy","title":"Deploy","text":"<p>Deployment to Databricks uses the Databricks SDK. Users have the option to control the job's configurations including the cluster and schedule. <pre><code>from rtdip_sdk.pipelines.deploy import DatabricksSDKDeploy, CreateJob, JobCluster, ClusterSpec, Task, NotebookTask, AutoScale, RuntimeEngine, DataSecurityMode, CronSchedule, Continuous, PauseStatus\nfrom rtdip_sdk.authentication.azure import DefaultAuth\n\ndef deploy():\n    credential = DefaultAuth().authenticate()\n    access_token = credential.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n\n    DATABRICKS_WORKSPACE = \"{databricks-workspace-url}\"\n\n    # Create clusters\n    cluster_list = []\n    cluster_list.append(JobCluster(\n        job_cluster_key=\"pipeline-cluster\",\n        new_cluster=ClusterSpec(\n            node_type_id=\"Standard_E4ds_v5\",\n            autoscale=AutoScale(min_workers=1, max_workers=8),\n            spark_version=\"13.3.x-scala2.12\",\n            data_security_mode=DataSecurityMode.SINGLE_USER,\n            runtime_engine=RuntimeEngine.STANDARD\n        )\n    ))\n\n    # Create tasks\n    task_list = []\n    task_list.append(Task(\n        task_key=\"pipeline\",\n        job_cluster_key=\"pipeline-cluster\",\n        notebook_task=NotebookTask(\n            notebook_path=\"{path/to/pipeline.py}\"\n        )\n    ))\n\n    # Create a Databricks Job for the Task\n    job = CreateJob(\n        name=\"rtdip-miso-batch-pipeline-job\",\n        job_clusters=cluster_list,\n        tasks=task_list,\n        continuous=Continuous(pause_status=PauseStatus.UNPAUSED)\n    )\n\n    # Deploy to Databricks\n    databricks_pipeline_job = DatabricksSDKDeploy(databricks_job=job, host=DATABRICKS_WORKSPACE, token=access_token, workspace_directory=\"{path/to/databricks/workspace/directory}\")\n    databricks_pipeline_job.deploy()\n\n    cluster_list = []\n    cluster_list.append(JobCluster(\n        job_cluster_key=\"maintenance-cluster\",\n        new_cluster=ClusterSpec(\n            node_type_id=\"Standard_E4ds_v5\",\n            autoscale=AutoScale(min_workers=1, max_workers=3),\n            spark_version=\"13.3.x-scala2.12\",\n            data_security_mode=DataSecurityMode.SINGLE_USER,\n            runtime_engine=RuntimeEngine.PHOTON\n        )\n    ))\n\n    task_list = []\n    task_list.append(Task(\n        task_key=\"rtdip-miso-maintenance-task\",\n        job_cluster_key=\"maintenance-cluster\",\n        notebook_task=NotebookTask(\n            notebook_path=\"{path/to/maintenance.py}\"\n        )\n    ))\n\n    # Create a Databricks Job for the Task\n    job = CreateJob(\n        name=\"rtdip-miso-maintenance-job\",\n        job_clusters=cluster_list,\n        tasks=task_list,\n        schedule=CronSchedule(\n            quartz_cron_expression=\"4 * * * * ?\",\n            timezone_id=\"UTC\",\n            pause_status=PauseStatus.UNPAUSED\n        )\n    )\n\n    # Deploy to Databricks\n    databricks_pipeline_job = DatabricksSDKDeploy(databricks_job=job, host=DATABRICKS_WORKSPACE, token=access_token, workspace_directory=\"{path/to/databricks/workspace/directory}\")\n    databricks_pipeline_job.deploy()\n\nif __name__ == \"__main__\":\n    deploy()\n</code></pre></p>"},{"location":"sdk/examples/query/Circular-Average/","title":"Circular Average","text":"<p>Circular Average - A function that receives a dataframe of raw tag data and computes the circular mean for samples in a range, returning the results.</p>"},{"location":"sdk/examples/query/Circular-Average/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the RTDIP SDK as specified in the Getting Started section.</p> <p>This example is using DefaultAuth() and DatabricksSQLConnection() to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by PYODBCSQLConnection(), TURBODBCSQLConnection() or SparkConnection().</p>"},{"location":"sdk/examples/query/Circular-Average/#parameters","title":"Parameters","text":"Name Type Description tag_names list List of tagname or tagnames [\"tag_1\", \"tag_2\"] start_date str Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) end_date str End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) time_interval_rate str The time interval rate (numeric input) time_interval_unit str The time interval unit (second, minute, day, hour) lower_bound int Lower boundary for the sample range upper_bound int Upper boundary for the sample range include_bad_data bool Include \"Bad\" data points with True or remove \"Bad\" data points with False"},{"location":"sdk/examples/query/Circular-Average/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .circular_average(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n        time_interval_rate=\"15\",\n        time_interval_unit=\"minute\",\n        lower_bound=\"0\",\n        upper_bound=\"360\",\n    )\n)\n\nprint(data)\n</code></pre>"},{"location":"sdk/examples/query/Circular-Standard-Deviation/","title":"Circular Standard Deviation","text":"<p>Circular Standard Deviation - A function that receives a dataframe of raw tag data and computes the circular standard deviation for samples assumed to be in the range, returning the results.</p>"},{"location":"sdk/examples/query/Circular-Standard-Deviation/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the RTDIP SDK as specified in the Getting Started section.</p> <p>This example is using DefaultAuth() and DatabricksSQLConnection() to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by PYODBCSQLConnection(), TURBODBCSQLConnection() or SparkConnection().</p>"},{"location":"sdk/examples/query/Circular-Standard-Deviation/#parameters","title":"Parameters","text":"Name Type Description tag_names list List of tagname or tagnames [\"tag_1\", \"tag_2\"] start_date str Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) end_date str End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) time_interval_rate str The time interval rate (numeric input) time_interval_unit str The time interval unit (second, minute, day, hour) lower_bound int Lower boundary for the sample range upper_bound int Upper boundary for the sample range include_bad_data bool Include \"Bad\" data points with True or remove \"Bad\" data points with False"},{"location":"sdk/examples/query/Circular-Standard-Deviation/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .circular_standard_deviation(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n        time_interval_rate=\"15\",\n        time_interval_unit=\"minute\",\n        lower_bound=\"0\",\n        upper_bound=\"360\",\n    )\n)\n\nprint(data)\n</code></pre>"},{"location":"sdk/examples/query/Interpolate/","title":"Interpolate","text":"<p>Interpolate - takes resampling one step further to estimate the values of unknown data points that fall between existing, known data points. In addition to the resampling parameters, interpolation also requires:</p> <p>Interpolation Method - Forward Fill, Backward Fill or Linear</p>"},{"location":"sdk/examples/query/Interpolate/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the RTDIP SDK as specified in the Getting Started section.</p> <p>This example is using DefaultAuth() and DatabricksSQLConnection() to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by PYODBCSQLConnection(), TURBODBCSQLConnection() or SparkConnection().</p>"},{"location":"sdk/examples/query/Interpolate/#parameters","title":"Parameters","text":"Name Type Description tag_names list List of tagname or tagnames [\"tag_1\", \"tag_2\"] start_date str Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) end_date str End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) time_interval_rate str The time interval rate (numeric input) time_interval_unit str The time interval unit (second, minute, day, hour) include_bad_data bool Include \"Bad\" data points with True or remove \"Bad\" data points with False"},{"location":"sdk/examples/query/Interpolate/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .interpolate(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n        time_interval_rate=\"15\",\n        time_interval_unit=\"minute\",\n    )\n)\n\nprint(data)\n</code></pre>"},{"location":"sdk/examples/query/Interpolation-at-Time/","title":"Interpolation at Time","text":"<p>Interpolation at Time - works out the linear interpolation at a specific time based on the points before and after. This is achieved by providing the following parameter:</p> <p>Timestamps - A list of timestamp or timestamps</p>"},{"location":"sdk/examples/query/Interpolation-at-Time/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the RTDIP SDK as specified in the Getting Started section.</p> <p>This example is using DefaultAuth() and DatabricksSQLConnection() to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by PYODBCSQLConnection(), TURBODBCSQLConnection() or SparkConnection().</p>"},{"location":"sdk/examples/query/Interpolation-at-Time/#parameters","title":"Parameters","text":"Name Type Description tag_names str List of tagname or tagnames [\"tag_1\", \"tag_2\"] timestamps list List of timestamp or timestamps in the format YYY-MM-DDTHH:MM:SS or YYY-MM-DDTHH:MM:SS+zz:zz where %z is the timezone. (Example +00:00 is the UTC timezone) window_length int Add longer window time in days for the start or end of specified date to cater for edge cases. include_bad_data bool Include \"Bad\" data points with True or remove \"Bad\" data points with False"},{"location":"sdk/examples/query/Interpolation-at-Time/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .interpolation_at_time(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        timestamp_filter=[\"2023-01-01T09:30:00\", \"2023-01-02T12:00:00\"],\n    )\n)\n\nprint(data)\n</code></pre>"},{"location":"sdk/examples/query/Metadata/","title":"Metadata","text":"<p>Metadata queries provide contextual information for time series measurements and include information such as names, descriptions and units of measure.</p>"},{"location":"sdk/examples/query/Metadata/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the RTDIP SDK as specified in the Getting Started section.</p> <p>This example is using DefaultAuth() and DatabricksSQLConnection() to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by PYODBCSQLConnection(), TURBODBCSQLConnection() or SparkConnection().</p>"},{"location":"sdk/examples/query/Metadata/#parameters","title":"Parameters","text":"Name Type Description tag_names (optional, list) Either pass a list of tagname/tagnames [\"tag_1\", \"tag_2\"] or leave the list blank [] or leave the parameter out completely"},{"location":"sdk/examples/query/Metadata/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .metadata(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n    )\n)\n\nprint(data)\n</code></pre>"},{"location":"sdk/examples/query/Plot/","title":"Plot","text":"<p>Plot enables changing the frequency of time series observations with aggregations for Average, Min, Max, First Last and Standard Deviation. This is achieved by providing the following parameters:</p> <p>Sample Rate - (deprecated) Sample Unit - (deprecated) Time Interval Rate - The time interval rate Time Interval Unit - The time interval unit (second, minute, day, hour)</p>"},{"location":"sdk/examples/query/Plot/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the RTDIP SDK as specified in the Getting Started section.</p> <p>This example is using DefaultAuth() and DatabricksSQLConnection() to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by PYODBCSQLConnection(), TURBODBCSQLConnection() or SparkConnection().</p>"},{"location":"sdk/examples/query/Plot/#parameters","title":"Parameters","text":"Name Type Description tag_names list List of tagname or tagnames [\"tag_1\", \"tag_2\"] start_date str Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) end_date str End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) time_interval_rate str The time interval rate (numeric input) time_interval_unit str The time interval unit (second, minute, day, hour) include_bad_data bool Include \"Bad\" data points with True or remove \"Bad\" data points with False"},{"location":"sdk/examples/query/Plot/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .plot(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n        time_interval_rate=\"15\",\n        time_interval_unit=\"minute\",\n    )\n)\n\nprint(data)\n</code></pre>"},{"location":"sdk/examples/query/Raw/","title":"Raw","text":"<p>Raw facilitates performing raw extracts of time series data, typically filtered by a Tag Name or Device Name and an event time.</p>"},{"location":"sdk/examples/query/Raw/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the RTDIP SDK as specified in the Getting Started section.</p> <p>This example is using DefaultAuth() and DatabricksSQLConnection() to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by PYODBCSQLConnection(), TURBODBCSQLConnection() or SparkConnection().</p>"},{"location":"sdk/examples/query/Raw/#parameters","title":"Parameters","text":"Name Type Description tag_names list List of tagname or tagnames [\"tag_1\", \"tag_2\"] start_date str Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) end_date str End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) include_bad_data bool Include \"Bad\" data points with True or remove \"Bad\" data points with False"},{"location":"sdk/examples/query/Raw/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .raw(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n        include_bad_data=True,\n    )\n)\n\nprint(data)\n</code></pre>"},{"location":"sdk/examples/query/Resample/","title":"Resample","text":"<p>Resample enables changing the frequency of time series observations. This is achieved by providing the following parameters:</p> <p>Sample Rate - (deprecated) Sample Unit - (deprecated) Time Interval Rate - The time interval rate Time Interval Unit - The time interval unit (second, minute, day, hour) Aggregation Method - Aggregations including first, last, avg, min, max</p>"},{"location":"sdk/examples/query/Resample/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the RTDIP SDK as specified in the Getting Started section.</p> <p>This example is using DefaultAuth() and DatabricksSQLConnection() to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by PYODBCSQLConnection(), TURBODBCSQLConnection() or SparkConnection().</p>"},{"location":"sdk/examples/query/Resample/#parameters","title":"Parameters","text":"Name Type Description tag_names list List of tagname or tagnames [\"tag_1\", \"tag_2\"] start_date str Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) end_date str End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) time_interval_rate str The time interval rate (numeric input) time_interval_unit str The time interval unit (second, minute, day, hour) agg_method str Aggregation Method (first, last, avg, min, max) include_bad_data bool Include \"Bad\" data points with True or remove \"Bad\" data points with False"},{"location":"sdk/examples/query/Resample/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .resample(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n        time_interval_rate=\"15\",\n        time_interval_unit=\"minute\",\n        agg_method=\"first\",\n    )\n)\n\nprint(data)\n</code></pre>"},{"location":"sdk/examples/query/Summary/","title":"Summary","text":"<p>Summary facilitates performing a summary of statisics of time series data, typically filtered by a Tag Name or Device Name and an event time.</p>"},{"location":"sdk/examples/query/Summary/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the RTDIP SDK as specified in the Getting Started section.</p> <p>This example is using DefaultAuth() and DatabricksSQLConnection() to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by PYODBCSQLConnection(), TURBODBCSQLConnection() or SparkConnection().</p>"},{"location":"sdk/examples/query/Summary/#parameters","title":"Parameters","text":"Name Type Description tag_names list List of tagname or tagnames [\"tag_1\", \"tag_2\"] start_date str Start date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) end_date str End date (Either a date in the format YY-MM-DD or a datetime in the format YYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) include_bad_data bool Include \"Bad\" data points with True or remove \"Bad\" data points with False"},{"location":"sdk/examples/query/Summary/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .summary(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n    )\n)\n\nprint(data)\n</code></pre>"},{"location":"sdk/examples/query/Time-Weighted-Average/","title":"Time Weighted Average","text":"<p>Time Weighted Averages provide an unbiased average when working with irregularly sampled data. The RTDIP SDK requires the following parameters to perform time weighted average queries:</p> <p>Window Size Mins - (deprecated) Time Interval Rate - The time interval rate Time Interval Unit - The time interval unit (second, minute, day, hour) Window Length - Adds a longer window time for the start or end of specified date to cater for edge cases Step - Data points with step \"enabled\" or \"disabled\". The options for step are \"true\", \"false\" or \"metadata\" as string types. For \"metadata\", the query requires that the TagName has a step column configured correctly in the meta data table</p>"},{"location":"sdk/examples/query/Time-Weighted-Average/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the RTDIP SDK as specified in the Getting Started section.</p> <p>This example is using DefaultAuth() and DatabricksSQLConnection() to authenticate and connect. You can find other ways to authenticate here. The alternative built in connection methods are either by PYODBCSQLConnection(), TURBODBCSQLConnection() or SparkConnection().</p>"},{"location":"sdk/examples/query/Time-Weighted-Average/#parameters","title":"Parameters","text":"Name Type Description tag_names list List of tagname or tagnames [\"tag_1\", \"tag_2\"] start_date str Start date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) end_date str End date (Either a utc date in the format YYYY-MM-DD or a utc datetime in the format YYYY-MM-DDTHH:MM:SS or specify the timezone offset in the format YYYY-MM-DDTHH:MM:SS+zz:zz) time_interval_rate str The time interval rate (numeric input) time_interval_unit str The time interval unit (second, minute, day, hour) window_length int Add longer window time in days for the start or end of specified date to cater for edge cases include_bad_data bool Include \"Bad\" data points with True or remove \"Bad\" data points with False step str Data points with step \"enabled\" or \"disabled\". The options for step is defaulted to \"metadata\" which will retrieve the step value from the metadata table or step can be specified as \"true\" or \"false\""},{"location":"sdk/examples/query/Time-Weighted-Average/#example","title":"Example","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .time_weighted_average(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n        time_interval_rate=\"15\",\n        time_interval_unit=\"minute\",\n        step=\"true\",\n    )\n)\n\nprint(data)\n</code></pre>"},{"location":"sdk/pipelines/components/","title":"Pipeline Components","text":""},{"location":"sdk/pipelines/components/#overview","title":"Overview","text":"<p>The Real Time Data Ingestion Pipeline Framework supports the following component types:</p> <ul> <li>Sources - connectors to source systems</li> <li>Transformers - perform transformations on data, including data cleansing, data enrichment, data aggregation, data masking, data encryption, data decryption, data validation, data conversion, data normalization, data de-normalization, data partitioning etc</li> <li>Destinations - connectors to sink/destination systems </li> <li>Utilities - components that perform utility functions such as logging, error handling, data object creation, authentication, maintenance etc</li> <li>Secrets - components that facilitate accessing secret stores where sensitive information is stored such as passwords, connectiong strings, keys etc</li> </ul>"},{"location":"sdk/pipelines/components/#component-types","title":"Component Types","text":"Python Apache Spark Databricks <p>Component Types determine system requirements to execute the component:</p> <ul> <li>Python - components that are written in python and can be executed on a python runtime</li> <li>Pyspark - components that are written in pyspark can be executed on an open source Apache Spark runtime</li> <li>Databricks - components that require a Databricks runtime</li> </ul> <p>Note</p> <p>RTDIP are continuously adding more to this list. For detailed information on timelines, read this blog post and check back on this page regularly.</p>"},{"location":"sdk/pipelines/components/#sources","title":"Sources","text":"<p>Sources are components that connect to source systems and extract data from them. These will typically be real time data sources, but also support batch components as these are still important and necessary data souces of time series data in a number of circumstances in the real world.</p> Source Type Python Apache Spark Databricks Azure AWS Delta Delta Sharing Autoloader Eventhub Eventhub Kafka IoT Hub Kafka Kinesis MISO Daily Load ISO MISO Historical Load ISO PJM Daily Load ISO PJM Historical Load ISO CAISO Daily Load ISO CAISO Historical Load ISO ERCOT Daily Load ISO Weather Forecast API V1 Weather Forecast API V1 Multi ECMWF MARS Weather Forecast MFFBAS API ENTSO-E API <p>Note</p> <p>This list will dynamically change as the framework is further developed and new components are added.</p>"},{"location":"sdk/pipelines/components/#transformers","title":"Transformers","text":"<p>Transformers are components that perform transformations on data. These will target certain data models and common transformations that sources or destination components require to be performed on data before it can be ingested or consumed.</p> Transformer Type Python Apache Spark Databricks Azure AWS Binary To String OPC Publisher OPCUA Json To Process Control Data Model OPC Publisher OPCAE Json To Process Control Data Model Fledge OPCUA Json To Process Control Data Model EdgeX OPCUA Json To Process Control Data Model SSIP PI Binary Files To Process Control Data Model SSIP PI Binary JSON To Process Control Data Model SEM Json To Process Control Data Model Honeywell APM Json To Process Control Data Model Process Control Data Model To Honeywell APM Json Mirico Json To Process Control Data Model Pandas to PySpark DataFrame Conversion PySpark to Pandas DataFrame Conversion MISO To Meters Data Model Raw Forecast to Weather Data Model PJM To Meters Data Model ERCOT To Meters Data Model CAISO To Meters Data Model ECMWF NC Forecast Extract Point To Weather Data Model ECMWF NC Forecast Extract Grid To Weather Data Model <p>Note</p> <p>This list will dynamically change as the framework is further developed and new components are added.</p>"},{"location":"sdk/pipelines/components/#destinations","title":"Destinations","text":"<p>Destinations are components that connect to sink/destination systems and write data to them. </p> Destination Type Python Apache Spark Databricks Azure AWS Delta Delta Merge Eventhub Kakfa Eventhub Kakfa Kinesis Rest API Process Control Data Model To Delta Process Control Data Model Latest Values To Delta EVM <p>Note</p> <p>This list will dynamically change as the framework is further developed and new components are added.</p>"},{"location":"sdk/pipelines/components/#utilities","title":"Utilities","text":"<p>Utilities are components that perform utility functions such as logging, error handling, data object creation, authentication, maintenance and are normally components that can be executed as part of a pipeline or standalone.</p> Utility Type Python Apache Spark Databricks Azure AWS Spark Session Spark Configuration Delta Table Create Delta Table Optimize Delta Table Vacuum AWS S3 Bucket Policy AWS S3 Copy ADLS Gen 2 ACLs Azure Autoloader Resources Spark ADLS Gen 2 Service Principal Connect <p>Note</p> <p>This list will dynamically change as the framework is further developed and new components are added.</p>"},{"location":"sdk/pipelines/components/#secrets","title":"Secrets","text":"<p>Secrets are components that perform functions to interact with secret stores to manage sensitive information such as passwords, keys and certificates.</p> Secret Type Python Apache Spark Databricks Azure AWS Databricks Secret Scopes Hashicorp Vault Azure Key Vault <p>Note</p> <p>This list will dynamically change as the framework is further developed and new components are added.</p>"},{"location":"sdk/pipelines/components/#conclusion","title":"Conclusion","text":"<p>Components can be used to build RTDIP Pipelines which is described in more detail here.</p>"},{"location":"sdk/pipelines/framework/","title":"RTDIP Ingestion Pipeline Framework","text":"<p>RTDIP has been built to simplify ingesting and querying time series data. The RTDIP Ingestion Pipeline Framework creates streaming and batch ingestion pipelines according to requirements of the source of the data and needs of the data consumer. RTDIP Pipelines focuses on the ingestion of data into the platform.</p>"},{"location":"sdk/pipelines/framework/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have followed the installation instructions as specified in the Getting Started section and follow the steps which highlight the installation requirements for Pipelines. In particular:</p> <ol> <li>RTDIP SDK Installation</li> <li>Java - If your pipeline steps utilize pyspark then Java must be installed.</li> </ol> <p>RTDIP SDK installation</p> <p>Ensure you have installed the RTDIP SDK, as a minimum, as follows: <pre><code>pip install \"rtdip-sdk[pipelines]\"\n</code></pre></p> <p>For all installation options please see the RTDIP SDK installation instructions.</p>"},{"location":"sdk/pipelines/framework/#overview","title":"Overview","text":"<p>The goal of the RTDIP Ingestion Pipeline framework is to:</p> <ol> <li>Support python and pyspark to build pipeline components</li> <li>Enable execution of sources, transformers, destinations and utilities components in a framework that can execute them in a defined order</li> <li>Create modular components that can be leveraged as a step in a pipeline task using Object Oriented Programming techniques included Interfaces and Implementations per component type</li> <li>Deploy pipelines to popular orchestration engines</li> <li>Ensure pipelines can be constructed and executed using the RTDIP SDK and rest APIs</li> </ol>"},{"location":"sdk/pipelines/framework/#jobs","title":"Jobs","text":"<p>The RTDIP Data Ingestion Pipeline Framework follow sthe typical convention of a job that users will be familiar with if they have used orchestration engines such as Apache Airflow or Databricks Workflows.</p> <p>A pipline job consists of the following components:</p> <pre><code>erDiagram\n  JOB ||--|{ TASK : contains\n  TASK ||--|{ STEP : contains\n  JOB {\n    string name\n    string description\n    list task_list\n  }\n  TASK {\n    string name\n    string description\n    string depends_on_task\n    list step_list\n    bool batch_task\n  }\n  STEP {\n    string name\n    string description\n    list depends_on_step\n    list provides_output_to_step\n    class component\n    dict component_parameters\n  }</code></pre> <p>As per the above, a pipeline job consists of a list of tasks. Each task consists of a list of steps. Each step consists of a component and a set of parameters that are passed to the component. Dependency Injection will ensure that each component is instantiated with the correct parameters. </p> <p>More Information about Pipeline Jobs can be found here.</p>"},{"location":"sdk/pipelines/framework/#runtime-environments","title":"Runtime Environments","text":"Python Apache Spark Databricks Delta Live Tables <p>Note</p> <p>RTDIP are continuously adding more to this list. For detailed information on timelines, read this blog post and check back on this page regularly.</p> <p>Pipelines can run in multiple environment types. These include:</p> <ul> <li>Python: Components written in python and executed on a python runtime</li> <li>Pyspark: Components written in pyspark and executed on an open source Apache Spark runtime</li> <li>Databricks: Components written in pyspark and executed on a Databricks runtime</li> <li>Delta Live Tables: Components written in pyspark and executed on a Databricks Delta Live Tables runtime</li> </ul> <p>Runtimes will take precedence depending on the list of components in a pipeline task.</p> <ul> <li>Pipelines with at least one Databricks or DLT component will be executed in a Databricks environment</li> <li>Pipelines with at least one Pyspark component will be executed in a Pyspark environment</li> <li>Pipelines with only Python components will be executed in a Python environment</li> </ul>"},{"location":"sdk/pipelines/framework/#conclusion","title":"Conclusion","text":"<p>Find out more about the components that can be used by the RTDIP Ingestion Pipeline Framework here.</p>"},{"location":"sdk/pipelines/jobs/","title":"Jobs","text":"<p>In a production environment, pipelines will be run as jobs that are either batch jobs executed on a schedule or a streaming job executed to be run continuously. </p>"},{"location":"sdk/pipelines/jobs/#build-a-pipeline","title":"Build a Pipeline","text":""},{"location":"sdk/pipelines/jobs/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have followed the installation instructions as specified in the Getting Started section and follow the steps which highlight the installation requirements for Pipelines. In particular:</p> <ol> <li>RTDIP SDK Installation</li> <li>Java - If your pipeline steps utilize pyspark then Java must be installed.</li> </ol> <p>RTDIP SDK installation</p> <p>Ensure you have installed the RTDIP SDK, as a minimum, as follows: <pre><code>pip install \"rtdip-sdk[pipelines]\"\n</code></pre></p> <p>For all installation options please see the RTDIP SDK installation instructions.</p>"},{"location":"sdk/pipelines/jobs/#import","title":"Import","text":"<p>Import the required components of a Pipeline Job.</p> <pre><code>from rtdip_sdk.pipelines.execute import PipelineJob, PipelineStep, PipelineTask\nfrom rtdip_sdk.pipelines.sources import SparkEventhubSource\nfrom rtdip_sdk.pipelines.transformers import BinaryToStringTransformer\nfrom rtdip_sdk.pipelines.destinations import SparkDeltaDestination\nfrom rtdip_sdk.pipelines.secrets import PipelineSecret, DatabricksSecrets\nimport json\n</code></pre>"},{"location":"sdk/pipelines/jobs/#steps","title":"Steps","text":"<p>Pipeline steps are constructed from components and added to a Pipeline task as a list. Each component is created as a <code>PipelineStep</code> and populated with the following information.</p> Parameter Description Requirements Name Each component requires a unique name that also facilitates dependencies between each component Contains only letters, numbers and underscores Description A brief description of each component Will populate certain components of a runtime such as Delta Live Tables Component The component Class Populate with the Class Name Component Parameters Configures the component with specific information, such as connection information and component specific settings Use Pipeline Secrets for sensitive Information Depends On Step Specifies any component names that must be executed prior to this component A python list of component names Provides Output To Step Specifies any component names that require this component's output as an input A python list of component names <pre><code>step_list = []\n\n# read step\neventhub_configuration = {\n    \"eventhubs.connectionString\": PipelineSecret(type=DatabricksSecrets, vault=\"test_vault\", key=\"test_key\"),\n    \"eventhubs.consumerGroup\": \"$Default\",\n    \"eventhubs.startingPosition\": json.dumps({\"offset\": \"0\", \"seqNo\": -1, \"enqueuedTime\": None, \"isInclusive\": True})\n}    \nstep_list.append(PipelineStep(\n    name=\"test_step1\",\n    description=\"test_step1\",\n    component=SparkEventhubSource,\n    component_parameters={\"options\": eventhub_configuration},\n    provide_output_to_step=[\"test_step2\"]\n))\n\n# transform step\nstep_list.append(PipelineStep(\n    name=\"test_step2\",\n    description=\"test_step2\",\n    component=BinaryToStringTransformer,\n    component_parameters={\n        \"source_column_name\": \"body\",\n        \"target_column_name\": \"body\"\n    },\n    depends_on_step=[\"test_step1\"],\n    provide_output_to_step=[\"test_step3\"]\n))\n\n# write step\nstep_list.append(PipelineStep(\n    name=\"test_step3\",\n    description=\"test_step3\",\n    component=SparkDeltaDestination,\n    component_parameters={\n        \"destination\": \"test_table\",\n        \"options\": {},\n        \"mode\": \"overwrite\"    \n    },\n    depends_on_step=[\"test_step2\"]\n))\n</code></pre>"},{"location":"sdk/pipelines/jobs/#tasks","title":"Tasks","text":"<p>Tasks contain a list of steps. Each task is created as a <code>PipelineTask</code> and populated with the following information.</p> Parameter Description Requirements Name Each task requires a unique name Contains only letters, numbers and underscores Description A brief description of the task Will populate certain components of a runtime such as Delta Live Tables Step List A python list of steps that are to be executed by the task A list of step names that contain only letters, numbers and underscores Batch Task The task should be executed as a batch task Optional, defaults to False <pre><code>task = PipelineTask(\n    name=\"test_task\",\n    description=\"test_task\",\n    step_list=step_list,\n    batch_task=True\n)\n</code></pre>"},{"location":"sdk/pipelines/jobs/#jobs_1","title":"Jobs","text":"<p>Jobs contain a list of tasks. A job is created as a <code>PipelineJob</code> and populated with the following information.</p> Parameter Description Requirements Name The Job requires a unique name Contains only letters, numbers and underscores Description A brief description of the job Will populate certain components of a runtime such as Delta Live Tables Version Enables version control of the task for certain environments Follow semantic versioning Task List A python list of tasks that are to be executed by the  job A list of task names that contain only letters, numbers and underscores <pre><code>pipeline_job = PipelineJob(\n    name=\"test_job\",\n    description=\"test_job\", \n    version=\"0.0.1\",\n    task_list=[task]\n)\n</code></pre>"},{"location":"sdk/pipelines/jobs/#execute","title":"Execute","text":"<p>Pipeline Jobs can be executed directly if the run environment where the code has been written facilitates it. To do so, the above Pipeline Job can be executed as follows:</p> <p>Pyspark Installation</p> <p>Ensure you have Java installed in your environment and you have installed pyspark using the below command: <pre><code>pip install \"rtdip-sdk[pipelines,pyspark]\"\n</code></pre></p> <pre><code>from rtdip_sdk.pipelines.execute import PipelineJobExecute\n\npipeline = PipelineJobExecute(pipeline_job)\n\nresult = pipeline.run()\n</code></pre>"},{"location":"sdk/pipelines/jobs/#conclusion","title":"Conclusion","text":"<p>The above sets out how a Pipeline Job can be constructed and executed. Most pipelines, however, will be exevcuted by orchestration engines. See the Deploy section for more information above how Pipeline Jobs can be deployed and executed in this way.</p>"},{"location":"sdk/pipelines/deploy/apache-airflow/","title":"Apache Airflow","text":""},{"location":"sdk/pipelines/deploy/apache-airflow/#databricks-provider","title":"Databricks Provider","text":"<p>Apache Airflow can orchestrate an RTDIP Pipeline that has been deployed as a Databricks Job. For further information on how to deploy an RTDIP Pipeline as a Databricks Job, please see here. </p> <p>Databricks has also provided more information about running Databricks jobs from Apache Airflow here.</p>"},{"location":"sdk/pipelines/deploy/apache-airflow/#prerequisites","title":"Prerequisites","text":"<ol> <li>An Apache Airflow instance must be running.</li> <li>Authentication between Apache Airflow and Databricks must be configured.</li> <li>The python packages <code>apache-airflow</code> and <code>apache-airflow-providers-databricks</code> must be installed.</li> <li>You have created an RTDIP Pipeline and deployed it to Databricks.</li> </ol>"},{"location":"sdk/pipelines/deploy/apache-airflow/#example","title":"Example","text":"<p>The <code>JOB ID</code> in the example below can be obtained from the Databricks Job.</p> <pre><code>from airflow import DAG\nfrom airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator\nfrom airflow.utils.dates import days_ago\n\ndefault_args = {\n  'owner': 'airflow'\n}\n\nwith DAG('databricks_dag',\n  start_date = days_ago(2),\n  schedule_interval = None,\n  default_args = default_args\n  ) as dag:\n\n  opr_run_now = DatabricksRunNowOperator(\n    task_id = 'run_now',\n    databricks_conn_id = 'databricks_default',\n    job_id = JOB_ID\n  )\n</code></pre>"},{"location":"sdk/pipelines/deploy/databricks/","title":"Databricks Workflows","text":"<p>Deploying to Databricks is simplified using the RTDIP SDK as this method of deployment will handle the setup of the libraries and spark configuration directly from the components being used in your pipeline.</p>"},{"location":"sdk/pipelines/deploy/databricks/#prerequisites","title":"Prerequisites","text":"<ul> <li>This deployment method expects to deploy a local file to Databricks Workflows</li> </ul>"},{"location":"sdk/pipelines/deploy/databricks/#import","title":"Import","text":"<pre><code>from rtdip_sdk.pipelines.deploy import DatabricksSDKDeploy, CreateJob, JobCluster, ClusterSpec, Task, NotebookTask, ComputeSpecKind, AutoScale, RuntimeEngine, DataSecurityMode\n</code></pre>"},{"location":"sdk/pipelines/deploy/databricks/#authentication","title":"Authentication","text":"Azure Active DirectoryDatabricks <p>Refer to the Azure Active Directory documentation for further options to perform Azure AD authentication, such as Service Principal authentication using certificates or secrets. Below is an example of performing default authentication that retrieves a token for Azure Databricks. </p> <p>Also refer to the Code Reference for further technical information.</p> <pre><code>from rtdip_sdk.authentication import authenticate as auth\n\nauthentication = auth.DefaultAuth().authenticate()\naccess_token = authentication.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n</code></pre> <p>Note</p> <p>If you are experiencing any trouble authenticating please see Troubleshooting - Authentication</p> <p>Refer to the Databricks documentation for further information about generating a Databricks PAT Token. Below is an example of performing default authentication that retrieves a token for a Databricks Workspace. </p> <p>Provide your <code>dbapi.....</code> token to the <code>access_token</code> in the examples below.</p> <pre><code>access_token = \"dbapi..........\"\n</code></pre>"},{"location":"sdk/pipelines/deploy/databricks/#deploy","title":"Deploy","text":"<p>Deployments to Databricks are done using the Databricks SDK. The Databricks SDK enables users to control exactly how they deploy their RTDIP Pipelines to Databricks.</p> <p>Any of the Classes below can be imported from the following location:</p> <pre><code>from rtdip_sdk.pipelines.deploy import *\n</code></pre> <p>Parameters for a Databricks Job can be managed using the following Classes:</p> Class Description ClusterSpec Provides Parameters for setting up a Databricks Cluster JobCluster Sets up a Jobs Cluster as defined by the provided <code>DatabricksCluster</code> Task Defines the setup of the Task at the Databricks Task level including Task specific Clusters, Libraries, Schedules, Notifications and Timeouts CreateJob Defines the setup at the Job level including Clusters, Libraries, Schedules, Notifications, Access Controls, Timeouts and Tags NotebookTask Provides the Notebook information to the <code>Task</code> DatabricksSDKDeploy Leverages the Databricks SDK to deploy the job to Databricks Workflows <p>Note</p> <p>All classes for deployment are available from the Databricks SDK and can be accessed using <code>from rtdip_sdk.pipelines.deploy import</code> and choosing the classes you need for your Databricks deployment</p>"},{"location":"sdk/pipelines/deploy/databricks/#example","title":"Example","text":"<p>A simple example of deploying an RTDIP Pipeline Job to an Azure Databricks Job is below.</p> <pre><code>databricks_host_name = \"{databricks-host-url}\" #Replace with your databricks workspace url\n\n# Setup a Cluster for the Databricks Job\ncluster_list = []\ncluster_list.append(JobCluster(\n    job_cluster_key=\"test_cluster\",\n    new_cluster=ClusterSpec(\n        node_type_id=\"Standard_E4ds_v5\",\n        autoscale=AutoScale(min_workers=1, max_workers=3),\n        spark_version=\"13.2.x-scala2.12\",\n        data_security_mode=DataSecurityMode.SINGLE_USER,\n        runtime_engine=RuntimeEngine.PHOTON\n    )\n))\n\n# Define a Notebook Task for the Databricks Job\ntask_list = []\ntask_list.append(Task(\n    task_key=\"test_task\",\n    job_cluster_key=\"test_cluster\",\n    notebook_task=NotebookTask(\n        notebook_path=\"/directory/to/pipeline.py\"\n    )\n))\n\n# Create a Databricks Job for the Task\njob = CreateJob(\n    name=\"test_job_rtdip\",\n    job_clusters=cluster_list,\n    tasks=task_list\n)\n\n# Deploy to Databricks\ndatabricks_job = DatabricksSDKDeploy(databricks_job=job, host=databricks_host_name, token=access_token)\n\ndeploy_result = databricks_job.deploy()\n</code></pre>"},{"location":"sdk/pipelines/deploy/databricks/#launch","title":"Launch","text":"<p>Once a job is deployed to Databricks, it can be executed immediately using the following code.</p> <pre><code># Run/Launch the Job in Databricks\nlaunch_result = databricks_job.launch()\n</code></pre>"},{"location":"sdk/pipelines/deploy/databricks/#stop","title":"Stop","text":"<p>A job that is running and is deployed to Databricks, can be cancelled using the following code.</p> <p>```python</p>"},{"location":"sdk/pipelines/deploy/databricks/#runlaunch-the-job-in-databricks","title":"Run/Launch the Job in Databricks","text":"<p>stop_result = databricks_job.stop()</p>"},{"location":"sdk/queries/connectors/","title":"Connectors","text":"<p>RTDIP SDK provides functionality to connect to and query its data using connectors. Below is a list of the available connectors.</p>"},{"location":"sdk/queries/connectors/#odbc","title":"ODBC","text":""},{"location":"sdk/queries/connectors/#databricks-sql-connector","title":"Databricks SQL Connector","text":"<p>Enables connectivity to Databricks using the Databricks SQL Connector which does not require any ODBC installation. </p> <p>For more information refer to this documentation and for the specific implementation within the RTDIP SDK, refer to this link.</p> <pre><code>from rtdip_sdk.connectors import DatabricksSQLConnection\n\nserver_hostname = \"server_hostname\"\nhttp_path = \"http_path\"\naccess_token = \"token\"\n\nconnection = DatabricksSQLConnection(server_hostname, http_path, access_token)\n</code></pre> <p>Replace server_hostname, http_path and access_token with your own information.</p>"},{"location":"sdk/queries/connectors/#pyodbc-sql-connector","title":"PYODBC SQL Connector","text":"<p>PYDOBC is a popular python package for querying data using ODBC. Refer to their documentation for more information about pyodbc, how to install it and how you can leverage it in your code.</p> <p>Warning</p> <p>The RTDIP SDK does not specify <code>pyodbc</code> as one of its package dependencies. It will need to be installed into your environment separately.</p> <p>View information about how pyodbc is implemented in the RTDIP SDK here.</p> <pre><code>from rtdip_sdk.connectors import PYODBCSQLConnection\n\nserver_hostname = \"server_hostname\"\nhttp_path = \"http_path\"\naccess_token = \"token\"\ndriver_path = \"/Library/simba/spark/lib/libsparkodbc_sbu.dylib\"\n\nconnection = PYODBCSQLConnection(driver_path, sever_hostname, http_path, access_token)\n</code></pre> <p>Replace server_hostname, http_path and access_token with your own information.</p>"},{"location":"sdk/queries/connectors/#turbodbc-sql-connector","title":"TURBODBC SQL Connector","text":"<p>Turbodbc is a powerful python ODBC package that has advanced options for querying performance. Find out more about installing it on your operation system and what Turbodbc can do here and refer to this documentation for more information about how it is implemented in the RTDIP SDK.</p> <p>Warning</p> <p>The RTDIP SDK does not specify <code>turbodbc</code> as one of its package dependencies. It will need to be installed into your environment separately.</p> <pre><code>from rtdip_sdk.connectors import TURBODBCSQLConnection\n\nserver_hostname = \"server_hostname\"\nhttp_path = \"http_path\"\naccess_token = \"token\"\n\nconnection = TURBODBCSQLConnection(server_hostname, http_path, access_token)\n</code></pre> <p>Replace server_hostname, http_path and access_token with your own information.</p>"},{"location":"sdk/queries/connectors/#spark","title":"Spark","text":""},{"location":"sdk/queries/connectors/#spark-connector","title":"Spark Connector","text":"<p>The Spark Connector enables querying of data using a Spark Session. This is useful for querying local instances of Spark or Delta. However, the most useful application of this connector is to leverage Spark Connect to enable connecting to a remote Spark Cluster to provide the compute for the query being run from a local machine.</p> <pre><code>from rtdip_sdk.connectors import SparkConnection\n\nspark_server = \"spark_server\"\naccess_token = \"my_token\"\n\nspark_remote = \"sc://{}:443;token={}\".format(spark_server, access_token)\nconnection = SparkConnection(spark_remote=spark_remote)\n</code></pre> <p>Replace the access_token with your own authentiction token.</p>"},{"location":"sdk/queries/functions/","title":"Functions","text":"<p>The RTDIP SDK enables users to perform complex queries, including aggregation on datasets within the Platform. Please find below the various types of queries available for specific dataset types. These SDK Functions are also supported by the RTDIP API Docker Image.</p>"},{"location":"sdk/queries/functions/#time-series-events","title":"Time Series Events","text":""},{"location":"sdk/queries/functions/#raw","title":"Raw","text":"<p>Raw facilitates performing raw extracts of time series data, typically filtered by a Tag Name or Device Name and an event time.</p>"},{"location":"sdk/queries/functions/#latest","title":"Latest","text":"<p>Latest queries provides the latest event values. The RTDIP SDK requires the following parameters to retrieve the latest event values: - TagNames - A list of tag names</p>"},{"location":"sdk/queries/functions/#resample","title":"Resample","text":"<p>Resample enables changing the frequency of time series observations. This is achieved by providing the following parameters:</p> <ul> <li>Time Interval Rate - The time interval rate</li> <li>Time Interval Unit - The time interval unit (second, minute, day, hour)</li> <li>Aggregation Method - Aggregations including first, last, avg, min, max</li> </ul>"},{"location":"sdk/queries/functions/#plot","title":"Plot","text":"<p>Plot enables changing the frequency of time series observations and performing Average, Min, Max, First, Last and StdDev aggregations. This is achieved by providing the following parameters:</p> <ul> <li>Time Interval Rate - The time interval rate</li> <li>Time Interval Unit - The time interval unit (second, minute, day, hour)</li> </ul>"},{"location":"sdk/queries/functions/#interpolate","title":"Interpolate","text":"<p>Interpolate - takes resampling one step further to estimate the values of unknown data points that fall between existing, known data points. In addition to the resampling parameters, interpolation also requires:</p> <ul> <li>Interpolation Method - Forward Fill, Backward Fill or Linear</li> </ul>"},{"location":"sdk/queries/functions/#interpolation-at-time","title":"Interpolation at Time","text":"<p>Interpolation at Time - works out the linear interpolation at a specific time based on the points before and after. This is achieved by providing the following parameter:</p> <ul> <li>Timestamps - A list of timestamp or timestamps</li> </ul>"},{"location":"sdk/queries/functions/#time-weighted-averages","title":"Time Weighted Averages","text":"<p>Time Weighted Averages provide an unbiased average when working with irregularly sampled data. The RTDIP SDK requires the following parameters to perform time weighted average queries:</p> <ul> <li>Time Interval Rate - The time interval rate</li> <li>Time Interval Unit - The time interval unit (second, minute, day, hour)</li> <li>Window Length - Adds a longer window time for the start or end of specified date to cater for edge cases</li> <li>Step - Data points with step \"enabled\" or \"disabled\". The options for step are \"true\", \"false\" or \"metadata\" as string types. For \"metadata\", the query requires that the TagName has a step column configured correctly in the meta data table</li> </ul>"},{"location":"sdk/queries/functions/#circular-averages","title":"Circular Averages","text":"<p>Circular Averages computes the circular average for samples in a range. The RTDIP SDK requires the following parameters to perform circular average queries:</p> <ul> <li>Time Interval Rate - The time interval rate</li> <li>Time Interval Unit - The time interval unit (second, minute, day, hour)</li> <li>Lower Bound - The lower boundary for the sample range</li> <li>Upper Bound - The upper boundary for the sample range</li> </ul>"},{"location":"sdk/queries/functions/#circular-standard-deviations","title":"Circular Standard Deviations","text":"<p>Circular Standard Deviations computes the circular standard deviations for samples assumed to be in the range. The RTDIP SDK requires the following parameters to perform circular average queries:</p> <ul> <li>Time Interval Rate - The time interval rate</li> <li>Time Interval Unit - The time interval unit (second, minute, day, hour)</li> <li>Lower Bound - The lower boundary for the sample range</li> <li>Upper Bound - The upper boundary for the sample range</li> </ul>"},{"location":"sdk/queries/functions/#summary","title":"Summary","text":"<p>Summary computes a summary of statistics (Avg, Min, Max, Count, StDev, Sum, Variance).</p>"},{"location":"sdk/queries/functions/#time-series-metadata","title":"Time Series Metadata","text":""},{"location":"sdk/queries/functions/#metadata","title":"Metadata","text":"<p>Metadata queries provide contextual information for time series measurements and include information such as names, descriptions and units of measure.</p> <p>Note</p> <p>RTDIP are continuously adding more to this list so check back regularly.</p>"},{"location":"sdk/queries/functions/#query-examples","title":"Query Examples","text":"<p>For examples of how to use the RTDIP functions, click the following links:</p> <ul> <li> <p>Raw</p> </li> <li> <p>Resample</p> </li> <li> <p>Interpolate</p> </li> <li> <p>Interpolation at Time</p> </li> <li> <p>Time Weighted Averages</p> </li> <li> <p>Circular Averages</p> </li> <li> <p>Circular Standard Deviations</p> </li> <li> <p>Metadata</p> </li> </ul>"},{"location":"sdk/queries/databricks/databricks-sql/","title":"Query Databricks SQL using the RTDIP SDK","text":"<p>This article provides a guide on how to use RTDIP SDK to query data via Databricks SQL. Before getting started, ensure you have installed the RTDIP Python Package and check the RTDIP Installation Page for all the required prerequisites.</p>"},{"location":"sdk/queries/databricks/databricks-sql/#how-to-use-rtdip-sdk-with-databricks-sql","title":"How to use RTDIP SDK with Databricks SQL","text":"<p>The RTDIP SDK has rich support of querying data using Databricks SQL, such as allowing the user to authenticate, connect and/or use the most commonly requested methods for manipulating time series data accessible via Databricks SQL.</p>"},{"location":"sdk/queries/databricks/databricks-sql/#authentication","title":"Authentication","text":"Azure Active DirectoryDatabricks <p>Refer to the Azure Active Directory documentation for further options to perform Azure AD authentication, such as Service Principal authentication using certificates or secrets. Below is an example of performing default authentication that retrieves a token for Azure Databricks. </p> <p>Also refer to the Code Reference for further technical information.</p> <pre><code>from rtdip_sdk.authentication import authenticate as auth\n\nauthentication = auth.DefaultAuth().authenticate()\naccess_token = authentication.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n</code></pre> <p>Note</p> <p>If you are experiencing any trouble authenticating please see Troubleshooting - Authentication</p> <p>Refer to the Databricks documentation for further information about generating a Databricks PAT Token. Below is an example of performing default authentication that retrieves a token for a Databricks Workspace. </p> <p>Provide your <code>dbapi.....</code> token to the <code>access_token</code> in the examples below.</p> <pre><code>access_token = \"dbapi..........\"\n</code></pre>"},{"location":"sdk/queries/databricks/databricks-sql/#connect-to-databricks-sql","title":"Connect to Databricks SQL","text":"<p>The RTDIP SDK offers several ways to connect to a Databricks SQL Warehouse.</p> Databricks SQL ConnectorPYODBCTURBODBC <p>The simplest method to connect to RTDIP and does not require any additional installation steps.</p> <pre><code>from rtdip_sdk.connectors import DatabricksSQLConnection\n\nserver_hostname = \"server_hostname\"\nhttp_path = \"http_path\"\naccess_token = \"token\"\n\nconnection = DatabricksSQLConnection(server_hostname, http_path, access_token)\n</code></pre> <p>Replace server_hostname, http_path and access_token with your own information.</p> <p>For more information about each of the connection methods, please see Code Reference and navigate to the required section.</p> <p>A popular library that python developers use for ODBC connectivity but requires more setup steps.</p> <p>ODBC or JDBC are required to leverage PYODBC. Follow these instructions to install the drivers in your environment.</p> <ul> <li> <p>Microsoft Visual C++ 14.0 or greater is required. Get it from Microsoft C++ Build Tools</p> </li> <li> <p>Driver paths can be found on PYODBC Driver Paths</p> </li> </ul> <pre><code>from rtdip_sdk.connectors import PYODBCSQLConnection\n\nserver_hostname = \"server_hostname\"\nhttp_path = \"http_path\"\naccess_token = \"token\"\ndriver_path = \"/Library/simba/spark/lib/libsparkodbc_sbu.dylib\"\n\nconnection = PYODBCSQLConnection(driver_path, sever_hostname, http_path, access_token)\n</code></pre> <p>Replace server_hostname, http_path and access_token with your own information.</p> <p>For more information about each of the connection methods, please see Code Reference and navigate to the required section.</p> <p>The RTDIP development team have found this to be the most performant method of connecting to RTDIP leveraging the arrow implementation within Turbodbc to obtain data, but requires a number of additional installation steps to get working on OSX, Linux and Windows</p> <ul> <li>ODBC or JDBC are required to leverage TURBODBC. Follow these instructions to install the drivers in your environment.</li> <li>Boost needs to be installed locally to use the TURBODBC SQL Connector (Optional)</li> </ul> <pre><code>from rtdip_sdk.connectors import TURBODBCSQLConnection\n\nserver_hostname = \"server_hostname\"\nhttp_path = \"http_path\"\naccess_token = \"token\"\n\nconnection = TURBODBCSQLConnection(server_hostname, http_path, access_token)\n</code></pre> <p>Replace server_hostname, http_path and access_token with your own information.</p> <p>For more information about each of the connection methods, please see Code Reference and navigate to the required section.</p>"},{"location":"sdk/queries/databricks/databricks-sql/#functions","title":"Functions","text":"<p>Finally, after authenticating and connecting using one of the methods above, you have access to the commonly requested RTDIP functions such as Resample, Interpolate, Raw, Time Weighted Averages or Metadata. </p> <p>1. To use any of the RTDIP functions, use the commands below.</p> <pre><code>from rtdip_sdk.queries import resample\nfrom rtdip_sdk.queries import interpolate\nfrom rtdip_sdk.queries import raw\nfrom rtdip_sdk.queries import time_weighted_average\nfrom rtdip_sdk.queries import metadata\n</code></pre> <p>2. From functions you can use any of the following methods.</p>"},{"location":"sdk/queries/databricks/databricks-sql/#resample","title":"Resample","text":"<pre><code>resample.get(connection, parameters_dict)\n</code></pre>"},{"location":"sdk/queries/databricks/databricks-sql/#interpolate","title":"Interpolate","text":"<pre><code>interpolate.get(connection, parameters_dict)\n</code></pre>"},{"location":"sdk/queries/databricks/databricks-sql/#raw","title":"Raw","text":"<pre><code>raw.get(connection, parameters_dict)\n</code></pre>"},{"location":"sdk/queries/databricks/databricks-sql/#time-weighted-average","title":"Time Weighted Average","text":"<pre><code>time_weighted_average.get(connection, parameter_dict)\n</code></pre>"},{"location":"sdk/queries/databricks/databricks-sql/#metadata","title":"Metadata","text":"<pre><code>metadata.get(connection, parameter_dict)\n</code></pre> <p>For more information about the function parameters see Code Reference and navigate through the required function.</p>"},{"location":"sdk/queries/databricks/databricks-sql/#example","title":"Example","text":"<p>This is a code example of the RTDIP SDK Interpolate function. You will need to replace the parameters with your own requirements and details. If you are unsure on the options please see Code Reference - Interpolate and navigate to the attributes section. </p> <pre><code>from rtdip_sdk.authentication import authenticate as auth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import interpolate\n\nauthentication = auth.DefaultAuth().authenticate()\naccess_token = authentication.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", access_token)\n\nparameters = {\n    \"business_unit\": \"{business_unit}\", \n    \"region\": \"{region}\",\n    \"asset\": \"{asset}\", \n    \"data_security_level\": \"{date_security_level}\",\n    \"data_type\": \"{data_type}\", #options are float, integer, string and double (the majority of data is float)\n    \"tag_names\": [\"{tag_name_1}, {tag_name_2}\"],\n    \"start_date\": \"2022-03-08\", #start_date can be a date in the format \"YYYY-MM-DD\" or a datetime in the format \"YYYY-MM-DDTHH:MM:SS\"\n    \"end_date\": \"2022-03-10\", #end_date can be a date in the format \"YYYY-MM-DD\" or a datetime in the format \"YYYY-MM-DDTHH:MM:SS\"\n    \"time_interval_rate\": \"1\", #numeric input\n    \"time_interval_unit\": \"hour\", #options are second, minute, day or hour\n    \"include_bad_data\": True #boolean options are True or False\n}\n\nresult = interpolate.get(connection, parameters)\nprint(result)\n</code></pre> <p>Note</p> <p>If you are having problems please see Troubleshooting for more information.</p>"},{"location":"sdk/queries/databricks/databricks-sql/#conclusion","title":"Conclusion","text":"<p>Congratulations! You have now learnt how to use the RTDIP SDK. Please check back for regular updates and if you would like to contribute, you can open an issue on GitHub. See the Contributing Guide for more help.</p>"},{"location":"sdk/queries/databricks/sql-warehouses/","title":"SQL Warehouses","text":"<p>In order to connect to the data using the RTDIP SDK you will require Databricks SQL Warehouse information. Retrieve this information from your Databricks Workspace by following the steps below:</p> <ol> <li>Login to your Databricks Workspace</li> <li>Switch to the SQL Option in the Workspace</li> <li>Select the SQL Warehouse</li> <li>Click on the Details tab</li> <li>Copy the Host Name and HTTP Path details</li> </ol>"},{"location":"sdk/queries/databricks/troubleshooting/","title":"Troubleshooting","text":""},{"location":"sdk/queries/databricks/troubleshooting/#cannot-install-pyodbc","title":"Cannot install pyodbc","text":"<p>Microsoft Visual C++ 14.0 or greater is required to install pyodbc. Get it with Microsoft C++ Build Tools</p>"},{"location":"sdk/queries/databricks/troubleshooting/#cannot-build-wheels-using-legacy-setuppy","title":"Cannot build wheels (Using legacy setup.py)","text":"<p>To install rtdip-sdk using setup.py, you need to have wheel installed using the following command:</p> <pre><code>pip install wheel\n</code></pre>"},{"location":"sdk/queries/databricks/troubleshooting/#authentication","title":"Authentication","text":"<p>For Default Credential authentication, a number of troubleshooting options are available here.</p> <p>For Visual Studio Code errors, the version of Azure Account extension is installed(0.9.11) - To authenticate in Visual Studio Code, ensure version 0.9.11 or earlier of the Azure Account extension is installed. To track progress toward supporting newer extension versions, see this GitHub issue. Once installed, open the Command Palette and run the Azure: Sign In command</p>"},{"location":"sdk/queries/databricks/troubleshooting/#exception-has-occurred-typeerror-module-object-is-not-callable","title":"Exception has occurred: TypeError 'module' object is not callable","text":"<p>Ensure you are importing and using the RTDIP SDK functions correctly. You will need to give the module a name and reference it when using the function. See below for a code example. </p> <pre><code>from rtdip_sdk.authentication import authenticate as auth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import interpolate\n\nauthentication = auth.DefaultAuth().authenticate()\naccess_token = authentication.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", access_token)\n\ndict = {\n    \"business_unit\": \"{business_unit}\", \n    \"region\": \"{region}\", \n    \"asset\": \"{asset}\", \n    \"data_security_level\": \"{date_security_level}\",\n    \"data_type\": \"{data_type}\", #options are float, integer, string and double (the majority of data is float)\n    \"tag_names\": [\"{tag_name_1}, {tag_name_2}\"],\n    \"start_date\": \"2022-03-08\", #start_date can be a date in the format \"YYYY-MM-DD\" or a datetime in the format \"YYYY-MM-DDTHH:MM:SS\"\n    \"end_date\": \"2022-03-10\", #end_date can be a date in the format \"YYYY-MM-DD\" or a datetime in the format \"YYYY-MM-DDTHH:MM:SS\"\n    \"time_interval_rate\": \"1\", #numeric input\n    \"time_interval_unit\": \"hour\", #options are second, minute, day, hour\n    \"include_bad_data\": True #boolean options are True or False\n}\n\nresult = interpolate.get(connection, dict)\nprint(result)\n</code></pre>"},{"location":"sdk/queries/databricks/troubleshooting/#databricks-odbcjdbc-driver-issues","title":"Databricks ODBC/JDBC Driver issues","text":""},{"location":"sdk/queries/databricks/troubleshooting/#general-troubleshooting","title":"General Troubleshooting","text":"<p>Most issues related to the installation or performance of the ODBC/JDBC driver are documented here.</p>"},{"location":"sdk/queries/databricks/troubleshooting/#odbc-with-a-proxy","title":"ODBC with a proxy","text":"<p>Follow this document to use the ODBC driver with a proxy.  </p>"},{"location":"sdk/queries/spark/spark-connect/","title":"Spark Connect","text":"<p>Spark Connect was released in Apache Spark 3.4.0 to enable a decoupled client-server architecture that allows remote connectivity to Spark clusters using the Spark DataFrame API.</p> <p>This means any Spark cluster could provide compute to a spark job and therefore enables options such as Spark on Kubernetes, Spark running locally or Databricks Interactive Clusters to be leveraged in the RTDIP SDK to perform time series queries.</p>"},{"location":"sdk/queries/spark/spark-connect/#prerequisites","title":"Prerequisites","text":"<p>Please ensure that you have followed the instructions to enable Spark Connect on your Spark cluster and that you are using a <code>pyspark&gt;=3.4.0</code>. If you are connecting to Databricks, then install <code>databricks-connect&gt;=13.0.1</code> instead of <code>pyspark</code>.</p>"},{"location":"sdk/queries/spark/spark-connect/#example","title":"Example","text":"<p>Below is an example of connecting to Spark using Spark Connect.</p> <pre><code>from rtdip_sdk.connectors import SparkConnection\n\nspark_server = \"sparkserver.com\"\naccess_token = \"my_token\"\n\nspark_remote = \"sc://{}:443;token={}\".format(spark_server, access_token)\nconnection = SparkConnection(spark_remote=spark_remote)\n</code></pre> <p>Replace the access_token with your own information(this assumes an access token is required to authenticate with the remote Spark server).</p>"},{"location":"university/overview/","title":"University","text":""},{"location":"university/essentials/overview/","title":"Overview","text":""},{"location":"university/essentials/api/authentication/","title":"Authentication","text":"<p>RTDIP REST APIs require Azure Active Directory Authentication and passing the token received as an <code>authorization</code> header in the form of a Bearer token. An example of the REST API header is <code>Authorization: Bearer &lt;&lt;token&gt;&gt;</code></p>"},{"location":"university/essentials/api/authentication/#end-user-authentication","title":"End User Authentication","text":"<p>If a developer or business user would like to leverage the RTDIP REST API suite, it is recommended that they use the Identity Packages provided by Azure to obtain a token.</p> <ul> <li>REST API</li> <li>.NET</li> <li>Java</li> <li>Python</li> <li>Javascript</li> </ul> <p>Note</p> <p>Note that the above packages have the ability to obtain tokens for end users and service principals and support all available authentication options. </p> <p>Ensure to install the relevant package and obtain a token.</p> <p>See the examples section to see various authentication methods implemented.</p> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/api/authentication/#course-progress","title":"Course Progress","text":"<ul> <li> Overview</li> <li> SDK</li> <li> Power BI</li> <li> APIs<ul> <li> Overview</li> <li> Authentication</li> <li> Swagger</li> <li> Postman</li> <li> Exercise</li> </ul> </li> <li> Excel Connector</li> </ul>"},{"location":"university/essentials/api/exercise/","title":"Exercise","text":"<p>In this exercise, you will learn how to run APIs on Swagger and Postman. </p> <ol> <li> <p>Go to your RTDIP Swagger page and click the green Authorize button on the right.</p> </li> <li> <p>Call a <code>Raw Get</code> query to retrieve some data from your time series data source.</p> </li> <li> <p>Now call a query to <code>Resample Get</code> this data to a 15 minute interval average.</p> </li> <li> <p>Convert the resample query to an <code>Interpolation Get</code> query that executes the <code>linear</code> interpolation method.</p> </li> <li> <p>Finally, try calling a <code>Time Weighted Average Get</code> query on the data, with <code>Step</code> set to False.</p> </li> </ol>"},{"location":"university/essentials/api/exercise/#additional-task","title":"Additional Task","text":"<ol> <li> <p>Similarly, on Postman, run <code>Raw Get</code> to retrieve some data from your time series data source. You will need to pass a bearer token in the Authorization section. </p> </li> <li> <p>Repeat exercise 2-5 using Postman.</p> </li> </ol> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/api/exercise/#course-progress","title":"Course Progress","text":"<ul> <li> Introduction</li> <li> SDK</li> <li> Power BI</li> <li> APIs<ul> <li> Overview</li> <li> Authentication</li> <li> Swagger</li> <li> Postman</li> <li> Exercise</li> </ul> </li> <li> Excel Connector</li> </ul>"},{"location":"university/essentials/api/overview/","title":"RTDIP REST APIs","text":"<p>RTDIP provides REST API endpoints for querying data in the platform. The APIs are a wrapper to the python RTDIP SDK and provide similar functionality for users and applications that are unable to leverage the python RTDIP SDK. It is recommended to read the RTDIP SDK documentation and in particular the Functions section for more information about the options and logic behind each API. </p> <p>RTDIP API's are designed with the intention of running small to medium queries, rather than large queries to reduce network latency, increase performance and maintainability.  </p> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/api/overview/#course-progress","title":"Course Progress","text":"<ul> <li> Introduction</li> <li> SDK</li> <li> Power BI</li> <li> APIs<ul> <li> Overview</li> <li> Authentication</li> <li> Swagger</li> <li> Postman</li> <li> Exercise</li> </ul> </li> <li> Excel Connector</li> </ul>"},{"location":"university/essentials/api/postman/","title":"What is Postman?","text":"![postman](assets/postman.png){width=40%}  <p>Postman is an API platform for building and using APIs. Some features of Postman include:</p> <ul> <li>API repository -  Easily store, catalog and collaborate your APIs in a central platform.</li> <li>Tools - Includes a set of tools that helps accelerate the API lifecyle from design, testing, documenting and sharing of APIs</li> <li>Workspaces - Helps organise your APIs and collaborate with teams across your organisation. </li> </ul> <p>Developers widely use Postman to simplify the process of testing APIs by providing a user-friendly interface for making requests, viewing responses and debugging issues. </p> <p>To learn more about Postman, see Postman Documentation.</p> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/api/postman/#course-progress","title":"Course Progress","text":"<ul> <li> Introduction</li> <li> SDK</li> <li> Power BI</li> <li> APIs<ul> <li> Overview</li> <li> Authentication</li> <li> Swagger</li> <li> Postman</li> <li> Exercise</li> </ul> </li> <li> Excel Connector</li> </ul>"},{"location":"university/essentials/api/swagger/","title":"What is Swagger?","text":"<p>Swagger is a set of open-source tools built around the OpenAPI Specification that can help you design, build, document and consume REST APIs. OpenAPI Specification is an API description format for REST APIs. An OpenAPI file will typically allow you to describe your entire API, including:</p> <ul> <li>Available operations your API supports</li> <li>Your API parameters and what it returns</li> <li>Authentication methods</li> <li>Contact information, license, terms of use and other information</li> </ul> <p>Some of the featured Swagger tools are:</p> <ul> <li>Swagger Editor - a browser-based editor where you can write OpenAPI definitions.</li> <li>Swagger UI - renders OpenAPI definitions as interactive document.</li> <li>Swagger Codegen - generates sever stubs and client libraries from OpenAPI definitions.</li> </ul> <p>To find out more information about Swagger, see Swagger Documentation.</p> <p> </p>"},{"location":"university/essentials/api/swagger/#rtdip-rest-api-endpoints","title":"RTDIP REST API Endpoints","text":"<p>RTDIP REST API documentation is available in a number of formats, as described below. </p>  ![rest](../../../api/images/open-api.png){width=50%}  <p>RTDIP REST APIs are built to OpenAPI standard 3.0.2. You can obtain the OpenAPI JSON schema at the following endpoint of your deployed APIs <code>https://{domain name}/api/openapi.json</code></p>  ![rest](../../../api/images/swagger.png){width=50%}  <p>It is recommended to review the Swagger documentation that can be found at the following endpoint of your deployed APIs <code>https://{domain name}/docs</code> for more information about the parameters and options for each API. It is also possible to try out each API from this link.</p>  ![rest](../../../api/images/redoc-logo.png){width=50%}  <p>Additionally, further information about each API can be found in Redoc format at  the following endpoint of your deployed APIs <code>https://{domain name}/redoc</code></p> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/api/swagger/#course-progress","title":"Course Progress","text":"<ul> <li> Introduction</li> <li> SDK</li> <li> Power BI</li> <li> APIs<ul> <li> Overview</li> <li> Authentication</li> <li> Swagger</li> <li> Postman</li> <li> Exercise</li> </ul> </li> <li> Excel Connector</li> </ul>"},{"location":"university/essentials/excel-connector/dashboard/","title":"Exercise: Creating a Simple Dashboard with Live Data","text":"<p>All the functions (except Metadata) are capable of refreshing at a set interval with the <code>refreshIntervalSeconds</code> parameter. </p> <p>For our final exercise, we will put everything together and create a simple dashboard.</p> <p>Here are the steps:</p> <ol> <li>Create a query for a tag that has live updating data and put the end date as the future (or now via <code>*</code>).</li> <li>Create a scatter or line chart with your data.</li> <li>Either with the taskpane <code>Advanced Parameters</code> or by editing your formula, set <code>refreshIntervalSeconds</code> to a number (min value <code>10</code>).</li> <li>Watch your chart update with live data.</li> </ol> <p>Note: Currently, if you require excel to recognise dates on plots you will need to do <code>=VALUE(your_date_cell)</code>. </p>"},{"location":"university/essentials/excel-connector/dashboard/#additional-task","title":"Additional Task","text":"<p>Look at the dashboard and you will see which functions are streaming and which are not - it is always a good idea to check this in case you have any stray functions running.</p> <p>That's the end of this lesson - if you have any feedback about the Excel Add-in we'd love to hear it!</p> <p> \u2190 Previous</p> <p>Congratulations on finishing RTDIP Essentials! </p>"},{"location":"university/essentials/excel-connector/dashboard/#course-progress","title":"Course Progress","text":"<ul> <li> Overview</li> <li> SDK</li> <li> Power BI</li> <li> APIs</li> <li> Excel Connector<ul> <li> Overview</li> <li> Getting Started</li> <li> Exercise: Exploring the Taskpane</li> <li> Exercise: Exploring the Functions</li> <li> Exercise: Creating a Simple Dashboard with Live Data</li> </ul> </li> </ul>"},{"location":"university/essentials/excel-connector/functions/","title":"Exercise: Exploring the Functions","text":"<p>As you will have seen in the previous execise, the Excel Add-in gets data from the RTDIP API with custom functions. In fact, each api route has it's own custom function in Excel.</p> <p>Try these exercises to get familiar with the functions:</p> <ol> <li>Write a function directly by referencing parameter values to cells. First, place your parameters in cells (e.g. In cell <code>B2</code> put your tagname). Then, in the cell where you want your data, write <code>=RTDIP.</code> and you will see the various functions available. Excel will hint which parameters go where.</li> <li>Refactor a formula in your sheet from a previous exercise and change the inputs to reference cells.</li> </ol> <p>A function may look like:  <code>=RTDIP.RAW(\"apiUrl\", \"region\", etc...)</code></p>"},{"location":"university/essentials/excel-connector/functions/#additional-task","title":"Additional Task","text":"<ol> <li>Try removing optional parameters. These are shown with square brackets around them, for example <code>[includeBadData]</code>. If not input, the defaults will be input behind the scenes.</li> </ol> <p>Let's continue to the final section:</p> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/excel-connector/functions/#course-progress","title":"Course Progress","text":"<ul> <li> Overview</li> <li> SDK</li> <li> Power BI</li> <li> APIs</li> <li> Excel Connector<ul> <li> Overview</li> <li> Getting Started</li> <li> Exercise: Exploring the Taskpane</li> <li> Exercise: Exploring the Functions</li> <li> Exercise: Creating a Simple Dashboard with Live Data</li> </ul> </li> </ul>"},{"location":"university/essentials/excel-connector/getting-started/","title":"Getting Started","text":"<p>Note: Although the images reference windows, all the functionality will work on mac. </p> <p>To get started, open up Microsoft Excel and go to the <code>Home</code> tab. On the right you should see the RTDIP Taskpane like this (if not, you may need to click <code>Add-ins</code>):</p> <p>Once opened, the set-up screen will show and will ask for your API URL. This will be the same URL as in the previous lessons in the format <code>https://YOUR_ORGANISATION_DOMAIN/api/v1</code></p> <p>After this, you should see our friendly taskpane (you are now completely set-up and ready to make some queries!):</p> <p>Let's move onto the next section:</p> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/excel-connector/getting-started/#course-progress","title":"Course Progress","text":"<ul> <li> Overview</li> <li> SDK</li> <li> Power BI</li> <li> APIs</li> <li> Excel Connector<ul> <li> Overview</li> <li> Getting Started</li> <li> Exercise: Exploring the Taskpane</li> <li> Exercise: Exploring the Functions</li> <li> Exercise: Creating a Simple Dashboard with Live Data</li> </ul> </li> </ul>"},{"location":"university/essentials/excel-connector/overview/","title":"RTDIP Excel Add-in","text":"<p>Note: This course assumes you already have the RTDIP Add-in installed by your organisation. </p> <p>The RTDIP Excel Add-in is one of the simplest ways to get timeseries data from databricks to Microsoft Excel.</p> <p>Behind the scenes the add-in sends requests to the RTDIP API, but as you'll see in this lesson, the entryway is simplified including authentication via SSO, date parsing and more!</p> <p>For now, here's a sneak peak of the task pane:</p> <p>If your course is facillitated, your facilitator will have a completed Excel workbook specific to your organisation </p> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/excel-connector/overview/#course-progress","title":"Course Progress","text":"<ul> <li> Overview</li> <li> SDK</li> <li> Power BI</li> <li> APIs</li> <li> Excel Connector<ul> <li> Overview</li> <li> Getting started fetching your first data</li> <li> Exercise: Exploring the Taskpane</li> <li> Exercise: Exploring the Functions</li> <li> Exercise: Creating a Simple Dashboard with Live Data</li> </ul> </li> </ul>"},{"location":"university/essentials/excel-connector/taskpane/","title":"Exercise: Exploring the Taskpane","text":"<p>The taskpane is the UI that guides you through and manages queries to the RTDIP API via your Excel sheet. All the API query types are supported, and can be accessed via the dropdown menu.</p> <p>When you click run on the task pane, it inserts a formula with the specified parameters into a cell. We'll dive deeper into these custom functions in the next exercise. </p> <p>Try these exercises to get familiar with the taskpane:</p> <ol> <li>Run a <code>Raw</code> query by filling in the parameters as you did in the API lesson.</li> <li>Run an <code>Interpolate</code> query in the same way.</li> <li>Try the shorthand parameters, for example rather than <code>todays date</code> you can do <code>*</code>, and <code>yesterdays date</code> you can do <code>*-1d</code>.</li> <li>Search for a different tag with the tag explorer  and add one to your query.</li> <li>Explore the dashboard  and <code>edit</code>, <code>delete</code> or <code>refresh</code> one of your queries.</li> </ol>"},{"location":"university/essentials/excel-connector/taskpane/#additional-task","title":"Additional Task","text":"<ol> <li>Swtich to the <code>SQL</code> form and write a SQL query (note: these do not have to be timeseries tables)</li> <li>Open up the settings and change the look of the headers (or even turn them off).</li> <li>Look at the <code>Advanced Parameters</code> and try changing them (do not change the refresh interval, we will do this in the final exercise).</li> </ol> <p>Onto the next section: exploring the functions directly!</p> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/excel-connector/taskpane/#course-progress","title":"Course Progress","text":"<ul> <li> Overview</li> <li> SDK</li> <li> Power BI</li> <li> APIs</li> <li> Excel Connector<ul> <li> Overview</li> <li> Getting Started</li> <li> Exercise: Exploring the Taskpane</li> <li> Exercise: Exploring the Functions</li> <li> Exercise: Creating a Simple Dashboard with Live Data</li> </ul> </li> </ul>"},{"location":"university/essentials/powerbi/exercise/","title":"Exercise","text":"<p>In this exercise, you will connect to Power BI and build a simple dashboard using the data from your time series data source. </p> <ol> <li> <p>Open Power BI Desktop aand establish a new Azure Databricks Connection. Provide the connection details for your Databricks SQL Warehouse.</p> </li> <li> <p>Select your table that contains the time series data and load it into Power BI using DirectQuery.</p> </li> <li> <p>Build a simple line chart that shows the time series data.</p> </li> <li> <p>Add filters to select the time range and the identifier.</p> </li> </ol>"},{"location":"university/essentials/powerbi/exercise/#additional-task","title":"Additional Task","text":"<ol> <li>Build a slider filter for selecting a time range</li> </ol> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/powerbi/exercise/#course-progress","title":"Course Progress","text":"<ul> <li> Overview</li> <li> SDK</li> <li> Power BI </li> <li> APIs</li> <li> Excel Connector</li> </ul>"},{"location":"university/essentials/powerbi/overview/","title":"Power BI Overview","text":"<p>Microsoft Power BI is a business analytics service that provides interactive visualizations with self-service business intelligence capabilities that enable end users to create reports and dashboards by themselves without having to depend on information technology staff or database administrators.</p> ![Power BI Databricks](images/databricks_powerbi.png){width=50%} <p>When you use Azure Databricks as a data source with Power BI, you can bring the advantages of Azure Databricks performance and technology beyond data scientists and data engineers to all business users.</p> <p>You can connect Power BI Desktop to your Azure Databricks clusters and Databricks SQL warehouses by using the built-in Azure Databricks connector. You can also publish Power BI reports to the Power BI service and enable users to access the underlying Azure Databricks data using single sign-on (SSO), passing along the same Azure Active Directory credentials they use to access the report.</p> <p>For more information on how to connect Power BI with databricks, see here.</p>"},{"location":"university/essentials/powerbi/overview/#power-bi-installation-instructions","title":"Power BI Installation Instructions","text":"<ol> <li>Install Power BI Desktop application from Microsoft Store using your Microsoft Account to sign in.</li> </ol> ![Power BI Desktop](images/power-bi-desktop.png) <ol> <li> <p>Open Power BI desktop.</p> </li> <li> <p>Click on Home, Get data and More... </p> </li> <li> <p>Search for Azure Databricks and click Connect.  </p> </li> <li> <p>Fill in the details and click OK.</p> </li> <li> <p>Connect to the RTDIP data using your Databricks SQL Warehouse connection details including Hostname and HTTP Path. For Data Connectivity mode, select DirectQuery.</p> </li> <li> <p>Click Azure Active Directory, Sign In and select Connect. In Power Query Editor, there are different tables for different data types. </p> </li> <li> <p>Once connected to the Databricks SQL Warehouse, navigate to the Business Unit in the navigator bar on the left and select the asset tables for the data you wish to use in your report. There is functionality to select multiple tables if required. Click Load to get the queried data.</p> </li> </ol> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/powerbi/overview/#course-progress","title":"Course Progress","text":"<ul> <li> Overview</li> <li> SDK</li> <li> Power BI<ul> <li> Overview</li> <li> Exercise    </li> </ul> </li> <li> APIs</li> <li> Excel Connector</li> </ul>"},{"location":"university/essentials/rtdip/architecture/databricks/","title":"Architecture","text":""},{"location":"university/essentials/rtdip/architecture/databricks/#databricks","title":"Databricks","text":"<p>RTDIP integrates with Databricks and supports executing time series queries or ingesting data. Queries are executed using either Databricks SQL Warehouses or Spark Connect. Data Ingestion can be run and orchestrated using Databricks Workflows or Delta Live Tables.</p> <p>For further information about Databricks, please refer to:</p> <ul> <li>Databricks SQL</li> <li>Databricks Workflows</li> <li>Delta Live Tables</li> </ul> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/rtdip/architecture/databricks/#course-progress","title":"Course Progress","text":"<ul> <li> Introduction<ul> <li> Overview</li> <li> Prerequisites</li> <li> Architecture<ul> <li> Queries</li> <li> Pipelines</li> <li> Databricks</li> </ul> </li> <li> Getting Started</li> </ul> </li> <li> SDK</li> <li> Power BI</li> <li> APIs</li> <li> Excel Connector</li> </ul>"},{"location":"university/essentials/rtdip/architecture/pipelines/","title":"Architecture","text":""},{"location":"university/essentials/rtdip/architecture/pipelines/#pipelines","title":"Pipelines","text":"<p>Not in scope for this particular course but it is worth mentioning that RTDIP also provides the ability to create and manage time series ingestion pipelines. Pipelines are a series of steps that are executed in sequence to process time series data. Pipeline components consist of data sources, data sinks, and processing steps.</p> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/rtdip/architecture/pipelines/#course-progress","title":"Course Progress","text":"<ul> <li> Introduction<ul> <li> Overview</li> <li> Prerequisites</li> <li> Architecture<ul> <li> Queries</li> <li> Pipelines</li> <li> Databricks</li> </ul> </li> <li> Getting Started</li> </ul> </li> <li> SDK</li> <li> Power BI</li> <li> APIs</li> <li> Excel Connector</li> </ul>"},{"location":"university/essentials/rtdip/architecture/queries/","title":"Architecture","text":""},{"location":"university/essentials/rtdip/architecture/queries/#queries","title":"Queries","text":"<p>RTDIP provides the ability to execute time series queries on the data stored in the RTDIP platform. The queries can be executed using the RTDIP SDK or APIs, queries such as raw, resample, interpolation, interpolate at time, time-weighted average, circular averages, circular standard deviation, latest, plot, summary, and metadata.</p> <p>The RTDIP Essentials course will focus on RTDIP queries in the sections that follow.</p> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/rtdip/architecture/queries/#course-progress","title":"Course Progress","text":"<ul> <li> Introduction<ul> <li> Overview</li> <li> Prerequisites</li> <li> Architecture<ul> <li> Queries</li> <li> Pipelines</li> <li> Databricks</li> </ul> </li> <li> Getting Started</li> </ul> </li> <li> SDK</li> <li> Power BI</li> <li> APIs</li> <li> Excel Connector</li> </ul>"},{"location":"university/essentials/rtdip/introduction/overview/","title":"Course Overview","text":"<p>Essentials</p> <p>Welcome to the RTDIP Essentials training course. This course introduces you to the Real Time Data Ingestion Platform, a scalable solution for ingesting and processing data from a variety of time series data sources. </p> <p>You will learn how to execute  By the end of this course, you will have a good understanding of:</p> <ul> <li>The RTDIP architecture</li> <li>How to use the SDK to interact with the RTDIP platform</li> <li>How to use the APIs to execute time series queries</li> <li>Build visualizations and dashboards in Power BI</li> </ul> <p> Next \u2192</p>"},{"location":"university/essentials/rtdip/introduction/overview/#course-progress","title":"Course Progress","text":"<ul> <li> Introduction<ul> <li> Overview</li> <li> Prerequisites</li> <li> Architecture</li> <li> Getting Started</li> </ul> </li> <li> SDK</li> <li> Power BI</li> <li> APIs</li> <li> Excel Connector</li> </ul>"},{"location":"university/essentials/rtdip/introduction/prerequisites/","title":"Course Prerequisites","text":"<p>Before you begin the course, ensure you obtain the following prerequisites(from your istructor or from your environment if you are doing this on your own):</p>"},{"location":"university/essentials/rtdip/introduction/prerequisites/#development-environment","title":"Development Environment","text":"<ul> <li>Python &gt;=3.10,&lt;3.13</li> <li>An IDE such as Visual Studio Code or PyCharm</li> <li>Postman via the app, web browser or as an extension on Visual Studio Code</li> </ul>"},{"location":"university/essentials/rtdip/introduction/prerequisites/#system-requirements","title":"System Requirements","text":"<ul> <li>A Cluster for executing Spark SQL - If using Databricks, this would typically be a Databricks SQL Warehouse and its associated connection details:<ul> <li>Server Hostname</li> <li>HTTP Path</li> </ul> </li> <li>Access to Power BI</li> </ul>"},{"location":"university/essentials/rtdip/introduction/prerequisites/#data-requirements","title":"Data Requirements","text":"<ul> <li>Access to a time series table that has, as a minimum:<ul> <li>An identifier column</li> <li>A timestamp column</li> <li>A value column</li> </ul> </li> </ul> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/rtdip/introduction/prerequisites/#course-progress","title":"Course Progress","text":"<ul> <li> Introduction<ul> <li> Overview</li> <li> Prerequisites</li> <li> Architecture</li> <li> Getting Started</li> </ul> </li> <li> SDK</li> <li> Power BI</li> <li> APIs</li> <li> Excel Connector</li> </ul>"},{"location":"university/essentials/sdk/authentication/azure/","title":"Authentication","text":""},{"location":"university/essentials/sdk/authentication/azure/#azure-active-directory","title":"Azure Active Directory","text":"<p>The following section describes authentication using Azure Active Directory..</p> <p>Note</p> <p>If you are using the SDK directly in Databricks please note that DefaultAuth will not work.</p> <p>1. Import rtdip-sdk authentication methods with the following:</p> <pre><code>from rtdip_sdk.authentication import azure as auth\n</code></pre> <p>2. Use any of the following authentication methods. Replace tenant_id , client_id, certificate_path or client_secret with your own details.</p> Default AuthenticationCertificate AuthenticationClient Secret Authentication <pre><code>credential = auth.DefaultAuth().authenticate()\n</code></pre> <pre><code>credential = auth.CertificateAuth(tenant_id, client_id, certificate_path).authenticate()\n</code></pre> <pre><code>credential = auth.ClientSecretAuth(tenant_id, client_id, client_secret).authenticate()\n</code></pre> <p>3. The methods above will return back a Client Object. The following example will show you how to retrieve the access_token from a credential object. The access token will be used in later steps to connect to RTDIP via the three options (Databricks SQL Connect, PYODBC SQL Connect, TURBODBC SQL Connect).</p>"},{"location":"university/essentials/sdk/authentication/azure/#tokens","title":"Tokens","text":"<p>Once authenticated, it is possible to retrieve tokens for specific Azure Resources by providing scopes when retrieving tokens. Please see below for examples of how to retrieve tokens for Azure resources regularly used in RTDIP.</p> Databricks <pre><code>access_token = credential.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\n</code></pre> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/sdk/authentication/azure/#course-progress","title":"Course Progress","text":"<ul> <li> Introduction</li> <li> SDK<ul> <li> Authentication<ul> <li> Overview    </li> <li> Azure Active Directory</li> <li> Databricks</li> </ul> </li> <li> Connectors</li> <li> Queries</li> </ul> </li> <li> Power BI</li> <li> APIs</li> <li> Excel Connector</li> </ul>"},{"location":"university/essentials/sdk/authentication/databricks/","title":"Databricks","text":"<p>Databricks supports authentication using Personal Access Tokens (PAT) and information about this authentication method is available here.</p>"},{"location":"university/essentials/sdk/authentication/databricks/#authentication","title":"Authentication","text":"<p>To generate a Databricks PAT Token, follow this guide and ensure that the token is stored securely and is never used directly in code.</p> <p>Your Databricks PAT Token can be used in the RTDIP SDK to authenticate with any Databricks Workspace or Databricks SQL Warehouse and simply provided in the <code>access_token</code> fields where tokens are required in the RTDIP SDK.</p>"},{"location":"university/essentials/sdk/authentication/databricks/#example","title":"Example","text":"<p>Below is an example of using a Databricks PAT Token for authenticating with a Databricks SQL Warehouse.</p> <pre><code>from rtdip_sdk.connectors import DatabricksSQLConnection\n\nserver_hostname = \"server_hostname\"\nhttp_path = \"http_path\"\naccess_token = \"dbapi.......\"\n\nconnection = DatabricksSQLConnection(server_hostname, http_path, access_token)\n</code></pre> <p>Replace server_hostname, http_path with your own information and specify your Databricks PAT token for the access_token. </p> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/sdk/authentication/databricks/#course-progress","title":"Course Progress","text":"<ul> <li> Introduction</li> <li> SDK<ul> <li> Getting Started</li> <li> Authentication<ul> <li> Overview      </li> <li> Azure Active Directory</li> <li> Databricks</li> </ul> </li> <li> Connectors</li> <li> Queries</li> </ul> </li> <li> Power BI</li> <li> APIs</li> <li> Excel Connector</li> </ul>"},{"location":"university/essentials/sdk/authentication/overview/","title":"RTDIP Authentication","text":"<p>RTDIP supports multiple authentication methods to secure access to the platform. These methods include:</p> <ul> <li>Azure Active Directory</li> <li>Databricks Token</li> </ul> <p>The following sections will cover how to perform secure authentication using these methods.</p> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/sdk/authentication/overview/#course-progress","title":"Course Progress","text":"<ul> <li> Overview</li> <li> Architecture</li> <li> SDK<ul> <li> Getting Started</li> <li> Authentication<ul> <li> Overview      </li> <li> Azure Active Directory</li> <li> Databricks</li> </ul> </li> <li> Connectors</li> <li> Queries</li> </ul> </li> <li> Power BI</li> <li> APIs</li> <li> Excel Connector</li> </ul>"},{"location":"university/essentials/sdk/connectors/databricks-sql-connector/","title":"Databricks SQL Connector","text":"<p>Enables connectivity to Databricks using the Databricks SQL Connector which does not require any ODBC installation. </p> <p>For more information refer to this documentation and for the specific implementation within the RTDIP SDK, refer to this link.</p> <pre><code>from rtdip_sdk.connectors import DatabricksSQLConnection\n\nserver_hostname = \"server_hostname\"\nhttp_path = \"http_path\"\naccess_token = \"token\"\n\nconnection = DatabricksSQLConnection(server_hostname, http_path, access_token)\n</code></pre> <p>Replace server_hostname, http_path and access_token with your own information.</p> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/sdk/connectors/databricks-sql-connector/#course-progress","title":"Course Progress","text":"<ul> <li> Introduction</li> <li> SDK<ul> <li> Authentication</li> <li> Connectors<ul> <li> Overview    </li> <li> Databricks SQL</li> <li> ODBC</li> <li> Spark</li> <li> Exercise</li> </ul> </li> <li> Queries</li> </ul> </li> <li> Power BI</li> <li> APIs</li> <li> Excel Connector</li> </ul>"},{"location":"university/essentials/sdk/connectors/exercise/","title":"Exercise","text":"<p>In this exercise, you will obtain an access token for Azure AD using the RTDIP SDK and then use it to authenticate with a Databricks SQL Warehouse.</p> <ol> <li> <p>Create a new python file.</p> </li> <li> <p>Import the necessary classes from the RTDIP SDK.</p> </li> <li> <p>Authenticate with Azure AD using the <code>DefaultAuth</code> method.</p> </li> <li> <p>Retrieve the access token.</p> </li> <li> <p>Connect to the Databricks SQL Warehouse using the relevant connector.</p> </li> <li> <p>Run your code and ensure that you can connect to the Databricks SQL Warehouse succesfully.</p> </li> </ol> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/sdk/connectors/exercise/#course-progress","title":"Course Progress","text":"<ul> <li> Introduction</li> <li> SDK<ul> <li> Authentication</li> <li> Connectors<ul> <li> Overview</li> <li> Databricks SQL</li> <li> ODBC</li> <li> Spark</li> <li> Exercise</li> </ul> </li> <li> Queries</li> </ul> </li> <li> Power BI    </li> <li> APIs</li> <li> Excel Connector</li> </ul>"},{"location":"university/essentials/sdk/connectors/odbc-connectors/","title":"ODBC Connectors","text":""},{"location":"university/essentials/sdk/connectors/odbc-connectors/#pyodbc-sql-connector","title":"PYODBC SQL Connector","text":"<p>PYDOBC is a popular python package for querying data using ODBC. Refer to their documentation for more information about pyodbc, how to install it and how you can leverage it in your code.</p> <p>Warning</p> <p>The RTDIP SDK does not specify <code>pyodbc</code> as one of its package dependencies. It will need to be installed into your environment separately.</p> <p>View information about how pyodbc is implemented in the RTDIP SDK here.</p> <pre><code>from rtdip_sdk.connectors import PYODBCSQLConnection\n\nserver_hostname = \"server_hostname\"\nhttp_path = \"http_path\"\naccess_token = \"token\"\ndriver_path = \"/Library/simba/spark/lib/libsparkodbc_sbu.dylib\"\n\nconnection = PYODBCSQLConnection(driver_path, sever_hostname, http_path, access_token)\n</code></pre> <p>Replace server_hostname, http_path and access_token with your own information.</p>"},{"location":"university/essentials/sdk/connectors/odbc-connectors/#turbodbc-sql-connector","title":"TURBODBC SQL Connector","text":"<p>Turbodbc is a powerful python ODBC package that has advanced options for querying performance. Find out more about installing it on your operation system and what Turbodbc can do here and refer to this documentation for more information about how it is implemented in the RTDIP SDK.</p> <p>Warning</p> <p>The RTDIP SDK does not specify <code>turbodbc</code> as one of its package dependencies. It will need to be installed into your environment separately.</p> <pre><code>from rtdip_sdk.connectors import TURBODBCSQLConnection\n\nserver_hostname = \"server_hostname\"\nhttp_path = \"http_path\"\naccess_token = \"token\"\n\nconnection = TURBODBCSQLConnection(server_hostname, http_path, access_token)\n</code></pre> <p>Replace server_hostname, http_path and access_token with your own information.</p> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/sdk/connectors/odbc-connectors/#course-progress","title":"Course Progress","text":"<ul> <li> Introduction</li> <li> SDK<ul> <li> Authentication</li> <li> Connectors<ul> <li> Overview    </li> <li> Databricks SQL</li> <li> ODBC</li> <li> Spark</li> <li> Exercise</li> </ul> </li> <li> Queries</li> </ul> </li> <li> Power BI    </li> <li> APIs</li> <li> Excel Connector</li> </ul>"},{"location":"university/essentials/sdk/connectors/overview/","title":"RTDIP Connectors","text":"<p>Integration and connectivity to RTDIP is facilitated through the use of connectors. Users require connectivity from various tools and applications to RTDIP and the connectors provided with the RTDIP SDK enable this. As an overview of the connectors, the following are available:</p> <ul> <li>Databricks SQL Connector: This is the default connector used in the SDK and is the simplest to use as there are no additional installation requirements. Additionally, this connector provides adequate performance for most use cases.</li> <li>ODBC Connector: In certain scenarios, users may want to leverage the Spark SIMBA ODBC driver. This requires the user to install and setup the driver in their environment prior to use, after which it can leverage Turbodbc or Pyodbc for connectivity to RTDIP.</li> <li>Spark Connector: This connector supports workloads that are running in a spark environment such as Databricks or where Spark Connect is required.</li> </ul> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/sdk/connectors/overview/#course-progress","title":"Course Progress","text":"<ul> <li> Overview</li> <li> Architecture</li> <li> SDK<ul> <li> Getting Started</li> <li> Authentication</li> <li> Connectors<ul> <li> Overview    </li> <li> Databricks SQL</li> <li> ODBC</li> <li> Spark</li> <li> Exercise</li> </ul> </li> <li> Queries</li> </ul> </li> <li> Power BI</li> <li> APIs</li> <li> Excel Connector</li> </ul>"},{"location":"university/essentials/sdk/connectors/spark-connector/","title":"Spark Connector","text":"<p>The Spark Connector enables querying of data using a Spark Session. This is useful for querying local instances of Spark or Delta. However, the most useful application of this connector is to leverage Spark Connect to enable connecting to a remote Spark Cluster to provide the compute for the query being run from a local machine.</p> <pre><code>from rtdip_sdk.connectors import SparkConnection\n\nspark_server = \"spark_server\"\naccess_token = \"my_token\"\n\nspark_remote = \"sc://{}:443;token={}\".format(spark_server, access_token)\nconnection = SparkConnection(spark_remote=spark_remote)\n</code></pre> <p>Replace the access_token with your own authentiction token.</p> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/sdk/connectors/spark-connector/#course-progress","title":"Course Progress","text":"<ul> <li> Introduction</li> <li> SDK<ul> <li> Authentication</li> <li> Connectors<ul> <li> Overview</li> <li> Databricks SQL</li> <li> ODBC</li> <li> Spark</li> <li> Exercise</li> </ul> </li> <li> Queries</li> </ul> </li> <li> Power BI    </li> <li> APIs</li> <li> Excel Connector</li> </ul>"},{"location":"university/essentials/sdk/getting-started/exercise/","title":"Exercise","text":"<p>It's time to confirm your environment is set up correctly so that you can progress to the next steps of the course.</p> <ol> <li> <p>Ensure you have an IDE (Visual Studio Code or Pycharm) installed.</p> </li> <li> <p>Either download the Postman App, use Postman Web Browser or install Postman as an extention on Visual Studio Code. Then create a free account. </p> </li> <li> <p>Ensure you have the right version of python installed on your machine. You can check this by running the following command in your terminal:     <pre><code>python --version\n</code></pre></p> </li> <li> <p>Ensure you have the right version of pip installed on your machine. You can check this by running the following command in your terminal:     <pre><code>pip --version\n</code></pre></p> </li> <li> <p>Create a python <code>rtdip-sdk</code> environment, activate it and install the latest version of rtdip-sdk and validate its installed correctly by running the following commands in your terminal:     <pre><code>python -m venv rtdip-sdk\nsource rtdip-sdk/bin/activate\npip install rtdip-sdk\n</code></pre></p> </li> </ol> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/sdk/getting-started/exercise/#course-progress","title":"Course Progress","text":"<ul> <li> Overview</li> <li> Power BI</li> <li> SDK</li> <li> APIs</li> <li> Excel Connector</li> </ul>"},{"location":"university/essentials/sdk/getting-started/installation/","title":"Installation","text":"<p>RTDIP SDK is a PyPi package that can be found here. On this page you can find the project description,  release history, statistics, project links and maintainers.</p> <p>Features of the SDK can be installed using different extras statements when installing the rtdip-sdk package:</p> QueriesPipelinesPipelines + Pyspark <p>When installing the package for only quering data, simply specify  in your preferred python package installer:</p> <pre><code>pip install rtdip-sdk\n</code></pre> <p>RTDIP SDK can be installed to include the packages required to build, execute and deploy pipelines. Specify the following extra [pipelines] when installing RTDIP SDK so that the required python packages are included during installation.</p> <pre><code>pip install \"rtdip-sdk[pipelines]\"\n</code></pre> <p>RTDIP SDK can also execute pyspark functions as a part of the pipelines functionality. Specify the following extra [pipelines,pyspark] when installing RTDIP SDK so that the required pyspark python packages are included during installation.</p> <pre><code>pip install \"rtdip-sdk[pipelines,pyspark]\"\n</code></pre> <p>Java</p> <p>Ensure that Java is installed prior to installing the rtdip-sdk with the [pipelines,pyspark]. See here for more information.</p> <p>The following provides examples of how to install the RTDIP SDK package with Pip, Conda or Micromamba. Please note the section above to update any extra packages to be installed as part of the RTDIP SDK.</p> PipCondaMicromamba <p>To install the latest released version of RTDIP SDK from PyPi use the following command:</p> <pre><code>pip install rtdip-sdk\n</code></pre> <p>If you have previously installed the RTDIP SDK and would like the latest version, see below:</p> <pre><code>pip install rtdip-sdk --upgrade\n</code></pre> <p>To create an environment, you will need to create a environment.yml file with the following:</p> <pre><code>name: rtdip-sdk\nchannels:\n    - conda-forge\n    - defaults\ndependencies:\n    - python==3.12\n    - pip\n    - pip:\n        - rtdip-sdk\n</code></pre> <p>Run the following command:</p> <pre><code>conda env create -f environment.yml\n</code></pre> <p>To update an environment previously created:</p> <pre><code>conda env update -f environment.yml\n</code></pre> <p>To create an environment, you will need to create a environment.yml file with the following:</p> <pre><code>name: rtdip-sdk\nchannels:\n    - conda-forge\n    - defaults\ndependencies:\n    - python==3.12\n    - pip\n    - pip:\n        - rtdip-sdk\n</code></pre> <p>Run the following command:</p> <pre><code>micromamba create -f environment.yml\n</code></pre> <p>To update an environment previously created:</p> <pre><code>micromamba update -f environment.yml\n</code></pre> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/sdk/getting-started/installation/#course-progress","title":"Course Progress","text":"<ul> <li> Introduction<ul> <li> Overview</li> <li> Prerequisites</li> <li> Architecture</li> <li> Getting Started<ul> <li> Prerequisites</li> <li> Installation</li> <li> Exercise</li> </ul> </li> </ul> </li> <li> SDK</li> <li> Power BI</li> <li> APIs</li> <li> Excel Connector</li> </ul>"},{"location":"university/essentials/sdk/getting-started/prerequisites/","title":"Getting Started","text":""},{"location":"university/essentials/sdk/getting-started/prerequisites/#prerequisites","title":"Prerequisites","text":""},{"location":"university/essentials/sdk/getting-started/prerequisites/#python","title":"Python","text":"<p>There are a few things to note before using the RTDIP SDK. The following prerequisites will need to be installed on your local machine.</p> <p>Python version 3.10 or higher (and &lt; 3.13) should be installed. Check which python version you have with the following command:</p> <pre><code>python --version\n</code></pre> <p>Find the latest python version here and ensure your python path is set up correctly on your machine.</p>"},{"location":"university/essentials/sdk/getting-started/prerequisites/#python-package-installers","title":"Python Package Installers","text":"<p>Installing the RTDIP can be done using a package installer, such as Pip, Conda or Micromamba.</p> PipCondaMicromamba <p>Ensure your pip python version matches the python version on your machine. Check which version of pip you have installed with the following command:</p> <pre><code>pip --version\n</code></pre> <p>There are two ways to ensure you have the correct versions installed. Either upgrade your Python and pip install or create an environment.</p> <pre><code>python -m pip install --upgrade pip\n</code></pre> <p>Check which version of Conda is installed with the following command:</p> <pre><code>conda --version\n</code></pre> <p>If necessary, upgrade Conda as follows:</p> <pre><code>conda update conda\n</code></pre> <p>Check which version of Micromamba is installed with the following command:</p> <pre><code>micromamba --version\n</code></pre> <p>If necessary, upgrade Micromamba as follows:</p> <pre><code>micromamba self-update\n</code></pre> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/sdk/getting-started/prerequisites/#course-progress","title":"Course Progress","text":"<ul> <li> Introduction<ul> <li> Overview</li> <li> Prerequisites</li> <li> Architecture</li> <li> Getting Started<ul> <li> Prerequisites</li> <li> Installation</li> <li> Exercise</li> </ul> </li> </ul> </li> <li> SDK</li> <li> Power BI</li> <li> APIs</li> <li> Excel Connector</li> </ul>"},{"location":"university/essentials/sdk/queries/exercise/","title":"Exercise","text":"<p>Its time to start running some time series queries using the RTDIP SDK.</p> <ol> <li> <p>Using the python file you created in the previous exercise, import the necessary time series query classes from the RTDIP SDK.</p> </li> <li> <p>Pass the connector you created in the previous exercise to the time series query class.</p> </li> <li> <p>Run a <code>Raw</code> query to retrieve some data from your time series data source.</p> </li> <li> <p>Now run a query to <code>Resample</code> this data to a 15 minute interval average.</p> </li> <li> <p>Convert the resample query to an <code>Interpolation</code> query that executes the <code>linear</code> interpolation method.</p> </li> <li> <p>Finally, try running a <code>Time Weighted Average</code> query on the data, with <code>Step</code> set to False.</p> </li> </ol>"},{"location":"university/essentials/sdk/queries/exercise/#additional-task","title":"Additional Task","text":"<ol> <li>The data returned from these queries is in the form of a pandas DataFrame. Use the <code>matplotlib</code> or <code>plotly</code> library to plot the data returned from the <code>Time Weighted Average</code> query.</li> </ol> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/sdk/queries/exercise/#course-progress","title":"Course Progress","text":"<ul> <li> Introduction</li> <li> SDK<ul> <li> Authentication</li> <li> Connectors</li> <li> Queries</li> </ul> </li> <li> Power BI    </li> <li> APIs</li> <li> Excel Connector</li> </ul>"},{"location":"university/essentials/sdk/queries/sql/","title":"SQL","text":""},{"location":"university/essentials/sdk/queries/sql/#execute","title":"Execute","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import SQLQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    SQLQueryBuilder()\n    .get(\n        connection=connection, \n        sql_query=\"SELECT * FROM {tablename_or_path}\"\n    )\n)\n\nprint(data)\n</code></pre> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/sdk/queries/sql/#course-progress","title":"Course Progress","text":"<ul> <li> Introduction</li> <li> SDK<ul> <li> Authentication</li> <li> Connectors</li> <li> Queries<ul> <li> Time Series</li> <li> SQL</li> <li> Weather</li> <li> Exercise</li> </ul> </li> </ul> </li> <li> Power BI        </li> <li> APIs</li> <li> Excel Connector</li> </ul>"},{"location":"university/essentials/sdk/queries/timeseries/","title":"Queries","text":""},{"location":"university/essentials/sdk/queries/timeseries/#time-series","title":"Time Series","text":"<p>The RTDIP SDK enables users to perform complex queries, including aggregation on datasets within the Platform. Please find below the various types of queries available for specific dataset types. These SDK Functions are also supported by the RTDIP API Docker Image.</p>"},{"location":"university/essentials/sdk/queries/timeseries/#raw","title":"Raw","text":"<p>Raw facilitates performing raw extracts of time series data, typically filtered by a Tag Name or Device Name and an event time.</p> <pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .raw(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n        include_bad_data=True,\n    )\n)\n\nprint(data)\n</code></pre>"},{"location":"university/essentials/sdk/queries/timeseries/#resample","title":"Resample","text":"<p>Resample enables changing the frequency of time series observations. This is achieved by providing the following parameters:</p> <ul> <li>Time Interval Rate - The time interval rate</li> <li>Time Interval Unit - The time interval unit (second, minute, day, hour)</li> <li>Aggregation Method - Aggregations including first, last, avg, min, max</li> </ul> <pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .resample(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n        time_interval_rate=\"15\",\n        time_interval_unit=\"minute\",\n        agg_method=\"first\",\n    )\n)\n\nprint(data)\n</code></pre>"},{"location":"university/essentials/sdk/queries/timeseries/#interpolate","title":"Interpolate","text":"<p>Interpolate - takes resampling one step further to estimate the values of unknown data points that fall between existing, known data points. In addition to the resampling parameters, interpolation also requires:</p> <ul> <li>Interpolation Method - Forward Fill, Backward Fill or Linear</li> </ul> <pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .interpolate(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n        time_interval_rate=\"15\",\n        time_interval_unit=\"minute\",\n    )\n)\n\nprint(data)\n</code></pre>"},{"location":"university/essentials/sdk/queries/timeseries/#interpolate-at-time","title":"Interpolate At Time","text":"<p>Interpolation at Time - works out the linear interpolation at a specific time based on the points before and after. This is achieved by providing the following parameter:</p> <ul> <li>Timestamps - A list of timestamp or timestamps</li> </ul> <pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .interpolation_at_time(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        timestamp_filter=[\"2023-01-01T09:30:00\", \"2023-01-02T12:00:00\"],\n    )\n)\n\nprint(data)\n</code></pre>"},{"location":"university/essentials/sdk/queries/timeseries/#time-weighted-average","title":"Time Weighted Average","text":"<p>Time Weighted Averages provide an unbiased average when working with irregularly sampled data. The RTDIP SDK requires the following parameters to perform time weighted average queries:</p> <ul> <li>Time Interval Rate - The time interval rate</li> <li>Time Interval Unit - The time interval unit (second, minute, day, hour)</li> <li>Window Length - Adds a longer window time for the start or end of specified date to cater for edge cases</li> <li>Step - Data points with step \"enabled\" or \"disabled\". The options for step are \"true\", \"false\" or \"metadata\" as string types. For \"metadata\", the query requires that the TagName has a step column configured correctly in the meta data table</li> </ul> <pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .time_weighted_average(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n        time_interval_rate=\"15\",\n        time_interval_unit=\"minute\",\n        step=\"true\",\n    )\n)\n\nprint(data)\n</code></pre>"},{"location":"university/essentials/sdk/queries/timeseries/#circular-averages","title":"Circular Averages","text":"<p>Circular Averages computes the circular average for samples in a range. The RTDIP SDK requires the following parameters to perform circular average queries:</p> <ul> <li>Time Interval Rate - The time interval rate</li> <li>Time Interval Unit - The time interval unit (second, minute, day, hour)</li> <li>Lower Bound - The lower boundary for the sample range</li> <li>Upper Bound - The upper boundary for the sample range</li> </ul> <pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .circular_average(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n        time_interval_rate=\"15\",\n        time_interval_unit=\"minute\",\n        lower_bound=\"0\",\n        upper_bound=\"360\",\n    )\n)\n\nprint(data)\n</code></pre>"},{"location":"university/essentials/sdk/queries/timeseries/#circular-standard-deviation","title":"Circular Standard Deviation","text":"<p>Circular Standard Deviations computes the circular standard deviations for samples assumed to be in the range. The RTDIP SDK requires the following parameters to perform circular average queries:</p> <ul> <li>Time Interval Rate - The time interval rate</li> <li>Time Interval Unit - The time interval unit (second, minute, day, hour)</li> <li>Lower Bound - The lower boundary for the sample range</li> <li>Upper Bound - The upper boundary for the sample range</li> </ul> <pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .circular_standard_deviation(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n        time_interval_rate=\"15\",\n        time_interval_unit=\"minute\",\n        lower_bound=\"0\",\n        upper_bound=\"360\",\n    )\n)\n\nprint(data)\n</code></pre>"},{"location":"university/essentials/sdk/queries/timeseries/#latest","title":"Latest","text":"<p>Latest queries provides the latest event values. The RTDIP SDK requires the following parameters to retrieve the latest event values: - TagNames - A list of tag names</p> <pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .latest(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n    )\n)\n\nprint(data)\n</code></pre>"},{"location":"university/essentials/sdk/queries/timeseries/#plot","title":"Plot","text":"<p>Plot enables changing the frequency of time series observations and performing Average, Min, Max, First, Last and StdDev aggregations. This is achieved by providing the following parameters:</p> <ul> <li>Time Interval Rate - The time interval rate</li> <li>Time Interval Unit - The time interval unit (second, minute, day, hour)</li> </ul> <pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .plot(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n        time_interval_rate=\"15\",\n        time_interval_unit=\"minute\",\n    )\n)\n\nprint(data)\n</code></pre>"},{"location":"university/essentials/sdk/queries/timeseries/#summary","title":"Summary","text":"<p>Summary computes a summary of statistics (Avg, Min, Max, Count, StDev, Sum, Variance).</p> <pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .summary(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n        start_date=\"2023-01-01\",\n        end_date=\"2023-01-31\",\n    )\n)\n\nprint(data)\n</code></pre>"},{"location":"university/essentials/sdk/queries/timeseries/#metadata","title":"Metadata","text":"<p>Metadata queries provide contextual information for time series measurements and include information such as names, descriptions and units of measure.</p> <pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import TimeSeriesQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    TimeSeriesQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .metadata(\n        tagname_filter=[\"{tag_name_1}\", \"{tag_name_2}\"],\n    )\n)\n\nprint(data)\n</code></pre> <p> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/sdk/queries/timeseries/#course-progress","title":"Course Progress","text":"<ul> <li> Introduction</li> <li> SDK<ul> <li> Authentication</li> <li> Connectors</li> <li> Queries<ul> <li> Time Series</li> <li> SQL</li> <li> Weather</li> <li> Exercise</li> </ul> </li> </ul> </li> <li> Power BI        </li> <li> APIs</li> <li> Excel Connector</li> </ul>"},{"location":"university/essentials/sdk/queries/weather/","title":"Weather","text":""},{"location":"university/essentials/sdk/queries/weather/#raw-point","title":"Raw Point","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import WeatherQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    WeatherQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .raw_point(\n        start_date=\"{start_date}\",\n        end_date=\"{end_date}\",\n        forecast_run_start_date=\"{forecast_run_start_date}\",\n        forecast_run_end_date=\"{forecast_run_end_date}\",\n        lat=\"{latitude}\",\n        lon=\"{longitude}\",\n    )\n)\n\nprint(data)\n</code></pre>"},{"location":"university/essentials/sdk/queries/weather/#raw-grid","title":"Raw Grid","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import WeatherQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    WeatherQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .raw_grid(\n        start_date=\"{start_date}\",\n        end_date=\"{end_date}\",\n        forecast_run_start_date=\"{forecast_run_start_date}\",\n        forecast_run_end_date=\"{forecast_run_end_date}\",\n        min_lat=\"{minimum_latitude}\",\n        min_lon=\"{minimum_longitude}\",\n        max_lat=\"{maximum_latitude}\",\n        max_lon=\"{maximum_longitude}\",\n    )\n)\n\nprint(data)\n</code></pre>"},{"location":"university/essentials/sdk/queries/weather/#latest-point","title":"Latest Point","text":"<pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import WeatherQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    WeatherQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .latest_point(\n        lat=\"{latitude}\",\n        lon=\"{longitude}\",\n    )\n)\n\nprint(data)\n</code></pre>"},{"location":"university/essentials/sdk/queries/weather/#latest-grid","title":"Latest Grid","text":"<p><pre><code>from rtdip_sdk.authentication.azure import DefaultAuth\nfrom rtdip_sdk.connectors import DatabricksSQLConnection\nfrom rtdip_sdk.queries import WeatherQueryBuilder\n\nauth = DefaultAuth().authenticate()\ntoken = auth.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token\nconnection = DatabricksSQLConnection(\"{server_hostname}\", \"{http_path}\", token)\n\ndata = (\n    WeatherQueryBuilder()\n    .connect(connection)\n    .source(\"{tablename_or_path}\")\n    .latest_grid(\n        min_lat=\"{minimum_latitude}\",\n        min_lon=\"{minimum_longitude}\",\n        max_lat=\"{maximum_latitude}\",\n        max_lon=\"{maximum_longitude}\",\n    )\n)\n\nprint(data)\n</code></pre> \u2190 Previous Next \u2192</p>"},{"location":"university/essentials/sdk/queries/weather/#course-progress","title":"Course Progress","text":"<ul> <li> Introduction</li> <li> SDK<ul> <li> Authentication</li> <li> Connectors</li> <li> Queries<ul> <li> Time Series</li> <li> SQL</li> <li> Weather</li> <li> Exercise</li> </ul> </li> </ul> </li> <li> Power BI        </li> <li> APIs</li> <li> Excel Connector</li> </ul>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/archive/2022/","title":"2022","text":""}]}