{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Necessary Imports & Functions",
   "id": "405bc88f23144136"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T01:13:10.763846Z",
     "start_time": "2025-01-29T01:13:04.666226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from matplotlib import ticker\n",
    "\n",
    "from rtdip_sdk.pipelines.data_quality import KSigmaAnomalyDetection, IntervalFiltering, MissingValueImputation, NormalizationMean, NormalizationZScore, NormalizationMinMax, Denormalization, DuplicateDetection, data_manipulation, IdentifyMissingDataPattern\n",
    "from rtdip_sdk.pipelines.transformers.spark.machine_learning.columns_to_vector import ColumnsToVector\n",
    "from rtdip_sdk.pipelines.forecasting import ArimaPrediction, LinearRegression\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Hide every warning\n",
    "import warnings\n",
    "# Comment first line for developement, comment second line for presentation\n",
    "warnings.filterwarnings('ignore')\n",
    "#warnings.filterwarnings('default')\n",
    "\n",
    "spark_session = SparkSession.builder.master(\"local[2]\").appName(\"test\").getOrCreate()\n"
   ],
   "id": "4cdd749090d2ecfc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timm638/miniconda3/envs/amos2024ws01-rtdip-data-quality-checker/lib/python3.11/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n",
      "25/01/29 02:13:09 WARN Utils: Your hostname, anons-Laptop resolves to a loopback address: 127.0.1.1; using 10.100.32.12 instead (on interface wlo1)\n",
      "25/01/29 02:13:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/29 02:13:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Wrapper function for creating a plot with PyPlotLib\n",
    "def create_value_plot(value_df = None, secondary_value_df = None, title = 'Unnamed Plot', x_label = 'Index', y_label = 'Primary Label', secondary_y_label = 'Secondary Value', share_y_axis = False, xlim = None, ylim = None, y_sec_lines = None, linestyle = '--', grid = 'y'):\n",
    "    value_df.index = pd.to_datetime(value_df['EventTime'], format='mixed')\n",
    "    value_df = value_df['Value']\n",
    "    if secondary_value_df is not None:\n",
    "        secondary_value_df.index = pd.to_datetime(secondary_value_df['EventTime'], format='mixed')\n",
    "        secondary_value_df = secondary_value_df['Value']\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(value_df, linestyle, linewidth=2)\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_xticks(ax.get_xticks())\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%d.%m.%Y \\n %H:%M:%S\"))\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylim([value_df.min(), value_df.max()] if ylim is None else ylim)\n",
    "    ax.set_xlim([value_df.index.min(), value_df.index.max()] if xlim is None else xlim)\n",
    "    if grid is not None:\n",
    "        ax.grid(axis=grid)\n",
    "    else:\n",
    "        ax.grid(False)\n",
    "\n",
    "    if secondary_value_df is not None:\n",
    "        ax2 = ax.twinx()\n",
    "        color='tab:red'\n",
    "        ax2.set_ylabel(secondary_y_label, color=color)\n",
    "        ax2.plot(secondary_value_df, linestyle, color=color, linewidth=1,)\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "        ax2.set_ylim(ax.get_ylim() if share_y_axis else [secondary_value_df.min(), secondary_value_df.max()])\n",
    "        ax2.set_xlim(ax.get_xlim())\n",
    "        if y_sec_lines is not None:\n",
    "            for y_sec_line in y_sec_lines:\n",
    "                ax2.axhline(y_sec_line, color= 'gray', linestyle = linestyle, linewidth=2)\n",
    "\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "# Adds artificial spikes to data\n",
    "def add_extreme_spikes(value_df, entries_to_spike=25):\n",
    "    random.seed(0)\n",
    "    samples = value_df.sample(n=entries_to_spike, random_state=1)\n",
    "    mask = value_df.index.isin(samples.index)\n",
    "    value_df.loc[mask, 'Value'] = value_df.loc[mask, 'Value'].apply(lambda x: x + 5.0 if random.random() < 0.5 else x - 5.0)\n",
    "    return value_df\n",
    "\n",
    "# Duplicate data points artificially in data\n",
    "def add_duplicate_data(value_df, entries_to_duplicate=25):\n",
    "    random.seed(0)\n",
    "    samples = value_df.head(n=entries_to_duplicate)\n",
    "    value_df = pd.concat([value_df, samples])\n",
    "    return value_df\n"
   ],
   "id": "d318dcec4c319cd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Introduction\n",
    "\n",
    "I'm [insert name], a Data Engineer at Shell. In the following presentation, we are going to showcase the RTDIP Pipeline, which can be used to easily analyse heterogeneous Big Data. This is done by pre-processing and analysing some data which was recorded from a collection of sensors located at refineries or other facilities by Shell.\n",
    "\n",
    "### Source of Data\n",
    "The first plot shows all the data contained in the file. For sake of brevity, we will only analyse a specific subset of the data shown in the second plot."
   ],
   "id": "942aa03292165b9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load & prepare data for usage by filtering to single source and converting columns to correct type\n",
    "source_df = pd.read_pickle('./ExampleData.pkl')\n",
    "tagname_to_select = '.0MI22YDSAXA3E0:S20'\n",
    "source_df = source_df[source_df['TagName'] == tagname_to_select]\n",
    "source_df['EventTime'] = pd.to_datetime(source_df['EventTime'], format='mixed')\n",
    "source_df = source_df.sort_values(by='EventTime')\n",
    "source_df.index = source_df['EventTime']\n",
    "full_source_df = source_df\n",
    "\n",
    "start_time = pd.to_datetime('2024-06-24 0:00:00', format='%Y-%m-%d %X')\n",
    "end_time = pd.to_datetime('2024-06-25 0:00:00', format='%Y-%m-%d %X')\n",
    "source_df = source_df[start_time:end_time]\n",
    "\n",
    "source_df = add_extreme_spikes(source_df)\n",
    "source_df = add_duplicate_data(source_df)\n",
    "source_df = source_df.sort_index()\n",
    "\n",
    "full_source_df = pd.concat([full_source_df.loc[:start_time], source_df[start_time:end_time], full_source_df.loc[end_time:]], axis=0)\n",
    "\n",
    "# We plot here the whole data range of one source\n",
    "create_value_plot(full_source_df, title='Sensor Data', y_label='Value', x_label='Time', linestyle='-')\n",
    "create_value_plot(source_df, title='Sensor Data [Subset]', y_label='Value', x_label='Time', linestyle='-')\n",
    "\n",
    "df_0 = spark_session.createDataFrame(source_df, ['TagName', 'EventTime', 'Status', 'Value'])"
   ],
   "id": "c0ad9654fcf3b930",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Normalization\n",
    "First the data is normalised, which is useful for further preprocessing & potential machine learning applications. We will process the whole data through the pipeline in normalised form and can denormalize it back to its original form at the of it. That the normalisation only changes the scale of the data and not its shape is shown in the following plot. Note the different axis range on the left and right side of the plot."
   ],
   "id": "7740bd80a8d3cbc3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prepare Pipeline Compoment\n",
    "z_score_norm = NormalizationZScore(df=df_0, column_names=['Value'], in_place=True)\n",
    "# Execute the normalization on the dataframe\n",
    "df_1 = z_score_norm.filter()"
   ],
   "id": "e907c0f00254bfa4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "create_value_plot(value_df=df_0.toPandas(), secondary_value_df=df_1.toPandas(), title='Sensor Data [Subset] - Normalization', y_label = 'Value',  secondary_y_label='Z-Score Normalized Value', linestyle='-', grid='y')",
   "id": "2faaa6cf4a26e01d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Duplicate Detection\n",
    "*Scenario*:\n",
    "In datasets, some datapoints could be duplicated because of communication errors or sensor errors. Cleaning these will decrease the size of the dataframe without losing any information.\n"
   ],
   "id": "68223445c673bf96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print('Before Duplicate Removal')\n",
    "df_1.filter('EventTime = \"2024-06-24 00:00:31.212\"').show(10, False)\n",
    "random.seed(0)\n",
    "df_2 = DuplicateDetection(df=df_1, primary_key_columns=['EventTime']).filter()\n",
    "print('After Duplicate Removal')\n",
    "df_2.filter('EventTime = \"2024-06-24 00:00:31.212\"').show(10, False)\n"
   ],
   "id": "1b0c33a1e7ac5741",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Sigma Anomaly Detection\n",
    "In raw sensor data, there are sometimes measurement outliers or errors, which pollute the data.\n",
    "Identifying these spikes and removing them improves the data quality."
   ],
   "id": "b7e0231cbe85428b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "random.seed(0)\n",
    "# Execute Pipeline\n",
    "df_3 = KSigmaAnomalyDetection(spark_session, df=df_2, column_names=['Value'], k_value=2.0, use_median=False).filter()"
   ],
   "id": "7551768526701dbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Transform Data back into pandas\n",
    "create_value_plot(value_df=df_2.toPandas(), secondary_value_df=df_3.toPandas(), title='Sensor Data [Subset] - Anomalies removed', share_y_axis=True, y_label = 'Values', secondary_y_label='Cleaned up values', ylim=[-4, 4], y_sec_lines = [-2.0, 2.0], linestyle='-', grid=None)"
   ],
   "id": "4b5d5c20cd8e4ac4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Value Imputation\n",
    "*Scenario*:\n",
    "Sometimes some data points is missing or the time differences between two neighbouring datapoints is too large to be processed effectively. To fix that issue, Value Imputation is used to fill in these gaps with meaningful values. This is necessary in some situations, where the following components expects a dataset with regularly spaced interval between teo neighbouring points.\n",
    "\n"
   ],
   "id": "c1f4a80eff23b79d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "random.seed(0)\n",
    "# Execute Pipeline\n",
    "start_time = pd.to_datetime('2024-06-24 3:00:00', format='%Y-%m-%d %X')\n",
    "end_time = pd.to_datetime('2024-06-24 4:00:00', format='%Y-%m-%d %X')\n",
    "# Slicing by end dates is only possible if index is sorted by time\n",
    "# df_3_gap = df_3.filter(F.col('EventTime').between(start_time, end_time))\n",
    "# df_4 = df_3.join(df_3_gap, on='EventTime', how='left_anti')\n",
    "# df_4_gap = MissingValueImputation(spark_session, df=df_3_gap, tolerance_percentage=10).filter()\n",
    "# df_4 = df_4.unionByName(df_4_gap).sort(['EventTime'])\n",
    "df_4 = MissingValueImputation(spark_session, df=df_3, tolerance_percentage=1).filter()\n",
    "# df_4.show()"
   ],
   "id": "2dbe14e5d6a0aceb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_3_pd = df_3.toPandas().set_index('EventTime', drop=False)\n",
    "df_4_pd = df_4.toPandas().set_index('EventTime', drop=False)\n",
    "# Transform Data back into pandas\n",
    "create_value_plot(value_df=df_4_pd[start_time:end_time], secondary_value_df=df_3_pd[start_time:end_time], title='Sensor Data [Subset] - Value Imputation', share_y_axis=True, y_label = 'Imputated Values', secondary_y_label='Original Values', ylim=[-2.0, 2.0], linestyle='o', grid='x')\n",
    "print(df_4.toPandas().shape[0])"
   ],
   "id": "48e1630a0a44550c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ARIMA Prediction\n",
    "*Scenario*:\n",
    "We want to predict how the signal continues after the end of data. For that we will train an ARIMA-forecasting-model on the hundred last points and predict 100 more points from that sample. The ARIMA Model generated good forecast if good parameters are supplied to the model."
   ],
   "id": "1cd3977a7a88dbfb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Reuse Output Dataframe from Interval Filtering, since that reduces amount of points needed to a manageable amount\n",
    "df_5_a = ArimaPrediction(past_data=df_4, to_extend_name=tagname_to_select, number_of_data_points_to_analyze=100, number_of_data_points_to_predict=100,\n",
    "                             order=(3,0,1), seasonal_order=(2,1,1,50), trend='n').filter()"
   ],
   "id": "49854878cac8863f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_5_a_pd = df_5_a.toPandas()\n",
    "df_5_a_pd['EventTime'] = pd.to_datetime(df_5_a_pd['EventTime'], format='mixed')\n",
    "df_5_a_pd = df_5_a_pd.set_index('EventTime', drop=False)\n",
    "df_5_a_pd['Value'] = pd.to_numeric(df_5_a_pd['Value'])\n",
    "plot_begin_time = pd.to_datetime('2024-06-24 12:00:00', format='%Y-%m-%d %X')\n",
    "plot_split = pd.to_datetime('2024-06-25 0:00:00', format='%Y-%m-%d %X')\n",
    "plot_end_time = pd.to_datetime('2024-06-25 1:00:00', format='%Y-%m-%d %X')\n",
    "create_value_plot(value_df=df_5_a_pd[plot_begin_time:plot_split], secondary_value_df=df_5_a_pd[plot_split:plot_end_time], share_y_axis=True,\n",
    "                    ylim=(-2, 2), title='ARIMA Prediction', xlim=[plot_begin_time, plot_end_time], linestyle='-', grid='x')"
   ],
   "id": "1ee23cf80c5e803a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Denormalization\n",
    "After we processed the data in our pipeline, the data is denormalized back to its real values. This is done by providing a reference to the normalizing component to the denormalizer. After the denormalization, the data could be further used for Machine Learning or Manual Data Analysis."
   ],
   "id": "ac8fc037fb99b40a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prepare Pipeline Compoment\n",
    "z_score_denorm = Denormalization(df=df_5_a, normalization_to_revert=z_score_norm)\n",
    "# Execute the normalization on the dataframe\n",
    "df_6 = z_score_denorm.filter()"
   ],
   "id": "7aa8f4d525c93213",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "create_value_plot(value_df=df_6.toPandas(), title='Sensor Data [Subset] - Renormalized', y_label = 'Z-Score Renormalized Value', linestyle='-')",
   "id": "6cb928df6634ceaa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "62d2e8efed32b91f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
