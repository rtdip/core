{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# fmt: off",
   "id": "dda968bcf450cdb9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Necessary Imports",
   "id": "405bc88f23144136"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import antigravity",
   "id": "c568100723ab9f48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "from rtdip_sdk.pipelines.data_quality import KSigmaAnomalyDetection, IntervalFiltering, ArimaPrediction, ARIMAResults, MissingValueImputation\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "import math\n",
    "import random\n",
    "\n",
    "spark_session = SparkSession.builder.master(\"local[2]\").appName(\"test\").getOrCreate()\n"
   ],
   "id": "4cdd749090d2ecfc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Table of content:\n",
    "- Normalization\n",
    "    - Z-Score\n",
    "    - Minmax\n",
    "    - Mean \n",
    "- Duplicate Detection \n",
    "- Sigma Anomaly Detection\n",
    "- Interval Filtering\n",
    "- ARIMA\n",
    "- Value Imputation"
   ],
   "id": "fbecd2a5be1e2e19"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Source of Data\n",
    "\n",
    "The source data is sourced from ENTSO-E (European Network of Transmission System Operators for Electricity) and contains the amount of energy generated 2024 in Germany split up by generation type.\n",
    "\n",
    "Link: https://transparency.entsoe.eu/generation/r2/actualGenerationPerProductionType/show?name=&defaultValue=false&viewType=GRAPH&areaType=CTY&atch=false&datepicker-day-offset-select-dv-date-from_input=D&dateTime.dateTime=18.11.2024+00:00|CET|DAYTIMERANGE&dateTime.endDateTime=18.11.2024+00:00|CET|DAYTIMERANGE&area.values=CTY|10Y1001A1001A83F!CTY|10Y1001A1001A83F&productionType.values=B01&productionType.values=B25&productionType.values=B02&productionType.values=B03&productionType.values=B04&productionType.values=B05&productionType.values=B06&productionType.values=B07&productionType.values=B08&productionType.values=B09&productionType.values=B10&productionType.values=B11&productionType.values=B12&productionType.values=B13&productionType.values=B14&productionType.values=B20&productionType.values=B15&productionType.values=B16&productionType.values=B17&productionType.values=B18&productionType.values=B19&dateTime.timezone=CET_CEST&dateTime.timezone_input=CET+(UTC+1)+/+CEST+(UTC+2)#"
   ],
   "id": "f3ff2fc5d8c1ce3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "source_df = pd.read_csv('./Actual Generation per Production Type_2024-2025.csv')\n",
    "\n",
    "df = source_df\n",
    "df['MTU-Start'] = pd.to_datetime(df['MTU'].apply(lambda x: x.split('-')[0]), dayfirst=True)\n",
    "df['Solar  - Actual Aggregated [MW]'] = pd.to_numeric(df['Solar  - Actual Aggregated [MW]'], errors='coerce')\n",
    "df = df.set_index('MTU-Start')\n",
    "\n",
    "def create_1_day_dataframe(only_aggregated_value : bool = False, add_timeevent_col : bool = False, add_tagname : bool = False,\n",
    "                           add_status : bool = False) -> pd.DataFrame:\n",
    "    return _create_dataframe('07.04.2024', '08.04.2024', only_aggregated_value, add_timeevent_col, add_tagname, add_status)\n",
    "\n",
    "def create_2_day_dataframe(only_aggregated_value : bool = False, add_timeevent_col : bool = False, add_tagname : bool = False,\n",
    "                           add_status : bool = False) -> pd.DataFrame:\n",
    "    return _create_dataframe('07.04.2024', '09.04.2024', only_aggregated_value, add_timeevent_col, add_tagname, add_status)\n",
    "\n",
    "def create_1_week_dataframe(only_aggregated_value : bool = False, add_timeevent_col : bool = False, add_tagname : bool = False,\n",
    "                            add_status : bool = False) -> pd.DataFrame:\n",
    "    return _create_dataframe('07.04.2024', '14.04.2024', only_aggregated_value, add_timeevent_col, add_tagname, add_status)\n",
    "\n",
    "def create_2_week_dataframe(only_aggregated_value : bool = False, add_timeevent_col : bool = False, add_tagname : bool = False,\n",
    "                            add_status : bool = False) -> pd.DataFrame:\n",
    "    return _create_dataframe('07.04.2024', '21.04.2024', only_aggregated_value, add_timeevent_col, add_tagname, add_status)\n",
    "\n",
    "def _create_dataframe(start : str, end : str, only_aggregated_value : bool = False, add_timeevent_col : bool = False, add_tagname : bool = False,\n",
    "                      add_status : bool = False) -> pd.DataFrame:\n",
    "    start_time = pd.to_datetime(start, dayfirst=True)\n",
    "    end_time = pd.to_datetime(end, dayfirst=True)\n",
    "\n",
    "    output_df = df[start_time:end_time].copy(deep=True)\n",
    "    if only_aggregated_value:\n",
    "        output_df = output_df['Solar  - Actual Aggregated [MW]'].to_frame()\n",
    "    if add_timeevent_col:\n",
    "        output_df['EventTime'] = pd.to_datetime(df[start_time:end_time].index)\n",
    "    if add_tagname:\n",
    "        output_df = output_df.assign(TagName='Sensor1')\n",
    "    if add_status:\n",
    "        output_df = output_df.assign(Status='Good')\n",
    "\n",
    "    return output_df\n",
    "\n"
   ],
   "id": "35b5539f1a942fcf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Normalization\n",
    "*Scenario*:\n",
    "We want to normalize sensor data, so it better used in other components or Machine Learning applications.\n",
    "We show our three implemented methods of normalization: Z-score, min-max & mean.\n",
    "The denormalization of data is also supported, e. g. for storing processed data in a human-readable format."
   ],
   "id": "c4ec1308480790a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_normalization_plot(input_df = None, normalized_df = None, title = 'Solar Energy produced in Germany', ylabel_2 = 'Normalized Value', share_y_axis = False):\n",
    "    fig, ax = plt.subplots()\n",
    "    solar_line = ax.plot(input_df['Solar  - Actual Aggregated [MW]'], label='Solar', linewidth=2)\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_xticks(ax.get_xticks())\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=60)\n",
    "    ax.set_ylabel('Produced Energy [MW]')\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylim([0, input_df['Solar  - Actual Aggregated [MW]'].max()])\n",
    "    ax.set_xlim([input_df.index.min(), input_df.index.max()])\n",
    "    ax.grid(axis='y')\n",
    "    \n",
    "    if normalized_df is not None:\n",
    "        ax2 = ax.twinx()\n",
    "        color='tab:red'\n",
    "        ax2.set_ylabel(ylabel_2, color=color)\n",
    "        ax2.plot(normalized_df['value'], color=color, linewidth=3, linestyle = '--')\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "        ax2.set_ylim([normalized_df['value'].min(), normalized_df['value'].max()] if not share_y_axis else [0, input_df['Solar  - Actual Aggregated [MW]'].max()])\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "create_normalization_plot(create_1_week_dataframe())"
   ],
   "id": "b56655334b5a4524"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Setup Data\n",
    "april_df = create_1_week_dataframe()\n",
    "s_df = spark_session.createDataFrame(april_df['Solar  - Actual Aggregated [MW]'].to_frame(), ['value'])\n",
    "# Prepare Pipelines\n",
    "min_max_norm = NormalizationMinMax(df=s_df, column_names=['value'], in_place=True)\n",
    "mean_norm = NormalizationMean(df=s_df, column_names=['value'],\n",
    "                              in_place=True)\n",
    "z_score_norm = NormalizationZScore(df=s_df, column_names=['value'],\n",
    "                                   in_place=True)\n",
    "# Normalize with all 3 methods\n",
    "min_max_df = min_max_norm.filter().toPandas().set_index(april_df.index)\n",
    "mean_df = mean_norm.filter().toPandas().set_index(april_df.index)\n",
    "z_score_df = z_score_norm.filter()\n",
    "# Denormalization of Z-Score DF\n",
    "denorm = Denormalization(df=z_score_df, normalization_to_revert=z_score_norm)\n",
    "z_score_df = z_score_df.toPandas().set_index(april_df.index)\n",
    "denorm_df = denorm.filter().toPandas().set_index(april_df.index)"
   ],
   "id": "e907c0f00254bfa4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "create_normalization_plot(input_df=april_df, normalized_df=min_max_df, title='Solar Energy produced in Germany - Min Max Normalization', ylabel_2='Min Max Normalized Value')\n",
    "create_normalization_plot(input_df=april_df, normalized_df=mean_df, title='Solar Energy produced in Germany - Mean Normalization', ylabel_2='Mean Normalized Value')\n",
    "create_normalization_plot(input_df=april_df, normalized_df=z_score_df, title='Solar Energy produced in Germany - Z-Score Normalization', ylabel_2='Z-Score Normalized Value')\n",
    "create_normalization_plot(input_df=april_df, normalized_df=denorm_df, title='Solar Energy produced in Germany - Denormalized from Z-Score Normalization', ylabel_2='Denormalized Value')"
   ],
   "id": "2faaa6cf4a26e01d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Duplicate Detection\n",
    "*Scenario*:\n",
    "Because of some error reading in the sensor data, some entries are duplicated.\n",
    "Cleaning these will decrease the size of the dataframe without losing any information.\n"
   ],
   "id": "68223445c673bf96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dd_df = pd.DataFrame(data={\n",
    "    'TagName' : ['Temperature_Sensor', 'Temperature_Sensor', 'Temperature_Sensor'],\n",
    "    'EventTime' : ['2024-01-02 20:03:46.000', '2024-01-02 20:03:46.000', '2024-01-02 20:03:47.000'],\n",
    "    'Value' : [5.0, 5.0, 7.0]\n",
    "})\n",
    "print('Before Duplicate Detection')\n",
    "print(dd_df.head())\n",
    "\n",
    "s_dd_df = spark_session.createDataFrame(dd_df, ['TagName', 'EventTime', 'Value'])\n",
    "output_dd_df = DuplicateDetection(df=s_dd_df).filter().toPandas().sort_values('EventTime')\n",
    "print('\\nAfter Duplicate Detection')\n",
    "print(output_dd_df.head())\n",
    "\n"
   ],
   "id": "1b0c33a1e7ac5741"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Sigma Anomaly Detection\n",
    "*Scenario*:\n",
    "In raw sensor data, there are sometimes measurement outliers or errors, which pollute the data.\n",
    "Identifying these spikes and removing them improves the data quality."
   ],
   "id": "b7e0231cbe85428b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "random.seed(0)\n",
    "# Create data with randomized spikes\n",
    "sa_april_df = create_1_week_dataframe(only_aggregated_value=True, add_timeevent_col=True)\n",
    "for i in range(0, 20):\n",
    "    sa_april_df.loc[random.choice(sa_april_df.index.to_list()), 'Solar  - Actual Aggregated [MW]'] = 100000\n",
    "s_dd_df = spark_session.createDataFrame(sa_april_df, ['Solar  - Actual Aggregated [MW]', 'EventTime'])\n",
    "# Execute Pipeline\n",
    "output_dd_ps_df = KSigmaAnomalyDetection(spark_session, df=s_dd_df, column_names=['Solar  - Actual Aggregated [MW]'], k_value=3.0, use_median=False).filter()\n",
    "# Transform Data back into pandas\n",
    "output_dd_df = output_dd_ps_df.toPandas().set_index('EventTime')\n",
    "output_dd_df['value'] = output_dd_df['Solar  - Actual Aggregated [MW]']\n",
    "    \n",
    "create_normalization_plot(input_df=sa_april_df, normalized_df=output_dd_df, title='Solar Energy produced in Germany - With Anomalies', share_y_axis=True, ylabel_2='Corrected Produced Energy (k sigma filtering) [MW]')\n"
   ],
   "id": "7551768526701dbd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Interval Filtering\n",
    "*Scenario*:\n",
    "Interval Filtering ensures, that the points in a dataframe are roughly equally in time, so that between two adjacent points roughly the same time period passes.\n",
    "When working with too large data sets, some models are computationally too expensive to train on the whole dataset.\n",
    "Interval Filtering can be used to reduce the amount of points in the data set, which allows it to be used e.g. for ARIMA-modelling."
   ],
   "id": "6ff17321695f0b6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Data Preparation\n",
    "april_df = create_2_week_dataframe()\n",
    "intf_april_df = create_2_week_dataframe(only_aggregated_value=True, add_timeevent_col=True)\n",
    "intf_april_df['EventTime'] = pd.Series(intf_april_df['EventTime'], dtype=\"string\")\n",
    "intf_ps_df = spark_session.createDataFrame(intf_april_df, ['Solar  - Actual Aggregated [MW]', 'EventTime'])\n",
    "# Pipeline Execution\n",
    "output_intf_df = IntervalFiltering(spark_session, df=intf_ps_df, interval=2, interval_unit='hours', time_stamp_column_name='EventTime').filter().toPandas()\n",
    "# Postprocessing of DataFrame\n",
    "output_intf_df['value'] = output_intf_df['Solar  - Actual Aggregated [MW]']\n",
    "output_intf_df['EventTime'] = pd.to_datetime(output_intf_df['EventTime'])\n",
    "output_intf_df = output_intf_df.set_index('EventTime')\n",
    "\n",
    "create_normalization_plot(input_df=april_df,normalized_df=output_intf_df, ylabel_2='Interval filtered Value to 6 hours')"
   ],
   "id": "42a4060dadf17ad2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ARIMA Prediction\n",
    "*Scenario*:\n",
    "We want to predict how much solar energy will be used next week. For that we will apply the reduced data from Interval Filtering on a ARIMA-Model."
   ],
   "id": "1cd3977a7a88dbfb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Reuse Output Dataframe from Interval Filtering, since that reduces amount of points needed to a manageable amount\n",
    "arima_comp_df = output_intf_df.copy(deep=True)\n",
    "arima_comp_df.drop(arima_comp_df.tail(1).index, inplace=True)\n",
    "half_size = math.ceil(arima_comp_df.shape[0]/2)\n",
    "arima_input_df = arima_comp_df.head(half_size)\n",
    "arima_ps_df = spark_session.createDataFrame(\n",
    "        arima_input_df,\n",
    "        ['Solar  - Actual Aggregated [MW]', 'EventTime']\n",
    "    )\n",
    "# Pipeline Execution\n",
    "arima_comp = ArimaPrediction(arima_ps_df, column_name='Solar  - Actual Aggregated [MW]', number_of_data_points_to_analyze=half_size, number_of_data_points_to_predict=half_size,\n",
    "                             order=(2,0,0), seasonal_order=(5,0,0,half_size/7), trend='n')\n",
    "forecasted_df = arima_comp.filter().toPandas()\n",
    "# Postprocessing Data\n",
    "\n",
    "forecasted_df['value'] = forecasted_df['Solar  - Actual Aggregated [MW]']\n",
    "forecasted_df['EventTime'] = pd.to_datetime(arima_comp_df.index)\n",
    "forecasted_df = forecasted_df.set_index('EventTime')\n",
    "\n",
    "create_normalization_plot(input_df=april_df, normalized_df=forecasted_df.tail(half_size), title='Solar Energy produced in Germany - Prediction with ARIMA', ylabel_2='Forecasted Solar - Actual Aggregated [MW]', share_y_axis=True)\n"
   ],
   "id": "49854878cac8863f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Value Imputation\n",
    "*Scenario*:\n",
    "Sometimes a value is missing because of sensor error or similar issues. This makes it difficult to use these time series on algorithms expecting a time series without gaps. To fix that issue, Value Imputation is used to fill in these gaps with meaningful values.\n",
    "\n"
   ],
   "id": "c1f4a80eff23b79d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "random.seed(0)\n",
    "# Create data with randomized gaps\n",
    "vi_april_df = create_1_week_dataframe(only_aggregated_value=True, add_timeevent_col=True, add_tagname=True, add_status=True)\n",
    "for i in range(0, 50):\n",
    "    chosen_index = random.choice(sa_april_df.index.to_list())\n",
    "    vi_april_df.loc[chosen_index, 'Solar  - Actual Aggregated [MW]'] = np.NaN\n",
    "    vi_april_df.loc[chosen_index, 'Status'] = 'Bad'\n",
    "vi_ps_df = spark_session.createDataFrame(vi_april_df, ['Value', 'EventTime', 'TagName', 'Status'])\n",
    "print('Before Value Imputation')\n",
    "vi_ps_df.show()\n",
    "# Execute Pipeline\n",
    "output_vi_ps_df = MissingValueImputation(spark_session, df=vi_ps_df).filter()\n",
    "print('After Value Imputation')\n",
    "output_vi_ps_df.show()\n",
    "# Transform Data back into pandas\n",
    "output_vi_df = output_dd_ps_df.toPandas().set_index('EventTime')\n",
    "output_vi_df['value'] = output_vi_df['Solar  - Actual Aggregated [MW]']\n",
    "\n",
    "create_normalization_plot(input_df=vi_april_df, title='Solar Energy produced in Germany - With Anomalies', share_y_axis=True, ylabel_2='Corrected Produced Energy (value imputation) [MW]')\n",
    "\n",
    "create_normalization_plot(input_df=output_vi_df, title='Solar Energy produced in Germany - Corrected Anomalies')\n"
   ],
   "id": "2dbe14e5d6a0aceb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f1343361a401bf29"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
